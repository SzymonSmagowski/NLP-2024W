{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/165 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 165/165 [00:02<00:00, 57.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>split</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-szum-20131104-22-19.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>‚Äì ≈ªyjemy w czasach okre≈õlanych mianem stulecia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-szum-20131104-22-19.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>Redaktorki Display zdoby≈Çy siƒô na wysi≈Çek przy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-szum-20131104-22-19.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>Umo≈ºliwi≈Ço mi to jednak dok≈Çadne zapoznanie si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-szum-20131104-22-19.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>G≈Ç√≥wna czƒô≈õƒá realizacji to metalowy szkielet s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-szum-20131104-22-19.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>W Warszawie, poza ko≈Ñczonym budynkiem na Wybrz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417029</th>\n",
       "      <td>W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt</td>\n",
       "      <td>eval</td>\n",
       "      <td>:Q8JFN&lt;\u0001GIQ&lt; B√∂8;P\u0001E8\u0001A«¥QPB\u0001GFCJB@\u0001@EEP:?\u0001;Q@&lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417030</th>\n",
       "      <td>W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt</td>\n",
       "      <td>eval</td>\n",
       "      <td>\u0001'@K:?&lt;CC \u00019P√∂F\u0001E@&lt;\u00019P√∂F\u0001C@K&lt;I8KLIFQE8N:8 \u0001NQ@...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417031</th>\n",
       "      <td>W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt</td>\n",
       "      <td>eval</td>\n",
       "      <td>\u0001:QP\u0001DF…õ&lt;\u0001A&lt;;E8B\u0001FBI&lt;»¥CFE8\u0001=FID8\u0001JKFJLEB}N\u0001JGF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417032</th>\n",
       "      <td>W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt</td>\n",
       "      <td>eval</td>\n",
       "      <td>\u0001 C8K&lt;&gt;F\u0001GIF&gt;I8DFN«õ\u0001IFQGI8N«¥\u0001'@K :?&lt;CC8\u0001FKN@&lt;I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417033</th>\n",
       "      <td>W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt</td>\n",
       "      <td>eval</td>\n",
       "      <td>:«õ:\u0001Q8I&lt;Q&lt;INFN8«£\u0001√∞F9I8Q√≠\u0001;C8\u000198I;Q@&lt;A\u0001F&gt;}C E&lt;&gt;...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>417034 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  split  \\\n",
       "0                          1-szum-20131104-22-19.txt  train   \n",
       "1                          1-szum-20131104-22-19.txt  train   \n",
       "2                          1-szum-20131104-22-19.txt  train   \n",
       "3                          1-szum-20131104-22-19.txt  train   \n",
       "4                          1-szum-20131104-22-19.txt  train   \n",
       "...                                              ...    ...   \n",
       "417029  W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt   eval   \n",
       "417030  W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt   eval   \n",
       "417031  W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt   eval   \n",
       "417032  W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt   eval   \n",
       "417033  W_J_T_Mitchell_Czego_chca_obrazy_ksiazka.txt   eval   \n",
       "\n",
       "                                                     text  \n",
       "0       ‚Äì ≈ªyjemy w czasach okre≈õlanych mianem stulecia...  \n",
       "1       Redaktorki Display zdoby≈Çy siƒô na wysi≈Çek przy...  \n",
       "2       Umo≈ºliwi≈Ço mi to jednak dok≈Çadne zapoznanie si...  \n",
       "3       G≈Ç√≥wna czƒô≈õƒá realizacji to metalowy szkielet s...  \n",
       "4       W Warszawie, poza ko≈Ñczonym budynkiem na Wybrz...  \n",
       "...                                                   ...  \n",
       "417029  :Q8JFN<\u0001GIQ< B√∂8;P\u0001E8\u0001A«¥QPB\u0001GFCJB@\u0001@EEP:?\u0001;Q@<...  \n",
       "417030  \u0001'@K:?<CC \u00019P√∂F\u0001E@<\u00019P√∂F\u0001C@K<I8KLIFQE8N:8 \u0001NQ@...  \n",
       "417031  \u0001:QP\u0001DF…õ<\u0001A<;E8B\u0001FBI<»¥CFE8\u0001=FID8\u0001JKFJLEB}N\u0001JGF...  \n",
       "417032  \u0001 C8K<>F\u0001GIF>I8DFN«õ\u0001IFQGI8N«¥\u0001'@K :?<CC8\u0001FKN@<I...  \n",
       "417033  :«õ:\u0001Q8I<Q<INFN8«£\u0001√∞F9I8Q√≠\u0001;C8\u000198I;Q@<A\u0001F>}C E<>...  \n",
       "\n",
       "[417034 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Directory containing text files\n",
    "data_input_txt = Path('data/txts')\n",
    "data_output_splits = Path('data/splits')\n",
    "data_output_splits.mkdir(parents=True, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# List to store train and evaluation data for each file\n",
    "data_records = []\n",
    "\n",
    "# Reading and splitting all text files from the directory\n",
    "for file_name in tqdm(os.listdir(data_input_txt)):\n",
    "    if file_name.endswith('.txt'):\n",
    "        with open(data_input_txt / file_name, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            if len(text) < 1000:\n",
    "                continue  # skip files with less than 1000 characters\n",
    "            text_lines = text.split('\\n')\n",
    "            train_lines, eval_lines = train_test_split(text_lines, test_size=0.2, random_state=42)\n",
    "            for line in train_lines:\n",
    "                data_records.append({\n",
    "                    'file_name': file_name,\n",
    "                    'split': 'train',\n",
    "                    'text': line\n",
    "                })\n",
    "            for line in eval_lines:\n",
    "                data_records.append({\n",
    "                    'file_name': file_name,\n",
    "                    'split': 'eval',\n",
    "                    'text': line\n",
    "                })\n",
    "\n",
    "df = pd.DataFrame(data_records)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is the DataFrame created from your text files\n",
    "# The df columns: 'file_name', 'split', 'text'\n",
    "\n",
    "# Create separate DataFrames for train and validation\n",
    "train_df = df[df['split'] == 'train'][['text']]\n",
    "eval_df = df[df['split'] == 'eval'][['text']]\n",
    "\n",
    "# Convert DataFrames to Hugging Face Dataset objects\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 333562/333562 [00:59<00:00, 5606.22 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83472/83472 [00:14<00:00, 5632.47 examples/s]\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "c:\\Users\\Filip\\anaconda3\\envs\\ASP\\lib\\site-packages\\transformers\\quantizers\\auto.py:186: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "c:\\Users\\Filip\\anaconda3\\envs\\ASP\\lib\\site-packages\\transformers\\training_args.py:1570: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  0%|          | 200/500343 [05:01<208:15:14,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6983, 'grad_norm': 2.4613547325134277, 'learning_rate': 1.9686219646247629e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 400/500343 [10:03<209:20:24,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.6591, 'grad_norm': 2.609933376312256, 'learning_rate': 3.9672229439392424e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 600/500343 [15:03<208:50:39,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.373, 'grad_norm': 1.5534205436706543, 'learning_rate': 5.965823923253723e-07, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 665/500343 [16:40<199:03:19,  1.43s/it]"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Set Hugging Face API token\n",
    "os.environ['HUGGINGFACE_API_KEY'] = 'Here-Key'\n",
    "\n",
    "# Configuration parameters\n",
    "model_name = \"bunnycore/LLama-3.1-3b-rp-lora\"\n",
    "dataset_name = \"wikitext\"\n",
    "output_dir = \"./resultssss\"\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "optim = \"adamw_torch\"\n",
    "save_steps = 1000\n",
    "logging_steps = 200\n",
    "learning_rate = 5e-5\n",
    "weight_decay = 0.01\n",
    "fp16 = True\n",
    "bf16 = False\n",
    "max_grad_norm = 1.0\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.1\n",
    "group_by_length = True\n",
    "lr_scheduler_type = \"linear\"\n",
    "packing = False\n",
    "max_seq_length = 512\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 8\n",
    "use_4bit = True\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "use_nested_quant = True\n",
    "device_map = \"auto\"\n",
    "\n",
    "# Reduce dataset size for training\n",
    "small_train_dataset = train_dataset\n",
    "small_eval_dataset = eval_dataset\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ['HUGGINGFACE_API_KEY'])\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_seq_length)\n",
    "\n",
    "tokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "\n",
    "# Load pre-trained model with quantization configuration for QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    token=os.environ['HUGGINGFACE_API_KEY']\n",
    ")\n",
    "\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"self_attn.k_proj\", \"self_attn.v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM\n",
    ")\n",
    "\n",
    "# Initialize LoRA with quantized model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "\n",
    "# Custom Data Collator to return loss\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "trainer.save_model(output_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Testing the fine-tuned model\n",
    "test_input = \"The history of natural language processing\"\n",
    "\n",
    "# Ensure input tensor is on the correct device\n",
    "test_input_ids = test_input_ids.to(model.device)\n",
    "\n",
    "# Create attention mask to avoid warnings during generation\n",
    "attention_mask = (test_input_ids != tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate text with attention mask and device consistency\n",
    "generated_text = model.generate(\n",
    "    test_input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    max_length=500\n",
    ")\n",
    "\n",
    "# Print the generated output\n",
    "print(tokenizer.decode(generated_text[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ASP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
