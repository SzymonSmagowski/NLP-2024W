{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling chunk 1 of 10 for book 'Balladyna'...\n",
      "Filling chunk 2 of 10 for book 'Balladyna'...\n",
      "Filling chunk 3 of 10 for book 'Balladyna'...\n",
      "Filling chunk 4 of 10 for book 'Balladyna'...\n",
      "Filling chunk 5 of 10 for book 'Balladyna'...\n",
      "Filling chunk 6 of 10 for book 'Balladyna'...\n",
      "Filling chunk 7 of 10 for book 'Balladyna'...\n",
      "Filling chunk 8 of 10 for book 'Balladyna'...\n",
      "Filling chunk 9 of 10 for book 'Balladyna'...\n",
      "Filling chunk 10 of 10 for book 'Balladyna'...\n",
      "Filling chunk 1 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 2 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 3 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 4 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 5 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 6 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 7 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 8 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 9 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 10 of 10 for book 'Dziady_(Mickiewicz)'...\n",
      "Filling chunk 1 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 2 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 3 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 4 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 5 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 6 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 7 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 8 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 9 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 10 of 10 for book 'Konrad_Wallenrod'...\n",
      "Filling chunk 1 of 10 for book 'Kordian'...\n",
      "Filling chunk 2 of 10 for book 'Kordian'...\n",
      "Filling chunk 3 of 10 for book 'Kordian'...\n",
      "Filling chunk 4 of 10 for book 'Kordian'...\n",
      "Filling chunk 5 of 10 for book 'Kordian'...\n",
      "Filling chunk 6 of 10 for book 'Kordian'...\n",
      "Filling chunk 7 of 10 for book 'Kordian'...\n",
      "Filling chunk 8 of 10 for book 'Kordian'...\n",
      "Filling chunk 9 of 10 for book 'Kordian'...\n",
      "Filling chunk 10 of 10 for book 'Kordian'...\n",
      "Filling chunk 1 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 2 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 3 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 4 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 5 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 6 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 7 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 8 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 9 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 10 of 10 for book 'Lalka_(Prus)'...\n",
      "Filling chunk 1 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 2 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 3 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 4 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 5 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 6 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 7 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 8 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 9 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 10 of 10 for book 'Ogniem_i_mieczem'...\n",
      "Filling chunk 1 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 2 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 3 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 4 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 5 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 6 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 7 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 8 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 9 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 10 of 10 for book 'Pan_Tadeusz_(wyd._1834)'...\n",
      "Filling chunk 1 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 2 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 3 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 4 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 5 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 6 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 7 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 8 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 9 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 10 of 10 for book 'Pan_Wołodyjowski'...\n",
      "Filling chunk 1 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 2 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 3 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 4 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 5 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 6 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 7 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 8 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 9 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 10 of 10 for book 'Potop_(Sienkiewicz)'...\n",
      "Filling chunk 1 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 2 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 3 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 4 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 5 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 6 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 7 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 8 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 9 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 10 of 10 for book 'Quo_vadis'...\n",
      "Filling chunk 1 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 2 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 3 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 4 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 5 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 6 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 7 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 8 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 9 of 10 for book 'Sonety_Adama_Mickiewicza'...\n",
      "Filling chunk 10 of 10 for book 'Sonety_Adama_Mickiewicza'...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from openai import OpenAI\n",
    "#330min run1\n",
    "###########################\n",
    "# CONFIGURE DATA DIRECTORY\n",
    "###########################\n",
    "DATA_DIR = r\"D:\\\\mgr\\\\nlp\\\\core\\\\data_processing_scripts\"\n",
    "# Adjust the above path as needed.\n",
    "\n",
    "###########################\n",
    "# OPENAI / HF CLIENT SETUP\n",
    "###########################\n",
    "with open(\"key.env\", \"r\") as file:\n",
    "    api_key = file.read().strip()\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://a1v7omv60l5kze8p.us-east-1.aws.endpoints.huggingface.cloud/v1/\", \n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "def call_model(prompt,\n",
    "               model=\"tgi\",\n",
    "               max_tokens=200,\n",
    "               temperature=0.5,\n",
    "               stream=False,\n",
    "               **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function to call the model with a given prompt and return the response text.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=stream,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        full_output = []\n",
    "        for chunk in response:\n",
    "            delta = chunk.choices[0].delta.get(\"content\", \"\")\n",
    "            full_output.append(delta)\n",
    "        return \"\".join(full_output)\n",
    "    else:\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "###########################\n",
    "# TASK FUNCTIONS (Polish prompts)\n",
    "###########################\n",
    "\n",
    "###########################\n",
    "# LIMIT TO FIRST 1000 BLANKS\n",
    "###########################\n",
    "def limit_to_first_1000_blanks(text):\n",
    "    \"\"\"\n",
    "    Finds sequences of underscores (placeholders).\n",
    "    Returns a substring of `text` up to (and including) the 1000th placeholder,\n",
    "    if there are more than 1000 placeholders.\n",
    "    Otherwise, returns the entire text.\n",
    "    \"\"\"\n",
    "    # This regex finds any sequence of 2 or more underscores as a \"placeholder.\"\n",
    "    placeholders = list(re.finditer(r'_{2,}', text))\n",
    "\n",
    "    if len(placeholders) <= 1000:\n",
    "        return text\n",
    "    else:\n",
    "        # We only keep up to the end of the 1000th placeholder\n",
    "        end_1000 = placeholders[999].end()  # 999 because zero-based index\n",
    "        truncated_text = text[:end_1000]\n",
    "        return truncated_text\n",
    "\n",
    "###########################\n",
    "# SPLIT TEXT INTO 10 CHUNKS\n",
    "###########################\n",
    "def split_into_10_chunks(text):\n",
    "    \"\"\"\n",
    "    Split 'text' into at most 10 roughly equal-length chunks by character count.\n",
    "    Returns a list of string chunks.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    length = len(text)\n",
    "    if length == 0:\n",
    "        return [text]  # Edge case: empty text\n",
    "\n",
    "    # We'll do integer division plus 1 to ensure we don't lose any chars\n",
    "    chunk_size = (length // 10) + 1\n",
    "    \n",
    "    start = 0\n",
    "    for _ in range(9):  # first 9 chunks\n",
    "        end = start + chunk_size\n",
    "        if end > length:\n",
    "            end = length\n",
    "        chunks.append(text[start:end])\n",
    "        start = end\n",
    "        if start >= length:\n",
    "            break\n",
    "    \n",
    "    # final chunk\n",
    "    if start < length:\n",
    "        chunks.append(text[start:])\n",
    "\n",
    "    return chunks\n",
    "\n",
    "###########################\n",
    "# TASK FUNCTION (Polish prompt)\n",
    "###########################\n",
    "def fill_in_blanks(book_title, blanks_text):\n",
    "    \"\"\"\n",
    "    Prompt in Polish to fill in the blanks.\n",
    "    We assume we only want the model to return the words needed\n",
    "    for each blank, in order.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Oto fragment z brakującymi słowami (podkreśleniami) z książki pod tytułem \"{book_title}\".\n",
    "Tekst do uzupełnienia:\n",
    "{blanks_text}\n",
    "\n",
    "Twoje zadanie: uzupełnij brakujące słowa w języku polskim. \n",
    "Zwróć mi wyłącznie listę słów, które powinny się tam pojawić w kolejności, w jakiej występują. \n",
    "Nie dodawaj żadnych komentarzy ani dodatkowego tekstu, tylko słowa rozdzielone spacją lub znakami nowej linii.\n",
    "\"\"\"\n",
    "    return call_model(prompt)\n",
    "\n",
    "\n",
    "def answer_questions(book_title, questions_list):\n",
    "    \"\"\"\n",
    "    Prompt in Polish to answer questions in Polish.\n",
    "    \"\"\"\n",
    "    answers = []\n",
    "    for question in questions_list:\n",
    "        prompt = f\"\"\"Masz pytanie dotyczące książki pt. \"{book_title}\".\n",
    "Pytanie: {question}\n",
    "Proszę odpowiedz zwięźle w języku polskim.\n",
    "\"\"\"\n",
    "        answer = call_model(prompt)\n",
    "        answers.append(answer.strip())\n",
    "    return answers\n",
    "\n",
    "def summarize_text(book_title, text_sections):\n",
    "    \"\"\"\n",
    "    Prompt in Polish to summarize text in Polish.\n",
    "    \"\"\"\n",
    "    summarized_sections = []\n",
    "    for section in text_sections:\n",
    "        prompt = f\"\"\"Masz następujący fragment z książki pt. \"{book_title}\".\n",
    "Fragment: \n",
    "{section}\n",
    "\n",
    "Zadanie: Streść ten fragment w sposób zwięzły w języku polskim.\n",
    "\"\"\"\n",
    "        summary = call_model(prompt)\n",
    "        summarized_sections.append(summary.strip())\n",
    "    return summarized_sections\n",
    "\n",
    "def translate_text(book_title, text_sections):\n",
    "    \"\"\"\n",
    "    Prompt in Polish, but request the translation in English.\n",
    "    \"\"\"\n",
    "    translated_sections = []\n",
    "    for section in text_sections:\n",
    "        prompt = f\"\"\"Masz fragment z książki pt. \"{book_title}\" w języku polskim:\n",
    "{section}\n",
    "\n",
    "Zadanie: Przetłumacz powyższy fragment na język angielski.\n",
    "\"\"\"\n",
    "        translation = call_model(prompt)\n",
    "        translated_sections.append(translation.strip())\n",
    "    return translated_sections\n",
    "\n",
    "###########################\n",
    "# MAIN SCRIPT\n",
    "###########################\n",
    "\n",
    "def main():\n",
    "    # 1. Handle BLANKS\n",
    "    blanks_input_dir = os.path.join(DATA_DIR, \"blanks\", \"blanks\")\n",
    "    blanks_output_dir = os.path.join(DATA_DIR, \"blanks\", \"solutions_auto\")\n",
    "    os.makedirs(blanks_output_dir, exist_ok=True)\n",
    "    for filename in os.listdir(blanks_input_dir):\n",
    "        if filename.endswith(\"_blanks.txt\"):\n",
    "            book_title = filename.replace(\"_blanks.txt\", \"\")  # e.g. \"BookA\"\n",
    "            input_path = os.path.join(blanks_input_dir, filename)\n",
    "            \n",
    "            # Create book-specific output directory\n",
    "            book_output_dir = os.path.join(blanks_output_dir, book_title)\n",
    "            os.makedirs(book_output_dir, exist_ok=True)\n",
    "            \n",
    "            # We'll name the output file \"solutions.txt\"\n",
    "            output_path = os.path.join(book_output_dir, \"solutions.txt\")\n",
    "            \n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                original_blanks_text = f_in.read()\n",
    "\n",
    "            # 1) Limit to the first 1000 placeholders\n",
    "            truncated_blanks_text = limit_to_first_1000_blanks(original_blanks_text)\n",
    "\n",
    "            # 2) Split the truncated text into up to 10 chunks\n",
    "            text_chunks = split_into_10_chunks(truncated_blanks_text)\n",
    "\n",
    "            all_words = []\n",
    "            for chunk_index, chunk_text in enumerate(text_chunks, start=1):\n",
    "                if chunk_text.strip() == \"\":\n",
    "                    # If there's an empty chunk, skip\n",
    "                    continue\n",
    "\n",
    "                # 3) Call the model on this chunk\n",
    "                print(f\"Filling chunk {chunk_index} of {len(text_chunks)} for book '{book_title}'...\")\n",
    "                filled_text = fill_in_blanks(book_title, chunk_text)\n",
    "\n",
    "                # 4) Split the model response into words/tokens\n",
    "                words_in_chunk = filled_text.split()\n",
    "                all_words.extend(words_in_chunk)\n",
    "\n",
    "            # 5) Write the combined words (from all chunks) to file, one per line\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                for w in all_words:\n",
    "                    f_out.write(w.strip() + \"\\n\")\n",
    "\n",
    "\n",
    "    # 2. Handle Q&A\n",
    "    qa_input_dir = os.path.join(DATA_DIR, \"q_a\", \"questions\")\n",
    "    qa_output_dir = os.path.join(DATA_DIR, \"q_a\", \"answers_auto\")\n",
    "    os.makedirs(qa_output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(qa_input_dir):\n",
    "        if filename.endswith(\"_questions.txt\"):\n",
    "            book_title = filename.replace(\"_questions.txt\", \"\")  # e.g. \"BookA\"\n",
    "            input_path = os.path.join(qa_input_dir, filename)\n",
    "            \n",
    "            # Create book-specific output directory\n",
    "            book_output_dir = os.path.join(qa_output_dir, book_title)\n",
    "            os.makedirs(book_output_dir, exist_ok=True)\n",
    "            \n",
    "            output_path = os.path.join(book_output_dir, \"answers.txt\")\n",
    "\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                questions = [line.strip() for line in f_in if line.strip()]\n",
    "\n",
    "            answers = answer_questions(book_title, questions)\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                for ans in answers:\n",
    "                    f_out.write(ans + \"\\n\")\n",
    "\n",
    "    # 3. Handle SUMMARIES\n",
    "    summary_input_dir = os.path.join(DATA_DIR, \"summary\", \"source\")\n",
    "    summary_output_dir = os.path.join(DATA_DIR, \"summary\", \"summaries_auto\")\n",
    "    os.makedirs(summary_output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(summary_input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            book_title = filename.replace(\".txt\", \"\")  # e.g. \"BookA\"\n",
    "            input_path = os.path.join(summary_input_dir, filename)\n",
    "            \n",
    "            # Create book-specific output directory\n",
    "            book_output_dir = os.path.join(summary_output_dir, book_title)\n",
    "            os.makedirs(book_output_dir, exist_ok=True)\n",
    "            \n",
    "            output_path = os.path.join(book_output_dir, \"summaries.txt\")\n",
    "\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                content = f_in.read()\n",
    "                sections = content.split(\"===\")\n",
    "\n",
    "            summaries = summarize_text(book_title, sections)\n",
    "            final_text = \"\\n===\\n\".join(summaries)\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                f_out.write(final_text)\n",
    "\n",
    "    # 4. Handle TRANSLATIONS\n",
    "    trans_input_dir = os.path.join(DATA_DIR, \"trans\", \"source\")\n",
    "    trans_output_dir = os.path.join(DATA_DIR, \"trans\", \"translations_auto\")\n",
    "    os.makedirs(trans_output_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(trans_input_dir):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            book_title = filename.replace(\".txt\", \"\")  # e.g. \"BookA\"\n",
    "            input_path = os.path.join(trans_input_dir, filename)\n",
    "            \n",
    "            # Create book-specific output directory\n",
    "            book_output_dir = os.path.join(trans_output_dir, book_title)\n",
    "            os.makedirs(book_output_dir, exist_ok=True)\n",
    "            \n",
    "            output_path = os.path.join(book_output_dir, \"translations.txt\")\n",
    "\n",
    "            with open(input_path, \"r\", encoding=\"utf-8\") as f_in:\n",
    "                content = f_in.read()\n",
    "                sections = content.split(\"===\")\n",
    "\n",
    "            translations = translate_text(book_title, sections)\n",
    "            final_text = \"\\n===\\n\".join(translations)\n",
    "\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "                f_out.write(final_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
