{
    "page_content": "Figure 10: Evaluating Explanation Methods for Feature Engineering. The graph illustrates the outcome of an experiment where users were tasked with selecting which algorithm would perform best in real-world scenarios.\n\nEach shaded line represents the average accuracy achieved by subjects starting from one of the initial ten instances, while each solid line indicates the average performance across all paths per round of interaction.\n\nIn this study, users inspected a set of explanations provided by either greedy or LIME algorithms. The position and order of these explanations were randomized between subjects. Based on their examination, users chose which algorithm they believed would excel in real-world applications.\n\nThe explanations were generated using two different instance selection methods: Random Pick (RP) and Submodular Pick (SP). To account for the potential variability in user performance, we ran each experiment with 100 participants. The results are displayed below.\n\nAs seen from Figure 10, all methods demonstrated a high level of accuracy in selecting the superior algorithm, underscoring the value of explanations in determining which model to trust. Notably, using test set accuracy would have led to incorrect selections.\n\nThe introduction of Submodular Pick (SP) significantly enhanced users' ability to choose the top-performing classi\ufb01er compared to Random Pick (RP). LIME outperformed greedy in both cases.",
    "metadata": {
        "source": "../../data/pdfs\\JP_2939672.2939778.pdf",
        "chunk_idx": 40
    }
}