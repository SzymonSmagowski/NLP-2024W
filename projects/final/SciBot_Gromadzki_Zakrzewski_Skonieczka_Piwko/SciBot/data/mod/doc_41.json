{
    "page_content": "Here is the modified document:\n\n6 We plan to further investigate optimization mechanisms in future work. Our current results suggest that using the empirical mean performance of \u03b8inc yields superior outcomes overall.\n\nHowever, we note that determining the optimal problem formulation remains unclear, as our goal is to obtain a diverse set of configurations with high Expected Improvement (EI). This ambiguity underscores the complexity of our approach.\n\nTo efficiently utilize computational resources, SMBO frequently evaluates numerous configurations per iteration. Given the relatively low cost of batch EI computations, we opt for calculating EI for an additional 10,000 randomly-sampled configurations; these are then sorted in descending order of EI. Interestingly, the top results of local search typically exhibited larger EI than all randomly sampled configurations.\n\nHaving compiled this list of 100,010 configurations based on the model, we interleave randomly-sampled configurations to provide unbiased training data for future models. Specifically, we alternate between configurations from the list and additional configurations sampled uniformly at random.\n\n4.4 Theoretical Analysis of SMAC and ROAR\n\nIn this section, we present a convergence proof for SMAC (and ROAR) in finite configuration spaces. Since Intensify always compares at least two configurations against the current incumbent, at least one randomly sampled configuration is evaluated in every iteration of SMBO. Consequently, in finite configuration spaces, each configuration has a positive probability of being selected in each iteration. Coupled with the fact that Intensify increases the number of runs used to evaluate each configuration unboundedly...",
    "metadata": {
        "source": "../../data/pdfs\\Hutter et al. - 2011 - Sequential Model-Based Optimization for General Algorithm Configuration.pdf",
        "chunk_idx": 40
    }
}