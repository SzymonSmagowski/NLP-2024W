{
    "page_content": "We evaluate the performance of several deep learning architectures on tabular data. The selected models are based on prominent literature in the field: \nMLP : a basic Multilayer Perceptron, enhanced by incorporating PyTorch's adaptive ReduceOnPlateau scheduler for dynamic learning rate adjustments.\nResnet : similar to MLP, this Residual Network variant includes dropout regularization, batch/layer normalization, and skip connections to alleviate overfitting issues.\nFT_Transformer : a simplified Transformer-based model that integrates module embeddings for both categorical and numerical features. Notably, this architecture has been thoroughly benchmarked against tree-based models and other specialized tabular models in prior work by Gorishniy et al. [2021], making it an apt representative of deep learning's capabilities on tabular data when numerical features only are used.\n\n(Note: the rest of the text was truncated)",
    "metadata": {
        "source": "../../data/pdfs\\JP_NeurIPS-2022-why-do-tree-based-models-still-outperform-deep-learning-on-typical-tabular-data-Paper-Datasets_and_Benchmarks.pdf",
        "chunk_idx": 16
    }
}