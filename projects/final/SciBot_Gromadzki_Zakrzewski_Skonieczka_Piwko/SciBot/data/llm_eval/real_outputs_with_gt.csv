llm,question,context,answer,ex_time,ground_truth
qwen2.5:3b,"What does the term ""learn to optimize"" mean?
","Document 1: 
 NatlSciRev ,2024,Vol.11,nwae132
Solver 4
Solver 1
Solver 2
Solver 3
Configuration space of algorithms
(solver class)
Facilitate the tedious trial-and-error process for
getting a good solver for an unseen instance
Solver 5
Unseen instances
Training set Solver class
Optimization problem class
Knowledge
Learning
Vehicle routing
problem
Machine learning
Traveling
salesman
problem
Problem Instance 1
Problem Instance 2
Problem Instance 3
Optimization problems
Figure 1 Illustration of the general idea of L2O. 
the same general paradigm that is very similar to 
the typical ML paradigm. That is, they leverage on 
a set of training instances from the target optimiza- 
tion problem class to gain something that would help 
alleviate the tedious algorithm configuration process 
on unseen problem instances, as i l lustrated in Fig. 1 . 
This article puts them under the umbrella term learn 
to optimize (L2O) and provides a brief overview on 
the general ideas as well as critical issues of L2O, in- 
cluding the training process, theoretical foundations 
as well as generalization issues. For more algorithmic 
details on NN-based solvers and heuristic solvers, 
readers are referred to [13 ,14 ] and [15 ,16 ], respec- 
tively. 

Document 2: 
 m∑
i=1
U(s, Xi),
where Xi is a random variable, representing an instance randomly drawn from X.
Notice that Um(s) is a random variable.
We deﬁne two properties of SPPAs, generalization and optimality. An SPPA
learning algorithm generalizes well, when the utility on all instances is close to the
utility on the training set. An SPPA learning algorithm is optimal, if the optimal
SPPA on the training set is close to the optimal result on the set of all instances.
These properties are formalized by the following deﬁnition.
Learning parallel portfolios of algorithms 99
Deﬁnition 6.1 We say that an SPPA learning algorithm mean-generalizes,i ff o ra n y
0 <ϵ and 0 <δ< 1 it outputs an SPPA s ∈ S,f o rw h i c h
P [Um(s) − E [U(s, X)] >ϵ ] ≤ δ.
Let the globally optimal algorithm be:
s∗ = arg max
s∈S
E [U(s, X)] .
We say that an SPPA learning algorithm ismean optimal,i ff o ra l l0 <ϵ and 0<δ< 1
it outputs a schedule s
P
[
E
[
U(s∗, X)
]
− E [U(s, X)] >ϵ 

Document 3: 
 model and select conﬁgurations, respectively.
Input :Target algorithm Awith parameter conﬁguration space Θ; instance set Π; cost
metric ˆc
Output :Optimized (incumbent) parameter conﬁguration, θinc
1 [R, θinc] ←Initialize(Θ, Π);
2 repeat
3 [M,tfit ] ←FitModel(R);
4 [ ⃗Θnew,tselect] ←SelectConﬁgurations(M, θinc, Θ);
5 [R,θinc] ←Intensify( ⃗Θnew, θinc, M, R, tfit + tselect, Π, ˆc);
6 until total time budget for conﬁguration exhausted;
7 return θinc;
parameter optimization (SPO) toolbox—which has received considerable attention in the
evolutionary algorithms community—provides many features that facilitate the manual
analysis and optimization of algorithm parameters; it also includes an automated SMBO
procedure for optimizing continuous parameters on single instances. We started our
own work in SMBO by comparing SKO vs SPO, studying their choices for the four
SMBO components [18]. We demonstrated that component Intensify mattered most, and
improved it in our SPO + algorithm [18]. Subsequently, we showed how to reduce the
overhead incurred by construction and use of response surface models via approximate
GP models. We also eliminated the need for a costly initial design by interleaving ran- 

Document 4: 
 this is to be taken as either the efficiency or the 
effectiveness of the solver.
r = n! Ts
Tns
The Challenges 
As presented in the introductory paragraphs, a 
divide-and-conquer approach can be lucky. A run 
can benefit from a good split, which brings a solu -
tion at the beginning of some subspace and allows 
for an early stop. In contrast, a different division 
can decrease performance. What is interesting here 
is that adding resources can decrease the perform -
ance since it can produce more demanding sub -
spaces. 
Even if portfolio-based approaches are less prone 
to this problem, extending the size of a portfolio 
can still be detrimental to its performance. In gen -
eral, this increases the overhead, due to more fre -
quent and broader clause sharing, and worsens 
cache-congestion issues. A priori, the question of 
deciding the most effective number of resources to 
use against a given formula is a difficult one. 
One possible direction of research is to extend 
automatic tuning techniques. These approaches 
use machine learning to craft a predictive function 
that relates the features of an instance and the 
parameters of a given solver to its expected run 
time. This function can be learned and tested 
offline against a large set of representative 
instances and used at run time to configure a solver 
and maximize its performance. This offline 
approach assumes that a large and representative 

Document 5: 
 OPT := inf
θ∈Θ
R(θ).
The search for the optimal conﬁguration is generally too ambitious, as the total runtime
required for the conﬁgurator must be extraordinarily large (possibly inﬁnite) to guarantee
that the best algorithm conﬁguration returned by the conﬁgurator is in fact the optimal
one with high probability.
As a workaround, one can leverage the idea underlying PAC learning (Valiant, 1984) to
the problem at hand. The basic idea is to relax the goal of ﬁnding the optimal conﬁguration
itself and, instead, ﬁnd a conﬁguration that is considered to be “good enough”. As there are
potentially several such “good enough” conﬁgurations 6, this relaxation of the goal allows
the search to be completed in less (and, thus, feasible) time. In this context, “good enough”
means that the expected runtime is only worse than the optimal expected runtime up to
a multiplicative factor of 1 + ε for some ﬁxed precision parameter ε > 0. Formally, a
conﬁguration is said to be ε-optimal (“good enough”) iﬀ
Ei∼P(c(i,θ)) ≤(1 + ε)OPT.
However, this relaxation of the target is problematic in the context of AC problems, since the
runtimes of conﬁgurations often exhibit a heavy-tailed distribution. Indeed, it is not diﬃcult
to construct an example based on such distributions in which any (sensible) conﬁgurator
would, in the worst case, take inﬁnitely long to ﬁnd an ε-optimal conﬁguration; see for 

Document 6: 
 tions of learning machines, which lead to objective 
functions that favor different optimization methods. 
As a result, many optimization techniques, including 
convex optimization methods [1 ], gradient descent 
[2 ], heuristic search [3 ] as well as evolutionary algo- 
rithms (E A s) [4 ], have been utilized by the ML com- 
munity in the past decades. 
Mainstream optimization algorithms are usu- 
ally highly configurable parameterized algorithms, 
i.e. they involve tunable parameters. Performance of 
such algorithms may vary significantly with different 
parameter settings. Hence, to solve the optimization 
problem induced by a learning task, tedious efforts 
are typically needed to configure the optimization al- 
gorithm. Consider the case of training an NN with 
stochastic gradient descent (SGD). It is widely ac- 
knowledged that the training performance is sensi- 
tive to the learning rate [5 ], a parameter of SGD. 
How to determine the optimal or at least appropri- 
ate learning rate has become a major hurdle for train- 
ing NNs, especially for large-scale NNs [6 ], or when 
the objective function is not an explicit differentiable 
function of the weights of an NN, as in the typical re- 
inforcement learning setting [7 ]. 
In order to reduce the human labor required, 
many studies have attempted to automate the config- 
uration of optimization algorithms [6 ,8 ]. However, 

","The term ""learn to optimize"" (L2O) refers to a method where training set instances from a target optimization problem class are used to gain knowledge that can help alleviate the tedious process of algorithm configuration for unseen problem instances. This approach is similar to the typical machine learning paradigm, with L2O under an umbrella term and includes considerations such as the training process, theoretical foundations, and generalization issues.",2.757843494415283,"The term ""learn to optimize"" (L2O) refers to a paradigm in which machine learning techniques are utilized to assist in the optimization process for various optimization problems. This approach leverages a set of training instances from a specific class of optimization problems to develop a solution that can alleviate the often tedious and lengthy trial-and-error process commonly associated with configuring algorithms for unseen instances. The overarching goal is to enhance the efficiency and effectiveness of solving optimization problems by using learned knowledge to predict or determine better algorithm configurations for new instances that have not been encountered before."
qwen2.5:3b,"Please give some examples of metaheuristics.
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 aside any metaphorical language. We utterly believe that eﬀorts in 
this direction should be intensiﬁed, building upon the initial postula- 
tions established in some recent works [105,106] . Speciﬁcally, meta- 
heuristics components (including search operators and algorithmic 
behavioral patterns) and interfaces between them should be stan- 
dardized towards objectively assessing similarities and diﬀerences 
between metaphor-based solvers [107] . A novel metaphor is by no 
means a suﬃcient guarantee for a signiﬁcant scientiﬁc contribution. 
• The third of these reasons is methodological, i.e., the relevance of 
some of the building blocks of the overall algorithm. A particular 
algorithm can include a given component (for example, a local op- 
timizer) that can be of relevance even if the algorithm as a whole 
is not completely competitive with respect to the prevailing liter- 
ature. A good example supporting this claim can be observed in 
co-evolutionary frameworks, which usually include a procedure to 
identify the subcomponents that will be individually co-evolved. In 
those cases, even if the subcomponent optimizer is not very sophis- 
ticated, the co-evolutionary framework can be relevant by itself. In 
this sense, it is important to select the appropriate framework to 
highlight the desired characteristic of the proposed algorithm, as dis- 
cussed in Section 3.1 . Following the same example of subcomponents 
identiﬁcation, a researcher focused on large-scale global optimiza- 

Document 3: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

Document 4: 
 erations Research54(1):99–114.
Bartz-Beielstein, T. 2006. Experimental Research in Evolution-
ary Computation. Springer V erlag.
Battiti, R., and Brunato, M. 2005. Reactiv e search: machine
learning for memory-based heuristics. Technical Report DIT-05-
058, Universit`a Degli Studi Di Trento, Trento, Italy.
Birattari, M.; St¨utzle, T.; Paquete, L.; and V arrentrapp, K. 2002.
A racing algorithm for conﬁguring metaheuristics. In Proc. of
GECCO-02, 11–18.
Birattari, M. 2004. The Problem of Tuning Metaheuristics as
Seen from a Machine Learning Perspective. Ph.D. Dissertation,
Universit´e Libre de Bruxelles, Brussels, Belgium.
Coy, S. P .; Golden, B. L.; Runger, G. C.; and Wasil, E. A. 2001.
Using experimental design to ﬁnd effective parameter settings for
heuristics. Journal of Heuristics7(1):77–97.
E´en, N., and S¨orensson, N. 2003. An extensible SA T solver. In
Proc. of SAT-03, 502–518. 

Document 5: 
 art algorithm, MOS, has been clearly surprised by SHADE-ILS and 
MLSHADE-SPA, hence becoming the most competitive algorithms 
(with a preference by SHADE-ILS, by its better performance and sim- 
plicity). 
8. Conclusions and outlook 
In this work we have stressed on the need for circumventing com- 
mon mistakes and ﬂaws observed in the ﬁeld of bio-inspired optimiza- 
tion, particularly when new meta-heuristic algorithms are proposed and 
experimentally validated over benchmarks designed to this end. Specif- 
ically, we have reviewed and critically analyzed contributions deal- 
ing with experimental recommendations and practices related to meta- 
heuristics. Following our literature study, we have prescribed a set of 
methodological recommendations for preparing a successful proposal 
of bio-inspired meta-heuristic algorithms, from the deﬁnition of the ex- 
perimentation to the presentation of the results. A number of useful 
techniques (graphically summarized in Fig. 9 ) have been suggested for 
prospective studies to implement our proposed methodological frame- 
work, in an attempt at ensuring fairness, coherence and soundness in 
future studies on the topic. Two diﬀerent case studies have been de- 
signed to exemplify the application of our prescribed methodology, dis- 
cussing on the results of the application of each guideline. Although both 
case studies deal with well-known benchmarks, we envision that our 
methodology can be a core part of the design process of meta-heuristic 

Document 6: 
 achieved using several heuristic mechanisms that interact in complex, non-intuitive
ways. For example, a DPLL-style complete solver for SAT (a prototypical NP-
complete problem with important applications in the design of reliable soft- and
hardware) may use different heuristics for selecting variables to be instantiated and
the values ﬁrst explored for these variables, as well as heuristic mechanisms for
managing and using logical constraints derived from failed solution attempts. The
activation, interaction and precise behaviour of those mechanisms is often controlled
by parameters, and the settings of such parameters have a substantial impact on the
Holger H. Hoos
Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver,
BC, V6T 1Z4, Canada, e-mail: hoos@cs.ubc.ca
1 We note that the use of heuristic techniques does not imply that the resulting algorithms are nec-
essarily incomplete or do not have provable performance guarantees, but often results in empirical
performance far better than the bounds guaranteed by rigorous theoretical analysis.
Y . Hamadi et al. (eds.),Autonomous Search,
DOI 10.1007/978-3-642-21434-9 3,
© Springer-Verlag Berlin Heidelberg 2011
37
38 Holger H. Hoos
efﬁcacy with which a heuristic algorithm solves a given problem instance or class of 

","Metaheuristics are high-level methodologies or general algorithmic frameworks that guide the search process to find good solutions in complex optimization problems. Here are some examples:

1. Simulated Annealing (SA)
2. Genetic Algorithms (GA)
3. Tabu Search (TS)
4. Ant Colony Optimization (ACO)
5. Harmony Search (HS)
6. Particle Swarm Optimization (PSO)

These metaheuristics have been widely used and studied in the field of optimization, often with variations and enhancements to improve their performance on specific problems.",2.751359462738037,"Examples of metaheuristics include Simulated Annealing (SA), Tabu Search (TS), Evolutionary Algorithms (EAs), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO)."
qwen2.5:3b,"What is the ""no free lunch"" theorem about?
","Document 1: 
 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTA TION, VOL. 1, NO. 1, APRIL 1997 67
No Free Lunch Theorems for Optimization
David H. Wolpert and William G. Macready
Abstract— A framework is developed to explore the connection
between effective optimization algorithms and the problems they
are solving. A number of “no free lunch” (NFL) theorems are
presented which establish that for any algorithm, any elevated
performance over one class of problems is offset by perfor-
mance over another class. These theorems result in a geometric
interpretation of what it means for an algorithm to be well
suited to an optimization problem. Applications of the NFL
theorems to information-theoretic aspects of optimization and
benchmark measures of performance are also presented. Other
issues addressed include time-varying optimization problems and
a priori“head-to-head” minimax distinctions between optimiza-
tion algorithms, distinctions that result despite the NFL theorems’
enforcing of a type of uniformity over all algorithms.
Index T erms— Evolutionary algorithms, information theory,
optimization.
I. I NTRODUCTION
T
HE past few decades have seen an increased interest
in general-purpose “black-box” optimization algorithms
that exploit limited knowledge concerning the optimization
problem on which they are run. In large part these algorithms
have drawn inspiration from optimization processes that occur
in nature. In particular, the two most popular black-box 

Document 2: 
 function, and will produce .
The proof is completed if we show that there is no cost
function so that produces a sample containing and
and such that produces a sample containing and .
There are four possible pairs of samples to consider:
i) ;
ii) ;
iii) ;
iv) .
Since if its ﬁrst point is a , jumps to which is where
starts, when ’s ﬁrst point is a its second point must
equal ’s ﬁrst point. This rules out possibilities i) and ii).
For possibilities iii) and iv), by ’s sample we know that
must be of the form , for some variable . For
case iii), would need to equal , due to the ﬁrst point in
’s sample. For that case, however, the second point sees
would be the value at , which is , contrary to hypothesis.
For case iv), we know that the would have to equal , due
to the ﬁrst point in ’s sample. That would mean, however,
that jumps to for its second point and would therefore
see a , contrary to hypothesis.
Accordingly, none of the four cases is possible. This is
a case both where there is no symmetry under exchange of
’s between and , and no symmetry under exchange of
histograms.
A PPENDIX G 

Document 3: 
 The sum over can be accomplished in the same manner
is summed over. In fact, all the sums over all can
be done, leaving
(7)
In this last step, the statistical independence of and has
been used.
Further progress depends on whether represents or
. We begin with analysis of the case. For this case
, since only reﬂects cost
values from the last cost function, . Using this result gives
The ﬁnal sum over is a constant equal to the number of
ways of generating the sample from cost values drawn
from . The important point is that it is independent of
the particular . Because of this the sum over can be
evaluated eliminating the dependence
This completes the proof of Theorem 2 for the case of .
The proof of Theorem 2 is completed by turning to the
case. This is considerably more difﬁcult since
cannot be simpliﬁed so that the sums over cannot be
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 79
decoupled. Nevertheless, the NFL result still holds. This is
proven by expanding (7) over possible values
(8)
The innermost sum over only has an effect on the
, term so it contributes ,
. This is a constant, equal to . This
leaves
The sum over is now simple
The above equation is of the same form as (8), only with a
remaining sample of size rather than . Consequently, in
an analogous manner to the scheme used to evaluate the sums 

Document 4: 
 always use algorithm . As applied to this case, they
mean that for ﬁxed and ,i f does better (on average)
with the algorithms in some set , then does better (on
average) with the algorithms in the set of all other algorithms.
In particular, if for some favorite algorithms a certain “well-
behaved”results in better performance than does the random
, then that well-behaved gives worse than random behavior
on the set all remaining algorithms. In this sense, just as there
are no universally efﬁcacious search algorithms, there are no
universally benignwhich can be assured of resulting in better
than random performance regardless of one’s algorithm.
In fact, things may very well be worse than this. In super-
vised learning, there is a related result [11]. Translated into
the current context, that result suggests that if one restricts
sums to only be over those algorithms that are a good match
to , then it is often the case that “stupid” choosing
procedures—like the irrational procedure of choosing the
algorithm with the less desirable—outperform “intelligent”
ones. What the set of algorithms summed over must be in
order for a rational choosing procedure to be superior to an
irrational procedure is not currently known.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 77
VIII. C ONCLUSIONS
A framework has been presented in which to compare 

Document 5: 
 values after distinct cost evaluations produced by using a
particular instantiation of an evolutionary algorithm?”
At ﬁrst glance this seems to be an intractable question,
but the NFL theorem provides a way to answer it. This is
because—according to the NFL theorem—the answer must be
independent of the algorithm used to generate. Consequently,
3 In particular, one may want to impose restrictions on /80 /40 /102 /41 . For instance,
one may wish to only consider /80 /40 /102 /41 that are invariant under at least partial
relabeling of the elements in /88 , to preclude there being an algorithm that will
assuredly “luck out” and land on /109/105 /110 /120 /50/88 /102 /40 /120 /41 on its very ﬁrst query.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 73
we can choose an algorithm for which the calculation is
tractable.
Theorem 3: For any algorithm, the fraction of cost func-
tions that result in a particular histogram is
For large enough , this can be approximated as
where is the entropy of the distribution , and
is a constant that does not depend on .
This theorem is derived in Appendix C. If some of the are
zero, the approximation still holds, only with redeﬁned to 

Document 6: 
 to-head minimax distinctions. Such an analysis is beyond the
scope of this paper.)
On the other hand, having the sums be equal does carry ob-
vious implications for whether there are head-to-head minimax
distinctions. For example, if both algorithms are deterministic,
then for any particular
equals one for one pair and zero for all others. In such
a case, is just the number
of that result in the pair .S o
implies
that there are no head-to-head minimax distinctions between
and . The converse, however, does not appear to hold. 4
4 Consider the grid of all /40 /122/59 /122 /48 /41 pairs. Assign to each grid point the number
of /102 that result in that grid point’s /40 /122/59 /122 /48 /41 pair. Then our constraints are i)
by the hypothesis that there are no head-to-head minimax distinctions, if grid
point /40 /122 /49 /59/122 /50 /41 is assigned a nonzero number, then so is /40 /122 /50 /59/122 /49 /41 and ii) by
the no-free-lunch theorem, the sum of all numbers in row /122 equals the sum
of all numbers in column /122 . These two constraints do not appear to imply
that the distribution of numbers is symmetric under interchange of rows and 

","The ""No Free Lunch"" (NFL) theorem states that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. The NFL theorems enforce a type of uniformity across all algorithms, indicating that no single algorithm can be universally superior or inferior to others without context-specific biases.",2.572277069091797,"The ""no free lunch"" (NFL) theorem for optimization establishes that any optimization algorithm's elevated performance over a specific class of problems is counterbalanced by a corresponding decrease in performance over other classes of problems. This implies that no algorithm performs better than random search across all possible optimization problems; each algorithm may excel in particular scenarios but will underperform in others, resulting in an overall average performance that is uniform when considering all possible problems. Thus, the theorem highlights the limitations of universally effective optimization algorithms, emphasizing that there are no algorithms that can guarantee superior performance on all types of optimization tasks."
qwen2.5:3b,"What is the concept behind Parallel Algorithm Portfolio?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 general-ized to denote a combination of different algorithms intended to outperform the search performance of any of its constituent algorithms [3, 6, 11-14]. Here we extend that formulation, so that an algorithm portfolio schedules its constituent algorithms to run concurrently on a set of processors. Let an algorithm be any CSP solver, as described in the previous section. Given a set A = {a1, a2, …, am} of m algorithms, a set P = {x1, x2, …, xn} of n problems, and a set of B consecutive time intervals T = {t1, t2, …, tB}, a simple schedule Sk for a prob-lem on a single processor specifies 

Document 4: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 5: 
 of processors [22], decomposition methods have shown their potential on many more processors (e.g., 64 cores or more in [19]). We will explore this in future work.  
9   Conclusions This paper presents WG, a constructor for non-parallel algorithm portfolios based on case-based reasoning and a greedy algorithm. It formulates parallel algorithm portfo-lio construction as an integer-programming problem, and generalizes WG to RSR-WG, a constructor for parallel algorithm portfolios based on a property of the optimal solution to the inherent integer-programming problem. To address a set of problems one at a time, RSR-WG creates portfolios of deterministic algorithms offline. Exper-iments show that the parallel algorithm portfolios produced by RSR-WG are statisti-cally significantly better than those produced by naïve parallel versions of popular portfolio constructors. Moreover, with only a few additional processors, RSR-WG portfolios are competitive with an oracle solver on a single processor.  Acknowledgements. This research was supported in part by the National Science Foundation under grants IIS-0811437, CNS-0958379 and CNS-0855217, and the City University of New York High Performance Computing Center. 

Document 6: 
 Ann Math Artif Intell (2006) 48:85–106
DOI 10.1007/s10472-007-9050-9
Learning parallel portfolios of algorithms
Marek Petrik ·Shlomo Zilberstein
Published online: 25 May 2007
© Springer Science + Business Media B.V. 2007
Abstract A wide range of combinatorial optimization algorithms have been devel-
oped for complex reasoning tasks. Frequently, no single algorithm outperforms all
the others. This has raised interest in leveraging the performance of a collection
of algorithms to improve performance. We show how to accomplish this using a
Parallel Portfolio of Algorithms (PPA). A PPA is a collection of diverse algorithms
for solving a single problem, all running concurrently on a single processor until
a solution is produced. The performance of the portfolio may be controlled by
assigning different shares of processor time to each algorithm. We present an
effective method for ﬁnding a PPA in which the share of processor time allocated
to each algorithm is ﬁxed. Finding the optimal static schedule is shown to be an
NP-complete problem for a general class of utility functions. We present bounds on
the performance of the PPA over random instances and evaluate the performance
empirically on a collection of 23 state-of-the-art SAT algorithms. The results show
signiﬁcant performance gains over the fastest individual algorithm in the collection.
Keywords Algorithm portfolios ·Resource bounded reasoning ·
Combinatorial optimization
Mathematics Subject Classiﬁcations (2000) 68T05 ·68T20 

","The concept of a Parallel Algorithm Portfolio involves creating a collection of diverse algorithms for solving a single problem, all running concurrently on a single processor until a solution is produced. The performance can be controlled by assigning different shares of processor time to each algorithm. Finding an optimal static schedule for such portfolios is shown to be an NP-complete problem for certain utility functions.

The main advantages include leveraging parallelism to effectively utilize modern hardware and using several different solvers to tackle computationally hard problems, as seen in sequential and adaptive solvers. Parallel Algorithm Portfolios are also designed with generalization potential, allowing them to run on multiple processors by scheduling algorithms concurrently across them.",2.925741910934448,"The concept behind a Parallel Algorithm Portfolio (PAP) involves leveraging multiple algorithms simultaneously to solve problems more efficiently than any single algorithm could on its own. A PAP consists of a diverse set of algorithms that run concurrently on a processor, each contributing to the overall problem-solving process. The performance of the portfolio can be enhanced by dynamically allocating processor time shares to each algorithm, which allows for optimal utilization of their strengths. The goal is to improve performance in solving complex reasoning tasks and combinatorial optimization problems by combining the strengths of various algorithms, especially when no single algorithm consistently outperforms the others. This approach allows for greater efficiency and improved outcomes compared to relying on a single algorithm or a naive combination of them."
qwen2.5:3b,"Please provide some approaches to how Parallel Algorithm Portfolio can be constructed.
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately
allocate computational resources to different solvers, while the
parallel portfolios do not necessarily require any extra resource
allocation since each solver is simply assigned with the same
amount of resources. Third, a parallel portfolio could be eas-
ily converted to a sequential portfolio by using the algorithm
selection methods [39] to build selectors on the solvers in the
portfolio, which means the portfolios generated by construc-
tion approaches (e.g., the approach proposed in this article)
could be further used for constructing sequential portfolios.
In this article, we propose a novel approach called the
generative adversarial solver trainer (GAST) for the auto-
matic construction of parallel portfolios. Unlike the existing
construction approaches, GAST would generate additional
training instances and construct a parallel portfolio with the
dynamically changing training set. More speciﬁcally, GAST
puts instance generation and portfolio construction in an adver-
sarial game. The instance generation aims to generate the hard
problem instances that could not be solved well by the cur-
rent portfolio, while the portfolio construction aims to ﬁnd a
new component solver for the portfolio to better solve these 

Document 3: 
 1 for j := 1 ... n do
2 obtain portfolio c(j)
1:k by running AC on conﬁguration space ∏k
l=1{(c) | c ∈ C} on I using m for time t/n
3 choose ˆc1:k ∈ arg minc(j)
1:k |j∈{1...n} m(c(j)
1:k, I) that achieved best performance on I according to m
4 return ˆc1:k
3.1.1. Simultaneous conﬁguration of all component solvers (Global)
Our ﬁrst portfolio conﬁguration method is the straightforward extension of standard algorithm conﬁguration to the 
construction of a parallel portfolio (see Algorithm 1). Speciﬁcally, if the given solver has ℓ parameters, we treat the portfolio 
c1:k as a single algorithm with ℓ · k parameters inducing a conﬁguration space of size |C|k, and conﬁgure it directly. As 
noted above, we identify a single conﬁguration as the best of n independent runs of AC . These runs can be performed in 
parallel, meaning that this procedure requires wall clock time t/n if n machines—one for each AC run—with k cores are 
available. The used CPU time will be the given time budget t for Lines 1 and 2 in Algorithm 1 and some small overhead 
ϵ to choose the best portfolio in Line 3. The scalability of this approach is limited by the fact that the global conﬁguration 

Document 4: 
 Learning Algorithm Portfolios for Parallel Execution Xi Yun1 and Susan L. Epstein1, 2  1 Department of Computer Science, The Graduate School of The City University of New York, New York, NY 10016, USA 2 Department of Computer Science, Hunter College of The City University of New York, New York, NY 10065, USA xyun@gc.cuny.edu, susan.epstein@hunter.cuny.edu 
Abstract. Portfolio-based solvers are both effective and robust, but their prom-ise for parallel execution with constraint satisfaction solvers has received rela-tively little attention. This paper proposes an approach that constructs algorithm portfolios intended for parallel execution based on a combination of case-based reasoning, a greedy algorithm, and three heuristics. Empirical results show that this method is efficient, and can significantly improve performance with only a few additional processors. On problems from solver competitions, the resultant algorithm portfolios perform nearly as well as an oracle. Keywords: constraint satisfaction, algorithm portfolio, parallel processing, ma-chine learning. 

Document 5: 
 sign of parallel algorithm portfolios—sets of solvers that are run in parallel on a given instance of a decision problem, such 
as SAT, until the ﬁrst of them ﬁnds a solution [39,27].
In this work,1 we study generic methods for solving a problem we call Automatic Construction of Parallel Portfolios 
(ACPP): automatically constructing a static2 parallel solver from a sequential solver or a set of sequential solvers. This 
task can be understood as falling within the programming by optimization paradigm [34] in that it involves the design of 
software in which many design decisions have been deliberately left open during the development process (here exposed as 
parameters of SAT solvers) to be made automatically later (here by means of an automated algorithm conﬁgurator) in order 
to obtain optimized performance for speciﬁc use cases. Hence, all that is required by our ACPP methods is a sequential 
solver whose conﬁguration space contains complementary conﬁgurations.
We study three variants of the ACPP problem. First, we consider building parallel portfolios starting from a single, 
highly parametric sequential solver design. However, for well-studied problems (e.g., SAT), there often exist a wide range of 
different solvers that contribute to the state of the art (see, e.g., [73]). Complementarities among such solvers can be ex-
ploited by algorithm portfolios, whether driven by algorithm selection (like SATzilla [72]) or by parallel execution (such as 

Document 6: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

","To construct parallel algorithm portfolios, effective approaches include leveraging techniques such as greedy algorithms and case-based reasoning combined with heuristics. This involves using a combination of methods like adaptive parameter control, reactive search, and hyper-heuristics. These methods dynamically determine the best solver setting while solving problem instances to find optimal configurations. Additionally, novel strategies can involve generating additional training instances and constructing a portfolio based on these dynamic changes in the training set. The generative adversarial solver trainer (GAST) is one such approach that uses an adversarial game where instance generation aims to generate difficult problems not solvable by the current portfolio, while portfolio construction seeks to find new component solvers for improvement.",3.219645977020264,"Several approaches can be used for constructing Parallel Algorithm Portfolios, as highlighted in the provided context:

1. **Generative Adversarial Solver Trainer (GAST)**: This novel approach focuses on dynamic instance generation in conjunction with portfolio construction. GAST creates challenging problem instances that the current portfolio struggles to solve, then seeks to integrate new solvers into the portfolio to better address these difficulties.

2. **Case-Based Reasoning and Heuristic Combination**: An approach combining case-based reasoning with greedy algorithms and heuristics can efficiently construct parallel algorithm portfolios. This method has demonstrated substantial performance improvements, especially with the addition of a few processors.

3. **Automatic Construction of Parallel Portfolios (ACPP)**: This method involves using an existing sequential solver or multiple sequential solvers to build a static parallel portfolio. It exploits the complementary configurations of sequential solvers and applies automatic algorithm configuration to optimize performance for specific problems.

4. **Simultaneous Configuration**: A straightforward method extends the standard algorithm configuration approach to create a parallel portfolio. All component solvers are configured simultaneously, treating the portfolio as a single algorithm with multiple parameters, allowing the configuration to achieve enhanced performance.

5. **Parallel Execution of Solvers**: The fundamental concept of a parallel portfolio is to run multiple solvers in parallel on a given problem instance until one solver finds a solution. This allows for the leveraging of the strengths of various solvers collectively.

6. **Adaptive Mechanisms**: Incorporating adaptive strategies, like reactive search or hyper-heuristics, can help dynamically tailor the solver configurations during problem-solving, enabling the portfolio to adjust to the specific needs of different instances in real-time.

Overall, these methods emphasize the importance of parallelism and the strategic combination of different solving techniques to enhance computational efficiency."
qwen2.5:3b,"Can co-evolution be used in Parallel Algorithm Portfolio construction?
","Document 1: 
 in a series of benchmark problems. Finally, the paper will end with
a brief conclusion and a discussion of future work in Section 5.
2 RELATED WORK
2.1 Portfolio of evolutionary algorithm
In the areas of evolutionary algorithms, algorithm portfolio is ap-
plied to increase the probability of finding a better solution by
allocating computational resources to several complementary algo-
rithms. The algorithm portfolio frameworks in the literature can
be classified into two categories as the parallel-based framework
and the sequential-based framework.
For the parallel-based framework, all candidates will run simul-
taneously in multiple sub-processes. Population-based algorithm
portfolio (PAP) is a typical example [16], which allocates compu-
tational resources before the optimization according to the prior
knowledge.Eachalgorithmhasitsownpopulationandevolveinde-
pendently,buttheinformationissharedamongdifferentalgorithms
bymigrationstrategy.Besides,otherparallel-basedportfolioframe-
works like AMALGAM-SO [21] and the UMOEAs [6] collect the
performance of algorithms during the optimisation process and
allocate more resources to the better algorithm.
On the other hand, the sequential based framework only runs
onealgorithmatmostofthetimeduringtheprocessofoptimisation.
Different from the parallel-based algorithm portfolio, this kind of
frameworktrytoselectthebestalgorithmindifferentoptimisation 

Document 2: 
 tic is likely to have the right properties for
this approach to be useful, thus making it
very general in terms of applications.
So far we have assumed that the com-
ponent algorithms are completely inde -
pendent of each other and do not com-
municate. They can be thought of as
“competing” with one another for ma-
chineresources.However,allowingforco -
operation or dependencies among the in-
dividualalgorithmswhiletheyarerunning
simultaneously can improve performance
(18, 19). This possibility raises the inter-
estingquestionoftheextenttowhichour
economics approach to portfolio algo-
rithms can also benefit from cooperation.
Basically, cooperation will introduce sta-
tistical correlations between the perfor -
mance of the individual algorithms, and
we will accordingly define the correlation
between them as
  
cov•t1,t2 
 var•t1 var•t2 
(3)
where cov(t1, t2) denotes the covariance of
theperformanceofthetwoalgorithms.The
effect of cooperation, when manifested in
negativecorrelations,istoincreasetheper-
formanceaswellasreducetherisk(Fig.5).
This change is easily understood in that
negative correlations mean that one algo-
rithmisparticularlygoodpreciselyonthose 

Document 3: 
 So far, we have shown that our procedures were able to construct effective parallel portfolios based on single solvers 
with rich design spaces. There is considerable evidence from the literature and from SAT competitions that strong portfolios 
can also be built by combining entirely different solvers in their default conﬁgurations (see, e.g., SATzilla [72], ppfolio [63]
and pfolioUZK [70]). For instance, ppfolio was obtained simply by combining the best solvers from the previous competition 
into a parallel portfolio. pfolioUZK included more state-of-the-art solvers from 2011 and relied on additional experiments 
to ﬁnd the best combination of solvers in a portfolio. Neither portfolio considers the conﬁguration space of the component 
solvers and therefore both can be seen as simple baselines for other parallelization approaches, including ours. However, 
ppfolio and pfolioUZK use Plingeling as a portfolio component. Since we aim to investigate the strength of our ACPP methods 
without additional human expert knowledge on parallel solving, we ﬁrst consider only sequential solvers as the basis for 
our ACPP approach. This section and the following section investigates the extension of our automatic techniques to the 
construction of portfolios based on the conﬁguration spaces spanned by such solver sets.
4.1. Approach
As long as all of our component solvers are sequential, we can simply use the ACPP procedures deﬁned in Section 3. 
We can accommodate the multi-solver setting by introducing a solver choice parameter for each portfolio component (see 

Document 4: 
 1
Few-shots Parallel Algorithm Portfolio Construction
via Co-evolution
Ke Tang, Senior Member, IEEE, Shengcai Liu, Member, IEEE, Peng Yang, Member, IEEE,
and Xin Yao, Fellow, IEEE
Abstract—Generalization, i.e., the ability of solving problem
instances that are not available during the system design and
development phase, is a critical goal for intelligent systems.
A typical way to achieve good generalization is to learn a
model from vast data. In the context of heuristic search, such a
paradigm could be implemented as conﬁguring the parameters
of a parallel algorithm portfolio (PAP) based on a set of
“training” problem instances, which is often referred to as PAP
construction. However, compared to traditional machine learning,
PAP construction often suffers from the lack of training instances,
and the obtained PAPs may fail to generalize well. This paper
proposes a novel competitive co-evolution scheme, named Co-
Evolution of Parameterized Search (CEPS), as a remedy to
this challenge. By co-evolving a conﬁguration population and an
instance population, CEPS is capable of obtaining generalizable
PAPs with few training instances. The advantage of CEPS in
improving generalization is analytically shown in this paper. Two
concrete algorithms, namely CEPS-TSP and CEPS-VRPSPDTW,
are presented for the Traveling Salesman Problem (TSP) and 

Document 5: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, co-evolution can indeed be used in parallel algorithm portfolio (PAP) construction. The paper introduces a competitive co-evolution scheme called CEPS for PAPs with few training instances. This approach co-evolves both a configuration population and an instance population to create generalizable PAPs.",2.1703577041625977,"Yes, co-evolution can be used in Parallel Algorithm Portfolio (PAP) construction. The context indicates that a competitive co-evolution scheme, specifically named Co-Evolution of Parameterized Search (CEPS), is proposed for this purpose. CEPS aims to enhance generalization in PAPs, which is particularly beneficial given the challenges of constructing effective portfolios from limited training instances. By co-evolving a population of configurations along with a population of problem instances, CEPS is capable of producing more generalizable PAPs, highlighting the advantage of this approach in constructing effective portfolios."
qwen2.5:3b,"What is the idea behind the PAP approach called ""GLOBAL""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 RDW 13.24 1.38 11.0 24.1 11.5–16.5 %
LEUKOCYTES (%) 7.61 3.16 0.76 33.82 %
LEUKOCYTES 7614.73 3190.62 760.0 33820.0 3500–10500µL
that have led to the production of a choice in a speciﬁc exam-
ple. The notion of explanation in this work is aligned with
Samek et al. ( 2019).
As stated previously, it is possible to analyze a model rela-
tion between input and output in two ways: looking at local
explanations and global interpretation/explanations (Molnar
2019). Here, we use these two terms interchangeably . While
global interpretations enlighten scientists about what features
may be more representative to the model given a data sam-
ple, local explanations show what led the model to the current
output.
In our scope, global interpretations can help healthcare
researchers to question data sample quality and also direct
the search for more relevant features. Single explanations of
the COVID-19 diagnosis can assist healthcare professionals
that are working directly with potentially infected patients in
their decision-making.
7.1 Global Interpretation
For the glass-box models, global interpretation can be quali-
tatively evaluated by analyzing its feature weights. For these
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 633
Table 2 Results of experiments 

Document 3: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 6: 
 (CEC2005) [12], denoted asfcec1 −fcec14. These 27 functions
span a diverse set of problem features, such as multimodality,
ruggedness, ill-conditioning, interdependency, etc. They pro-
vided an ideal platform for our investigation on reducing risk
on a large variety of problems. Short descriptions of these
functions are presented in Tables I and II. More details of these
functions can be found in [11] and [12]. In our experiments,
all the functions were solved in 30 dimensions.
B. Experimental Settings
All the results presented in this paper were obtained by
executing 30 independent runs for each experiment. Since
we expect the PAP framework to be general enough so that
alternative algorithms can be incorporated with little effort,
it should not rely much on the reﬁnement of the constituent
algorithms. Hence, we did not ﬁne-tune the parameters of
the constituent algorithms to ﬁt PAP. When implementing
SaNSDE, we used all the parameter settings suggested in
the original publication [7]. As suggested in [8], a linearly
decreasing inertia weight over the course of the search is
employed in our implementation of wPSO. The two coefﬁ-
cients of wPSO were both set to 1.49445. We assumed the
researchers who proposed G3PCX and CMA-ES are at the
best position to implement the two algorithms and ﬁne-tune
the parameters. Hence, we simply used the source code of 

","The PAP (Problem Analysis and Prioritization) approach, which includes various phases like PROBE (Problem Recognition), PLAN (Plan to Resolve Problem), and other steps, can have different components or variants. The concept of a ""GLOBAL"" phase is not universally standardized within the core PAP framework.

However, if we were to interpret the acronym ""GLOBAL"" as an extension or refinement of the PAP approach, it might suggest:

1. **G**lobal Impact: Consider broader system impacts and effects.
2. **L**everage Learning: Build on previous analyses for more informed decisions.
3. **O**ptimize Objectives: Ensure alignment with overall project objectives.
4. **R**esource Review: Assess and optimize resource allocation, including human resources, time, and finances.
5. **A**udience Alignment: Understand the perspectives and expectations of stakeholders involved.
6. **L**everage Lessons Learned: Apply lessons from similar projects or contexts.

In essence, a ""GLOBAL"" phase could be seen as an integrated approach that seeks to consolidate insights across various dimensions (global impact, learning leverage, objectives alignment, resource optimization, stakeholder alignment, and lessons application) in order to make more comprehensive decisions. 

This concept isn't specifically tied to any single PAP variant but could serve as a guiding principle for enhancing the effectiveness of problem analysis processes.",4.403626203536987,"The idea behind the PAP (Population Allocation Paradigm) approach called ""GLOBAL"" relates to investigating the performance of the algorithm across a wider set of problems as opposed to focusing solely on singular optimization issues. PAP seeks to reduce the risk by applying various population-based search algorithms to multiple problems simultaneously, rather than optimizing for just one. This framework allows these algorithms to interact and share insights through a multipopulation model where computation time (measured by fitness evaluations) is intelligently allocated among different constituent algorithms. This collaboration enables the algorithms to leverage diverse strengths and promote cooperation, thereby enhancing overall optimization performance. The goal is to foster a more robust algorithm performance that can adapt to various challenges presented by a broad spectrum of problems."
qwen2.5:3b,"What is the idea behind the PAP approach called ""PARHYDRA""?
","Document 1: 
 on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is
worthy of allocating the limited computation time to different
algorithms so that the overall performance can be enhanced.
In contrast, a speciﬁc topology/structure for the relationship
between subpopulations is usually explicitly deﬁned in existing
dEAs [26]–[29], [32], [33]. Third, we are more interested
in reducing the overall risk of an algorithm ona spectrum
of problems. This issue has never been investigated in the
literature of dEAs.
In addition to dEAs, another notable work that is related
to PAP is the isolation strategy employed by hierarchically
organized evolution strategies (HOES) [34], [35]. In HOES,
a population is partitioned into a number of subpopulations,
and ES is run on them with different control parameters (e.g.,
search step sizes). The isolation strategy works in such a
way that the subpopulations evolve separately for a predeﬁned
isolation period (e.g., a few generations), and then the control
parameters are updated based on the individuals generated in
the subpopulations during the isolation period. Unlike PAP,
the motivation of the isolation strategy is not to make use of 

Document 2: 
 SCENARIOS , THAT IS , SAT-SMALL-BIAS-1/2/3/4. T HE NAME OF THE
CONSTRUCTION APPROACH IS USED TO DENOTE THE PORTFOLIOS
CONSTRUCTED BY IT. “PARHYDRA-A” R EFERS TO PARHYDRA
CONFIGURING BASED ON AUGMENTED TRAINING SETS .AP A R - 1 0
SCORE IS SHOWN IN BOLDFACE IF IT WAS NOT SIGNIFICANTLY
DIFFERENT FROM THE BEST TEST PERFORMANCE IN THE SCENARIO
(ACCORDING TO A WILCOXON SIGNED -RANK TEST WITH p = 0.05)
To answer this question, in each of the eight SAT scenarios,
that is, SAT-SMALL/BIAS-1/2/3/4, we used the same instance
generation procedure as in GAST (lines 13–18 in Algorithm 1)
to generate a large set of instances based on the training set.
The size of the generated set is ﬁve times the size of the
training set. Recall that the training set contains 50 instances,
we thus obtained an augmented training set of 300 instances
in each SAT scenario, and then PARHYDRA was used to
construct a parallel portfolio on these augmented training sets,
and then the obtained portfolio was tested on the test sets. As
before, each portfolio was tested by running it on the test set 

Document 3: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 4: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 5: 
 be used to build parallel portfolios. (Building a parallel portfolio in this way has the added advantage that no instance 
features are required, since there is no need to select among algorithms.) We introduce some enhancements to this 
approach for the parallel portfolio setting (discussed in Section 3.1.2), and refer to our method as parHydra.
3. Some parallel solvers only achieve strong performance when running on more than one core; such solvers will not 
be found by a greedy approach like parHydra, which only adds one conﬁguration at a time and does not recognize 
interaction effects that arise between different threads of a parallel solver. To overcome this problem, we introduce a 
new method called parHydrab, which augments parHydra to train b solvers per iteration. This method trades off the 
computational beneﬁt of parHydra’s greedy approach with the greater coverage of Global.
We evaluated our ACPP methods on SAT. We chose this domain because it is highly relevant to academia and industry 
and has been widely studied. We thus had access to a wide range of strong, highly parametric solvers and were assured 
that the bar for demonstrating eﬃcacy of parallelization strategies was appropriately high. We note that our approach is not 
limited to SAT solvers and can be directly applied to other domains. To evaluate our methods in the single-solver setting, we 
studied both Lingeling and clasp: prominent, highly parametric state-of-the-art solvers for SAT. Lingeling won a gold medal in 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The PARHYDRA approach, within the broader context of Privacy by Design (PAP), likely refers to a specific methodology or set of principles for implementing privacy-enhancing technologies. However, without direct access to an official document or detailed explanation from the developers of this particular approach, it's challenging to provide precise information about exactly what PARHYDRA stands for and its full meaning.

In general, Privacy by Design (PAP) is a framework that aims to incorporate privacy considerations into the design phase of products and services, rather than treating them as an afterthought or an add-on. It was developed in response to the increasing concerns about data protection and privacy, especially in the digital age where personal information can be collected, analyzed, and used at unprecedented scales.

If ""PARHYDRA"" is meant to be a specific acronym within PAP, it might represent another layer of detail or further refinement in designing privacy-enhancing technologies. For example:

- **P**rivacy: Ensuring that data processing respects individuals' rights.
- **A**ccessibility: Making sure that only authorized personnel can access sensitive information.
- **R**epositioning: Changing the way data is used and stored to prevent misuse.
- **H**idden Data: Concealing data where possible to reduce exposure risks.
- **Y**ield Control: Limiting or controlling what can be done with collected data.
- **D**ata Privacy: Protecting personal information at all stages of its lifecycle.

These could be some interpretations of the letters in ""PARHYDRA,"" but I would need more context from a specific source to provide definitive answers. If you have access to official documentation or further details about PARHYDRA, that would help clarify the exact meaning and content of this approach within PAP.",5.699020862579346,"The PAP approach called ""PARHYDRA"" refers to a method for constructing parallel portfolios using augmented training sets. It focuses on creating a parallel portfolio by utilizing a large augmented training set generated from a smaller training set, thereby optimizing the performance of the portfolio on various instances. This approach is designed to leverage multiple algorithms simultaneously without relying on specific instance features, allowing for more effective performance in SAT scenarios. PARHYDRA enhances algorithm selection by training multiple solvers concurrently, recognizing interaction effects, and maximizing computational efficiency."
qwen2.5:3b,"What is the idea behind the PAP approach called ""CLUSTERING""?
","Document 1: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 2: 
 proposed a big data k-means clustering, and a big data fuzzy k-means missing values 
approach that resulted in robust and efficient output for big data and offered reasonable 
execution times. The two imputation techniques surpassed in most cases mean imputa -
tion and elimination of the instances with lost values during classification. Offer robust 
and efficient results for Big Data datasets, offering reasonable execution times. The fuzzy 
k-means approach was proved to provide better results for high percentages of miss -
ing values in the data, while the k-means performed better with the dataset that had 
lower amounts of missing values. Zhang et al. [102], also proposed a multiple imputation 
clustering based approach that handled missing values in big longitudinal trial data in 
e-Health. The proposed concept proved that it could be easily adapted for different types 
of clustering for big incomplete longitudinal trial data in eHealth services.
Ensemble methods
Ensemble methods are strategies that make multiple models and then combine them to 
produce a single improved result. This method usually produces more precise results 
than a single model would. This has been the case in machine learning competitions, 
where the triumphant models used ensemble techniques [103]. Studies have confirmed 
that ensemble missing data handling algorithms outperform single base machine learn -
ing algorithms [104–108]. Also, ensemble methods can be implemented in parallel com -
puting environments, which are necessary to process missing data in big datasets. These 
ensemble algorithms are a group of techniques that their decisions are combined in a 

Document 3: 
 Built upon automatic algorithm conﬁguration, the automatic
construction of parallel algorithm portfolios (PAPs) [11]–[15]
seeks to identify a set of conﬁgurations to form a PAP. Each
conﬁguration in the PAP is called a component solver. To
solve a problem instance, all the component solvers are run
independently, typically in parallel, to get multiple solutions.
Then, the best solution will be taken as the output of the PAP.
Although a PAP would consume much more computational
resources than a single-conﬁguration solver, it has two im-
portant advantages. First, the performance of a PAP on any
given instance is the best performance achieved among its
component solvers on the instance. In other words, by ex-
ploiting the complementarity between the component solvers,
a PAP could achieve a much stronger overall performance than
any of its component solver. Second, considering the great
development of parallel computing architectures [16] (e.g.,
multi-core CPUs) over the last decade, exploiting parallelism
has become very important in designing efﬁcient solvers for
computationally hard problems. PAPs employ parallel solution
strategies, and thus allow exploiting modern high-performance
computing facilities in an extremely simple way.
From the practical point of view, a PAP construction method
is expected to identify a PAP that generalizes well, i.e.,
performs well not only on the instance set used during the 

Document 4: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 limited time budget. Although numerous algorithms are readily
applicable for this type of problems, their performance usually
varies signiﬁcantly from problem to problem. This implies that
there is an inherent risk associated with the selection of an
algorithm. Unfortunately, identifying a suitable (or optimal)
algorithm for a speciﬁc problem is a nontrivial task due to the
lack of prior knowledge. The limited time budget also prohibits
us from trying out different algorithms and then choosing the
best one. Instead of betting the entire time budget on a single
algorithm, we proposed that such a risk can be reduced by
distributing the time budget to multiple algorithms. Based on
this idea, a general framework called PAP has been proposed
in the context of population-based search algorithms. PAP
typically consists of a number of constituent algorithms, each
of which is allowed to run with a portion of the time budget.
Allocation of computation time is implemented by dividing the
whole population into a number of subpopulations, and main-
taining one for each constituent algorithm. To further boost
the performance, interaction among constituent algorithms is
carried out through regularly migrating individuals among the
subpopulations. We proposed a pairwise metric to compare
the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget. 

","The CLUSTERING method for constructing PAPs involves instance grouping, where they first split the training instances into groups. Then, in each iteration, it identifies a single component solver that maximizes marginal performance contribution to the current PAP among these grouped instances. This approach uses instance-based clustering techniques to select and insert algorithms into the PAP iteratively.",2.2603116035461426,"The PAP approach called ""CLUSTERING"" is based on instance grouping. It involves splitting the training dataset into distinct groups or clusters before constructing the algorithm portfolio. This method focuses on organizing instances in a way that allows for more tailored and effective algorithm configurations, as it takes into account the similarities among the instances. By grouping similar instances together, the CLUSTERING approach aids in identifying which algorithms perform better on specific types of problems, thereby improving the overall performance of the PAP."
qwen2.5:3b,"What is the idea behind the PAP approach called ""PCIT""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 them.
PCIT conducts the instance transfer with the help of in-
cumbent configurations (i.e., the best configurations found
by the AC procedure). In each subset, the instances which
cannot be solved satisfactorily by the corresponding incum-
bent are identified as the ones that need to be transferred,
and the target subset of each transferred instance is deter-
mined according to how well the incumbent on the candi-
date subset could perform on the instance. In essence, the
incumbent on a subset can be seen as a common special
characteristic of those “similar” instances (in the sense they
share the same high-quality configurations) within the sub-
set, and PCIT uses it to identify those “dissimilar” instances
and find better subsets for them. In each subset, the perfor-
mance of the incumbent on each instance could be obtained
from the rundata collected from the AC procedure runs.
However, while determining the target subsets for the trans-
ferred instances, how well the incumbents on the candidate
subsets would perform on the transferred instances are un-
known. One way to obtain these performances is to actually
test these incumbents on the transferred instances, which
however would introduce considerable additional compu-
tational costs. To avoid this, PCIT builds empirical perfor-
mance models (EPM) (Hutter et al. 2014) based on the col-
lected rundata to predict these performances.
1562
Algorithm Framework 

Document 3: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 4: 
 of the more state-of-the-art Plingeling. Such results indicate
PCIT could identify powerful parallel portfolios, with little
human effort involved. Thus the portfolios constructed by
PCIT could conveniently provide at least two advantages.
That is, they are high-quality parallel solvers, and they could
be used as starting points for the development of more ad-
Table 4: Test results of parallel solvers on the test set of SAT-
Single and SAT-Multi. The performance of a solver is shown
in boldface if it was not significantly different from the best
performance (according to a permutation test with 100000
permutations and significance level p = 0.05).
SAT-Single SAT-Multi
#TOS PAR-10 PAR-1 #TOS PAR-10 PAR-1
PCIT 181 119 21 35 1164 219
Priss6 225 146 25 - - -
PfolioUZK - - - 36 1185 213
Plinegling-bbc452 276 32 33 1090 199
vanced parallel solvers.
Conclusion
In this paper we proposed a novel ACPP method, named
PCIT, which utilized an instance transfer mechanism to im-
prove the quality of the instance grouping. The experimen-
tal results on two widely studied problem domains, SAT and
TSP, have demonstrated the effectiveness of PCIT. Currently
PCIT relies on the instance features to build the EPM. Since 

Document 5: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","It appears there might be some confusion in your question. The term ""PAP"" doesn't directly relate to PCIT (Parent-Child Interaction Therapy). Let me clarify:

1. **PAP** usually stands for **Patient and Provider Approach**, which is more commonly used in healthcare contexts, particularly within evidence-based practice models like the PICO (Population/Intervention/Comparison/Outcome) approach.

2. **PCIT** stands for **Parent-Child Interaction Therapy**, which is an intensive behavioral treatment primarily designed to help children with attention deficit hyperactivity disorder (ADHD), Oppositional Defiant Disorder (ODD), and Conduct Disorder by improving parents' skills in managing their child's challenging behaviors.

If you were referring to PCIT, the idea behind this approach involves:

- **Parent Coaching**: The therapist works closely with parents on techniques to improve their interactions and manage difficult behavior of children.
  
- **Interactive Play Sessions**: Parents practice new skills during play sessions with their child, which helps reinforce these behaviors at home.

- **Behavioral Reinforcement**: Encourages positive reinforcement strategies to increase good behavior and reduce negative ones.

If you were actually asking about another concept related to PAP or PCIT, please provide more context so I can give a more accurate response.",4.086761474609375,"The PAP approach known as ""PCIT"" (Performance-based Instance Transfer) leverages an instance transfer mechanism aimed at enhancing the quality of instance grouping. It identifies instances that are poorly solved by existing configurations (the incumbents) and transfers these instances to more suitable subsets based on performance predictions. This is achieved by utilizing empirical performance models (EPM) to predict how well incumbents would perform on these transferred instances without the need for extensive additional computational costs. The effectiveness of PCIT has been demonstrated through experimental results on various problem domains, indicating that it can construct high-quality parallel solvers efficiently with minimal human intervention."
qwen2.5:3b,"What is the idea behind the PAP approach called ""CEPS""?
","Document 1: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

Document 2: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 3: 
 this approach no longer guarantees the validity of the above
assumption. As a consequence, Θ′may generalize worse than
Θ. A remedy to prevent this as much as possible is to increase
redundancy in the evolution of the PAP. More speciﬁcally, one
could repeat the conﬁguration-removal procedure to Θ for n
times, leading to ntemporary PAPs, ¯Θ1,..., ¯Θn; then for each
temporary PAP ¯Θ, the new conﬁgurations are identiﬁed and
inserted, leading to n new PAPs, Θ′
1,..., Θ′
n, each of which
is of the same size as Θ; ﬁnally, the PAP among them that
performs best against T ∪T′ is retained.
III. C O-EVOLUTION OF PARAMETERIZED SEARCH
By incorporating the above-described procedure into the co-
evolution process, we arrive at the proposed CEPS framework,
as demonstrated in Algorithm 1. In general, CEPS consists
of two major phases, i.e., an initialization phase (lines 2-
7), and a co-evolution phase (lines 8-27) which could be
further subdivided into alternating between the evolution of
the conﬁguration population (representing the PAP) (lines 10-
15) and the evolution of the instance population (representing
the training instances) (lines 17-26) for MaxIte iterations in
total. These modules are detailed as follows. 

Document 4: 
 contributions of this paper include:
1) A novel PAP construction framework, namely Co-
Evolution of Parameterized Search (CEPS), is proposed.
It is also shown that CEPS approximates a process that
minimizes the upper bound, i.e., a tractable surrogate,
of the generalization performance.
2) To demonstrate the implementation details of CEPS as
well as to assess its potential, concrete instantiations are
also presented for two hard optimization problems, i.e.,
TSP and the Vehicle Routing Problem with Simultane-
ous Pickup–Delivery and Time Windows (VRPSPDTW)
[18]. Computational studies conﬁrm that CEPS is able
to obtain PAPs with better generalization performance.
3) The proposal of CEPS extends the realm of Co-
Evolution, for the ﬁrst time, to evolving algorithm
conﬁgurations and problem instances. Since CEPS does
not invoke domain-speciﬁc knowledge, its potential ap-
plications can go beyond optimization problems, even to
planning and learning problems.
The rest of the paper is organized as follows. Section II in-
troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 cores each (2.20 GHz, 30 MB Cache), running Centos 7.5.
D. Results and Analysis
We report the #TOs, PAR-10 and PANC achieved by the
PAPs on the testing set in Table II and also visualize their
medians and variance across all the testing instances by box-
plots in Figure 1. Note the mean value is also plotted in Figure
1 (indicated by “ ▲”) to show that for a PAP how its PAR-
10/PANC is affected by the outliers (the timeout cases) which
would be hidden by boxplots. In Table II the #TOs, PAR-
10/PANC of a PAP is highlighted in grey if it achieved the best
performance. One could make three important observations
from these results. First, the PAPs obtained by CEPS have
the smallest number of timeouts in all the six experiments,
which means they have the highest success rate for solving the
testing instances among all the tested PAPs. Recall that CEPS
actively searches in the instance space to identify the hard-
to-solve instances for further improving the generalization of
the PAPs. Such a mechanism makes CEPS the method that is
least affected by the hard testing instances which signiﬁcantly
differs from the given training instances. This could be further
veriﬁed by Figure 1, in which CEPS is the method that has
the least gap between the mean value (which takes timeouts
into account) and median value (which naturally ﬁlters out the 

","The term ""CEPS"" in relation to the Policy Analysis for Change (PAP) approach does not have a standard or widely recognized meaning. It's possible there might be some confusion with specific projects, initiatives, or variations of PAP that use certain acronyms. Without more context about which project or initiative you're referring to, it's challenging to provide an accurate explanation.

The Policy Analysis for Change (PAP) approach is generally known as a systematic and analytical way of evaluating policies, programs, and strategies. It aims to enhance the effectiveness and efficiency of public policy by identifying problems, analyzing options, and recommending solutions. The process typically involves collecting and analyzing data, gathering expert opinions, and considering various perspectives.

If you could provide more details about which specific project or initiative under PAP uses ""CEPS,"" I'd be better equipped to give a relevant explanation. Otherwise, for the standard PAP approach, it's focused on enhancing policy analysis through systematic evaluation methods rather than having specific acronyms like CEPS.",3.354555130004883,"The Co-Evolution of Parameterized Search (CEPS) is a framework designed for constructing Parameterized Algorithms (PAPs) that aims to improve their generalization performance across a set of problems rather than on a single one. The key idea behind CEPS is to co-evolve both the configurations of the algorithms and the problem instances, where the framework consists of two major phases: an initialization phase and a co-evolution phase. In this approach, the algorithm dynamically searches the instance space to identify challenging instances that can enhance the generalization capabilities of the obtained PAPs. CEPS utilizes historical data from the generated instances to assist in tuning parameters effectively, thereby enabling better performance even in a zero-shot learning setting. This allows CEPS to be applicable beyond traditional optimization problems, extending its potential to areas like planning and learning. Overall, CEPS seeks to minimize the upper bounds of generalization performance while reducing the risk across multiple problems."
qwen2.5:3b,"Please give me the names of some state-of-the-art SAT solvers.
","Document 1: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 2: 
 Intelligence (AAAI’02), (pp. 674–681). AAAI Press / The MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building lo-
cal search sat solvers from components. In Proceedings of the Twenty-ﬁrst International Joint Conference
on Artiﬁcial Intelligence (IJCAI’09), (pp. 517–524).
Le Berre, D. & Simon, L. (2004). Fifty-ﬁve solvers in Vancouver: The SAT 2004 competition. In Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory and Applications of Satisﬁability Testing: Proceedings of the Seventh
International Conference (SAT’04), volume 3542 of Lecture Notes in Computer Science , (pp. 321–344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y . (2002). Learning the empirical hardness of optimization
problems: The case of combinatorial auctions. In Hentenryck, P. V . (Ed.), Principles and Practice of
Constraint Programming – CP 2002: Eighth International Conference , volume 2470 of Lecture Notes in
Computer Science, (pp. 556–572). Springer Verlag, Berlin, Germany. 

Document 3: 
 212
state-of-the-art SLS algorithms. SATenstein-LS has
41 parameters that control the selection and behavior of
its components, leading to a total conﬁguration space of
size 4.82 ×1012. Automatically-identiﬁed conﬁgurations of
SATenstein-LS have been shown to achieve better per-
formance than existing state-of-the-art SLS algorithms on
several well-known instance distributions.
Instances
We investigated the effectiveness of Hydra on four distri-
butions, drawing on well-known families of SAT instances.
Because no state-of-the-art SLS algorithms are able to prove
unsatisﬁability, we considered only satisﬁable instances. We
identiﬁed these by running all complete algorithms that won
a SAT competition category between 2002 and 2007 for
one hour. First, the BM data set is constructed from 500
instances taken from each of the six distributions used by
KhudaBukhsh et al. (2009) (
QCP, SWGCP, FACT, CBMC,
R3FIX, and HGEN), split evenly into training and test sets.
Second, the INDU data set is a mixture of 500 instances
from each of the CBMC and FACT distributions, again split
evenly into training and test sets. Third and fourth, the HAND
and RAND data sets include all satisﬁable instances from the
Random and Handmade categories of the SAT Competitions 

Document 4: 
 form (a conjunction of disjunctions of variables and their negations) and determine a satisﬁable assignment or prove that 
none exists. An incomplete SAT solver can ﬁnd satisfying assignments, but not prove unsatisﬁability.
Most state-of-the-art complete SAT solvers are based on conﬂict-driven clause learning (CDCL; [57]). Their parameters 
control variable selection for branching decisions, clause learning and restart techniques. State-of-the-art incomplete SAT 
solvers use stochastic local search (SLS; [38]), and their parameters control the selection of the variable whose value is 
modiﬁed in each local search step as well as the diversiﬁcation and additional intensiﬁcation strategies. Furthermore, there 
exist several preprocessing techniques (e.g., [21]) to simplify formulas and their parameters control how long and how 
aggressive preprocessing will be used – too much preprocessing can remove important structural information and hence, 
it can increase the hardness of formulas. The eﬃcacy of SAT solvers depends on multiple heuristic components whose 
basic functions and the interplay between them are controlled by parameters. Some parameters are categorical (e.g., choice 
between different search strategies in SLS), while many others are integer- or real-valued (e.g., the damping factor used in 
computing heuristic variable scores in CDCL).
Parallel SAT solvers have received increasing attention in recent years. ManySAT [30,31,29] was one of the ﬁrst parallel 

Document 5: 
 tial solver baselines, indicating the great benefit by combin-
ing complementary configurations obtained from a rich con-
figuration space.
Comparison with Hand-designed Parallel Solvers
To further evaluate the portfolios constructed by PCIT, we
compared them with the state-of-the-art manually designed
parallel solvers. Specifically, we considered the ones con-
structed for SAT. We chose Priss6 (Manthey, Stephan, and
Werner 2016) to compare with the one constructed in SAT-
Single, since Priss6 is the official parallel version of Riss6
(the base solver in SAT-Single). For the same reason, we
chose PfolioUZK (Wotzlaw et al. 2012) (the gold medal
winning solver of the parallel track of the SAT’12 Chal-
lenge) to compare with the one constructed in SAT-Multi.
Finally, we chose Plingeling (version bbc) (Biere 2016),
the gold medal winning solver of the parallel track of the
SAT’16 Competition, to compare with both. Note that all
the manually designed solvers considered here have imple-
mented far more advanced parallel solving strategies (e.g.,
clause sharing) than only independently running component
solvers in parallel. In the experiments the default settings of
these solvers were used and the same statistical tests as be-
fore were conducted. As shown in Table 4, on SAT-Single 

Document 6: 
 ACM 52(10):56–67.
Balyo, T.; Heule, M. J. H.; and J¨arvisalo, M., eds. 2016. Proceed-
ings of SAT Competition 2016: Solver and Benchmark Descrip-
tions, volume B-2016-1 of Department of Computer Science Se-
ries of Publications B. University of Helsinki.
Battiti, R.; Brunato, M.; and Mascia, F., eds. 2008. Reactive
Search and Intelligent Optimization. Springer.
Biere, A. 2016. Splatz, Lingeling, Plingeling, Treengeling, Yal-
SAT Entering the SAT Competition 2016. In Balyo et al. (2016),
44–45.
Burke, E. K.; Gendreau, M.; Hyde, M.; Kendall, G.; Ochoa, G.;
¨Ozcan, E.; and Qu, R. 2013. Hyper-heuristics: A Survey of the
State of the Art. Journal of the Operational Research Society
64(12):1695–1724.
Gomes, C. P., and Selman, B. 2001. Algorithm Portfolios. Arti-
ficial Intelligence 126(1-2):43–62.
Hamadi, Y ., and Wintersteiger, C. M. 2013. Seven Challenges in
Parallel SAT Solving. AI Magazine 34(2):99–106. 

","Some state-of-the-art SAT solvers include Hydra, MiniSAT, Glucose, Maple, CryptoMiniSat, and XpressSAT among others. These are part of a rich landscape of solver implementations that have emerged in recent years to tackle the complexity and scalability challenges associated with solving large SAT instances efficiently.",2.387084484100342,"Some state-of-the-art SAT solvers mentioned in the context include Priss6, PfolioUZK, Plingeling, and SATenstein-LS. Other solvers are also referenced, such as Splatz, Lingeling, and Treengeling."
qwen2.5:3b,"Please give me the names of some state-of-the-art TSP solvers.
","Document 1: 
 For surveys of the problem and its applications, the reader is referred to the excellent
volumes edited by Lawler et al. [24] and Gutin and Punnen [11].
Local search with k-exchange neighborhoods, k-opt, is the most widely used heu-
ristic method for the TSP. k-opt is a tour improvement algorithm, where in each step
k links of the current tour are replaced by k links in such a way that a shorter tour is
achieved.
It has been shown [7] that k-opt may take an exponential number of iterations and
that the ratio of the length of an optimal tour to the length of a tour constructed byk-opt
can be arbitrarily large when k ≤ n/2 − 5. Such undesirable cases, however, are very
rare when solving practical instances [31]. Usually, high-quality solutions are obtained
in polynomial time. This is, for example, the case for the Lin–Kernighan heuristic,
one of the most effective methods for generating optimal or near-optimal solutions
for the symmetric TSP. High-quality solutions are often obtained, even though only a
small part of the k-exchange neighborhood is searched.
In the original version of the Lin–Kernighan heuristic [ 25], the allowable
k-exchanges (or k-opt moves) are restricted to those that can be decomposed into a
2- or 3-exchange followed by a (possibly empty) sequence of 2-exchanges. This restric- 

Document 2: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 3: 
 10.0 
12.5 
15.0 
17.5 PAR-10 (c) TSP-3
Fig. 2: Visual comparison in boxplots of the medians and variance of the test performance of each TSP solver across the
testing instances. Note the mean value is also plotted, indicated by “ ▲”.
TABLE III: Comparison of the state-of-the-art TSP solvers
with the PAPs obtained by CEPS, on the testing set. #TOs
refers to number of total timeouts. PAR-10 is the penalized
average runtime-10. Performance of a solver is highlighted in
grey if it achieved the best testing performance.
TSP-1 TSP-2 TSP-3
#TOs PAR-10 #TOs PAR-10 #TOs PAR-10
LKH-default 131 30.84 137 31.98 150 34.73
LKH-tuned 29 8.23 34 9.40 27 7.67
EAX-default 69 17.98 73 18.91 69 17.95
EAX-tuned 33 10.97 30 10.38 29 10.12
VBS 6 3.82 7 4.26 6 4.13
CEPS 6 2.74 4 2.15 2 1.94
signiﬁcant performance improvement, though still obviously
falling behind of the PAPs obtained by CEPS. Second, the only
solver that could match the PAP’s performance level in one
of the three scenarios, is the VBS of the algorithm portfolio
considered by the algorithm selection approaches [29], [30].
However, in TSP-2 and TSP-3, the performance advantage of 

Document 4: 
 troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in
Section VI. Section VII concludes the paper with discussions.
II. P ARAMETERIZED SOLVERS MADE GENERALIZABLE
A. Notations and Problem Description
Assume a PAP is to be built for a problem class (e.g., TSP),
for which an instance of the problem class is denoted as s, and
the set of all possible sis denoted as Ω. Given a parameterized
algorithm, each component solver of the PAP is a conﬁguration
(full instantiation) of the algorithm. Generally speaking, the
parameterized algorithm can be any concrete computational
process, e.g., a traditional heuristic search process such as the
LK Heuristic for TSP or even a neural network [19]–[21] that
outputs a solution for a given instance of the target problem
class. Let θ denote a conﬁguration and let Θ denote a PAP
that contains K different conﬁgurations (component solvers),
i.e., Θ = {θ1,...,θ K}. The quality of a conﬁguration θ on a 

Document 5: 
 H. Trautmann, “Evolving diverse TSP instances by means of novel and
creative mutation operators,” in Proceedings of the 15th ACM/SIGEVO
Conference on Foundations of Genetic Algorithms, FOGA’2019. Pots-
dam, Germany: ACM, Aug 2019, pp. 58–71.
[38] Y . Nagata and S. Kobayashi, “A powerful genetic algorithm using edge
assembly crossover for the traveling salesman problem,” INFORMS
Journal on Computing , vol. 25, no. 2, pp. 346–363, 2013.
[39] X. Xie and J. Liu, “Multiagent optimization system for solving the
traveling salesman problem (tsp),” IEEE Transactions on Systems, Man,
and Cybernetics, Part B , vol. 39, no. 2, pp. 489–502, 2009.
[40] C. Wang, D. Mu, F. Zhao, and J. W. Sutherland, “A parallel simulated
annealing method for the vehicle routing problem with simultaneous
pickup-delivery and time windows,” Computers & Industrial Engineer-
ing, vol. 83, pp. 111–122, 2015.
[41] W. Huang and T. Zhang, “Vehicle routing problem with simultaneous
pick-up and delivery and time-windows based on improved global arti-
ﬁcial ﬁsh swarm algorithm,” Computer Engineering and Applications ,
vol. 52, no. 21, pp. 21–29, 2016. 

Document 6: 
 Portfolio-based algorithm selection for SAT,” Journal of Artiﬁcial In-
telligence Research, vol. 32, pp. 565–606, 2008.
[29] P. Kerschke, L. Kotthoff, J. Bossek, H. H. Hoos, and H. Trautmann,
“Leveraging TSP solver complementarity through machine learning,”
Evolutionary Computation, vol. 26, no. 4, pp. 597–620, 2018.
[30] K. Zhao, S. Liu, Y . Rong, and J. X. Yu, “Leveraging TSP solver
complementarity via deep learning,” arXiv preprint arXiv:2006.00715 ,
2020.
[31] L. Kotthoff, “Algorithm selection for combinatorial search problems: A
survey,” AI Magazine, vol. 35, no. 3, pp. 48–60, 2014.
[32] C. D. Rosin and R. K. Belew, “New methods for competitive coevolu-
tion,” Evolutionary Computation, vol. 5, no. 1, pp. 1–29, 1997.
[33] J. I. van Hemert, “Evolving combinatorial problem instances that are
difﬁcult to solve,” Evolutionary Computation , vol. 14, no. 4, pp. 433–
462, 2006.
[34] K. Helsgaun, “General k-opt submoves for the Lin-Kernighan TSP 

","There are several state-of-the-art algorithms and software for solving Traveling Salesman Problem (TSP). Here are some notable ones:

1. **Concorde**: This is one of the most famous exact TSP solvers, developed by Travis Kimes and others at AT&T Labs. It uses a combination of branch-and-price and local search methods.

2. **Chuffed/SAT**: While primarily a SAT solver, Chuffed can also be used to solve TSP problems effectively through heuristic-based approaches.

3. **VRP Solver (VROOM)**: This is specifically designed for Vehicle Routing Problems (VRPs), which is related but often solved using similar techniques as the TSP. VROOM uses integer programming and constraint satisfaction methods.

4. **LibTSP**: A library of functions to solve Travelling Salesman Problem instances, especially useful in research contexts due to its flexibility and ability to work with large datasets.

5. **CPLEX or Gurobi**: These are optimization software that can be used for TSP problems as mixed-integer programming (MIP) models. They offer advanced heuristics and branch-and-cut methods.

6. **ANTSYSV**: This solver uses Ant Colony Optimization, which is a metaheuristic inspired by the foraging behavior of ants.

7. **JULIA TSP Solver**: A specialized solver written in Julia that can handle large instances effectively using various heuristic approaches including genetic algorithms, tabu search, and local search.

8. **TSPLIB95**: An open-source library containing many benchmark problems and solvers (including some commercial ones), which is often used for evaluating new methods or comparing existing ones.

Each of these tools has its strengths and can be chosen based on the specific requirements of the TSP instance at hand, including size, complexity, and desired solution quality.",5.824650526046753,"Some state-of-the-art TSP solvers mentioned in the context are LKH (Lin-Kernighan heuristic), EAX (Edge Assembly Crossover), and VBS (Variable Neighborhood Search)."
qwen2.5:3b,"Please give me the names of some state-of-the-art Automatic Algorithm Configuration approaches.
","Document 1: 
 Journal of Artiﬁcial Intelligence Research 75 (2022) 425-487 Submitted 02/2022; published 10/2022
A Survey of Methods for Automated
Algorithm Conﬁguration
Elias Schede elias.schede@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Jasmin Brandt jasmin.brandt@upb.de
Alexander Tornede alexander.tornede@upb.de
Department of Computer Science,
Paderborn University, Paderborn, Germany
Marcel Wever marcel.wever@ifi.lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Viktor Bengs viktor.bengs@ifi.lmu.de
Institute of Informatics,
LMU Munich, Munich, Germany
Eyke H¨ ullermeier eyke@lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Kevin Tierney kevin.tierney@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews 

Document 2: 
 2 Related Work
2.1 Automatic Algorithm Conﬁguration
Several approaches exist in the literature for the automatic tuning
of algorithms. Some of these were created for a speciﬁc algorithm
or task. For example, [24] devises a modular algorithm for solving
constraint satisfaction problems (CSPs). Using a combination of ex-
haustive enumeration of all possible conﬁgurations and parallel hill-
climbing, the technique automatically conﬁgures the system for a
given set of training instances. Another approach, presented in [30],
focuses on the conﬁguration of adaptive algorithms, employing a se-
quential parameter optimization approach.
Other approaches automatically design and build an entire solver
to best tackle a set of example training instances. For example, [28]
uses genetic programming to create an evolutionary algorithm (EA).
Here the chromosome is an EA operation like the selection of par-
ents, mutation, or crossover, and the task is to ﬁnd a sequence of
the genetic programming operators that is best suited for the speci-
ﬁed problem. For SAT, [8] classiﬁes local search (LS) approaches by
means of context-free grammars. This approach then uses a genetic
programming approach to select a good LS algorithm for a given set
of instances.
There also exist approaches that are applicable to more general al-
gorithms. For example, in order to tune continuous parameters, [5]
suggests an approach that determines good parameters for individual
training instances. This approach ﬁrst evaluates the extreme param- 

Document 3: 
 470
A Survey of Methods for Automated Algorithm Configuration
General AC systems
D-SMAC https://github.com/tqichun/distributed-SMAC3
GPS https://github.com/YashaPushak/GPS
irace https://github.com/MLopez-Ibanez/irace
OAT (GGA) https://docs.optano.com/algorithm.tuner/current/
ParamILS https://www.cs.ubc.ca/labs/algorithms/Projects/ParamILS/
PyDGGA http://ulog.udl.cat/software/
REVAC https://github.com/ChrisTimperley/RubyREVAC
SMAC 3 https://github.com/automl/SMAC3
Benchmarks
AClib https://bitbucket.org/mlindauer/aclib2/src/master/
DAC https://github.com/automl/DAC
Table 7: List of available software in the realm of AC.
References
Belarmino Adenso-Diaz & Manuel Laguna. Fine-tuning of algorithms using fractional ex-
perimental designs and local search. Operations research, 54(1):99–114, 2006.
¨Ozg¨ ur Akg¨ un, Nguyen Dang, Ian Miguel, Andr´ as Z Salamon, & Christopher Stone. In- 

Document 4: 
 cal (e.g., choice of heuristic). Categorical parameters can be used to select and combine discrete
building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics); consequently,
our general view of algorithm conﬁguration includes the automated construction of a heuristic algo-
rithm from such building blocks. To the best of our knowledge, the methods discussed in this article
are yet the only general ones available for the conﬁguration of algorithms with many categorical
parameters.
We now give an overview of what follows and highlight our main contributions. After for-
mally stating the algorithm conﬁguration problem in Section 2, in Section 3 we describe ParamILS
(ﬁrst introduced by Hutter, Hoos & St ¨utzle, 2007), a versatile stochastic local search approach for
automated algorithm conﬁguration, and two of its instantiations, BasicILS and FocusedILS.
We then introduce adaptive capping of algorithm runs, a novel technique that can be used to
enhance search-based algorithm conﬁguration procedures independently of the underlying search
strategy (Section 4). Adaptive capping is based on the idea of avoiding unnecessary runs of the
algorithm to be conﬁgured by developing bounds on the performance measure to be optimized.
We present a trajectory-preserving variant and a heuristic extension of this technique. After dis-
cussing experimental preliminaries in Section 5, in Section 6 we present empirical evidence show-
ing that adaptive capping speeds up both BasicILS and FocusedILS. We also show that BasicILS
268 

Document 5: 
 rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 
Due to the separation between a tuning and a production phase, 
automatic algorithm conﬁguration is also known as oﬄine param- 
eter tuning to differentiate it from online approaches that adapt or 
control parameter settings while solving an instance [13,50] . Nev- 
ertheless, online approaches also contain parameters that need to 
be deﬁned oﬄine, for example, which and how parameters are 
adapted at run-time; such parameters and design choices can be 
conﬁgured by an oﬄine tuning method [59] . 
In our research on making the algorithm conﬁguration process 
more automatic , we have focused on racing approaches. Birattari 
et al. [19,20] proposed an automatic conﬁguration approach, F- 
Race, based on racing [64] and Friedman’s non-parametric two-way 
analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 

Document 6: 
 formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de-
veloped and used for more than a decade, and many of the fundamental techniques
date back even further. However, it has only recently become possible to effectively
solve complex conﬁguration problems involving target algorithms with dozens of
parameters, which are often categorical and conditional. This success is based in
3 Automated Algorithm Conﬁguration and Parameter Tuning 65
part on the increased availability of computational resources, but has mostly been
enabled by methodological advances underlying recent conﬁguration procedures.
Still, we see much room (and, indeed, need) for future work on automated algo-
rithm conﬁguration and parameter tuning methods. We believe that in developing
such methods, the fundamental features underlying all three types of methods dis-
cussed in this chapter can play an important role, and that the best methods will
employ combinations of these. We further believe that different conﬁguration pro-
cedures will likely be most effective for solving different types of conﬁguration
problems (depending, in particular, on the number and type of target algorithm pa-
rameters, but also on regularities in the parameter response). Therefore, we see a
need for research aiming to determine which conﬁgurator is most effective under 

","Some state-of-the-art approaches in Automatic Algorithm Configuration mentioned in the text include:

- ParamILS (first introduced by Hutter, Hoos & Stützle, 2007)
- BasicILS and FocusedILS, which are instantiations of ParamILS
- Adaptive capping of algorithm runs, a novel technique for enhancing search-based configuration procedures.

Additionally, the text mentions Iterated F-race (I/F-Race) as an approach based on racing and Friedman's non-parametric two-way analysis of variance by ranks.",3.1615681648254395,"Some state-of-the-art Automatic Algorithm Configuration approaches include D-SMAC, GPS, irace, OAT (GGA), ParamILS, PyDGGA, REVAC, and SMAC 3."
qwen2.5:3b,"What is the idea behind the AAC approach called ""irace""?
","Document 1: 
 analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 
the I/F-Race procedure is given in those publications, an imple- 
mentation was not made publicly available. The irace package im- 
plements a general iterated racing procedure, which includes I/F- 
Race as a special case. It also implements several extensions al- 
ready described by Birattari [19] , such as the use of the paired 
t -test instead of Friedman’s test. Finally, irace incorporates sev- 
eral improvements not published before, such as sampling from a 
truncated normal distribution, a parallel implementation, a restart 
strategy that avoids premature convergence, and an elitist racing 
procedure to ensure that the best parameter conﬁgurations found 
are also evaluated on the highest number of training instances. 
The paper is structured as follows. Section 2 introduces the 
algorithm conﬁguration problem and gives an overview of ap- 
proaches to automatic algorithm conﬁguration. Section 3 describes 
the iterated racing procedure as implemented in the irace pack- 
age as well as several further extensions including the elitist irace . 
Section 4 illustrates the steps followed to apply irace to two con- 

Document 2: 
 5.2. Multi-objective optimization metaheuristics 
Besides the application to the MOACO framework described 
above [58] , irace has been applied to aid in the design of other 
multi-objective optimization algorithms. Dubois-Lacoste et al. 
[31] used irace to tune a hybrid of two-phase local search and 
Pareto local search (TP + PLS) to produce new state-of-the-art al- 
gorithms for various bi-objective permutation ﬂowshop problems. 
Fisset et al. [33] used irace to tune a framework of multi-objective 
optimization algorithms for clustering. When applied to a suﬃ- 
ciently ﬂexible algorithmic framework, irace has been used to de- 
sign new state-of-the-art multi-objective evolutionary algorithms 
[16,17] . 
5.3. Anytime algorithms (improve time-quality trade-offs) 
There is often a trade-off between solution quality and com- 
putation time: Algorithms that converge quickly tend to produce 
better solutions for shorter runtimes, whereas more exploratory al- 
gorithms tend to produce better solutions for longer runtimes. Im- 
proving the anytime behavior of an algorithm amounts to improv- 
ing the trade-off curve between solution quality and computation 
time such that an algorithm is able to produce as high quality so- 
lutions as possible at any moment during their execution. López- 

Document 3: 
 effect on the budget consumed is lower. The same experiment for 
the ACOTSP scenario showed that the best conﬁgurations become 
worse when T new or T ﬁrst are increased. This is due to the fact that 
ACOTSP has a homogeneous training set and, therefore, sampling 
new candidates is more important than executing a large number 
of instances. 
5. Other applications of irace 
Since the ﬁrst version of the irace package became publicly 
available in 2012, there have been many other applications of 
irace . In this section, we provide a list of the applications of the 
irace package of which we are aware at the time of writing. Some 
of these applications go beyond what is traditionally understood as 
algorithm conﬁguration, demonstrating the ﬂexibility of irace . 
54 M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 
5.1. Algorithm conﬁguration 
The traditional application of irace is the automatic conﬁgura- 
tion of algorithms. Several publications have used irace when eval- 
uating or designing algorithms for problems such as the traveling 
salesman with time windows [60] , slot allocation [77] , generalized 
hub location [68] , ﬂow shop [14] , virtual machine placement [86] , 
online bin packing [92] , graph coloring [23] , image binarization 
[69] , network pricing [90] , combined routing and packing prob- 

Document 4: 
 ming by optimization paradigm [40] . Moreover, the importance of 
properly tuning the parameters of algorithms before analyzing and 
comparing them is becoming widely recognized. We hope that the 
development of the irace package will help practitioners and re- 
searchers to put these ideas into practice. 
Acknowledgements 
This work received support from the COMEX project (P7/36) 
within the Interuniversity Attraction Poles Programme of the Bel- 
gian Science Policy Oﬃce, and the EU FP7 ICT Project COLOMBO, 
Fig. A.1 1. Scheme of the user-provided components required by irace . 
Table A1 
Parameters of irace corresponding to the description 
of iterated racing given in Section 3.2 . The full list of 
irace parameters is available in the user guide. 
Iterated racing parameter irace parameter 
B maxExperiments 
C (cost measure) targetRunner 
μ mu 
N min minNbSurvival 
T ﬁrst firstTest 
T each eachTest 
Statistical test testType 
Cooperative Self-Organizing System for Low Carbon Mobility at 
Low Penetration Rates (agreement no. 318622). Thomas Stützle and 
Mauro Birattari acknowledge support from the Belgian F.R.S.-FNRS, 
of which they are Senior Research Associates. Leslie Pérez Cáceres 
acknowledges support of CONICYT Becas Chile. 
Appendix A. The irace package 

Document 5: 
 man problem with time windows: adapting algorithms from travel-time to 
makespan optimization. Appl Soft Comput 2013;13(9):3806–15 . 
[61] López-Ibáñez M., Dubois-Lacoste J., Pérez Cáceres L., Stützle T., Birattari M.. 
2016a. The irace package: Iterated racing for automatic algorithm conﬁgura- 
tion. http://iridia.ulb.ac.be/supp/IridiaSupp2016-003/ . 
[62] López-Ibáñez M, Pérez Cáceres L, Dubois-Lacoste J, Stützle T, Birattari M. The 
irace package: user guide. Technical Report TR/IRIDIA/2016-004. IRIDIA, Uni- 
versité Libre de Bruxelles, Belgium; 2016b . http://iridia.ulb.ac.be/IridiaTrSeries/ 
IridiaTr2016-004.pdf . 
[63] Marmion M-E , Mascia F , López-Ibáñez M , Stützle T . Automatic design of hy- 
brid stochastic local search algorithms. In: Blesa MJ, Blum C, Festa P, Roli A, 
Sampels M, editors. Hybrid metaheuristics, volume 7919 of lecture notes in 

Document 6: 
 ent robot behaviors and the criteria to transition between behav- 
iors. Each module can be further customized by means of several 
parameters. A particular combination of behaviors and transitions 
represents one controller, that is, an instance of the software that 
controls the robots in the swarm. The performance of a particu- 
lar controller is evaluated by means of multiple simulations. The 
search for the best controller over multiple training simulations is 
carried out by means of irace . The authors report that this system 
is not only able to outperform a previous system that used F-race 
[34] , but also a human designer, under the scenarios studied by 
them. 
6. Conclusion 
This paper presented the irace package, which implements the 
iterated racing procedure for automatic algorithm conﬁguration. It- 
erated racing is a generalization of the iterated F-race procedure. 
M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 55 
The primary purpose of irace is to automatize the arduous task of 
conﬁguring the parameters of an optimization algorithm. However, 
it may also be used for determining good settings in other compu- 
tational systems such as robotics, traﬃc light controllers, compil- 
ers, etc. The irace package has been designed with simplicity and 
ease of use in mind. Despite being implemented in R , no previ- 
ous knowledge of R is required. We included two examples for the 

","The idea behind the Automatic Algorithm Configuration (AAC) approach called ""irace"" involves using an iterative racing procedure to automate the process of configuring optimization algorithm parameters efficiently. The primary purpose of irace is to automatize the task of configuring the parameters of an optimization algorithm, making it easier and more systematic than manual tuning. It can also be used to find good settings in other computational systems like robotics or traffic light controllers, among others.",2.419574499130249,"The ""irace"" package implements an automatic algorithm configuration approach known as iterated racing. The key idea behind irace is to automate the process of tuning the parameters of optimization algorithms to improve their performance. It uses a statistical procedure to compare different parameter configurations, iteratively refining and sampling from the parameter space to identify the most effective settings. Irace incorporates various techniques such as parallel implementation, restart strategies to avoid premature convergence, and the ability to use different statistical tests for evaluating configurations. It has proven to be flexible and effective across various applications, including multi-objective optimization and robotics."
qwen2.5:3b,"What is the idea behind the AAC approach called ""SMAC""?
","Document 1: 
 intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods
SMAC Sequential model-based optimization for algorithm conﬁguration (SMAC) (Hutter
et al., 2011; Lindauer et al., 2021) can be seen as one of the ﬁrst fully-ﬂedged model-
based AC approaches, as it features solutions for many of the limitations of the previously
discussed SMBO techniques. SMAC generalizes TB-SPO to perform conﬁguration over
multiple problem instances so that it can support categorical parameters and handle tight
time constraints.
To support multiple problem instances, SMAC adapts the intensiﬁcation strategy of
TB-SPO to iteratively evaluate conﬁgurations on randomly sampled combinations of seeds
and problem instances. When doing so, it ensures that conﬁgurations are compared only
based on a performance estimate computed on the same randomly sampled set of problem
instances. Furthermore, SMAC’s surrogate model can generalize across problem instances
by incorporating problem instance features. To this end, a surrogate model is learned on
the joint problem instance and conﬁguration space to predict the performance of a given
conﬁguration on a given problem instance.
As a means to deal with a mixture of categorical and numerical parameters, SMAC 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 algorithm configuration framework. JArtifIntellRes 2009; 36 : 
267–306. 
9. Lindauer M, Eggensperger K, Feurer M et al. SMAC3: a ver- 
satile Bayesian optimization package for hyperparameter opti- 
mization. JMachLearnRes 2022; 23 : 54. 
10. Santu SKK, Hassan MM, Smith MJ et al. Automl to date and 
beyond: challenges and opportunities. ACMComputSurv 2022; 
54 : 175. 
Page7of9
Downloaded from https://academic.oup.com/nsr/article/11/8/nwae132/7639406 by guest on 18 September 2024 

Document 5: 
 more benchmark instances and compare its performance to
the best conﬁguration observed so far.
SMAC is an anytime algorithm that interleaves the ex-
ploration of new conﬁgurations with additional runs of the
current best conﬁguration to yield both better and more con-
ﬁdent results over time. As all anytime algorithms, SMAC
improves performance over time. While SMAC provably
converges for ﬁnite conﬁguration spaces, it often only ﬁnds
close-to-optimal conﬁgurations for realistic time budgets
and challenging conﬁguration problems.
Automated Portfolio Construction & Hydra
Our method for constructing sequential portfolios is closely
related to Hydra (Xu, Hoos, and Leyton-Brown 2010),
which automatically constructs selection-based portfolios.
Given a parameterized algorithm framework with a space
of algorithmsΘ, Hydra starts with an empty set of can-
didate algorithms C= ∅and iteratively calls an algorithm
conﬁguration method to add algorithms θ ∈Θ that comple-
ment Cbest. In each iteration, it constructs a portfolio from
the current set Cusing the portfolio-based algorithm selec-
tor SATzilla (Xu et al. 2008) and measures its performance
m(C,π) on each benchmark instance π ∈Π (in the ﬁrst it-
eration, m(∅,π) = −∞). The performance metric Hydra
maximizes via its algorithm conﬁguration method is then 

Document 6: 
 Anastacio and Hoos (2020) propose SMAC+PS, which integrates the idea of probabilis-
tic sampling known from irace into SMAC. This enhancement yields improvements over
both SMAC and irace in many cases. In particular, Anastacio and Hoos (2020) account for
the problem that many of the completely randomly sampled conﬁgurations by SMAC often
exhibit rather bad performance and thus, their evaluation yields only limited information.
To this end, the authors suggest to sample conﬁgurations according to a truncated normal
distribution centered around the default conﬁguration.
444
A Survey of Methods for Automated Algorithm Configuration
In (Lindauer & Hutter, 2018b) the authors suggest two diﬀerent strategies to warmstart
model-based AC approaches and apply their suggestions to SMAC, leading to signiﬁcant
speedups from days to hours of conﬁguration time. The idea underlying warmstarting is to
use the evaluations of conﬁgurations from previous runs, i.e., on diﬀerent problem instance
sets, to speed up the conﬁguration process in new runs of the conﬁgurator on a new set of
instances.
Distributed SMAC (Hutter et al., 2012) (D-SMAC) is an extension of SMAC leveraging
parallelization to speed up the conﬁguration process. The main idea behind D-SMAC is
to parallelize target algorithm runs onto available workers as much as possible. For this
purpose, it maintains a queue of target algorithm conﬁguration evaluations to be performed, 

","SMAC (Sequential Model-based Algorithm Configuration) is an anytime algorithm that interleaves exploration of new configurations with additional runs of the current best configuration. It aims to yield both better and more confident results over time, though it often only finds close-to-optimal configurations for realistic time budgets and challenging configuration problems. SMAC uses a model-based approach where it iteratively evaluates configurations on randomly sampled combinations of seeds and problem instances, ensuring comparisons are based on the same set of problem instances used to compute performance estimates.",2.607760429382324,"The idea behind the AAC (automatic algorithm configuration) approach called SMAC (Sequential Model-based Optimization for Algorithm Configuration) is to optimize the performance of algorithms by systematically searching for optimal hyperparameter configurations. SMAC achieves this by using a surrogate model that predicts the performance of different configurations based on previously evaluated instances, allowing it to generalize across multiple problem instances. It interleaves the exploration of new configurations with additional runs of the current best configuration, ensuring iterative improvement over time. SMAC also accommodates a mixture of categorical and numerical parameters, making it versatile for a variety of algorithmic applications. Additionally, enhancements like SMAC+PS and D-SMAC have been proposed to improve its performance and speed through more efficient sampling and parallelization strategies."
qwen2.5:3b,"What is the idea behind the AAC approach called ""GGA""?
","Document 1: 
 to be evaluated based on k diﬀerently parameterized instantiations of the UCB criterion.
GGA++ Ans´ otegui et al. (2015) adapt the model-free AC approach GGA to include a
surrogate model. More precisely, the authors use a surrogate model to evaluate the quality
of new conﬁgurations. They integrate this within a crossover operator and call it genetic
engineering. Recall that GGA contains both a competitive and non-competitive popula-
tion in which winning conﬁgurations from the races between members of the competitive
population are recombined with individuals from the non-competitive population. To this
end, the crossover operator generates individuals according to the parameter tree crossover
of the original GGA method and evaluates them using the surrogate. Note that rather
than predicting the solution quality or runtime directly, the surrogate predicts the rank the
individual would have in a tournament. The individuals with the best ranks are accepted
into the population of the next generation in the same way as in GGA.
While the GGA++ surrogate is based on a random forest model, it diﬀers in a key
way. The premise of a random forest is to equally approximate the underlying function over
the complete input space. In the case of AC, this is undesirable as only the areas of the
input space that correspond to high-quality conﬁgurations are of interest. Thus, the authors
present specialized splitting criteria that focuses on only the best conﬁgurations to increase 

Document 2: 
 Our approach uses a similar insight as Bansal and Viswanathan (1993) and Chen and Ludvigson
(2009), who propose using a given set of conditional GMM equations to estimate the SDF with
neural networks, but restrict themselves to a small number of conditioning variables. In order to
deal with the inﬁnite number of moment conditions we extend the classical GMM setup of Hansen
(1982) and Chamberlain (1987) by an adversarial network to select the optimal moment conditions.
A similar idea has been proposed by Lewis and Syrgkanis (2018) for non-parametric instrumental
variable regressions. Our problem is also similar in spirit to the Wasserstein GAN in Arjosvky,
Chintala, and Leon (2017) that provides a robust ﬁt to moments. The Generative Adversarial
Network (GAN) approach was ﬁrst proposed by Goodfellow et al. (2014) for image recognition.
In order to ﬁnd the hidden states in macroeconomic time series we propose the use of Recurrent
Neural Networks with Long-Short-Term-Memory (LSTM). LSTMs are designed to ﬁnd patterns
4We have shared our data and estimated models with Avramov, Cheng, and Metzker (2020). In their comparison
study Avramov, Cheng, and Metzker (2020) also include a portfolio derived from our GAN model. However, they do
not consider our SDF portfolio based on ω but use the SDF loadings β to construct a long-short portfolio based on 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 instead of the fully predetermined lifetimes in the original
GA VaPS and nAMGA VaPS.
Another variation of the GA VaPS lifetime scheme was
described by Bäck et al .[ 18]. The adaptive population GA
(APGA) uses the same lifetime allocation but differs from
GA VaPS in the reproduction cycle and in that, when in-
crementing the ages of individuals in each cycle, the best
individual of that cycle remains unchanged. This adaptive
population strategy is part of an ensemble and is described
in more detail in Section V. This method was also applied to
cooperative coevolution by Iorio and Li [ 109]. An analysis
of APGA by Lobo and Lima [ 135] shows theoretical and
experimental results suggesting an upper bound and a con-
verging population to a constant size that is determined by
the minimum and maximum lifetime parameters. The authors
conclude that the population is not adapted by the APGA
but the size parameter is in fact replaced by the two lifetime
parameters.
Cook and Tauritz [36] suggested two strategies for removing
the population size parameter. FiScIS-EA removes individuals
according to a survival probability, derived by linear scaling
of the individual’s ﬁtness in the range between the minimum
and maximum ﬁtness values present in the population. GC-EA
simply evades choosing a population size by maintaining a
population as large as is allowed by memory limitations. This
requires a well-chosen parent selection mechanism that scales
well, while a survival selection operator is still needed in 

Document 5: 
 3.4.5.2 Generative Adversarial Network Imputation
GANs consist of two parts— a generator and a discriminator
(Goodfellow et al., 2014 ). In an adversarial process, the
generator learns to generate samples that are as close as
possible to the data distribution, and the discriminator learns
to distinguish whether an example is true or generated.
Imputation approaches based on GANs include those in the
work ofYoon et al. (2018); Shang et al. (2017);a n dLi et al.
(2019). Here, we employ one of the most popular approaches of
GAN-based imputation, Genera tive Adversarial Imputation
Nets (GAIN) (Yoon et al., 2018). GAIN adapts the original
GAN architecture as follows. The generator ’s input is the
concatenation of the input data and a binary matrix that
represents the missing values. The discriminator learns to
reconstruct the mask matrix. Its input is the concatenation of
the generator’s output and a hint matrix, which reveals partial
information about the missingness of the original data. The
computation of the hint matrix incorporates the introduced
hyperparameterhint rate. A second hyperparameter α that
GAIN introduces helps to balance the generator ’s
performance for observed and missing values. For training,
we use Adam optimizer with default hyperparameters except
for the learning rate for the generator and the discriminator, 

Document 6: 
 obtain promising initial results, the combined evolution of solutions and their genetic opera-
tors is comparatively challenging, and it remains an open question whether autoconstructive
evolution is indeed superior to non-autoconstructive evolution.
Multi-armed bandits DAC can be cast as a multi-armed bandit problem, where each
conﬁguration represents an arm with an associated reward distribution. The agent is then
tasked to maximize the cumulative reward, i.e., “pull the arm“ that maximizes the reward.
Put diﬀerently, the agent aims to select a conﬁguration that works well for the current time
step. This setting of the bandit problem is particularly challenging because the reward
distributions associated with the respective arms change over time. In Fialho et al. (2010),
various multi-armed bandit approaches are compared with an approach that only considers
rewards obtained within a certain time frame (i.e., a sliding window) to allow for smoother
transitions between conﬁgurations. Note that in this work, the authors assume rewards to
be retrievable during an algorithm run. While this assumption holds for many optimization
approaches, it does not necessarily hold for all of them (e.g., the pre-processing/root node
phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions 

","The term ""GGA"" in the context of Alternative and Augmentative Communication (AAC) typically refers to a specific type of AAC system known as the ""Generalized Grapho-Grammar Approach."" This approach was developed by researchers working with individuals who have severe communication disabilities, aiming to provide more structured and meaningful ways for these individuals to communicate.

GGA stands for Generalized Grapho-Grammar Approach. It is not an abbreviation but rather a specific method within AAC that focuses on grapheme-based systems (systems where letters or symbols represent sounds) as a way to enhance communication for people who cannot use conventional speech due to disabilities, injuries, or other conditions.

Key points about GGA:

1. **Grapheme-Based Communication**: It uses visual representations of letters and words rather than relying solely on motor skills or environmental stimuli.
2. **Structured System**: Unlike some other AAC systems that are more open-ended, GGA provides a structured way to represent different concepts through predefined symbols and structures.
3. **Teaching Methodology**: The approach involves teaching the user how to use these graphemes within a defined set of rules (grammar) for constructing sentences or messages.

While specific details on ""GGA"" can vary depending on who is using the term, it generally refers to this structured grapheme-based communication method designed for people with severe speech and language difficulties.",4.20039439201355,"The GGA (Genetic Algorithm with Genetic Engineering) approach, as described in the context, involves enhancing the traditional genetic algorithm by integrating a surrogate model to evaluate the quality of new configurations. The surrogate model predicts the rank an individual would achieve in a tournament rather than directly predicting solution quality or runtime, which is important in finding high-quality configurations. GGA employs a competitive and non-competitive population structure, where successful configurations from competitions are recombined through a crossover operator. This adaptation aims to focus on high-performing individuals, utilizing specialized splitting criteria within a random forest model to refine the search process in automatic algorithm configuration."
qwen2.5:3b,"What is the idea behind the AAC approach called ""ISAC""?
","Document 1: 
 ISAC – Instance-Speciﬁc Algorithm Conﬁguration1
Serdar Kadioglu and Yuri Malitsky and Meinolf Sellmann and Kevin Tierney2
Abstract. We present a new method for instance-speciﬁc algorithm
conﬁguration (ISAC). It is based on the integration of the algorithm
conﬁguration system GGA and the recently proposed stochastic off-
line programming paradigm. ISAC is provided a solver with cate-
gorical, ordinal, and/or continuous parameters, a training benchmark
set of input instances for that solver, and an algorithm that com-
putes a feature vector that characterizes any given instance. ISAC
then provides high quality parameter settings for any new input in-
stance. Experiments on a variety of different constrained optimiza-
tion and constraint satisfaction solvers show that automatic algorithm
conﬁguration vastly outperforms manual tuning. Moreover, we show
that instance-speciﬁc tuning frequently leads to signiﬁcant speed-ups
over instance-oblivious conﬁgurations.
1 Introduction
When developing a new heuristic or complete algorithm for a con-
straint satisfaction or a constrained optimization problem, we fre-
quently face the problem of choice. There may be multiple branch-
ing heuristics that we can employ, different types of inference mech-
anisms, various restart strategies, or a multitude of neighborhoods to
choose from. Furthermore, the way in which the choices we make
affect one another is not readily known. The task of making these 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 in Ye et al. (2021) that performance gains can already be achieved when the algorithm
conﬁguration is adapted only once. Furthermore, the hyper-reactive approach of Ans´ otegui
et al. (2017) won several categories at the MaxSAT Evaluation 2016 (Argelich et al., 2016).
Thus, DAC oﬀers signiﬁcant potential for improving algorithms, however, it does require
algorithm designers to more deeply integrate their techniques with AC methods than was
performed in the past. In the following, we discuss the most frequently used approach to
DAC, reinforcement learning (RL). While it is the most popular choice, there also exist
other approaches such as policy portfolios, autoconstructive evolution, and multi-armed
bandits.
Reinforcement learning A challenge in DAC is to evaluate the quality of individual
parameter adjustments since feedback is obtained only at the end of processing a problem
instance. Due to this, it is natural to consider DAC within the realm of reinforcement
learning (we refer to this as DAC-RL). To this end, the DAC problem is modeled in terms of
a Markov decision process (MDP) (Puterman, 1994) based on which a policy π: I×Q→ Θ
is learned from data (Biedenkapp et al., 2020). As a reward signal, typically, the eventually
obtained performance value is propagated back to the agent. Using RL allows a wide variety
of researched mechanisms to be directly applied to DAC.
The roots of DAC extend to a variety of methods that use RL to control parameters of 

Document 4: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 5: 
 ACPP is closely related to the area of automatic algo-
rithm configuration, in which the task is to automatically
identify a high-quality configuration from a configuration
space. In the last few years, several high-performance AC
methods (which could handle considerable large configura-
tion spaces) such as ParamILS (Hutter et al. 2009), GGA
(Ans´otegui, Sellmann, and Tierney 2009), irace (L ´opez-
Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and Leyton-
Brown 2011) have been proposed. As a consequence, re-
cently there has been research interest in utilizing these
AC procedures to automatically identify useful portfolios
of configurations from large configuration spaces. Such at-
tempts were first done in constructing sequential portfolios.
The representative methods for solving this problem are Hy-
dra (Xu, Hoos, and Leyton-Brown 2010) and ISAC (Ka-
dioglu et al. 2010). The basic ideas of these methods were
then adapted to be used in constructing parallel portfolios
(i.e., ACPP), thus resulting in PARHYDRA and CLUSTER-
ING (Lindauer et al. 2017) (see the first section). The main
differences between the automatic construction of sequen-
tial portfolios and of parallel portfolios lie in two aspects: 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The ISAC (Instance-Specific Algorithm Configuration) approach, proposed by Kadioglu et al. in 2010, aims to identify a high-quality configuration specifically tailored for an unseen problem instance from a larger configuration space. Unlike typical AAC methods which search across a solver class and then apply the best solver found to a new instance without further customization, ISAC uses SMAC (SMAC3) to search within the configuration space and find the most effective algorithm directly for the given problem instance. This tailored approach allows for better performance optimization specific to that particular input.",2.891430377960205,"ISAC, or Instance-Specific Algorithm Configuration, is an approach aimed at adapting algorithm parameters specifically for different instances of a problem. The fundamental idea is that ISAC leverages a training benchmark set of input instances and uses a feature vector to characterize any given instance. By doing so, ISAC provides high-quality parameter settings tailored to the specifics of new input instances. This method has been shown to outperform manual tuning and typically yields significant speed-ups compared to instance-oblivious configurations. In essence, ISAC aims to customize algorithm performance by aligning configurations closely with the characteristics of the problem instance at hand."
qwen2.5:3b,"What is the difference between metaheuristics and heuristics?
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 principle difference between similarity and statistical learning (i.e., the two alternative approaches to evaluate the dif-
ference and divergence between two data items). Similarity can be defined over a pair of data items/samples while sta-
tistical measures require a large (theoretically infinite) number of independent data observations.
Another promising direction is to build Turing's type-B random machines (or unorganized machines) (Jiang &
Crookes, 2019; Webster, 2012), also random Boltzmann machines, which can possibly lead to a generalized AI. The
inclusion of new neuro-scientific findings into XAI models will make research on XAI more rationalized, and vice
versa: such a cross-disciplinary exploitation will make XAI not only meaningful for AI researchers but also help solve
century-old challenges on how to understand human intelligence, ultimately. Open research questions in this area
include: (i) how best to determine the network/model architecture?; (ii) how best to extract and represent features?;
(iii) what are the best distance metrics and what are the implications?; (iv) which is the best optimization method?; and
(v) how to determine the best set of prototypes that represent the data best (if a prototype-based method is being used)?
CONFLICT OF INTEREST
The authors have declared no conflicts of interest for this article.
DATA AVAILABILITY STATEMENT
Data sharing is not applicable to this article as no new data were created or analyzed in this study. 

Document 3: 
 the same standard. (In contrast, SAT solver competitions have compared solvers with a complex scoring function that includes the performance of all competitors [25].) As formulated here, the differences between two solvers may be simply in their choice of even a single technique, heuristic, or policy that sustains performance diver-sity. Thus an algorithm portfolio can be thought of as a mixture of experts [26], in-cluding variable-ordering and value-ordering heuristics, restart policies, and nogood learning methods. In particular, even if only one heuristic is available, the portfolio could consist of the heuristic and its opposite, or the heuristic and random selection. 

Document 4: 
 (Eds.), Handbook of Metaheuristics (pp. 321–353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search for classiﬁcation
and function approximation. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 59–66). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding the role of noise in stochastic local search: Analysis and experi-
ments. Artiﬁcial Intelligence, 172(8-9), 955–990.
Minton, S. (1993). An analytic learning system for specializing heuristics. In Bajcsy, R. (Ed.),Proceedings of
the Thirteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI’93), (pp. 922–929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically conﬁguring constraint satisfaction programs: A case study. Constraints,
1(1), 1–40.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conﬂicts: A heuristic repair 

Document 5: 
 PSO. 
For the methodological part of the comparisons, there are far more 
studies. Statistical tests, for instance, lay at the core of prior contribu- 
tions on this matter. However, such contributions are frequently writ- 
ten from a statistical point of view –l i k e the one by Dem š ar [39] –
making it diﬃcult for researchers in this ﬁeld to embrace their method- 
ological recommendations. More recently, some tutorials have tried to 
bring together the ﬁelds of meta-heuristics and inferential statistics [4] . 
Some examples can be found in [40] , in which a statistical treatment is 
suggested for distinguishing between measurements of performance in 
adaptive evolutionary algorithms. Another good example is [41] , which 
shows that in a popular real-parameter benchmark (CEC’2005), con- 
ditions needed for running parametric hypothesis tests did not hold, 
and non-parametric tests were thus recommended. More recently, in 
[42] , some recommendations for the comparison of evolutionary algo- 
rithms are provided, which can be even extrapolated to machine learn- 
ing benchmarks. 
Another important issue from a methodological point of view is the 
assessment of the performance of bio-inspired algorithms from the per- 
spective of the experimental design. Some studies [5] provide general 
recommendations to design experiments for the comparison of algo- 
rithms in a similar way to what we do in this contribution. However, 
these recommendations are far more general as it targets a broader scope 

Document 6: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

","Metaheuristics are high-level methodologies or general algorithmic templates that can solve a wide range of problems, unlike heuristics which may not be able to escape from local optima. Metaheuristics provide more robust solutions by not being limited to specific problem instances, as they operate at a higher level. Heuristics, on the other hand, are simpler strategies designed to quickly find good-enough solutions but might get stuck in local optima for complex problems.",2.5333497524261475,"The main difference between metaheuristics and heuristics lies in their scope and adaptability to specific problems. Heuristics are commonly problem-specific techniques that aim to produce solutions more quickly when classical methods are too slow. They typically focus on a specific problem domain and may produce sub-optimal solutions without addressing the possibility of escaping local optima.

On the other hand, metaheuristics are higher-level methodologies or algorithmic frameworks that can be applied to a wider range of problems. They do not adapt deeply to specific problems; instead, they provide a more generalized approach to problem-solving that allows for the exploration of the solution space and escaping local optima. Metaheuristics often incorporate stochastic components and are inspired by natural processes, making them versatile for tackling hard optimization problems across various domains."
qwen2.5:3b,"What factors and challenges influenced the development of the Automatic Algorithm Configuration approaches?
","Document 1: 
 ods for algorithm development. The methods used in this context 
include experimental design techniques [2,29] , racing approaches 
[20] , and algorithmic methods for parameter conﬁguration, such 
as heuristic search techniques [3,10,41,73,81] , and statistical mod- 
eling approaches [11,43] . These methods have led to an increasing 
automatization of the algorithm design and parameter setting pro- 
cess. 
Automatic algorithm conﬁguration can be described, from a ma- 
chine learning perspective, as the problem of ﬁnding good param- 
eter settings for solving unseen problem instances by learning on 
a set of training problem instances [19] . Thus, there are two clearly 
delimited phases. In a primary tuning phase, an algorithm con- 
ﬁguration is chosen, given a set of training instances representa- 
tive of a particular problem. In a secondary production (or test- 
ing) phase, the chosen algorithm conﬁguration is used to solve un- 
seen instances of the same problem. The goal in automatic algo- 
rithm conﬁguration is to ﬁnd, during the tuning phase, an algo- 
rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 

Document 2: 
 diﬀerent problems (SAT, MIP, ASP, etc.) of varying complexity (number of variables and
problem instances) for tasks of runtime or quality conﬁguration. For DAC, the DACBench
has been proposed (Eimer et al., 2021), although this does not support DAC settings en-
visioned, e.g., by hyper-reactive search. As an alternative to such libraries, AC methods
can also be benchmarked by using surrogate models that are trained on test instances in
advance, resulting in cheaper evaluations when testing (Eggensperger et al., 2018). The ex-
isting benchmarks fail to cover other conﬁgurations settings like the realtime conﬁguration
setting or the conﬁguration of multi-objective target algorithms.
11.4 Novel AC Methodologies
AC methods have become extremely sophisticated and cover a wide range of methodologies
including evolutionary algorithms, statistical tests, and learned surrogate models. There
nonetheless remain opportunities to improve current methods and create better AC algo-
rithms. We note that our goal in this section is not necessarily to specify the methodologies
of the future, but rather to identify the key challenges that remain in the hopes that engaged
466
A Survey of Methods for Automated Algorithm Configuration
readers will ﬁll these gaps with their ideas. To this end, we discuss several challenges faced
by AC approaches: comparing/ranking conﬁgurations, generating new conﬁgurations, and
selecting instances.
Comparing/ranking conﬁgurations This challenge can be summarized as follows: 

Document 3: 
 available for detecting (approximate) independencies. Although there is some evi-
dence that GGA can solve some moderately difﬁcult conﬁguration problems more
effectively than FocusedILS without capping [2], it appears to be unable to reach
the performance of FocusedILS version 2.3 with aggressive capping on the most
challenging conﬁgurations problems [40]. Unfortunately, GGA also offers less ﬂex-
ibility than FocusedILS in terms of the performance metric to be optimised. More
algorithm conﬁguration procedures based on evolutionary algorithms are covered in
Chapter 2 of this book.
Finally, work originating from the Ph.D. project of Hutter [32] has recently over-
come two major limitations of the sequential model-based optimisation methods
discussed in Section 3.4 of this chapter by introducing a procedure that can han-
dle categorical parameters while explicitly exploiting the fact that performance is
evaluated on a set of problem instances. There is some evidence that this procedure,
dubbedSequential Model-based Algorithm Conﬁguration (SMAC) , can, at least on
some challenging conﬁguration benchmarks, reach and sometimes exceed the per-
formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de- 

Document 4: 
 Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews
do not take into account all derivatives of the AC problem, nor do they oﬀer a complete
classiﬁcation scheme. To this end, we introduce taxonomies to describe the AC problem and
features of conﬁguration methods, respectively. We review existing AC literature within
the lens of our taxonomies, outline relevant design choices of conﬁguration approaches,
contrast methods and problem variants against each other, and describe the state of AC in
industry. Finally, our review provides researchers and practitioners with a look at future
research directions in the ﬁeld of AC.
1. Introduction
Diﬃcult computational problems must be regularly solved in many areas of industry and
academia, such as constraint satisfaction problems, Boolean satisﬁability problems (SAT),
vehicle routing problems, ﬁnding a proper machine learning model for a given dataset,
or computing highly complex simulations. Algorithms that were developed to solve such
problems usually have parameters that strongly inﬂuence the behavior of the respective
algorithm and also, for example, the runtime that is required to solve problem instances
©2022 AI Access Foundation. All rights reserved.
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney 

Document 5: 
 better performance than the top-ranked solver from the competition.
9. Related Work
Many researchers before us have been dissatisﬁed with manual algorithm conﬁguration, and various
ﬁelds have developed their own approaches for automatic parameter tuning. We start this section
with the most closely-related work—approaches that employ direct search to ﬁnd good parameter
conﬁgurations—and then describe other methods. Finally, we discuss work on related problems,
such as ﬁnding the best parameter conﬁguration or algorithm on a per-instance basis, and approaches
that adapt their parameters during an algorithm’s execution (see also Hoos, 2008, for further related
work on automated algorithm design).
8. BasicILS was used, because FocusedILS had not yet been developed when that study was conducted.
297
HUTTER , HOOS , LEYTON -BROWN & ST ¨UTZLE
9.1 Direct Search Methods for Algorithm Conﬁguration
Approaches for automated algorithm conﬁguration go back to the early 1990s, when a number of
systems were developed for adaptive problem solving. One of these systems is Composer (Gratch
& Dejong, 1992), which performs a hill-climbing search in conﬁguration space, taking moves if
enough evidence has been gathered to render a neighbouring conﬁguration statistically signiﬁcantly
better than the current conﬁguration. Composer was successfully applied to improving the ﬁve
parameters of an algorithm for scheduling communication between a collection of ground-based
antennas and spacecrafts (Gratch & Chien, 1996). 

Document 6: 
 ration.
1 Introduction
We consider the problem of automatic solver conﬁguration. Practically all solvers have
parameters that are partly ﬁxed by the programmer and partly set by the user. In recent
years, systems have been devised which automate the task of tuning parameters for a
given set of training instances that are assu med to represent typical instances for the
target algorithm.
There are several motivations for such an automation, the ﬁrst being that it is of
course time consuming to tune parameters and it may lead to better results when leaving
the conﬁguration of solvers to a computer rather than doing it by hand.
Moreover, it is conceivable that the existence of an effective tuning environment
will cause algorithm developers to paramete rize more aspects of their algorithms and
thus leave more freedom for algorithmic solutions that are automatically tailored to the
problems of individual users. In particular, many of the SAT solvers that are available
today have parameters which cannot be set through the command line. These parameters
have been ﬁxed to values that the developers have found beneﬁcial without knowledge
about the particular instances a user may want to use the solver for. Automatic parameter
tuning allows solvers to adapt to the ﬁnal environment in which they need to perform.
After being shipped, rather than relying on default parameters, an algorithm can be
⋆ This work was partly supported by the projects TIN2007-68005-C04-04 and TIN2006-15662-
C02-02 funded by the MEC, and by the the National Science Foundation through the Ca-
reer: Cornﬂower Project (award number 0644113). 

","The development of Automatic Algorithm Configuration (A2C) approaches is influenced by a multitude of factors and faces several significant challenges. Here are some key aspects to consider:

### Factors Influencing A2C Development

1. **Complexity and Size of Search Spaces**:
   - The search space for configuring algorithms can be vast, especially when considering the parameters, settings, and heuristics that can influence an algorithm's performance.

2. **Computational Resources**:
   - Determining the best configuration often requires extensive computational resources, which can be a limiting factor in some contexts.

3. **Algorithm Performance Metrics**:
   - The choice of metrics to evaluate algorithm performance is crucial for A2C approaches, affecting both the quality and diversity of the solutions generated.
  
4. **Quality Control and Reliability**:
   - Ensuring that configurations produce reliable and consistent results across different environments or datasets.

5. **Domain-Specific Knowledge**:
   - Understanding domain-specific characteristics can help in tailoring algorithm configurations to specific problem domains, which is important for achieving optimal performance.

### Challenges Faced by A2C Approaches

1. **Search Space Exploration**:
   - Developing effective search strategies that efficiently explore the complex solution space without getting stuck in local optima is a significant challenge.
   
2. **Computational Efficiency**:
   - Many A2C approaches need to be computationally efficient, as they often run multiple iterations and configurations, which can be resource-intensive.

3. **Handling Stochastic Behavior**:
   - Some algorithms exhibit stochastic behavior; thus, robust methods are required to handle the uncertainty introduced by randomness during configuration evaluation.

4. **Adaptability and Scalability**:
   - A2C should be adaptable to different algorithm types and settings while being scalable enough to handle complex configurations and large datasets.
  
5. **Generalization and Transfer Learning**:
   - Ensuring that configurations developed for one setting can generalize well to other similar contexts or domains.

6. **User Interaction and Trust**:
   - When algorithms are used in critical applications, there is often a need for user oversight and control, which presents challenges for purely automated approaches.
   
7. **Data Quality and Availability**:
   - In many cases, obtaining high-quality data for evaluating different configurations can be challenging or costly.

8. **Causal vs. Predictive Models**:
   - Different A2C methods may rely on causal versus predictive models to guide the search process; understanding when each is more appropriate and how to integrate both effectively poses another challenge.

### Summary

The development of Automatic Algorithm Configuration approaches hinges on addressing complex algorithm configuration spaces, optimizing resource usage, ensuring reliable performance, leveraging domain knowledge where applicable, and navigating computational challenges. To overcome these challenges successfully, A2C methods often need to be sophisticated, adaptive, and capable of handling a wide variety of configurations and settings.",9.001035690307617,"Factors influencing the development of Automatic Algorithm Configuration (AC) approaches include the need for efficient tuning of algorithm parameters to enhance performance and runtime for diverse computational problems. These problems often exhibit complexity due to the variation in the number of variables and instances.

Challenges faced by AC include:

1. **Comparing and Ranking Configurations**: Ensuring that different configurations can be effectively evaluated and ranked based on performance.

2. **Generating New Configurations**: Developing methods to produce novel configurations that may outperform existing ones.

3. **Selecting Instances**: Identifying which problem instances to use for tuning the configurations so that they generalize well to unseen instances.

Additionally, the advancement of methodologies like evolutionary algorithms, surrogate models, and sequential model-based optimization has improved AC, but there are still opportunities for refinement and development in areas not comprehensively covered by existing benchmarks and methodologies."
qwen2.5:3b,"What are the main benefits of AAC?
","Document 1: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 2: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 3: 
 phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions
with three themes in mind. First, we consider what is required for AC techniques to be
more widely adopted in industry. Second, we investigate algorithm conﬁguration settings
that have not yet been considered. Third, we consider what methodological advances could
move the ﬁeld forward.
11.1 Industry Adoption
Compared to AC, HPO has begun to be adopted in both industry and academia (Van der
Blom et al., 2021). Furthermore, HPO is the focus of a number of start-ups, such as
SigOpt (which was acquired by Intel (Intel Corporation, 2019)), MindsDB, pecan.ai, and
dotData, among many more. AC, however, has received comparatively little fanfare or
adoption. While some well-known AC tools have public releases, such as SMAC (Lindauer
et al., 2021), irace (L´ opez-Ib´ anez et al., 2016), and GGA (Ans´ otegui et al., 2009), we are
unaware of widespread use of these in industry. The company Optano GmbH released a 

Document 4: 
 a policy to adapt the conﬁguration dynamically. Note that even realtime AC commits to a
single conﬁguration when running a given instance, while DAC has the freedom to adjust
the conﬁguration according to target algorithm behavior during execution. Similar to oﬄine
AC, DAC can either focus on ﬁnding a policy for a set of problem instances or a policy that
is tailored towards a single problem instance (i.e., per-instance algorithm conﬁguration).
Two requirements must be met to implement DAC: (1) the algorithm in question needs
to support dynamic changes in its conﬁguration and (2) runtime information must be pro-
vided to describe the current state of the target algorithm.
DAC approaches consider two diﬀerent types of features: instance features I, which do
not change during target algorithm execution, and features encoding the internal state Q
of the algorithm. Examples of state features include the current iteration of a local search
algorithm, the current restart number of a SAT method, or the current solution quality for
optimization techniques.
461
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
Figure 7: Illustration of dynamic AC as presented in (Biedenkapp et al., 2020)
Biedenkapp et al. (2020) provide the ﬁrst formal deﬁnition of the DAC setting, however,
there is a signiﬁcant amount of earlier work for learning dynamic conﬁguration policies
(Lagoudakis & Littman, 2000, 2001; Pettinger & Everson, 2002). Such earlier works use 

Document 5: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 6: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

","Assistive Communication Aids (AAC) offer several key benefits for individuals with communication difficulties, including:

1. **Improved Social Interaction**: AAC tools can enhance a person's ability to communicate effectively, which in turn improves social interactions and relationships.

2. **Increased Self-Esteem and Confidence**: Being able to express oneself more clearly can significantly boost an individual’s self-esteem and confidence, leading to better mental health outcomes.

3. **Better Participation in Daily Activities**: AAC aids ensure that individuals with communication challenges are better integrated into everyday life activities such as work, school, or personal relationships.

4. **Enhanced Learning and Educational Opportunities**: AAC tools facilitate greater participation in educational settings by allowing users to communicate their needs, understand instructions, and express opinions clearly.

5. **Reduced Behavioral Problems**: Effective communication can help reduce frustration that often leads to behavioral issues, thus making environments more pleasant for all involved.

6. **Improved Occupational Functioning**: For adults, effective communication through AAC aids can enhance job performance by enabling them to participate fully in workplace interactions and fulfill their duties effectively.

7. **Support for Daily Living Skills**: Using AAC tools supports individuals with learning or developmental disabilities to acquire necessary skills for independent living.

8. **Cognitive Benefits**: The process of using an AAC device often involves problem-solving, decision-making, and planning, which can have cognitive benefits for users.

9. **Emotional Support Networks**: For families, AAC devices provide a means for family members to better understand the needs and emotions of individuals with communication challenges, strengthening familial bonds.

10. **Increased Independence**: Overall, AAC aids contribute to greater independence in all aspects of life by enabling people to communicate more effectively and independently.

These benefits highlight how AAC supports not only the individual but also their caregivers, teachers, employers, and peers, creating a more inclusive environment for everyone involved.",5.727841377258301,"The main benefits of Automatic Algorithm Configuration (AAC) include:

1. **Performance Improvement**: AAC methods enhance the performance of algorithms across various domains, such as the boolean satisfiability problem (SAT), the traveling salesman problem (TSP), answer set programming (ASP), and machine learning applications. This improvement is achieved by automatically tuning algorithm parameters to better fit specific problem instances.

2. **Dynamic Adjustment**: AAC enables the use of dynamic configurations that can adapt during execution, allowing for better performance based on the current state of the algorithm and the problem. This adaptability is especially beneficial when dealing with different problem instances and conditions.

3. **Diversity of Methods**: There are several AAC methods available, such as ParamILS, GGA/GGA+, irace, and SMAC, which offer various strategies for parameter optimization. This diversity allows for flexibility in choosing the most suitable method based on the specific context or requirements of the project.

4. **Reduction in Need for Manual Tuning**: By automating the configuration process, AAC reduces the burden on domain experts who would otherwise need to manually optimize parameters, saving time and resources while also potentially achieving better configurations than manual tuning.

Overall, AAC supports the effective utilization of algorithms by streamlining the configuration process and improving their adaptability to diverse problem scenarios."
qwen2.5:3b,"Please provide an overview of the types of AAC methods.
","Document 1: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 2: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 3: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

Document 4: 
 attempted. Nonetheless, the AC literature shows a surprising amount of hybridization of
local search, evolutionary and model-based methods. We hypothesize that there is still sig-
niﬁcant progress that can be made in the area of AC, despite the sophistication of current
methods, and are encouraged by the signiﬁcant increase in attention the ﬁeld has received,
in particular through the spread of HPO techniques. Finally, we especially encourage re-
searchers to address the real-world usability of AC techniques to ensure that the promising
performance gains the AC community is seeing can beneﬁt the world at large.
Acknowledgements
This work was partially supported by the German Research Foundation (DFG) within the
Collaborative Research Center “On-The-Fly Computing” (SFB 901/3 project no. 160364472)
and by the research training group “Dataninja” (Trustworthy AI for Seamless Problem
Solving: Next Generation Intelligence Joins Robust Data Analysis) funded by the Ger-
man federal state of North Rhine-Westphalia. We also would like to thank the anonymous
reviewers for their suggestions on the ﬁrst version of this manuscript.
13. Appendix
To help the reader navigate though the jungle of AC, we provide additional resources.
Table 6 contains a list of abbreviations with terms related to AC used within this work. In
addtion, we provide a list of software resources (Table 7) that contains currently available
tools for AC. We only include software that is widely used.
469 

Document 5: 
 algorithm conﬁgurators. To this end, we propose two classiﬁcation schemes: one for AC
problems, and one for algorithm conﬁgurators. Based on this, we structure and summarize
the available literature and classify existing problem variants as well as approaches to AC.
The remainder of the paper is structured as follows. First, in Section 2, we give a formal
introduction into the setting of algorithm conﬁguration, specify the scope of this survey, and
discuss the relation between AC, AS and HPO. In Section 3, we present the classiﬁcation
schemes for AC problems and approaches that are used, in turn, to describe and compare
existing algorithm conﬁgurators. In Sections 4 and 5, we survey algorithm conﬁguration
methods grouped by the property of whether these methods are model-free or leverage
a model respectively. Section 6 deals with theoretical guarantees that can be obtained.
Diﬀerent problem variants, such as realtime AC, instance-speciﬁc vs. feature-based, multi-
objective, and dynamic AC are discussed in Sections 7 to 10. Eventually, with the help of
our classiﬁcation schemes, we elaborate on appealing research directions in Section 11 and
conclude this survey in Section 12. A list of abbreviations used in this work can be found in
Table 6. In addition, we provide a list of useful software in Table 7. We note, however, that
this list is by no means exhaustive; it is meant to provide an idea about available software
at the time of publication.
2. Problem Formulation
2.1 Algorithm Conﬁguration
To describe the AC problem more formally, we introduce the following notation that is sim- 

Document 6: 
 signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement
learning problem by Sakurai et al.[ 173], Chen et al.[ 33] and
Pettinger and Everson [ 160]. Unlike the previous approaches,
these methods include the notion of state that is deﬁned
using feedback from the EA. For each distinct state, separate
preferences are learned for each operator and selection of the
operator to apply is based on the current state of the search.
An important issue with operator selection (as with all
adaptive parameter control) is the feedback used for control.
Veerapen et al .[ 198] presented and compared various utility
measures for variation operators. These measures are based on
a combination of exploration and exploitation measures and
use Pareto-dominance to evaluate operator utility. In another
paper, Whitacre et al .[ 204] make a distinction between
the source of feedback and any further statistical treatment
(a notion further elaborated in [112]). Several feedback sources
are suggested (including whether the offspring survives or
the number of generations it remains in the population).
The data received by these sources can be treated by av-
eraging them or by detecting the outliers in the sample. 

","Assistive Augmentative Communication (AAC) methods are designed to help people who have difficulty communicating effectively. There are several different types of AAC systems, each suited for individuals with varying levels of communication needs and abilities. Here’s a broad overview of some common types:

1. **Pronouncing Devices**: These devices use speech output technology such as text-to-speech or synthesized voices to produce speech.

2. **Pictorial Systems (PECS)**: Picture Exchange Communication System, which uses pictures or photographs for communication. This is often used by non-verbal children with autism and other developmental disabilities.

3. **Picture Communication Symbols (PCS) / Visual Supports**: These are primarily images that represent specific actions, objects, or ideas. They can be in books, signs, schedules, or digital apps.

4. **Gesture-Based Systems**: Devices like the DynaVox which uses gesture recognition for communication.

5. **Pawel’s Method**: A therapy method based on the idea of using symbols to help users understand and communicate their needs, preferences, and wants.

6. **Electronic Symbol Books / Electronic Communication Boards (ECBs)**: These are digital boards with touch screen technology that allow individuals to select from a range of images or words.

7. **Augmentative Communication Devices**: Handheld devices like the Tobii Dynavox, which use eye-tracking technology for communication input by users who may have limited motor abilities.

8. **Speech Generating Devices (SGDs)**: These are specialized electronic devices that can generate speech to help individuals express themselves when they have difficulty speaking.

9. **Picture Exchange Communication Systems (PECS)**: A method of teaching basic communication skills where a learner exchanges a picture or symbol with an adult for the item or activity desired, and is often used by non-verbal children with autism.

10. **Environmental Control Units (ECUs)**: These are typically designed for people who have severe motor disabilities but can still interact with their environment. They use touch screen technology to allow users to access environmental features such as lights and doors through pre-programmed settings or user-created commands.

Each of these methods has its own advantages, challenges, and is suited to different levels of communication needs and abilities. Choosing the appropriate AAC method typically involves an assessment by a speech-language pathologist or other health care professionals who understand the specific needs of the individual.",7.286456346511841,"Automatic Algorithm Configuration (AAC) methods can be categorized into two main components: sampling in the solver class and evaluating candidate solvers. 

1. **Sampling Methods:**
   - **Search-based Methods:** These methods include techniques such as hill climbing, local search, and evolutionary algorithms (EAs). They typically make minimal assumptions about the solver structure and do not require gradient information, allowing for a broad exploration of the solver space.
   - **Model-based Methods:** These methods aim to capture performance dependencies between solvers and their configurations by training surrogate models based on performance data from the configuration process. This approach is akin to performance prediction techniques that can improve the effectiveness of the sampling process.

2. **Evaluation Strategies:**
   - The evaluation of a candidate solver often employs different performance estimators, as indicated by the estimation errors that decrease with larger sample sizes. The performance of the estimator can depend on various factors, including the number of training instances and the specific configuration scenarios at hand.

3. **Specialized Approaches:**
   - **Sequential Model-based Optimization (SMBO):** SMBO approaches are significant in the AAC landscape and involve techniques like sequential kriging and parameter optimization, which are tailored to optimize black-box functions, especially those affected by noise.
   - **Hybrid Approaches:** There is a noted hybridization of local search, evolutionary techniques, and model-based methods in AAC literature, suggesting that combining these strategies can lead to advancements in performance.

4. **Adaptive Parameter Control:** Techniques that incorporate feedback mechanisms, often guided by reinforcement learning principles, are also part of AAC methods. These methods adjust parameters dynamically based on the observed performance of solutions during the search process.

The landscape of AAC methods continues to evolve, with researchers encouraged to explore new directions and enhance the usability of these techniques in real-world applications."
qwen2.5:3b,"Are Parallel Algorithm Portfolios connected with financial portfolios?
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 automatically producing effective parallel portfolios from multiple solvers, such as ppfolio and pfolioUZK, while exploiting 
the rich design spaces of these component solvers.
Katsirelos et al. [46] showed that an effective parallelization of a CDCL SAT solver does not merely hinge on picking a 
good clause sharing strategy, since it is not straightforward to obtain shorter resolution proofs by parallelization without 
essential changes of the underlying sequential reasoning mechanism. Our ACPP does not aim at parallelizing the resolution 
proof, but rather runs multiple algorithms and algorithm conﬁgurations in parallel, in order to maximize the probability 
that at least one of them ﬁnds a short proof quickly.
2.2. Related work
Well before there was widespread interest in multi-core computing, the potential beneﬁts offered by parallel algorithm 
portfolios were identiﬁed in seminal work by Huberman et al. [39]. Their notion of an algorithm portfolio is inspired by the 
use of portfolios of assets for risk management in ﬁnance and amounts to running multiple algorithms concurrently and 
independently on the same instance of a given problem, until one of them produces a solution. Gomes et al. [27] further 
investigated conditions under which such portfolios outperform their component solvers. Both lines of work considered 
prominent constraint programming problems (graph coloring and quasigroup completion), but neither presented methods 
for automatically constructing portfolio solvers. Parallel portfolios have since made practical impact, both in cases where 
the allocation of computational resources to algorithms in the portfolio is static [62,76] and where the component solvers 

Document 4: 
 is crucial for the effectiveness of any parallel portfolio.
According to (1), the performance of a parallel portfolio on
788 IEEE TRANSACTIONS ON CYBERNETICS, VOL. 52, NO. 2, FEBRUARY 2022
Algorithm 1 GAST
Input: base solvers B with conﬁguration space C; number of
component solvers k; instance set I; performance metric m;
algorithm conﬁgurator AC; independent conﬁgurator runs n; time
budgets tC, tV , tI for conﬁguration, validation and instance
generation respectively
Output:parallel portfolio c1:k
1: for i ← 1: k do
2: /*———— conﬁguration phase ———————*/
3: for j ← 1: n do
4: obtain a portfolio cj
1:i by running AC on conﬁguration space
{c1:i−1}×{ c|c ∈ C} using m for time tC
5: end for
6: validate c1
1:i, ...,cn
1:i on I using m for time tV
7: let c1:i ← arg mincj
1:i|j∈{1,...,n} P(cj
1:i, I) be the portfolio with
the best validation performance
8: /*———— instance-generation phase ————–*/ 

Document 5: 
 work relevant to parallel algorithm portfolios in-cludes online learning [2, 16] and methods that split problems [29, 30].) Effectively, the construction of a parallel algorithm portfolio to solve as many training problems as possible on K processors is an integer-programming (IP) prob-lem. The goal is to find the schedule S that specifies the time allotments to all algo-rithms on all processors, such that no problem can receive more than B time from all the processors together, and the total number of problems solved is a maximum. The expression (1 - )ij(tkj)) is 1 if problem xi is unsolved by algorithm aj after time tkj allo-cated to it 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Parallel algorithm portfolios are not directly connected to financial portfolios. The concept of an algorithm portfolio, inspired by asset management in finance, involves running multiple algorithms concurrently and independently on the same problem instance until one produces a solution. This is different from traditional financial portfolios which involve diversification for risk management purposes.",1.913195371627808,"Yes, Parallel Algorithm Portfolios are conceptually connected with financial portfolios. The notion of an algorithm portfolio is inspired by the use of portfolios of assets in finance for risk management. In both cases, the idea is to run multiple independent strategies (algorithms or financial assets) concurrently to maximize the chances of achieving a favorable outcome (finding a solution in the case of algorithms, or achieving financial returns in the case of investments). This parallel processing allows for greater resilience and effectiveness, as it leverages the strengths of diverse approaches to address a problem or market condition."
qwen2.5:3b,"What factors and challenges influenced the development of the Parallel Algorithm Portfolios?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 Historically, the parallel computing community quickly 
adopted combinatorial search as a playground for applications.
Search algorithms have the advantage of being conceptually 
simple (think of the most basic backtrack-style algorithm) and 
computationally demanding due to the (usually) exponential
size of the search space. In contrast, the search community did 
not really focus its research on parallelizing. The lack of proper 
Articles 
SUMMER 2013   99 Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 
Seven Challenges in 
Parallel SAT Solving 
Youssef Hamadi and Christoph M. Wintersteiger 
n This article provides a broad overview of the 
state of the parallel SAT solving field. A set of 
challenges to researchers is presented that, we 
believe, must be met to ensure the practical
applicability of parallel SAT solvers in the 
future. All these challenges are described infor -
mally but put into perspective with related 
research results, and a (subjective) grading of 
difficulty for each of them is provided.
infrastructure and, for many, the feeling that 
sequential algorithms were still full of research 
opportunities can go toward explaining that. In 
that community, parallelism was often only put in 
the perspectives of papers with no real perspec -
tives. This led to a situation where parallel search 
algorithms were designed by people with only one 
part of the required skills. 
Most computational problems solved on a com - 

Document 3: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 4: 
 rithm except some important parameter settings. Therefore, the
portfolio framework in PPTA framework can be parallel-based
framework as well as sequential-based framework.
The algorithm portfolio requires algorithm constituents to
be complementary so that the portfolio algorithm can handle
different types of problems. But it is not easy to select the com-
plementary algorithms which need to test algorithm candidates
in benchmark functions. In PPTA framework, the parameter
conﬁguration obtained from different kinds of problems are
naturally complementary thanks to the classiﬁcation of train-
ing instances. Therefore, we can obtain the complementary
algorithm much easier to construct the algorithm portfolio.
3) Robustness: For each algorithm constituent in the port-
folio, it is the best parameter conﬁguration found for the
speciﬁc kind of problems. Each algorithm is suitable for a
kind of problem so that the portfolio algorithm can handle
several kinds of problems. As a result, the optimization risk
is obviously reduced.
4) Efﬁciency: The time complexity of parameter tuning
algorithm in PPTA framework can be reduced signiﬁcantly
compared with commonly used parameter tuning methods. As
mentioned before, the performance metric only evaluates the
parameter candidates on one speciﬁc problem instance.
In the algorithm portfolio phase, the time complexity does-
n’t increase a lot compared with the single algorithm because
the operators of the algorithm in PPTA are the same with
single algorithm and the total computational resources (i.e.
max FEs) are also the same. The only difference is the
strategy allocating computational resources which varies from
algorithm to algorithm. 

Document 5: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

Document 6: 
 the former is to generate instances that are challenging for the
current portfolio, while the aim of the latter is to ﬁnd a new
component solver for the portfolio to better solve the newly gen-
erated instances. Applied to two widely studied problem domains,
that is, the Boolean satisﬁability problems (SAT) and the travel-
ing salesman problems (TSPs), the proposed approach identiﬁed
parallel portfolios with much better generalization than the ones
generated by the existing approaches when the training data
were scarce and biased. Moreover, it was further demonstrated
that the generated portfolios could even rival the state-of-the-art
manually designed parallel solvers.
Index Terms—Automatic portfolio construction (APC), genera-
tive adversarial approach, parallel algorithm portfolio, parameter
tuning.
I. I NTRODUCTION
M
ANY high-performance algorithms for solving com-
putationally hard problems, ranging from the exact
Manuscript received May 17, 2019; revised October 5, 2019 and March
4, 2020; accepted March 23, 2020. Date of publication April 29, 2020; date
of current version February 16, 2022. This work was supported in part by
the National Key Research and Development Program of China under Grant
2017YFB1003102, in part by the Guangdong Provincial Key Laboratory under
Grant 2020B121201001, in part by the Natural Science Foundation of China
under Grant 61672478, in part by the Program for Guangdong Introducing
Innovative and Enterpreneurial Teams under Grant 2017ZT07X386, in part 

","The development of Parallel Algorithm Portfolios (PAP) is influenced by several key factors and challenges. Here are some of the major ones:

### Factors Influencing Development:
1. **Computational Complexity**: The ability to solve problems more efficiently as computational resources increase, such as using multiple processors or accelerating computations with GPUs.
2. **Scalability**: Designing algorithms that can handle increasing problem sizes without significantly decreasing performance.
3. **Hardware Capabilities**: Advancements in hardware technologies like multi-core CPUs and specialized accelerators (GPUs, TPUs) have greatly influenced algorithm development to exploit these capabilities effectively.
4. **Algorithmic Innovation**: Developing new or optimizing existing algorithms to better fit the constraints of parallel computing architectures.
5. **Application Needs**: Tailoring PAPs to meet specific application requirements in fields like scientific computing, data analytics, and machine learning.

### Challenges Facing Development:
1. **Concurrency Management**: Managing multiple threads/processes can be complex due to potential race conditions, deadlocks, and synchronization issues across parallel tasks.
2. **Data Synchronization Over Scalability**: As the number of processors increases, ensuring that all parts of a computation synchronize correctly becomes more challenging.
3. **Load Balancing**: Distributing computational workloads efficiently among available processing units to maximize overall system throughput without creating bottlenecks or idle resources.
4. **Communication Overhead**: In distributed computing environments, communication between different nodes adds overhead which can limit scalability and performance gains.
5. **Programming Model Complexity**: Developing effective programming models that abstract parallelism so users don't need to worry about the underlying hardware specifics.
6. **Portability Across Architectures**: Ensuring algorithms are portable across various hardware configurations is crucial for broad applicability.
7. **Resource Management**: Efficiently managing resources like memory and storage, especially in large-scale systems where these become critical bottlenecks.

These factors and challenges have driven research into developing effective Parallel Algorithm Portfolios that can take full advantage of modern parallel computing architectures while minimizing potential pitfalls.",6.283138751983643,"The development of Parallel Algorithm Portfolios has been influenced by several factors and challenges. Key aspects include:

1. **Complementarity of Algorithms**: The success of parallel portfolios relies on the ability to select algorithm constituents that are complementary. This allows the portfolio to handle a variety of problems effectively, thus reducing optimization risk (Document 4).

2. **Efficient Use of Resources**: With the rise of parallel computing architectures, effective resource allocation that does not significantly increase time complexity compared to single algorithms is crucial (Document 4).

3. **Dynamic Load Balancing**: For parallel search methods, particularly in SAT solving, efficient dynamic load balancing and worker cooperation through conflict clause exchange are vital for performance (Document 5).

4. **Generalization in Learning**: The generation of instances that challenge current portfolios and the identification of new component solvers for better performance in learning are important (Document 6). These factors are critical when existing training data is scarce and biased.

5. **Research Perspectives**: The historical focus of the parallel computing community on combinatorial search, combined with the lack of dedicated research into parallelization from the search community, has created challenges in effectively designing parallel search algorithms (Document 2).

6. **Algorithm Structure Homogeneity**: The homogeneity of the underlying structure of individual-based algorithms allows for easier integration into parallel frameworks without the need for significant modifications (Document 3).

Overall, the interplay of these factors shapes the design, implementation, and performance of parallel algorithm portfolios, addressing challenges related to efficiency, generalization, and collaboration among different solvers."
qwen2.5:3b,"What are the main benefits of PAPs?
","Document 1: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 2: 
 on PAP with more than two constituent algorithms.
Two conclusions regarding the migration scheme of PAP
can be drawn from the superiority of PAP over the compared
algorithms. First, the migration scheme is of great impor-
tance to the success of PAP. Second, 1 andMAX GEN/20
are two appropriate and robust values for the parame-
tersmigration size and migration interval. To further
investigate the inﬂuence of these parameters on PAP, we
carried out a sensitivity analysis to check whether the per-
formance of PAP will change signiﬁcantly with other param-
eter settings. As stated in Section VI-B, 16 different pairs
ofmigration interval and migration size were tested for
every instantiation of PAP. For each pair, 30 independent
runs were executed on all 27 benchmark functions. Then, for
each instantiation on each function, Kruskal–Wallis one-way
analysis of variance by ranks was employed to test whether
the 16 pairs of parameters had led to signiﬁcantly different
performance. After that, for each instantiation of PAP, we
counted the number of the benchmark functions on which
all 16 pairs of parameters made no difference. The larger
the number, the more insensitive an instantiation is to the
parameters. For the sake of brevity, we only summarize in
Table IX these numbers for the 11 PAP instantiations, while
omit the full details. It can be observed that, in the worst case 

Document 3: 
 described as follows:
Ω(P, z) =max{max
θ∈P
Metric (θ, z), Metric ( ¯θ, z)}, (14)
where ¯θ represents the Restructure procedure. Note Equation (14) is slightly different from
Equation (11) (i.e., the performance of conventional PAPs) due to theRestructure procedure.
Initialization
member 
algorithm 
𝜃
1
𝑆
1
member 
algorithm 
𝜃
𝑖
𝑆
𝑖
member 
algorithm 
𝜃
𝑘
𝑆
𝑘
…
…
…
…
𝑆
𝑘
+
1
𝑆
𝑜𝑢𝑡𝑝𝑢𝑡
Comparison
Restructure
Improvement
Figure 1. Illustrations of MOEAs/PAP , the variant form of PAP for MOPs. Si represents the so-
lution set found by member algorithm θi, and Soutput represents the solution set ﬁnally returned
by MOEAs/PAP . The main difference of MOEAs/PAP from conventional PAPs lies in the way of
determining the ﬁnal output, as indicated by the dash box.
Electronics 2023, 12, 4639 7 of 18
4. Automatic Construction of MOEAs/PAP
As aforementioned, the member algorithms of a PAP could be automatically deter-
mined by solving the problem deﬁned in Equation (13). Below, we ﬁrst introduce the 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget.
To evaluate the effectiveness of PAP, 11 instantiations of
PAP were implemented based on four existing constituent
algorithms, including SaNSDE, wPSO, G3PCX, and CMA-
ES. The performance of each instantiation was compared to
PENG et al.: POPULATION-BASED ALGORITHM PORTFOLIOS FOR NUMERICAL OPTIMIZATION 799
its constituent algorithms on 27 benchmark functions. Our
experimental results showed that seven out of the 11 PAP in-
stantiations outperformed their constituent algorithms in terms
of solution quality and the proposed risk metric. Furthermore,
7 out of the 11 instantiations even achieved superior or compa-
rable performance in comparison with G-CMA-ES, which was
known to be superior to any of the four constituent algorithms.
Our empirical studies also revealed that PAP is capable of
increasing the probability of ﬁnding the global optimum and
is insensitive to control parameters of the migration scheme.
Further analyses have been conducted to investigate in what
circumstance PAP may outperform its constituent algorithms.
Complementarity was identiﬁed as a key issue.
Though PAP has been shown to be a promising framework, 

Document 6: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

","PAP stands for Positive Airway Pressure, typically used in respiratory care. The term is often applied to different devices and contexts, but I'll provide information on both PAP (as related to sleep apnea) and PAP therapy generally.

### Positive Airway Pressure (PAP)
Positive airway pressure (PAP) is a treatment for conditions such as obstructive sleep apnea, chronic obstructive pulmonary disease (COPD), and heart failure. Here are the main benefits of using PAP:

1. **Reduces Apneas**: PAP devices help keep the airways open by delivering a steady stream of air through a mask worn during sleep. This prevents the collapses in the airway that cause apneas.

2. **Improves Sleep Quality**: By reducing or eliminating obstructive sleep apnea, PAP can improve both the duration and quality of sleep.

3. **Reduces Daytime Symptoms**: Conditions like excessive daytime sleepiness, fatigue, and irritability are often reduced with effective use of PAP therapy.

4. **Prevents Other Complications**: Sleep apnea increases the risk of various health problems such as high blood pressure, heart disease, diabetes, and depression. By improving sleep quality, PAP can help reduce these risks.

5. **Enhanced Physical Performance**: Improved sleep from using PAP can lead to better physical performance during daily activities and sports.

6. **Reduces Cardiovascular Events**: For patients with COPD or heart failure, effective use of PAP has been shown to reduce the risk of cardiovascular events such as stroke, heart attack, and hospitalization.

### Sleep Apnea Treatment (PAP)

1. **Sleep Quality Improvement**: Effective PAP therapy can significantly improve sleep quality by reducing interruptions during sleep caused by apneas.

2. **Reduced Cardiovascular Risk**: It helps prevent or reduce cardiovascular complications associated with untreated sleep apnea such as hypertension, arrhythmias, and stroke.

3. **Mental Health Benefits**: Sleep is crucial for mental health; improved sleep can lead to better mood regulation, reduced anxiety, and decreased depression symptoms.

4. **Reduced Need for Antidepressants**: For patients who have tried conventional antidepressants but experienced no relief or side effects, PAP can be an effective alternative.

5. **Increased Energy Levels**: Better sleep leads to increased energy levels during the day, improving overall daily function and productivity.

### Other Applications

- **COPD Management**: Helps manage symptoms like shortness of breath by providing a constant stream of air through the airways.
  
- **Heart Failure**: Reduces fluid retention and improves heart function in patients with heart failure by reducing blood volume and pressure on the heart during sleep.

In summary, PAP therapy can significantly improve both physical health and quality of life for those suffering from conditions like obstructive sleep apnea. It's important to use it as directed under medical supervision to maximize its benefits.",8.961711645126343,"The main benefits of Population-Based Algorithms (PAPs) are:

1. **Ease of Implementation**: PAPs are straightforward to implement as they do not require complex resource allocation; each member algorithm is assigned an equal amount of resources.

2. **Enhanced Performance**: The performance of a PAP is determined by the best performance achieved among its member algorithms, allowing it to potentially outperform any single algorithm included within it.

3. **Utilization of Parallelism**: With the growth of parallel computing architectures, PAPs can leverage parallel solution strategies effectively, making them suitable for modern computing facilities.

4. **Diversity and Complementarity**: The effectiveness of a PAP relies on the diversity and complementarity among its member algorithms, which can improve the overall solution quality.

5. **Sensitivity to Parameters**: Certain PAP configurations have shown to be insensitive to control parameters, contributing to more robust performance across different scenarios.

6. **Empirical Success**: Experimental results indicate that many PAP instantiations can outperform their individual constituent algorithms and improve the likelihood of finding optimal solutions."
qwen2.5:3b,"What is algorithm for mosaic permutation test?
","Document 1: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

Document 2: 
 algorithm and their default values, (i.e. the original values) are
presented in Table II.
The gamma and mu in GA represent the crossover rate
and mutation rate [20]. In DE algorithm, policy represents the
choice of mutation operator andF, CR are two important pa-
rameters in mutation and crossover operators [21]. The policy
candidates contain/rand/1, /best/1, /current−to−best/1,
/best/2 and /rand/2. In PSO algorithm, c1, c2 are two ﬂying
factors for each particle [22].
1852
Authorized licensed use limited to: University College London. Downloaded on July 03,2020 at 02:38:54 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: The tuned parameters for test algorithms with
Dimension d = 30.
Types GA DE PSO
gamma mu policy F CR c1 c2
TB1 0.4294 0.1728 4 0.4155 0.5441 1.7848 0.5908
TB2 0.5972 0.0617 3 0.6553 0.5350 0.1467 3.0000
TB3 0.5493 0.9737 3 0.7791 0.9308 0.7161 1.9731
TB4 0.5329 0.6145 1 0.9584 0.9273 1.7811 1.7866
TB5 0.9964 0.0839 1 0.4453 0.0581 0.4452 3.0000
TC1 0.5972 0.0617 1 0.7791 0.9308 1.2194 2.3496 

Document 3: 
 The mosaic permutation test: an exact and nonparametric
goodness-of-fit test for factor models
Asher Spector ∗ Rina Foygel Barber † Trevor Hastie ∗ Ronald N. Kahn ‡
Emmanuel Cand` es∗§
September 30, 2024
Abstract
Financial firms often rely on fundamental factor models to explain correlations among asset
returns and manage risk. Yet after major events, e.g., COVID-19, analysts may reassess whether
existing risk models continue to fit well: specifically, after accounting for a set of known factor ex-
posures, are the residuals of the asset returns independent? With this motivation, we introduce the
mosaic permutation test, a nonparametric goodness-of-fit test for preexisting factor models. Our
method can leverage modern machine learning techniques to detect model violations while provably
controlling the false positive rate, i.e., the probability of rejecting a well-fitting model, without mak-
ing asymptotic approximations or parametric assumptions. This property helps prevent analysts
from unnecessarily rebuilding accurate models, which can waste resources and increase risk. To
illustrate our methodology, we apply the mosaic permutation test to the BlackRock Fundamental
Equity Risk (BFRE) model. Although the BFRE model generally explains the most significant
correlations among assets, we find evidence of unexplained correlations among certain real estate
stocks, and we show that adding new factors improves model fit. We implement our methods in 

Document 4: 
 the maximum absolute estimated correlation between asset j⋆ and any asset j ∈ Gd.
• Reset Gd⋆ = Gd⋆ ∪ {j⋆}.
Return: Partition G1, . . . , GD.
In particular, let \MaxCorrj := maxj′̸=j | ˆCj,j′| be the maximum absolute estimated correlation between
asset j and asset j′. Let S ⊂ [p] be the subset of indices of [ p] corresponding to the assets with the
ℓ largest values of \MaxCorrj, so |S| = ℓ. Finally, let ˆν denote the top eigenvector of ˆCS,S. We then
define ˆv as follows:
ˆvj =
(
ˆνj j ∈ S
0 else.
In other words, S is the support of ˆ v, and on S, ˆv equals the maximum eigenvalue of ˆCS,S. We
picked this algorithm because it is conceptually simple and computationally cheap, but we have not
explored other algorithms. Of course, the mosaic permutation test could be used in combination with
a maximum bi-cross validation R2 statistic based on any sparse PCA algorithm.
42 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","Algorithm 1 defines the mosaic permutation test. It involves inputs like asset returns, exposures, and tiles. The key steps include constructing a residual estimate ˆϵ that exactly preserves some independence properties of true residuals ϵ, then permuting observations within each tile to compute an adaptive p-value for quantifying correlations among columns of ˆϵ.",2.14713716506958,"The mosaic permutation test is a nonparametric goodness-of-fit test designed for factor models in finance. The algorithm consists of the following steps:

1. **Inputs**: It takes asset returns \( Y \in \mathbb{R}^{T \times p} \), exposures \( L_t \in \mathbb{R}^{p \times k} \) for \( t \in [T] \), and a set of tiles \( \{(B_m, G_m)\}^M_{m=1} \) along with a test statistic \( S : \mathbb{R}^{T \times p} \to \mathbb{R} \).

2. **OLS Estimate**: For each tile \( m = 1, \ldots, M \), the algorithm computes \( \hat{\epsilon}(m) \), which is the ordinary least squares (OLS) estimate of the residuals using only the data in that tile \( Y(m) \). The exposures for the assets in the \( m \)-th tile are denoted as \( L(m) \).

3. **Residual Estimate**: Using these estimates, the algorithm creates a mosaic residual estimate \( \hat{\epsilon} \in \mathbb{R}^{T \times p} \).

4. **Permutation**: The algorithm permutes the order of the residuals multiple times to generate permuted variants \( \tilde{\epsilon}^{(r)} \) for \( r = 1, \ldots, R \) and checks if permuting decreases the value of a chosen meta test-statistic.

5. **P-value Calculation**: Finally, it computes a p-value based on the number of times the permuted residuals yield a test statistic value lower than the original.

Overall, this method allows analysts to check for independence of residuals after accounting for known factor exposures without requiring strong parametric assumptions."
qwen2.5:3b,"What test statistic should be used for mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 power compared to the oracle tests, suggesting that the adaptive QMC statistic effectively adapts to
the unknown sparsity level. Furthermore, the MPT oracle and OLS double oracle—which use the
same test statistic but are applied to different residual estimates—have similar power. Indeed, the
average power difference is 3%, and the maximum power difference is 10%. This result should not
be too surprising, since ˆϵols and ˆϵ are estimating the same residuals ϵ, and thus the OLS and mosaic
statistic should be highly correlated. Indeed, in our application (Figure 4), in all three sectors, the
mosaic statistics are empirically ≥ 85% correlated with the OLS statistics. Thus, in this simulation,
the mosaic test is competitive with an oracle method based on OLS residuals.
7 Discussion
This paper introduces the mosaic permutation test, an exact and nonparametric goodness-of-fit test
for factor models with known exposures. In an empirical application to the BlackRock Fundamental
Equity Risk model, we demonstrate how to use the mosaic permutation test to diagnose financial
factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing 

Document 4: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","For the mosaic permutation test, the test statistic should be a function of the mosaic estimator ˆϵ of the residuals instead of a function of a conventional OLS estimator ˆϵols. This is due to the specific properties and construction of the mosaic residual estimate.",1.8970093727111816,"The mosaic permutation test employs a test statistic denoted as S(ˆϵ), which quantifies the correlations among the columns of the residual estimate ˆϵ. This statistic is central to the test as it allows for the computation of a significance threshold by permuting the observations within the rectangular tiles of ˆϵ. The test is designed to evaluate the independence properties of the residuals, making S(ˆϵ) a key component in assessing the goodness-of-fit in factor models."
qwen2.5:3b,"What is restriction for test statistic in mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when
estimating ˆϵ.
6 Do the mosaic residual estimates cause a loss of power?
Our method requires the test statistic S(ˆϵ) to be a function of mosaic residual estimates instead
of a function S(ˆϵols) of OLS residual estimates. We hope that the mosaic statistic is a good proxy
23
for the OLS statistic (which indeed seems to be the case in Figure 4), so this section analyzes via
simulations whether the mosaic test has lower power than an oracle test based on the OLS statistic.
Our simulations also show the effectiveness of the adaptive test statistic introduced in Section 5.1.
We conduct semisynthetic simulations where the exposures Lt are constant over time and equal to
the BFRE exposures for the financial sector on May 21st, 2020. We sample the factor returns Xtk
as i.i.d. t4 variables. The residuals satisfy ϵt = γt + Ztv for γt,j
i.i.d.∼ t4, Zt
i.i.d.∼ t4 and v ∈ Rp. In 

Document 5: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 6: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

","In the mosaic permutation test, the restriction on the test statistic S involves that it should be a function of the mosaic residual estimates (ˆϵ) instead of a function of OLS residual estimates (ˆϵols). The assumption is made to analyze whether this change affects the power of the test. Specifically, the test statistic only depends on Y through ˆϵols, making the residual bootstrap identical to the pairs bootstrap in this case.",2.2273967266082764,"The restriction for the test statistic in the mosaic permutation test is that it must be a function of the mosaic residual estimates \( \hat{\epsilon} \) instead of a function of the ordinary least squares (OLS) residual estimates \( \hat{\epsilon}_{ols} \). This means that the test statistic \( S(\hat{\epsilon}) \) is calculated using the newly introduced estimator \( \hat{\epsilon} \) which preserves some independence properties of the true residuals, rather than relying on the properties of the OLS residuals which do not meet the same independence criteria."
qwen2.5:3b,"What is the trick for allowiing the exposures to change with each observation in Mosaic permutation test?
","Document 1: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 2: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","The trick for allowing the exposures (Lt) to change with each observation in the Mosaic permutation test is to augment the exposure matrix L_t. Specifically, a new augmented exposure matrix L⋆_t is constructed:

L⋆_t := 

\[
\begin{cases}
h Lt \quad Lt+1 i & t \text{ is odd} \\
Lt-1 \quad Lt i & t \text{ is even}
\end{cases}
\]

∈ Rp×2k. By construction, L⋆_t only changes every two observations. This allows for testing without violating the independence assumptions of the test statistic.",3.157684803009033,"The trick for allowing the exposures to change with each observation in the Mosaic permutation test is to augment the exposure matrix. Specifically, instead of simply using the original exposure matrix \( L_t \), an augmented exposure matrix \( L^\star_t \) is defined, which is constructed to vary every two observations. The form of this augmented matrix is as follows:

\[
L^\star_t := 
\begin{cases}
\begin{pmatrix}
L_t \\ L_{t+1}
\end{pmatrix} & \text{if } t \text{ is odd} \\
\begin{pmatrix}
L_{t-1} \\ L_t
\end{pmatrix} & \text{if } t \text{ is even}
\end{cases}
\]

By this construction, \( L^\star_t \) only changes every two observations, and it still maintains that \( L_t \) is a submatrix of \( L^\star_t \). Therefore, the null hypothesis retains its validity for the augmented model, which allows the application of the mosaic permutation test without the need for \( L_t \) to be constant across all observations but instead ensures that it is constant within the tiles created for the permutation test."
qwen2.5:3b,"Is there any way of choice of tiling in Mosaic permutation test?
","Document 1: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 2: 
 setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining
provable false positive control. For example, it allows analysts to use regularized estimates of the
covariance matrix of the residuals ϵ, for example, via a graphical lasso (Friedman et al., 2007), and
it also permits the use of cross-validation to choose the regularization strength. The only restriction
5
Figure 3: Semisynthetic simulation with Xtk, ϵtj
i.i.d.
∼ N(0, 1), and the exposures are taken from the BFRE
model for the energy sector on May 21st, 2020. Note T = 350, p = 183, k = 18, and we use the test statistic
from Figure 1. Panel 2(a) shows that a naive residual permutation test (discussed in Section 2.1) inaccurately
simulates the null distribution of the test statistic S(ˆϵols)—in fact, the true null distribution and the estimated
one do not overlap. Panel 2(b) shows that naive bootstrap Z-statistics (discussed in Section 2.2) are not
approximately mean zero, nor do they have unit variance. All p-values based on these two naive methods are 

Document 3: 
 The proof is simple, but the notation is subtle. To ease comprehension, recall that by definition
ϵ(m) = ϵBm,Gm where Bm and Gm are random. In this proof, we will use the notation ϵBm,Gm instead
of ϵ(m) to make the dependence on Bm and Gm explicit.
Let T = {(Bm, Gm)}m∈[M] denote the choice of tiles and let τ = {(βm, γm)}m∈[M] denote an arbitrary
fixed tiling in the support of T . It suffices to show that for any fixed permutation matrices P1 ∈
R|β1|×|β1|, . . . , PM ∈ R|βM |×|βM |,
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ) | T= τ. (A.8)
To show this, we note that Assumption A.1 yields the marginal result that
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ). (A.9)
To convert this to a conditional result, note that sinceT is a function of ˆϵ (which itself is a deterministic 

Document 4: 
 In future work, we plan to extend the mosaic permutation test to apply to this new setting (among
others).
• Using regularization: Our methods currently require the use of unregularized OLS regressions within
each tile to estimate the residuals (see Remark 3). However, to increase power, it might be valuable
to develop methods that can use regularization.
• Robustness: It might be valuable to develop tests that are robust to slight inaccuracies in the expo-
sures Lt. Indeed, this could also help relax the assumption that Lt is locally constant (Assumption
3.1), since small within-tile changes in Lt could be viewed as small “inaccuracies.” Similarly, it
would be useful to develop theory quantifying the robustness of the existing test, i.e., by bounding
the excess error in some interpretable way.
25
8 Code and data availability
We implemented our methods in the python package mosaicperm. All other code used in the paper is
available at https://github.com/amspector100/mosaic_factor_paper/. Although we are not able
to make the BFRE model data available, we have provided a publicly available sample dataset that
allows one to obtain qualitatively similar results (see the GitHub repository for more details).
9 Acknowledgements
The authors would like to thank John Cherian, Kevin Guo, Guido Imbens, Lihua Lei, and Bobby Luo 

Document 5: 
 highly computationally efficient procedure compared to one which (e.g.) repeatedly cross-validates an
expensive machine learning algorithm.
5.2 Adaptively choosing the tiling
Although Section 3.3 gives a good default choice of tiling, another option is to learn a good choice
of tiles that “separate” assets whose idiosyncratic returns are correlated. However, in general, if the
tiling is chosen using Y, ˆϵ will not necessarily be invariant to any permutations under the null because
of the dependence between {(Bm, Gm)}M
m=1 and Y. In other words, naive “double dipping” leads to
inflated false positives.
However, we can sequentially choose the mth tile (Bm, Gm) based on the estimated residuals from the
previous m−1 tiles as long as our choice of (Bm, Gm) does not depend on the order of the rows within
each of the previous m − 1 tiles. Precisely, suppose that we can write Bm, Gm as functions bm, gm of
the previous tiles as well as auxiliary randomness Um
i.i.d.∼ Unif(0, 1):
Bm = bm(ˆϵ(1), . . . ,ˆϵ(m−1), Um) and Gm = gm(ˆϵ(1), . . . ,ˆϵ(m−1), Um). (5.5) 

Document 6: 
 returns ˆϵ·,1, ˆϵ·,2 will never be “separated” by different permutations. We address this problem in the
next section.
3.2 The mosaic permutation test
We now introduce the general mosaic permutation test, which is more powerful and stable than
the simple method in Section 3.1. As an added benefit, we will also make the test more robust to
autocorrelation and nonstationarity among the residuals ϵ1, . . . , ϵT .
In Section 3.1, we separated the asset returns Y ∈ RT×p into two disjoint groups, computed residual
estimates ˆϵ separately for each group, and then permuted within each group. Now, we partition the
data Y into an arbitrary number M of rectangles along both axes. Formally, for m = 1, . . . , M, let
Bm ⊂ [T] denote a subset or “batch” of observations and Gm ⊂ [p] denote a subset or “group” of
assets. We say {(Bm, Gm)}M
m=1 is a tiling if for every timepoint t and asset j, there is exactly one pair
(Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we 

","Yes, there is a way of choosing the tiling for the Mosaic permutation test. The document mentions that although Section 3.3 provides a good default choice of tiling, another option is to learn a good choice of tiles that ""separate"" assets whose idiosyncratic returns are correlated. Additionally, it discusses sequentially choosing the mth tile based on estimated residuals from previous tiles without allowing naive ""double dipping"".",2.70759654045105,"Yes, the choice of tiling in the mosaic permutation test can be adaptively learned to separate assets whose idiosyncratic returns are correlated. Although a default choice of tiling is provided in Section 3.3, analysts can sequentially choose each tile based on the estimated residuals from previous tiles, ensuring that the choice does not depend on the order of rows within the previous tiles. This process involves writing the pair of sets (Bm, Gm) as functions of previous tiles along with auxiliary randomness, allowing for an adaptive and potentially more effective selection of tiling."
qwen2.5:3b,"What is RNN with Long-Short-Term-Memory estimate in deep learning pricing asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper.
Hansen, L. P.(1982): “Large sample properties of generalized method of moments estimators,” Econo-
metrica, 50(4), 1029–1054.
Hansen, L. P., and R. Jagannathan(1997): “Assessing Speciﬁcation Errors in Stochastic Discount
Factor Models,” The Journal of Finance , 52(2), 557–590.
Heaton, J., N. Polson, and J. H. Witte(2017): “Deep learning for ﬁnance: Deep portfolios,” Applied
Stochastic Models in Business and Industry , 33(1), 3–12.
Hochreiter, S., and J. Schmidhuber(1997): “Long short-term memory,” Neural Computation, 9(8),
1735–1780.
Horel, E., and K. Giesecke(2020): “Towards Explainable AI: Signiﬁcance Tests for Neural Networks,”
Journal of Machine Learning Research, forthcoming . 

Document 3: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 4: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 5: 
 Rapach et al. 2013) and lasso (Chinco, Clark-Joseph & Ye 2019; Freyberger, Neuhierl & Weber
2020).
More recently, Gu, Kelly & Xiu (2020) conduct a wide-ranging analysis of machine learning
methods for return prediction, considering not only regularized linear methods but also more
cutting-edge nonlinear methods including random forest, boosted regression trees, and deep
learning.Their research illustrates the substantial gains of incorporating machine learning when
estimating expected returns. This translates into improvements in out-of-sample predictive R2
as well as large gains for investment strategies that leverage machine learning predictions. The
empirical analysis also identifies the most informative predictor variables, which helps facilitate
deeper investigation into economic mechanisms of asset pricing.
Machine learning also makes it possible to improve expected return estimates using predic-
tive information in complex and unstructured data sets. For example, Ke, Kelly & Xiu (2019)
propose a new supervised topic model for constructing return predictions from raw news text
and demonstrate its prowess for out-of-sample forecasting.Jiang,Kelly & Xiu (2021) and Obaid
& Pukthuanthong (2022) demonstrate how to tap return predictive information in image data
using machine learning models from the computer vision literature. Both text and image data
confer particularly strong return forecasting gains at short horizons of days and weeks and are 

Document 6: 
 characteristics. Feng, Polson, and Xu (2019) impose a no-arbitrage constraint by using a set of
pre-speciﬁed linear asset pricing factors and estimate the risk loadings with a deep neural network.
Rossi (2018) uses Boosted Regression Trees to form conditional mean-variance eﬃcient portfolios
based on the market portfolio and the risk-free asset. Our approach also yields the conditional
mean-variance eﬃcient portfolio, but based on all stocks. Gu, Kelly, and Xiu (2019) extend the
linear conditional factor model of Kelly, Pruitt, and Su (2019) to a non-linear factor model using an
autoencoder neural network. 3 We conﬁrm their crucial insight that imposing economic structure
2Other related work includes Sirignano, Sadhwani, and Giesecke (2020) who estimate mortgage prepayments,
delinquencies, and foreclosures with deep neural networks, Moritz and Zimmerman (2016) who apply tree-based
models to portfolio sorting and Heaton, Polson, and Witte (2017) who automate portfolio selection with a deep
neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the 

","Luyang Chen's work on using Recurrent Neural Networks (RNN) with Long Short-Term Memory (LSTM) cells for estimating asset prices likely refers to developing a deep learning-based model that can predict stock or financial asset prices. LSTM is a specific type of RNN architecture designed to address the vanishing gradient problem, which allows it to remember information over long periods, making it suitable for tasks involving sequential data such as time series prediction.

In this context, Chen's work would involve several steps:

1. **Data Preparation**: Collecting and preprocessing financial market data (e.g., historical prices, volumes, etc.).

2. **Model Architecture**: Designing an RNN with LSTM cells to capture temporal dependencies in the price sequences. LSTM units are particularly good at learning from past values because they have mechanisms that prevent vanishing or exploding gradients.

3. **Training**: Using a training dataset of historical asset pricing data to train the model, fine-tuning its parameters to minimize prediction errors.

4. **Prediction and Evaluation**: Testing the model's performance on unseen (validation) or test datasets by comparing predicted prices with actual ones, using metrics such as Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), etc., to gauge accuracy.

5. **Model Analysis**: Analyzing how well the LSTM-RNN model performs in predicting asset price movements and discussing its potential applications and limitations within financial market analysis.

By leveraging Luyang Chen's work, we would expect insights into more accurate forecasting of asset prices using advanced deep learning techniques, potentially contributing to better investment strategies or risk management practices.",5.099343776702881,"The context does not provide specific information about the use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) in the asset pricing models developed by Luyang Chen. However, it mentions the use of deep learning techniques for estimating an asset pricing model that accounts for various conditioning information and time-variations. LSTMs are typically utilized in scenarios where data has sequential characteristics, making them suitable for time-series forecasting in finance. They are particularly effective in capturing long-term dependencies in data, which is crucial for modeling the dynamic behavior of asset returns. Therefore, while the specific model by Chen does not detail RNN or LSTM use, these architectures can enhance the processing of temporal data in asset pricing models."
qwen2.5:3b,"What is loss function in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 In addition to the various layer-types of CNN architecture introduced in the previous
section, the ﬁnal classiﬁcation is achieved from the output layer that usually the last layer
of the FC layer, as shown in Figure 2. Different loss functions also affect the performance of
the CNN architecture and are applied to different visual tasks (e.g., image classiﬁcation,
face recognition, and object recognition). Here are some commonly used loss functions in
CNN-based image classiﬁcation methods (inherit the content of Section 2.1.2), as shown
in Table 1.
All in all, Softmax+Cross-Entropy has become the usual loss function of the CNN
model. There are also many improved versions based on it, such as center-loss [ 82], L-
Softmax [83], A-Softmax [ 84], AM-Softmax [ 85], PEDCC-loss [ 86], etc., which play an
important role in different visual tasks.
Remote Sens. 2021, 13, 4712 8 of 51
Table 1. Common loss functions for CNN models.
Loss Function Equation Characteristic
L1 (MAE) Loss(y, y∗) = 1
m ×
m
∑
i=1
⏐⏐⏐⏐y∗
i −yi
⏐⏐⏐⏐
This function is widely used in
regression problems. L1 Loss is called
mean absolute error (MAE)
L2 (MSE) Loss(y, y∗) = 1
m ×
m
∑
i=1
( 

Document 3: 
 We choose this speciﬁc linear version of the model as it maps directly into the linear approaches that
have already been successfully used in the literature. This linear framework essentially captures
the class of linear factor models. Appendix C provides a detailed overview of the various models
for conditional SDFs and their relationship to our framework.
II. Estimation
A. Loss Function and Model Architecture
The empirical loss function of our model minimizes the weighted sample moments which can
be interpreted as weighted sample mean pricing errors:
L(ω|ˆg,It,It,i) = 1
N
N∑
i=1
Ti
T

1
Ti
∑
t∈Ti
Mt+1Re
t+1,iˆg(It,It,i)

2
. (4)
13Kozak, Nagel, and Santosh (2020) consider also cross-products of the characteristics. They show that the PCA
rotation of the factors improves the pricing performance. Lettau and Pelger (2020) extend this important insight
to RP-PCA rotated factors. We consider PCA based factors in III.J. Our main analysis focuses on conventional
long-short factors as these are the most commonly used models in the literature.
14As before we deﬁne as tangency portfolio one of the portfolios on the global mean-variance eﬃcient frontier.
12
Electronic copy available at: https://ssrn.com/abstract=3350138 

Document 4: 
 part of network learning. The ﬁrst thing that backpropagation needs to determine is the
loss function of the model. There are many loss functions, see Section 2.2.5. The loss
function in the Figure 2 is L2 Loss:
Loss(y, y∗) = 1
m ×
m
∑
i=1
(y∗
i −yi)2 (2)
By calculating the mean square error between y∗ and y, the network weights w
and bias b can be updated by obtaining the partial derivative of the loss function: w′ =
w −η ×(∂Loss/∂w) and b′= b −η ×(∂Loss/∂b). With a suitable learning rate η, the loss
of the y and y∗can be gradually minimized. That is to make the y closer to y∗, so as to
achieve the effect of network training.
Remote Sens. 2021, 13, 4712 4 of 51
Figure 2. The structure of the MLP . It hasn input values and m output values, including k hidden
units. xn is the input value. The direction of the arrow is the direction in which the input value is
transmitted. The hidden unit is hk, it receives the input value of the previous layer. ym is the output
unit, and the real value is y∗m.
2.2. CNN Architecture
The main structure of CNN is the convolutional layer, pooling layer, nonlinear acti- 

Document 5: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 6: 
 The most common way is to translate the problem into an unconditional asset pricing model
on sorted portfolios. Under additional assumptions one could obtain a valid SDF Mt+1 conditional
on a set of asset-speciﬁc characteristics It,i by its projection on the return space:
Mt+1 = 1 −ω⊤
t Re
t with ωt,i = f(It,i),
where It,i is a vector of q characteristics observed for N stocks and f(·) is a general, potentially
nonlinear and non-separable function. Most of the reduced-form asset pricing models approximate
this function by a (potentially very large) set of simple managed portfolios fj(·), such that f(It,i) ≈∑˜Nbasis
j=1 fj (It,i) ˜wj. The SDF then becomes a linear combination of these managed portfolios with
constant weights ˜ωj:
Mt+1 = 1 −
˜Nbasis
∑
j=1
˜wj ˜Rt+1,j with ˜Rt+1,j =
N∑
i=1
fj (It,i) Re
t+1,i, (A.1)
where ˜Rt+1 are the returns of ˜Nbasis managed portfolios that correspond to diﬀerent basis functions
in the characteristic space. The number of basis portfolios increases by the complexity of the
basis functions and the number of characteristics. The most common managed portfolios are 

","In the deep learning asset pricing model proposed by Luyang Chen, the primary loss function used is the L2 Loss (also known as Mean Squared Error). This function measures how much the predicted values \( y \) deviate from the actual values \( y^* \), and it is calculated as:

\[ \text{Loss}(y, y^*) = \frac{1}{m} \sum_{i=1}^{m} (y^*_i - y_i)^2 \]

Where \( m \) is the number of outputs from the model.",3.291795253753662,"In the deep learning asset pricing model developed by Luyang Chen and co-authors, the empirical loss function minimizes the weighted sample moments, interpreted as weighted sample mean pricing errors. The loss function is mathematically represented as:

\[ L(\omega|\hat{g}, I_t, I_{t,i}) = \frac{1}{N} \sum_{i=1}^{N} T_i \left\| \frac{1}{T_i} \sum_{t \in T_i} M_{t+1} R_{e, t+1, i} \hat{g}(I_t, I_{t,i}) \right\|^2. \]

This function aims to optimize the model's performance in estimating asset prices based on conditioning information and the dynamic states of the economy."
qwen2.5:3b,"What is the model archtecture in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 the most promising direction for future empirical asset pricing research is developing a genuine
fusion of economic theory and machine learning. It is a natural marriage, as asset pricing theory
revolves around price formation through the aggregation of investor beliefs, which undoubtedly
enterpricesinsubtle,complex,andsometimessurprisingways.Atthesametime,machinelearning
constitutes a sophisticated collection of statistical models that flexibly adapt to settings with rich
and complex information sets.
Machine learning factor models are one such example of this fusion. Almost all leading theo-
reticalassetpricingmodelspredictalow-dimensionalfactorstructureinassetprices.Wherethese
models differ is in their predictions regarding the identity of the common factors. Much of the
frontier work in empirical asset pricing can be viewed as using the (widely agreed upon) factor
structure skeleton as a theory-based construct within which various machine learning schemes
are injected to conduct an open-minded investigation into the economic nature of the common
factors.
Our survey is inevitably selective and disproportionally influenced by our own research on
thesetopics.Wehavemainlyfocusedonmethodologicalcontributions,leavingadetailedreviewof
empirical discoveries via these methodologies for future work.A frequently discussed dichotomy
in the literature is observable factor versus latent factor models. While some of the methods we
discuss apply to observable factor settings (or hybrid settings),we have also skewed our coverage 

Document 3: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 4: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 5: 
 paper.
Feng, G., N. G. Polson, and J. Xu(2019): “Deep Learning in Characteristics-Sorted Factor Models,”
Working paper.
Freyberger, J., A. Neuhierl, and M. Weber(2020): “Dissecting characteristics nonparametrically,”
Review of Financial Studies, forthcoming , 33(5), 2326–2377.
Goodfellow, I., Y. Bengio, and A. Courville(2016): Deep Learning. MIT Press.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio (2014): “Generative adversarial nets,” in Advances in neural information processing
systems, pp. 2672–2680.
Gu, S., B. Kelly, and D. Xiu(2019): “Autoencoder Asset Pricing Models,” Journal of Econometrics,
forthcoming.
Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper. 

Document 6: 
 neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the
additional elements of ﬁnding the macroeconomic states and identifying the most robust conditioning instruments.
Fundamentally, our object of interest is the pricing kernel. Kelly, Pruitt, and Su (2019) obtain a multi-factor factor
model that maximizes the explained variation. The linear special case applies PCA to a set of characteristic based
factors to obtain a linear lower dimensional factor model, while their more general autoencoder obtains the loadings
to characteristic based factors that can depend non-linearly on the characteristics. We show in Section III.J how our
SDF framework and their conditional multi-factor framework can be combined to obtain an even better asset pricing
model.
5
Electronic copy available at: https://ssrn.com/abstract=3350138
on a machine learning algorithm can substantially improve the estimation. Bryzgalova, Pelger,
and Zhu (2020) use decision trees to build a cross-section of asset returns, that is, a small set of
basis assets that capture the complex information contained in a given set of stock characteristics.
Their asset pricing trees generalize the concept of conventional sorting and are pruned by a novel 

","The specific architecture details of the deep learning asset pricing model developed by Luyang Chen and his co-authors are not provided in the given context. The text describes their method but does not specify the exact neural network or feature extraction layers used in their model. However, they mention using a more realistic and flexible beta function compared to traditional linear specifications.",2.2481260299682617,"The deep learning asset pricing model constructed by Luyang Chen and colleagues utilizes a deep neural network architecture that incorporates a flexible non-linear factor model. This model aims to estimate stock returns and takes into account various conditioning information while accounting for time-variation. The innovations of their approach include the integration of the fundamental no-arbitrage condition in the criterion function, the use of an adversarial approach to construct informative test assets, and the extraction of economic states from macroeconomic time series. The model is designed to outperform traditional methods in terms of Sharpe ratio, explained variation, and pricing errors, ultimately identifying key factors that drive asset prices."
qwen2.5:3b,"What is done instead of comparing the variance of the factors before and after the break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 assumption violations.
In the current work, we extend the theory of generalized residuals to common factor
models with continuous MVs. This extension allows us to identify various sources of misfit
in common factor models, such as the nonlinear relationship between MVs and LVs,
heterogeneous variance of MVs conditional on LVs, and nonnormality of LVs. In addition,
we suggest a summary GOF statistic that complements the local statistics conditional on
LV values, which can facilitate an overall judgment on misfit. For all the proposed GOF
statistics, we derive asymptotic reference distributions to construct formal statistical tests.
Ultimately, our proposed framework is expected to complement conventional GOF
diagnostics for common factor models.
The rest of the paper is organized as follows. In Section 2, we introduce the
common factor model and present our extended theory of generalized residuals, along with
example GOF test statistics suitable for testing various model assumptions. In Section 3,
the performance of the proposed test statistics are evaluated by Monte Carlo studies. In
Section 4, our GOF testing methods are illustrated with a real data example. The paper is
concluded with a discussion of the main findings and future directions in Section 5.
2. Theory
2.1. Common Factor Model
2.1.1. Latent Variable Measurement Model
Let Yij ∈ Rbe the individuali’s response for manifest variable (MV)j, and
Xi = (Xi1, . . . , Xid)
⊤ 

Document 5: 
 spectively; and et = [ e1t,· · · ,eN t ]
′
is the N × 1 idiosyncratic error that may have serial and
cross-sectional dependence along with heteroskedasticit y . W e deﬁne π1 = T1/T ∈ (0,1) as the
break fraction, which is assumed to be a ﬁxed constant. This i mplies that T1 is a sequence that
depends on T. F or notational simplicity , we suppress the dependence of T1 on T.
W e are interested in testing the null hypothesis of no struct ural break in the factor loadings,
i.e.,
H0 : λi1 = λi2 ∀i. (2.2)
against the alternative hypothesis that a non-negligible p ortion of the cross sections have a break
in their loadings at a common time, i.e.,
H1 : λi1 ̸= λi2 for i∈ J (2.3)
where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally 

Document 6: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

","In the likelihood ratio (LR) test for structural changes in factor models, instead of directly comparing the variances of the factors before and after the break, we redefine the pre-break and post-break variance estimators. This approach allows us to handle cases where the variances under the alternative hypothesis can be a singular matrix, making the LR test more powerful than Wald-type tests.",2.3827013969421387,"Instead of directly comparing the variance of the factors before and after the break in the likelihood ratio (LR) test for structural changes in factor models, the approach transforms the structural change problem into a low-dimensional problem by exploring a special feature of the estimated factors. Specifically, the test takes into account that the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, which enhances the power of the LR test compared to Wald-type tests. This method allows for more powerful testing of changes in the estimated factors' variances without requiring a direct comparison of those variances."
qwen2.5:3b,"How to determine the number of break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 3: 
 100,500 0.154 0.081 0.017 0.106 0.053 0.017 0.108 0.054 0.01 3 0.063 0.031 0.010
200,200 0.080 0.032 0.005 0.072 0.036 0.013 0.084 0.030 0.00 7 0.052 0.023 0.006
200,500 0.156 0.080 0.022 0.108 0.056 0.018 0.102 0.043 0.01 1 0.066 0.027 0.007
500,500 0.158 0.084 0.020 0.121 0.069 0.020 0.111 0.059 0.01 4 0.066 0.039 0.014
23
T able 4: Size of structural break tests with unknown break da te in a factor model with t-distributed
factors. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.004 0.002 0.000 0.015 0.008 0.003 0.028 0.007 0.00 0 0.128 0.090 0.061
100,200 0.038 0.018 0.002 0.023 0.008 0.001 0.048 0.016 0.00 2 0.032 0.012 0.006 

Document 4: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

Document 5: 
 In this section, we estimate a factor model for the US industrial employment rates and apply
the proposed tests to check whether the factor loadings have undergone a structural change in
the past decade. Monthly data from January 2010 through Apri l 2022 are available from the US
Department of Labor for 84 industries. The data are a balance d panel with T = 147 ,N = 84.
W e use the information criteria ICp1 and ICp2 of Bai and Ng (2002), the ER (eigenvalue ratio)
and GR (growth ratio) of Ahn and Horenstein (2013), the empirical d istribution estimator of
Onatski (2010), and the bridge estimator of Caner and Han (20 14) to determine the number of
common factors in the data. The maximum number of factors is s et to 10. The estimated number
of factors by ICp1 and ICp2 is three; the methods of Onatski (2010) and Caner and Han (201 4)
detect two factors; and ER and GR of Ahn and Horenstein (2013) choose one common factor.
Therefore, we study the test results for the cases of one, two , and three common factors.
W e apply our LR and LR m tests to examine whether there exists a structural break in f actor
loadings and estimate the break date if the null hypothesis i s rejected. All of the settings for the
two tests are the same as those used in the simulation studies . 

Document 6: 
 References
Ahn, S. and Horenstein, A. 2013. Eigenvalue ratio test for the number of factors. Econometrica 81 ,
pp. 1203–1227.
Andrews, D.W.K., 1993. T ests for parameter instability and structural change with unknown
change point. Econometrica 61 , pp. 821–856.
Bai, J., 1997. Estimaing multiple breaks one at a time. Econometric Theory 13 , pp. 315–352.
Bai, J., 2000. V ector autoregressive models with structura l changes in regression coeﬃcients and
in variance-covariance matrices. Annals of Economics and Finance 1 , pp. 303–339.
43
Bai, J., 2010. Common breaks in means and variances for panel data. Journal of Econometrics 157 ,
pp. 78–92.
Bai, J. and Ng, S. 2002. Determining the number of factors in a pproximate factor models. Econo-
metrica 70 , pp. 191–221.
Bai, J., 2003. Inferential theory for factor models of large dimensions. Econometrica 71 , pp.
135–171.
Bai, J., Han, X., Shi, Y., 2020. Estimation and inference of c hange points in high-dimensional 

","The likelihood ratio test is often used to detect structural breaks or changes in parameters over time, which can be particularly relevant when analyzing factor models. Here's a step-by-step approach on how you might go about determining the number of breaks using the likelihood ratio test:

### 1. Model Specification:
- **Model with no breaks**: Fit a single model (e.g., a single-factor model) to your data.
- **Model with \(k\) breaks**: Fit a multiple break model where there are \(k\) points in time at which structural changes occur.

### 2. Likelihood Calculation:
For each model, calculate the maximum likelihood estimates and then compute the log-likelihood value.

### 3. Likelihood Ratio Test Statistic:
The likelihood ratio test statistic is calculated as follows:

\[
LR = -2 \times (\ln(\text{Likelihood with k breaks}) - \ln(\text{Likelihood without any breaks}))
\]

This test statistic follows a chi-squared distribution under the null hypothesis that there are no breaks.

### 4. Determining the Number of Breaks:
To determine how many breaks \(k\) to include in your model, you can perform multiple likelihood ratio tests at different values of \(k\).

#### Step-by-Step Procedure:

1. **Fit Models with Different Numbers of Breaks**:
   - Fit a single-break model (i.e., k=1).
   - Fit a two-break model (i.e., k=2).
   - Continue to fit models up to the point where adding an additional break does not significantly improve the log-likelihood.

2. **Calculate Likelihood Ratio Tests for Each Model**:
   For each \(k\), calculate the LR statistic using the formula above.

3. **Interpret Results**:
   If the LR test statistic is significant (e.g., if it exceeds a critical value from the chi-squared distribution at your chosen significance level, typically 0.05 or 0.1) and the corresponding \(p\)-value is less than your chosen significance level, then you can reject the null hypothesis of no breaks.

4. **Model Comparison**:
   Compare the LR statistics across different values of \(k\). Choose the number of breaks where adding another one does not significantly increase the log-likelihood (e.g., a significant p-value or a small LR statistic).

### 5. Consideration for Complex Factor Models:
For factor models, you might need to adjust your approach slightly. You could either fit multiple break points directly into the model framework or compare nested models with different numbers of breaks.

### Summary:

To determine the number of breaks using the likelihood ratio test in structural change detection within factor models:

- Fit models with increasing \(k\) (number of breaks).
- Use the LR test to decide at what point adding more breaks does not provide a significant improvement in fit.
- Choose the model that best balances model complexity and goodness-of-fit.

This process will help you identify the most parsimonious model that accounts for structural changes in your data.",9.419920682907104,"To determine the number of breaks in the likelihood ratio (LR) test for structural changes in factor models, the procedure involves the following steps:

1. **Initial Break Point**: Identify the initial break point, denoted as \( \hat{k}_1 \), using a method such as the Quasi-Maximum Likelihood (QML) method.

2. **Subsample Division**: Split the entire sample into two subsamples: one from the start to \( \hat{k}_1 \) and the other from \( \hat{k}_1 \) to the end of the sample.

3. **Conduct Sup-LR Tests**: For each subsample, conduct a sup-LR test with \( m = 1 \). If the null hypothesis is rejected at a specified significance level \( \alpha \), this suggests the presence of additional breaks.

4. **Estimate Further Break Points**: For the subsample where the null is rejected, estimate a new break point, and repeat the process by further dividing the subsample at this newly estimated break point. 

5. **Continue Until Non-Rejection**: Continue this splitting and testing process until the LR test does not reject the null hypothesis for all subsamples. 

6. **Count Breaks**: The number of break points is determined by counting the number of subsamples minus one. This gives a consistent estimate of the number of breaks when the significance level \( \alpha \) converges to zero slowly.

This method effectively tracks structural changes and their locations by continuously refining the sample based on the results of the statistical tests at each step."
qwen2.5:3b,"What is equivalent of a factor model with structural brak in factor loading?
","Document 1: 
 where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally
equivalent to a model with time-invariant loadings and pote ntially more pseudo-factors (e.g., HI,
2015; Baltagi et al., 2017). T o capture the factor dimension augmentation caused by the break,
we follow the framework of DBH (2022) and set r as the number of pseudo-factors in (2.1). W e
set
Λ1 = Λ B, Λ2 = Λ C,
5
where Λ is an N× rmatrix with full column rank r, B and C are r× rmatrices, rank( B) = r1 ≤ r,
and rank( C) = r2 ≤ r.
F or a given split point k, deﬁne
X(1)
k = [ x1 ,...,x k ]′, X(2)
k = [ xk+1,...,x T ]′, F(1)
k = [ f1,...,f k ]′ , F(2)
k = [ fk+1,...,f T ]′ ,
e(1) 

Document 2: 
 loadings βSDF as a function of characteristics on diﬀerent stocks for the estimation and evaluation.
Therefore, we can evaluate the model on stocks that are not available in the ﬁrst part of the data.
Importantly, the conditional model also allows us to directly study the economic sources of risk in
terms of ﬁrm-characteristics.
Appendix C.D. Conditional Factor Models
A conditional factor model assumes that the SDF is spanned by a linear combination of con-
ditional risk factors, and hence it restricts the basis assets that span the SDF. In contrast to the
60
Electronic copy available at: https://ssrn.com/abstract=3350138
unconditional model, the SDF weights and loadings of the conditional risk factors are a function
of the characteristics and hence time-varying. We use the Instrumented Principal Component
Analysis (IPCA) of Kelly, Pruitt, and Su (2019) to illustrate this setup. This conditional factor
model directly models individual stock returns as a function of characteristics given by Re
t+1,i =
b⊤
t,ifIPCA
t+1 + ϵt+1,i with bt,i = I⊤
i,tΓb. Instead of allowing the SDF to be Mt+1 = 1 −∑N
i=1 ωt,iRe
t+1,i,
it is restricted to Mt+1 = 1 −∑K
k=1 wIPCA
t,k fIPCA 

Document 3: 
 EC08CH03-Bai ARI 17 October 2016 11:32
where λ = vec
(
/Lambda1′)
. Consider the following restriction on the factor loadings:
λ = Bδ + C, (19)
where δ is a vector of free parameters with dim( δ) ≤ dim( λ). In general, B and C are known ma-
trices and vectors deﬁned by either identifying restrictions or other structural model restrictions.
In view of Equation 19, we may rewrite the restricted factor model in Equation 18 as
y = Zδ + vec( E),
where y = vec( X) − (IN ⊗ F)C and Z = [( IN ⊗ F)B]. If we impose some distributional assump-
tions on the error terms, for example, vec (E|Z) ∼ N(0 ,R ⊗ IT −s )f o rs o m e N × N positive
deﬁnite matrix R, such models can be estimated using the Bayesian algorithm from Bai & Wang
(2015).
11.3. Structural Vector Autoregression and Restricted Dynamic Factor Models
The dynamic factor models also bring new insight into the estimation of structural vector autore-
gression (SVAR) models with measurement errors. Consider a traditional SVAR given by
A( L)Zt = at , 

Document 4: 
 100,200 0.998 0.984 0.792 0.861 0.704 0.424 1.000 1.000 1.00 0 1.000 1.000 0.997
100,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
200,200 1.000 0.980 0.784 0.860 0.702 0.419 1.000 1.000 1.00 0 1.000 1.000 0.999
200,500 1.000 1.000 1.000 1.000 1.000 0.998 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
24
T able 6: Power of structural break tests with unknown break d ate as the magnitude of the break in factor
loadings increases. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
λi2 = λi1 + εi, εi
i.i.d.∼ N (0, b · Ir0 )
N = 100 , T = 100 , ρ = 0
b
0.0 0.015 0.005 0.000 0.026 0.007 0.002 0.055 0.018 0.003 0.158 0.111 0.074 

Document 5: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 6: 
 model that specifies the relationship between LVs and MVs.
A widely used LV measurement model is the common factor model (e.g., Bollen,
1989; Jöreskog, 1969; Kaplan, 2008; Kline, 2023; Lawley and Maxwell, 1971), particularly
when analyzing continuous data. The common factor model requires MVs to be linearly
dependent on LVs, aiming to attribute the dependencies among MVs to LVs. This model
has been widely used for testing theories about the number of LVs and the pattern of
MV-LV dependency (i.e., confirmatory factor analysis), or for learning these structures
from data (i.e., exploratory factor analysis).
Most of the commonly used measurement models, however, rely on strong
parametric assumptions. For example, in the common factor model, the conditional mean
of an MV given LVs is a linear function of LVs, and the conditional variance of an MV is
constant. In addition, it is often assumed that the MVs and LVs are normally distributed.
These restrictive assumptions may be violated when analyzing real-world data, resulting in
poor model-data fit and potentially erroneous inferences.
To prevent fallacious inferences, various goodness-of-fit (GOF) assessment tools
have been developed for common factor models. A majority of existing GOF diagnostics
rely on residual means, residual covariances, or deviances to reflect the discrepancies 

","In a factor model, the introduction of ""structural breaks"" or ""time-varying parameters"" into the factor loadings means that the relationship between factors and assets' returns may change over time. Typically, in standard factor models like the Fama-French three-factor model or Carhart four-factor model, it is assumed that these relationships are constant across different periods.

When we introduce structural breaks to the factor loadings, we are essentially allowing for changes in these relationships at specific points in time (the ""breaks""). This can be implemented in several ways:

1. **Piecewise Constant Loadings**: The factor loadings can have distinct values before and after each break point.
2. **Smooth Transition**: The factor loadings transition smoothly between their different states over a period of time, rather than being abruptly changed at the breakpoints.
3. **Time-Varying Coefficients Models**: Here, the coefficients (factor loadings) are allowed to vary continuously over time.

An equivalent approach can be taken by specifying these changes explicitly in the model itself. For instance:

- **Piecewise Model**: A common way to incorporate structural breaks is to split the data into different regimes based on certain criteria (e.g., market conditions), and estimate separate models for each regime.
  
  \[
  R_t = X_{t} \beta_1 + X_{c,t} \beta_2 + \epsilon_t
  \]

  Where:
  - \(X_{t}\) represents the factor loadings in one period (e.g., before a break point).
  - \(X_{c,t}\) represents the adjusted factor loadings after accounting for the structural change.

- **Smooth Transition Models**: These models incorporate smooth changes over time, often using trigonometric functions or polynomial splines to represent transitions between states.
  
  \[
  R_t = X_{t} \beta_1 + (X_c t) \beta_2 + (1 - X_c t)\beta_3 + \epsilon_t
  \]

  Where:
  - \(X_c\) is a function that smoothly transitions from zero to one over time, indicating the level of change.

- **State-Space Models**: These models allow for different regimes and can capture changes in factor loadings dynamically without needing to specify explicit break points.
  
In summary, an equivalent way to incorporate structural breaks into factor loading would involve specifying these changes explicitly within a model, whether through piecewise constant or smooth transitions. The exact implementation depends on the specifics of your data and research objectives.",7.633642673492432,"A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This transformation simplifies the analysis of structural changes by reducing it from a high-dimensional problem to a low-dimensional problem, allowing for the likelihood ratio (LR) test to be more powerful in detecting variance changes in the estimated factors."
qwen2.5:3b,"What is null hypothesis is likehood ratio test for structural changes in factor model?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 LQ(k) = klog(| ˜Σ1 (k)|) + ( T − k) log( | ˜Σ2(k)|)
= klog(|Q′ | · | ˆΣ1 (k)| · | Q|) + ( T − k) log( |Q′| · | ˆΣ2(k)| · | Q|)
= 2 Tlog(|Q|) + klog(| ˆΣ1 (k)|) + ( T − k) log( | ˆΣ2 (k)|)
The log-likelihood of no change for the entire sample become s
LQ0 = Tlog(|T−1
T∑
t=1
˜gt,Q ˜g′
t,Q|) = Tlog(|Q′ | · | T−1
T∑
t=1
ˆgt ˆg′
t| · | Q|) = 2 Tlog(|Q|)
because T−1 ∑ T
t=1 ˆgt ˆg′
t = Ir under (3.1). Thus, the likelihood ratio test under this alte rnative
identiﬁcation condition can be expressed as
LRQ(k) ≡ − klog(| ˜Σ1(k)|) − (T − k) log( | ˜Σ2 (k)|) + LQ0 

Document 3: 
 in Section 3.
2 Performance of default bootstrap and permutation methods
To review from Section 1, the problem statement is to test the following factor model:
Yt = LtXt + ϵt for t = 1, . . . , T, (2.1)
for outcomes Yt ∈ Rp, fixed and known exposures Lt ∈ Rp×k, unobserved factor returns Xt ∈ Rk and
residuals ϵt ∈ Rp. We seek to test the null hypothesis H0 that the time series of residuals for each
asset are independent:
H0 : ϵ·,1, ϵ·,2, . . . , ϵ·,p ∈ RT are jointly independent. (2.2)
Sections 2.1-2.2 now explain why naive permutation and bootstrap tests can yield false positive rates
of up to 100%, as in Figure 3. The main challenge is that the estimated OLS residuals ˆ ϵols do not
satisfy the same independence properties as the true residuals.
2.1 Naive residual permutation tests are invalid
For simplicity, we assume for this section that the vectors of residuals ϵ1, . . . , ϵT
i.i.d.∼ Pϵ are i.i.d. This
assumption plus H0 together imply that separately permuting the residuals of each asset does not
change the joint law of all of the residuals:
ϵ :=

 

Document 4: 
 aresettozero.
• Scenario2:Changeinonecolumn.
Let1−1 = (ρ(−)
ij ) with
ρ(−)
1j = ρ(−)
j1 =



sign(ρj−1)|ρ|
1+ 3(p− 1)ρ2/2
for j> 1,
1 for j= 1.
Theremainingcorrelationcoefficientsarezero.
• Scenario3:Allcorrelationcoefficientsarechanged.
Here,weput ρij = ρ|j−i| fori, j∈ {1, . . . ,p}.
• Scenario4:Changeinthenumberoffactors.
Thenumberoffactorsinthetruemodelisincreasedto K+ ˜Kwith˜K∈ {1, . . . ,10}.
Thesefourscenariosleadtodifferenttypesoffactormodelsunderthealternativehypothesis.Forinstance,incaseof
Scenario1,asinglechangeinthecorrelationmatrixofresidualsisassumed,whereasScenario2leadstochangesinthefirst
column(row)of 11.Scenario3correspondstochangesinallelementsof 11althoughtheirmagnitudebecomessmaller 

Document 5: 
 However, it can be shown that H′
0gt = R′ ηt for some orthonormal matrix R. Because a pre-
multiplication by R does not change the distribution of an independent standard normal vector,
we can still replace Ω with ˆΩ when simulating the limiting null distribution.
Note that the presence of Ω in (3.5) is due to the potentially m isspeciﬁed likelihood function
(3.2), which assumes that gt,t = 1 ,· · · ,T are i.i.d. Gaussian under the null hypothesis. If (3.2)
correctly speciﬁes the likelihood, then the limiting distr ibution reduces to
sup
[ǫT ]≤k≤[(1−ǫ)T ]
LR(k) ⇒ sup
π∈[ǫ,1−ǫ]
[W1(π) − πW1(1)]′[W1(π) − πW1(1)]
π(1 − π) (3.7)
where W1(π) is an r(r+ 1) /2 vector of independent Brownian motions. The distribution in (3.7)
is the same as that used in conventional supreme type tests fo r a structural break, and the critical
values can be found in Andrews (1993).
If gt,t = 1 ,· · · ,T are i.i.d. Gaussian and the null hypothesis is true, the sup- LR statistic is 

Document 6: 
 200,200 0.844 0.677 0.333 0.736 0.498 0.187 1.000 1.000 0.99 9 1.000 0.998 0.956
200,500 0.999 0.999 0.984 0.999 0.997 0.951 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 0.999 0.985 0.998 0.995 0.934 1.000 1.000 1.00 0 1.000 1.000 1.000
T able 9: Power of structural break tests with unknown break d ate in the case of factor mean change.
ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.048 0.014 0.000 0.071 0.026 0.005 0.128 0.049 0.00 5 0.196 0.106 0.037
100,200 0.202 0.103 0.018 0.215 0.109 0.022 0.269 0.141 0.02 8 0.551 0.335 0.101
100,500 0.653 0.499 0.221 0.649 0.508 0.222 0.678 0.531 0.25 6 0.967 0.919 0.611
200,200 0.199 0.096 0.020 0.215 0.107 0.019 0.277 0.149 0.03 2 0.556 0.341 0.115 

","The Likelihood Ratio Test (LRT) can be used to assess the significance of a change in the parameters of a factor model, particularly when testing for structural breaks. A factor model involves assuming that multiple assets' returns are driven by a common set of factors rather than by idiosyncratic shocks.

In this context, null hypothesis might refer to no structural changes (or a specified structure), and the alternative hypothesis is one or more points in time where there have been structural changes. 

A change point analysis could be done using an LRT in the following way:

1. **Fit Model with Structural Changes:** Fit the factor model assuming that there are specific points in time when the parameters of the model may differ from those under a null hypothesis.

2. **Fit Model Without Structural Changes (Baseline):** Fit another version of the same model where no structural changes are assumed, i.e., treating all parameter values as constant over time.

3. **Likelihood Ratio Test:** Compare the likelihoods obtained from fitting the two models: one with and one without structural changes. The test statistic is twice the difference in log-likelihoods between these two models (because the LRT tests if the null model fits better).

4. **Decision Rule:** If the p-value associated with this test statistic is less than a chosen significance level (e.g., 0.05), it suggests that there are structural breaks at some point(s) in time, and thus you reject the null hypothesis.

In summary, when using LRT for testing changes in structural parameters of a factor model, like other applications, you essentially test whether including additional parameters to account for these changes (structural breaks) significantly improves the fit of your model over simpler models that do not consider such breaks. If the LRT indicates a significant improvement due to structural changes, then rejecting the null hypothesis is supported.",6.049842596054077,"The null hypothesis (H0) for the likelihood ratio test for structural changes in a factor model typically posits that there are no structural changes in the factor loadings or variances across the observed period. In the context of the provided snippets, this means that the time series of residuals for each asset are jointly independent, indicating that any dependencies occurring in the data can be attributed to the specified factor model without any structural shifts. The alternative hypothesis posits that there is a structural change, which may include changes in the factor loadings or variances of the factors."
qwen2.5:3b,"Is it possible to test mutliple change with sup-LR test?
","Document 1: 
 limit is similar to that of the usual W ald test,
Waldm(k) ⇒ ∥ W(π) − πW(1)∥ 2
π(1 − π)
where W(π) is an r+ r(r+ 1) /2 = r(r+ 3) /2 standard Brownian motion. This test is nuisance-
parameter free.
Consider the simple weighting by k(T − k)/T2, let WWald = k(T − k)/T2 · Waldm(k). T ake
16
the maximum over the range say k= r+ 1,r + 2,· · · ,T − r− 1. The limiting distribution is simply
sup
π∈[0,1]
∥ W(π) − πW(1)∥ 2 ,
and the critical values do not depend on trimming parameter ǫ.
4.2 T esting multiple changes
In this section, we extend the sup-LR test to multiple changes. W e consider testing the null
hypothesis of no change versus the alternative hypothesis o f a prespeciﬁed number of changes. T o
allow for mchanges under the alternative hypothesis, let 0 = k0 <k1 <k2 <· · · <km <km+1 = T
and deﬁne
LR(k1 ,...,k m) = −
m∑
j=0 

Document 2: 
 the null hypothesis. As the value of a increases, all of the tests become more powerful, except
for W ald(HAC) and LM(HAC) when N = T = 100. The LR test has the highest power under
this DGP . The power of the LR m test increases at a slower rate as a increases. Our Theorem
3 demonstrates that the sup-LR diverges at the same rate as th e conventional sup-W ald when
both pre-break and post-break pseudo-factors have nonsing ular variances. However, our simulation
results indicate that sup-LR remains more powerful than sup -W ald under this scenario, as reported
in T able 7. Speciﬁcally , when the loading matrix undergoes a rotational change, sup-LR is notably
more powerful than sup-W ald with HAC variance. While we ackn owledge that the theoretical
power comparison under this setup may require analysis unde r local alternatives, we leave this as
a future research topic.
T able 8 presents the power against changes in the number of fa ctors. The post-break loadings
are equal to pre-break loadings multiplied by an r × r matrix, i.e., Λ 2 = Λ 1C. W e set C = 

Document 3: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 LRm (k1,...,k m) ⇒ sup
(π1 ,··· ,πm)∈Θ π,ǫ
m∑
j=0
∥ ¯Ω 1/2 [B(πj+1 )−B(πj )]∥ 2
πj+1 −πj
, (4.3)
where B(π) = U(π) − πU(1) is an (r+ r2) Brownian bridge process, and ¯Ω is deﬁned in Theorem
4.
These results depend on m, but mdoes not have to be correctly speciﬁed. W e can also consider
the double max type of test and the conditional test for multi ple breaks as in Bai and Perron (1998),
but we leave these as future research topics.
4.3 Determining the number of breaks
In practice, the number of breaks in a factor model is often unknown. W e follow Bai’s (1997)
sequential testing procedure to provide a consistent estim ate for the true number of breaks. The
procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples: 

Document 6: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

","Yes, it is possible to test multiple changes with the sup-LR test. The document mentions extending the sup-LR test to accommodate a prespecified number of changes by considering testing the null hypothesis of no change versus an alternative hypothesis of a specified number of changes.",1.8908123970031736,"Yes, it is possible to test for multiple changes using the sup-LR test. The context indicates that the sup-LR test has been extended to allow for multiple changes under the alternative hypothesis, enabling the testing of the null hypothesis of no change against a prespecified number of changes, as outlined in Document 1 and Document 5."
qwen2.5:3b,"What is misbehavior of the residual bootstrap in high dimension?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 ϵ = 2. Each simulation uses the standard estimate of σ2
ϵ from
the generated ϵi’s. rρ(κ; G) was computed using a ﬁrst run of simulations with
ϵi
iid∽ G. The Huber loss in this plot is Huber 1 and not the default Huber 1.345 of
the rlm function.
to bootstrapping from the residuals. In the case of random design, it makes also a lot
of intuitive sense to use the pairs bootstrap, since resampling the predictors might be
interpreted as mimicking the data generating process.
However, as in residual bootstrap, it is clear that the pairs bootstrap will have problems,
at least in quite high dimensions. In fact, when resampling the Xi’s from ˆF, the number
of times a certain vector Xi0 is picked has asymptotically Poisson(1) distribution. So the
expected number of diﬀerent vectors appearing in the bootstrapped design matrix X∗ is
n(1 −1/e). When p/n is large, with increasingly high probability the bootstrapped design
matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that 

Document 3: 
 accurate conﬁdence intervals in moderately high dimensions.
In Section 4, we discuss another resampling scheme, the jackknife. We focus on the
jackknife estimate of variance and show that it has similarly poor behavior in high dimen-
sions. In the case of L2 loss with Gaussian design matrices, we further prove that the
jackknife estimator over estimates the variance of our estimator by a factor of 1 /(1 −p/n);
we also provide corrections for other losses that improve the jackknife estimate of variance
in moderately high dimensions.
We rely on simulation results to demonstrate the practical impact of the failure of the
bootstrap. The settings for our simulations and corresponding theoretical analyses are ide-
alized, without many of the common settings of heteroskedasticity, dependency, outliers and
so forth that are known to be a problem for bootstrapping. This is intentional, since even
these idealized settings are suﬃcient to demonstrate that the standard bootstrap methods
have poor performance. For brevity, we give only brief descriptions of the simulations in
what follows; detailed descriptions can be found in AppendixD.1.
Similarly, we focus on the basic implementations of the bootstrap for linear models.
While there are many proposed alternatives (often for speciﬁc loss functions or types of
data), the standard methods we study are most commonly used and recommended in prac-
tice. Furthermore, to our knowledge none of the alternative bootstrap methods we have 

Document 4: 
 regression, by making assumptions on the diagonal entries of the hat matrix. In our context,
these assumptions would be satisﬁed only if p/n →0. Hence those interesting results do
not apply to the present study. We also note that Hall (1992, p. 167) contains cautionary
notes about using the bootstrap in high-dimension.
While there has not been much theoretical work on the bootstrap in the setting where
p/n →κ ∈(0,1), one early work of Bickel and Freedman (1983) considered bootstrap-
ping scaled residuals for least-squares regression when κ > 0. They show that when
p/n → κ ∈ (0,1), there exists a data-dependent direction c, such that c′ˆβ∗
LS does not
have the correct asymptotic distribution (Bickel and Freedman, 1983, Theorem 3.1, p.39),
i.e., its distribution is not conditionally in probability close to the sampling distribution of
c′ˆβLS. Furthermore, they show that when the errors in the model are Gaussian, under the
assumption that the diagonal entries of the hat matrix are not all close to a constant, the
empirical distribution of the residuals is a scaled-mixture of Gaussian, which is not close to
the original error distribution.
As we previously explained, in this work we instead only consider inference forpredeﬁned
contrasts υ′β. The important and interesting problems pointed out in Bickel and Freedman 

Document 5: 
 Journal of Machine Learning Research 19 (2018) 1-66 Submitted 1/17; Revised 12/17; Published 08/18
Can We Trust the Bootstrap in High-dimensions?
The Case of Linear Models
Noureddine El Karoui nkaroui@berkeley.edu, n.elkaroui@criteo.com
Criteo AI Lab
32 Rue Blanche
75009 Paris, France
and
Department of Statistics
University of California
Berkeley, CA 94270, USA
Elizabeth Purdom epurdom@stat.berkeley.edu
Department of Statistics
University of California
Berkeley, CA 94270, USA
Editor: Guy Lebanon
Abstract
We consider the performance of the bootstrap in high-dimensions for the setting of linear
regression, where p<n but p/nis not close to zero. We consider ordinary least-squares as
well as robust regression methods and adopt a minimalist performance requirement: can
the bootstrap give us good conﬁdence intervals for a single coordinate of β (where β is the
true regression vector)?
We show through a mix of numerical and theoretical work that the bootstrap is fraught
with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","The misbehavior of the residual bootstrap in high dimensions is that it tends to give anti-conservative estimates (increased Type I error), while true errors have a different distribution from residuals with more general aspects differing, such as marginal distributions. This behavior leads to poor inference for β when p/n grows large.",2.0386722087860107,"The misbehavior of the residual bootstrap in high dimensions is primarily due to the fact that the residuals tend to have a very different distribution compared to the true errors. Specifically, their distributions differ not only in basic properties such as variances but also in more general aspects, including their marginal distributions. This discrepancy can lead to anti-conservative estimates, resulting in inflated Type I error rates. When the ratio of predictors to observations (p/n) is large, the residual bootstrap fails to provide accurate inferences, highlighting the challenges of using standard bootstrap methods in high-dimensional regression settings."
qwen2.5:3b,"How misbehavior of the residual in high dimension can be explained?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 decent fit of the linear normal one-factor model, our method detected misfit in various
model assumptions: misfit in the LV density, the MV-level mean function, and the
MV-level variance function.
In factor analysis, it is common to assume the joint multivariate normality of MVs
and LVs in order to perform ML estimation. Various well-known methods exist for
assessing normality at the level of observed variables (e.g., Mardia, 1970; Mardia, 1980);
however, to our knowledge, assessing the normality assumption for LVs is less common.
This LV-level misfit can be conveniently assessed by generalized residuals, which sometimes
GENERALIZED RESIDUALS 29
can explain non-normality at the MV level, as is the case in our empirical example.
Furthermore, graphical plots such as the right panel of Figure 8 provide intuitive guidance
for model modification.
In the MV-level fit assessment, our method helps to identify curvilinear
relationships in the mean and/or variance functions. In the case of item 7 in our empirical
example, the misfitting linear normal factor model tended to overestimate the conditional
mean of log-RT while underestimating the conditional variance at the high end of the LV.
Such misfits can be particularly problematic if researchers’ focus is on investigating
extremely slow responders. Similar discussions can also be found in Liu and Wang (2022),
where similar patterns of curvatures in the conditional moment functions have been
identified using a semiparametric approach. 

Document 3: 
 P
→λ⋆. This gives that 2Λj is a
multiple of aχ2
1 variable with multiplicative factor given byκσ2
⋆/λ⋆.
This rough analysis shows that the distribution of the LLR in high dimensions deviates from aχ2
1 due
to the coupled eﬀects of two high-dimensional phenomena. The ﬁrst is the inﬂated variance of the MLE,
which is larger than classically predicted. The second comes from the termλ⋆, which is approximately equal
to Tr
(
H−1( ˆβ)
)
/n, where H( ˆβ) = ∇2ℓ( ˆβ) is the Hessian of the negative log-likelihood function. In the
classical setting, this Hessian converges to a population limit. This is not the case in higher dimensions and
the greater spread in the eigenvalues also contributes to the magnitude of the LRT.
5 Broader Implications and Future Directions
This paper shows that in high-dimensions, classical ML theory is unacceptable. Among other things, classical
theory predicts that the MLE is approximately unbiased when in reality it seriously overestimates eﬀect
magnitudes. Since the purpose of logistic modeling is to estimate the risk of a speciﬁc disease given a patient’s
observed characteristics, say, the bias of the MLE is extremely problematic. As we have seen, an immediate
consequence of the strong bias is that the MLE either dramatically overestimates, or underestimates, the
chance of being sick. The issue becomes increasingly severe as either the dimensionality or the signal 

Document 4: 
 data IRT (e.g., Wang, 2010).
Finally, exploring efficient ways to compute and visualize generalized residuals when
dealing with high-dimensional LVs is a potential topic for future research. When the latent
dimensionality is high, it is impossible to construct evaluation points using an outer
product grid as the number of evaluation points grows exponentially fast. For example, in
a five-dimensional case, examining residuals at just five different points per LV leads to
55 = 3, 125 evaluations, making it computationally intensive and posing a challenge for
generating graphical plots. Additionally, selecting which grids of LVs to summarize the
results would also be challenging. Formulating partially marginalized residuals conditioned
only on one or two LV(s) may yield more informative diagnostics.
GENERALIZED RESIDUALS 30
References
Alexander, P. A., Dumas, D., Grossnickle, E. M., List, A., & Firetto, C. M. (2016).
Measuring relational reasoning.The Journal of Experimental Education, 84(1),
119–151.
Bentler, P. M. (1990). Comparative fit indexes in structural models.Psychological bulletin,
107(2), 238.
Bentler, P. M. (1995).Eqs structural equations program manual(Vol. 6). Multivariate
software Encino, CA.
Bickel, P. J., & Doksum, K. A. (2015).Mathematical statistics: Basic ideas and selected 

Document 5: 
 pressed sensing. IEEE Transactions on Information Theory, 57(10):6920–6941, 2011.
[26] Noureddine El Karoui. Asymptotic behavior of unregularized and ridge-regularized high-dimensional
robust regression estimators: rigorous results.arXiv preprint arXiv:1311.2445, 2013.
[27] Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators.Probability Theory and Related Fields, pages
1–81, 2017.
23
[28] Noureddine El Karoui, Derek Bean, Peter J Bickel, Chinghway Lim, and Bin Yu. On robust regression
with high-dimensional predictors. Proceedings of the National Academy of Sciences, 110(36):14557–
14562, 2013.
[29] Jianqing Fan and Jinchi Lv. Nonconcave penalized likelihood with NP-dimensionality.IEEE Transac-
tions on Information Theory, 57(8):5467–5484, 2011.
[30] Yingying Fan, Emre Demirkaya, and Jinchi Lv. Nonuniformity of p-values can occur early in diverging
dimensions. https://arxiv.org/abs/1705.03604, May 2017.
[31] David Firth. Bias reduction of maximum likelihood estimates.Biometrika, 80(1):27–38, 1993. 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","In high-dimensional data analysis, residuals often exhibit certain behaviors that are not as apparent in lower dimensions. These behaviors can be largely explained by several key concepts from statistical theory and machine learning:

1. **Sparsity**: In high dimensions, it is likely for most of the features to have no or very little contribution to the model's predictions. This leads to a sparse residual space where only a few features are relevant.

2. **Collinearity**: High-dimensional data often suffers from multicollinearity issues because there might be many more features than observations. Residuals in such cases can exhibit patterns that reflect this collinearity, which is difficult to detect or handle in practice due to the curse of dimensionality.

3. **Outliers and Anomalies**: The presence of outliers becomes much more likely as you increase the number of dimensions. These are points that deviate significantly from other data points. Outliers can heavily influence residuals and make their analysis challenging.

4. **Noise Amplification**: As the number of features increases, so does the potential amplification of random noise. This is because each additional feature provides an opportunity for some noise to be assigned a non-zero residual value simply by chance.

5. **Overfitting**: High-dimensional data can often lead to overfitting where models perform well on training data but poorly on unseen data due to their complexity fitting not just the underlying pattern, but also noise and outliers in the dataset.

6. **Non-linear Relationships**: In high-dimensional spaces, capturing non-linear relationships becomes increasingly complex, leading to a higher likelihood of misbehaving residuals that do not follow expected linear patterns.

7. **Convergence Issues**: Algorithms used for training models may face convergence issues or slow performance as they attempt to navigate the high-dimensional space efficiently.

To explain these behaviors more concretely and develop effective strategies:

- For sparsity, one can use techniques like feature selection (e.g., LASSO) that aim to identify which features are truly important in explaining the model's predictions.
  
- Addressing collinearity might involve dimensionality reduction methods such as PCA or regularization techniques.

- Identifying and handling outliers through robust statistical measures or data cleaning processes is essential.

- For noise, strategies like smoothing splines, Gaussian Processes, or ensemble models that combine many weak learners can be employed to reduce the impact of noise.
  
- For overfitting, cross-validation methods are used to ensure models generalize well to unseen data. Techniques like dropout (in neural networks) also help mitigate this issue.

- To handle non-linearities, one might use kernel-based techniques or deep learning architectures that include multiple layers for capturing complex interactions between features.

In summary, misbehavior of residuals in high-dimensional settings can be explained through various statistical and computational phenomena such as sparsity, multicollinearity, noise amplification, overfitting, and the challenges posed by non-linear relationships. These issues often require a combination of model selection, feature engineering, and regularization techniques to manage effectively.",8.994425773620605,"The misbehavior of the residual bootstrap in high dimensions can be explained by the fact that the residuals tend to have a very different distribution compared to that of the true errors. This discrepancy is evident in both simple properties, such as variances, and more general aspects, including the marginal distributions. The work of El Karoui et al. (2013) characterizes the asymptotic relationship between the residuals and true errors in high dimensions, which reveals that as the dimensionality increases, the classical assumptions of the bootstrap no longer hold, leading to a poor approximation of confidence intervals and other statistical inference derived from bootstrap methods."
qwen2.5:3b,"What is a alternative residual bootstrap procedure called deconvolution?
","Document 1: 
 This explains why the scaled ˜ ri(i) is probably a good error distribution ϵ∗ to use in the
residual bootstrap when κis close to 0 or 1. We note that when κis close to 1, ˜ri(i) gives an
error distribution that is in general very diﬀerent from the distribution of ϵ. Our numerical
work of Section 2.3 shows that it is nonetheless a good error distribution from the point of
view of the residual bootstraps we consider.
Appendix B. Deconvolution Bootstrap
In the main text, we considered situations where our predictorsXi are i.i.d with an elliptical
distribution and assume for instance that Xi = λiξi, where ξi ∼N (0,Σ) and λi are i.i.d
scalar random variables with E
(
λ2
i
)
= 1. As described in the main text, if X is elliptical,
˜ei(i) is a convolution of the correct G distribution and a Normal distribution,
˜ei(i) ≃ϵi + ˜Zi,
where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the 

Document 2: 
 problem, and experiments demonstrate that increasing cardinality C is more effective
than going deeper or wider when we increase the capacity. The authors compared the
completely equivalent structures of the three mathematical calculations in Figure 18.
The experimental results show that block Figure 18c with grouped convolution is
more succinct and faster than the other two forms, and ResNeXt uses this structure as
a basic block.
5. Dilated Residual Networks (DRN). To solve the decrease in the resolution of the
feature map and the loss of feature information caused by downsampling. However,
simply removing subsampling steps in the network will reduce the receptive ﬁeld. So,
Yu et al. [120] introduced dilated convolutions that are used to increase the receptive
ﬁeld of the higher layers and replaced a subset of the internal downsampling layer
Remote Sens. 2021, 13, 4712 19 of 51
based on the residual network, compensating for the reduction in receptive ﬁeld
induced by removing subsampling. Compared to ResNet with the same parameter
amount, the accuracy of DRN is signiﬁcantly improved in image classiﬁcation.
6. Other models. Veit et al. [ 121] drops some of the layers of a trained ResNet and
still have comparable performance. Resnet in Resnet (RiR) [ 122] proposed a deep
dual-stream architecture that generalizes ResNets and standard CNNs and is easily
implemented with no computational overhead. DropBlock [123] technique discards 

Document 3: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 4: 
 and mild moment requirements. In implementing the two alternative residual-bootstrap
methods described above, we use ˆσϵ,LS as our estimate of σϵ, including for bootstrapping
robust regression where ρ(x) ̸= x2.
Performance in bootstrap inferenceIn Figure 2 we show the error rate of conﬁdence intervals
based on the two residual-bootstrap methods we proposed above. We see that both methods
control the Type I error, unlike bootstrapping directly from the residuals, and that both
methods are conservative. There is little diﬀerence between the two methods with this
sample size (n= 500), though with n= 100, we observe the deconvolution performance to
be worse in L1 (data not shown).
The deconvolution strategy, however, depends on the distribution of the design matrix,
which in these simulations we assumed was Gaussian (so we did not have to estimate λi’s).
For elliptical designs (λi ̸= 1), the error rate of the deconvolution method described above,
with no adaptation for the design, was similar to that of uncorrected residuals in high
dimensions (i.e., > 0.25 for p/n = 0 .5). Individual estimates of λi might improve the
deconvolution strategy, but this problem points to the general reliance of the deconvolution
method on precise knowledge about the design matrix. The bootstrap using standardized
predicted errors, on the other hand, had a Type I error for an elliptical design only slightly 

Document 5: 
 where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the
following bootstrap procedure:
1. Calculate the predicted errors, ˜ei(i)
2. Estimate |λi|∥ˆβρ(i) −β∥2 (the standard deviation of the ˜Zi)
3. Deconvolve in ˜ei(i) the error term ϵi from the ˜Zi term ;
4. Use the resulting estimates of G as the estimate of ˆG in residual bootstrapping.
34
Can We Trust the Bootstrap in High-dimensions?
B.1 Estimating ∥ˆβρ −β∥and the Variance of the Zi
Deconvolution methods that deconvolve ϵ from the ˜Zi require an estimate of the variance
of the ˜Zi. Equation (3) gives the variance as λ2
i∥ˆβρ(i) −β∥2
2, and we need to estimate this
quantity from the data. We use the approximation
∥ˆβρ(i) −β∥2 ≃∥ˆβρ −β∥2.
See AppendixA and references therein for justiﬁcation of this approximation.
Furthermore, as we note in the main text, in our implementation of this deconvolution in 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","The deconvolution bootstrap procedure mentioned in the text is referred to as Method 1 in Section 2.3 of the main text. This method uses deconvolution techniques to estimate G and then applies it for residual bootstrapping. The process involves estimating the variance of the predicted errors, deconvolving the error term ϵi from the ˜Zi terms, and using these estimates as an alternative in residual bootstrapping.",2.445326328277588,"The alternative residual bootstrap procedure called deconvolution involves several steps to estimate the distribution of the error term while accounting for the influence of the predictors. Specifically, the procedure includes:

1. Calculating the predicted errors \(\tilde{e}_i(i)\).
2. Estimating \(|\lambda_i|\| \hat{\beta}_{\rho}(i) - \beta \|_2\), which represents the standard deviation of the \(\tilde{Z}_i\) term.
3. Deconvolving in \(\tilde{e}_i(i)\) the error term \(\epsilon_i\) from the \(\tilde{Z}_i\) term.
4. Using the resulting estimates of \(G\) as the estimate of \(\hat{G}\) in residual bootstrapping.

This procedure is particularly useful in situations where the predictors have an elliptical distribution and is intended to improve the accuracy of bootstrap inference in high-dimensional settings."
qwen2.5:3b,"What is a alternative residual bootstrapinf from sttandarized residual?
","Document 1: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 2: 
 samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con-
ditions, our results allow the factor returns {Xt}T
t=1 to be arbitrarily non-stationary and the residuals
{ϵt}T
t=1 to be non-stationary across tiles. For instance, in our empirical application, we require that
the residuals are stationary within each week, but their distributions can change arbitrarily between
weeks.
To illustrate this contribution, we conduct semisynthetic simulations using the exposure matrix Lt
from the BFRE model for energy stocks on May 21st, 2020. For simplicity, we generate data Y from
Eq. 1.1 after sampling the residuals and factor returns as i.i.d. standard Gaussians, with T = 350
observations. We use the test statistic from Section 1.2. Figure 3 shows that in this simple Gaussian
setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining 

Document 3: 
 15
El Karoui and Purdom
Implications for the Bootstrap For the purposes of the residual-bootstrap, Theorem 1 im-
plies that diﬀerent methods of estimating the residual distribution ˆG will result in similar
residual-bootstrap conﬁdence intervals as p/n→1, if ˆGhas the same variance. This agrees
with our simulations, where both of our proposed bootstrap strategies set the variance of ˆG
equal to ˆσ2
ϵ,LS and both had similar performance in our simulations for large p/n. Further-
more, as we noted, for p/n closer to 1, they both had similar performance to a bootstrap
procedure that simply sets ˆG = N(0,ˆσ2
ϵ,LS) (Figure 2) (see also AppendixA.3 for further
discussion of residual bootstrap methods which draw from the “wrong” distribution, i.e.,
forms of wild bootstraps (Wu, 1986)).
We return speciﬁcally to the bootstrap based on ˜ri(i), the standardized predicted errors.
Equation (3) tells us that the marginal distribution of ˜ei(i) is a convolution of the distribution
of ϵi and a normal, with the variance of the normal governed by the term∥ˆβρ−β∥2. Theorem
1 makes rigorous our previous assertion that as p/n →1, the normal term will dominate
and the marginal distribution of ˜ei(i) will approach normality, regardless of the distribution 

Document 4: 
 well beyond the expected 5%: we observe error rates of 10-15% for p/n= 0.3 and approxi-
mately 20% for p/n= 0.5. We see similar error rates for other robust-regression methods,
such as L1 and Huber loss, and also for diﬀerent error distributions and distributions of X
(Supplementary Figures A-1 and A-2). We explain some of the reasons for these problems
in Subsection 2.2 below.
2.1 Bootstrapping from Corrected Residuals
While resampling directly from the uncorrected residuals is widespread and often given
as a standard bootstrap procedure (e.g., Koenker, 2005; Chernick, 1999), the discrepancy
between the distribution of ϵi and ei has spurred more reﬁned recommendations in the
8
Can We Trust the Bootstrap in High-dimensions?
case of least-squares: form corrected residuals ri = ei/√1 −hi and sample the ϵ∗
i from the
empirical distribution of the ri −¯r (see e.g., Davison and Hinkley, 1997).
This correction is known to exactly align the variance of ri with that of ϵi regardless
of the design vectors Xi or the true error distribution, using simply the fact that the hat
matrix is a rank min(n,p) orthogonal projection matrix. We see that for L2 loss it corrects 

Document 5: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","The paper suggests using corrected residuals, where ri = ei/√1 - hi, and sampling the ϵ* i from the empirical distribution of ri − ¯r. This correction aims to align the variance of ri with that of ϵi, regardless of the design vectors Xi or the true error distribution.",2.4224867820739746,"An alternative to the standard residual bootstrap is to form corrected residuals instead of resampling directly from the uncorrected residuals. For instance, the corrected residuals can be defined as \( r_i = \frac{e_i}{\sqrt{1 - h_i}} \), where \( e_i \) are the residuals from the model and \( h_i \) are the leverage values derived from the hat matrix. By sampling \( \epsilon^*_i \) from the empirical distribution of the corrected residuals \( r_i - \bar{r} \), the variance of the corrected residuals can be aligned with the variance of the true errors \( \epsilon_i \) regardless of the distribution of the design vectors \( X_i \) or the true error distribution. This method aims to address the discrepancy between the distribution of the residuals and the true errors, especially in high-dimensional settings."
qwen2.5:3b,"What is pair bootstrap?
","Document 1: 
 i = X′
iˆβρ + ϵ∗
i, from which ˆβ∗
ρ is computed. This
method of bootstrapping assumes that the linear model is correct for the mean of y (i.e.,
that E (yi) = X′
iβ); it also assumes ﬁxed Xi design vectors because the sampling is con-
ditional on the Xi. In the second method, called pairs bootstrap, ˆF is an estimate of the
joint distribution of the vector ( yi,Xi) ∈Rp+1 given by the empirical joint distribution
of {(yi,Xi)}n
i=1; the corresponding resampling method resamples the pairs ( yi,Xi). This
method makes no assumption about the mean structure of yand, by resampling the Xi, also
does not condition on the values of Xi. For this reason, pairs resampling is often considered
to be more generally applicable than residuals resampling (see e.g., Davison and Hinkley,
1997).
1.3 Background: High-dimensional Inference of Linear Models
Recent research shows that ˆβρ has very diﬀerent asymptotic properties when p/nhas a limit
κ that is bounded away from zero than it does in the classical setting where p/n→0 (see 

Document 2: 
 B.4 Bootstrap Estimates ˆβ∗from ˆG . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.5 Estimation of λ2
i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C Alternative Weight Distributions for Pairs Bootstrap 38
C.1 Case Σ ̸= Idp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D Description of Numerics 39
D.1 Simulation Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

Document 3: 
 v′ˆβ∗over repeated resampling from the pairs ( yi,Xi).
18
Can We Trust the Bootstrap in High-dimensions?
1 1
1
1
Ratio ( κ )
% Increase in Average CI Width
2
2
2
2
3
3
3
3
0.01 0.10 0.30 0.50
1
10
50
100
350 1
2
3
Normal
Ellip. Normal
Ellip. Exp
Figure 4: Comparison of width of 95% conﬁdence intervals of β1 for L2 loss:
Here we demonstrate the increase in the width of the conﬁdence interval due to
pairs bootstrapping. Shown on the y-axis is the percent increase of the average
conﬁdence interval width based on simulation ( n = 500), as compared to the
average for the standard conﬁdence interval based on normal theory in L2; the
percent increase is plotted against the ratio κ = p/n (x-axis). Shown are three
diﬀerent choices in simulating the entries of the design matrix X: (1) Xij ∼
N(0,1) (2) elliptical Xij with λi ∼ N(0,1) and (3) elliptical Xij with λi ∼
Exp(
√
2). The methods of simulation are the same as described in Figure 1;
exact values are given in Table A-2 in AppendixI. 

Document 4: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 5: 
 bootstrap and other resampling plans perform well when p is comparable to n.
Deﬁning success: accurate inference on β1 The common theoretical deﬁnition of whether
the bootstrap “works” is that the bootstrap distribution of the entire bootstrap estimate
ˆβ∗
ρ converges conditionally almost surely to the sampling distribution of the estimator ˆβρ
(see e.g., van der Vaart, 1998). The work of Bickel and Freedman (1983) on the residual
bootstrap for least squares regression, which we discuss in the background section 1.2, shows
that this theoretical requirement is not fulﬁlled even for the simple problem of least squares
regression.
In this paper, we choose to focus only on accurate inference for the projection of our
parameter on a pre-speciﬁed direction υ. More speciﬁcally, we concentrate only on whether
the bootstrap gives accurate conﬁdence intervals for υ′β. We think that this is the absolute
minimal requirement we can ask of a bootstrap inferential method, as well as one that is
meaningful from the standpoint of applied statistics. This is of course a much less stringent
requirement than performing well on complicated functionals of the whole parameter vector,
which is the implicit demand of standard deﬁnitions of bootstrap success. For this reason,
we focus throughout the exposition on inference for β1 (the ﬁrst element of β) as an example
of a pre-deﬁned direction of interest (where β1 corresponds to choosing υ = e1, the ﬁrst
canonical basis vector). 

Document 6: 
 matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that
situation makes little statistical sense (El Karoui, 2010, Subsection 4.4; Zheng et al., 2014).
For smaller ratios of p/n, we evaluate the performance of pairs bootstrapping on sim-
ulated data. We see that the performance of the bootstrap for inference also declines
17
El Karoui and Purdom
dramatically as the dimension increases, becoming increasingly conservative (Figure 1). In
pairs bootstrapping, the error rates of 95%-conﬁdence-intervals drop far below the nominal
5%, and are essentially zero for the ratio of p/n= 0.5. Like residual bootstrap, this overall
trend is seen for all the settings we simulated under (Supplemental Figures A-1, A-2). For
L1 loss, even ratios as small as 0 .1 yield incredibly conservative bootstrap conﬁdence inter-
vals for ˆβ1, with the error rate dropping to less than 0.01. For Huber and L2 losses, the
severe loss of power in our simulations starts for ratios of 0 .3.
A minimal requirement for the distribution of the bootstrapped data to give reasonable
inferences is that the variance of the bootstrap estimator ˆβ∗ 

","Pair bootstrap resamples pairs (yi, Xi) from the empirical joint distribution. It does not assume anything about the mean structure of y and thus does not condition on the values of Xi. This makes it more generally applicable than residual bootstrapping methods.",1.7507777214050293,"Pairs bootstrap is a resampling method used in statistical inference that involves resampling pairs of observations from a dataset, specifically the pairs of outcome variables (yi) and design vectors (Xi). Unlike residuals resampling, which assumes a specific mean structure for the outcome, pairs bootstrap does not make such assumptions about the mean of the outcome variable and allows for the resampling of the design vectors as well. This method is often considered to be more generally applicable because it does not condition on the values of Xi. It aims to estimate the joint distribution of the pairs (yi, Xi) based on their empirical distribution from the original sample."
qwen2.5:3b,"How central limit theorem for hyperrectangles work?
","Document 1: 
 using the empirical or multiplier bootstraps.
The results in this paper substantially extend those obtain ed in [17] where
we considered the class A = Am of sets of the form A = {w ∈ Rp :
maxj∈J wj ≤ a} for some a∈ R and J ⊂ { 1,...,p }, but in order to obtain
much better dependence on n, we employ new techniques. Most notably, as
the main ingredient in the new proof, we employ an argument in spired by
Bolthausen [10]. Our paper builds upon our previous work [17 ], which in
turn builds on a number of works listed in the bibliography (s ee [18] for a
detailed review and links to the literature).
The organization of this paper is as follows. In Section 2, we derive
a Central Limit Theorem (CLT) for hyperrectangles in high di mensions;
that is, we derive a bound on ρn(A) for A = Are being the class of all
hyperrectangles and show that the bound converges to zero un der certain
conditions even when pis potentially larger or much larger than n. In Section
3, we extend this result by showing that similar bounds apply for A ⊂
Asi(a,d) being a class of simple convex sets and for A = Asp(s) being the
class of all s-sparsely convex sets. In Section 4, we derive high dimensio nal 

Document 2: 
 Assumption F1. Assumption 6(ii) strengthens Bai’s (2003) A ssumption F3 and ensures the con-
sistency of the heteroskedasticity and autocorrelation co nsistent (HAC) covariance estimator of
ˆg′
t ˆgt − Ir . Assumption 7 follows from Assumption 10 of DBH (2022) or Ass umption 8 of HI (2015).
Assumption 8 states that a basic functional central limit th eorem holds for the sums of gt under
the null hypothesis of no break.
9
3.2 Limiting distribution of sup-LR under the null hypothes is
It is well known that ˆGis an estimator of GH, where H = (Λ
′
Λ/N)(G
′ ˆG/T)V−1
N T and VN T denotes
the eigenvalues of XX′/NT. Bai’s (2003) Proposition 1 shows that G
′ ˆG/T →p Σ−1/2
Λ ΥV1/2 , where
V is the probability limit of VN T and an r× r diagonal matrix of the eigenvalues of Σ 1/2
Λ ΣG Σ1/2
Λ ,
and Υ is the eigenvector of Σ 1/2
Λ ΣGΣ1/2
Λ . Thus, 

Document 3: 
 where the ﬁrst three lines follow from the properties of Fβ(w) and g(t) (recall
that m(w) = g(Fβ(w))), and the last inequality follows from Lemma A.1.
Here the constant C depends only on b. Likewise we have
P(Vn ≤ y− φ−1) ≥ P(SW
n ≤ y− φ−1) − Cφ−1 log1/2 p− |E[In]|.
The conclusion of the lemma follows from combining these ine qualities with
the bound on |E[In]| derived in Step 1. ■
Proof of Corollary 5.1. Pick any hyperrectangle
A= {w∈ Rp : wj ∈ [aj,bj] for all j = 1 ,...,p }.
For i = 1 ,...,n , consider the random vectors ˜Xi and ˜Yi in R2p deﬁned
by ˜Xij = Xij and ˜Yij = Yij for j = 1 ,...,p , and ˜Xij = −Xi,j−p and
˜Yij = −Yi,j−p for j = p+ 1,..., 2p. Then
P(SX
n ∈ A) = P( S
˜X
n ≤ y), P(SY
n ∈ A) = P( S
˜Y 

Document 4: 
 ) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely
convex sets are sets that can be represented as intersection s of many
convex sets whose indicator functions depend only on a small subset of
their arguments, with hyperrectangles being a special case .
1. Introduction
Let X1,...,X n be independent random vectors in Rp where p≥ 3 may be
large or even much larger than n. Denote by Xij the j-th coordinate of Xi,
so that Xi = ( Xi1,...,X ip)′. We assume that each Xi is centered, namely
E[Xij] = 0, and E[ X2
ij] <∞ for all i= 1 ,...,n and j = 1 ,...,p . Deﬁne the
normalized sum
SX
n := ( SX
n1,...,S X
np)′ := 1
√n
n∑
i=1
Xi.
We consider Gaussian approximation to SX
n , and to this end, let Y1,...,Y n 

Document 5: 
 by ψα(x) := exp( xα) − 1, and for a real-valued random variable ξ, we deﬁne
∥ξ∥ψα := inf {λ> 0 : E[ ψα(|ξ|/λ)] ≤ 1}.
For α∈ [1,∞), ∥ · ∥ψα is an Orlicz norm, while for α∈ (0,1), ∥ · ∥ψα is not a
norm but a quasi-norm, that is, there exists a constant Kα depending only
on α such that ∥ξ1 + ξ2∥ψα ≤ Kα(∥ξ1∥ψα + ∥ξ2∥ψα ). Throughout the paper,
we assume that n≥ 4 and p≥ 3.
2. High Dimensional CLT for Hyperrectangles
This section presents a high dimensional CLT for hyperrecta ngles. We
begin with presenting an abstract theorem (Theorem 2.1); th e bound in
Theorem 2.1 is general but depends on the tail properties of t he distri-
butions of the coordinates of Xi in a nontrivial way. Next we apply this
theorem under simple moment conditions and derive more expl icit bounds
(Proposition 2.1).
Let Are be the class of all hyperrectangles in Rp; that is, Are consists of
all sets A of the form 

Document 6: 
 arXiv:1412.3661v4  [math.ST]  8 Mar 2016
CENTRAL LIMIT THEOREMS AND BOOTSTRAP IN
HIGH DIMENSIONS
VICTOR CHERNOZHUKOV, DENIS CHETVERIKOV, AND KENGO KATO
Abstract. This paper derives central limit and bootstrap theorems for
probabilities that sums of centered high-dimensional rand om vectors hit
hyperrectangles and sparsely convex sets. Speciﬁcally, we derive Gauss-
ian and bootstrap approximations for probabilities P( n− 1/ 2 ∑ n
i=1 Xi ∈
A) where X1, . . . , Xn are independent random vectors in Rp and A is
a hyperrectangle, or, more generally, a sparsely convex set , and show
that the approximation error converges to zero even if p = pn → ∞ as
n → ∞ and p ≫ n; in particular, p can be as large as O(eCnc
) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely 

","The central limit theorem for hyperrectangles in this paper works by deriving bounds on probabilities that sums of centered high-dimensional random vectors hit hyperrectangles and sparsely convex sets. Specifically, it provides Gaussian and bootstrap approximations for probabilities \( P\left(\sqrt{n} \sum_{i=1}^n X_i \in A\right) \), where \(X_1, ..., X_n\) are independent random vectors in \(R^p\) and \(A\) is a hyperrectangle or more generally, a sparsely convex set. The approximation error converges to zero even if \(p = pn \to \infty\) as \(n \to \infty\) and when \(p \gg n\); it also holds uniformly over all hyperrectangles or more generally, sparsely convex sets without restrictions on the correlation structure among coordinates of \(X_i\).",3.7645089626312256,"The Central Limit Theorem (CLT) for hyperrectangles in high dimensions, as described in the provided snippets, involves the analysis of sums of independent random vectors and how these sums behave when they are constrained to lie within hyperrectangles or sparsely convex sets. The key points are as follows:

1. **Definition of Hyperrectangles**: A hyperrectangle in \( \mathbb{R}^p \) is defined as a set \( A = \{ w \in \mathbb{R}^p : w_j \in [a_j, b_j] \text{ for all } j = 1, \ldots, p \} \).

2. **Random Vectors**: The CLT applies to independent random vectors \( X_1, \ldots, X_n \) in \( \mathbb{R}^p \), where the number of dimensions \( p \) can be much larger than the number of observations \( n \).

3. **Normalized Sums**: The normalized sum \( S_n^X = \frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \) is considered. The paper derives Gaussian approximations for the probabilities that \( S_n^X \) lies in a specified hyperrectangle.

4. **Convergence of Approximation Error**: The approximation error (the difference between the true distribution of \( S_n^X \) and the Gaussian distribution) converges to zero under certain conditions, even when \( p \) grows significantly with \( n \).

5. **Generalization to Sparsely Convex Sets**: The results hold not just for hyperrectangles but also for more general sparsely convex sets, which are combinations of several convex sets.

6. **Independence of Correlation Structure**: The results are robust in that they do not require any specific assumptions about the correlation structure of the random variables involved.

In summary, the CLT for hyperrectangles establishes that even in high dimensions, under appropriate conditions, the sum of independent centered random vectors will converge in distribution to a multivariate normal distribution when constrained to fall within hyperrectangles or certain convex shapes."
qwen2.5:3b,"What is high dimensional CLT for simple and sparsely conver sets?
","Document 1: 
 In addition, denote
D(1)
n =
(B2
nlog7(pn)
n
)1/6
, D(2)
n,q =
(B2
nlog3(pn)
n1−2/q
)1/3
. (9)
An application of Theorem 2.1 under these conditions leads t o the following
proposition.
Proposition 2.1(High Dimensional CLT for Hyperrectangles) . Suppose
that conditions (M.1) and (M.2) are satisﬁed. Then under (E.1), we have
ρn(Are) ≤ CD(1)
n ,
where the constant C depends only on b; while under (E.2), we have
ρn(Are) ≤ C{D(1)
n + D(2)
n,q},
where the constant C depends only on b and q.
3. High Dimensional CLT for Simple and Sparsely Convex Sets
In this section, we extend the results of Section 2 by conside ring larger
classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather 

Document 2: 
 mT = max i≤N
∑
j≤N
1
(
σij ̸= 0
)
.
The sparsity assumption puts an upper bound assumption on mT in the sense that
m2
T = o
( T
r2 log ( N )
)
.
In this formulation, the number of factors r is allowed to be large and grows with T .U s i n g
principal components estimators under the normalization 1
T
∑T
t=1 Ft F′
t = Ir , one can decompose
the sample covariance of Xt as
SX = ˆ/Lambda1ˆ/Lambda1+
N∑
i=r+1
ˆμi ˆξi ˆξ
′
i ,
where ˆμi and ˆξi are the i-th leading eigenvalues and eigenvectors of SX , respectively. In the high-
dimensional setup, the sample covariance might be singular and provides a poor estimator for the
population covariance. For example, when N > T , the rank of SX can never exceed T ,w h e r e a s
the theoretical covariance /Sigma1X always has rank N . To overcome this problem, we may apply the
thresholding technique to the component ∑N 

Document 3: 
 kw≤ ak for all k= 1 ,...,m }
for some unit vectors vk ∈ Sp−1 and coeﬃcients ak, k = 1 ,...,m . If the
number of non-zero elements of each vk does not exceed s, this A is an
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 9
s-sparsely convex set. Yet another example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj for all j = 1 ,...,p and w2
1 + w2
2 ≤ c}
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , j = 1 ,...,p , and 0 <c ≤ ∞ . This
A is a 2-sparsely convex set. A more complicated example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj,w2
k + w2
l ≤ ckl, for all j,k,l = 1 ,...,p }
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , 0 < ckl ≤ ∞ , j,k,l = 1 ,...,p .
This A is a 2-sparsely convex set. Finally, consider the set 

Document 4: 
 classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather
large. In addition, it allows us to derive similar bounds for classes of sparsely
convex sets. These classes in turn may be of interest in stati stics where
sparse models and techniques have been of canonical importa nce in the past
years.
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 7
3.1. Simple convex sets. Consider a closed convex set A⊂ Rp. This set
can be characterized by its support function:
SA : Sp−1 → R ∪ {∞}, v ↦→ SA(v) := sup {w′v: w∈ A},
where Sp−1 := {v ∈ Rp : ∥v∥ = 1 }; in particular, A = ∩v∈Sp− 1 {w ∈ Rp :
w′v≤ S A(v)}. We say that the set Ais m-generated if it is generated by the
intersection of m half-spaces (that is, A is a convex polytope with at most
m facets). The support function SA of such a set A can be characterized 

Document 5: 
 CLT AND BOOTSTRAP IN HIGH DIMENSIONS 3
because they allow us to approximate the distributions of va rious key sta-
tistics that arise in inference for high-dimensional model s. For example,
the probability that a collection of Kolmogorov-Smirnov ty pe statistics falls
below a collection of thresholds
P
(
max
j∈Jk
SX
nj ≤ tk for all k= 1 ,...,κ
)
= P
(
SX
n ∈ A
)
can be approximated by P( SY
n ∈ A) within the error margin ρn(Are); here
{Jk} are (non-intersecting) subsets of {1,...,p }, {tk} are thresholds in the
interval ( −∞,∞), κ ≥ 1 is an integer, and A ∈ A re is a hyperrectangle of
the form {w∈ Rp : max j∈Jk wj ≤ tk for all k= 1 ,...,κ }. Another example
is the probability that a collection of Pearson type statist ics falls below a
collection of thresholds
P
(
∥(SX
nj)j∈Jk ∥2 ≤ tk for all k= 1 ,...,κ
)
= P
(
SX 

Document 6: 
 It is worthwhile to mention that a notable example where the t ransformed
variables ˜Xi = ( v′Xi)v∈V(Am) satisfy condition (E.1 ′) is the case where each
Xi obeys a log-concave distribution. Recall that a Borel proba bility measure
µ on Rp is log-concave if for any compact sets A1,A2 in Rp and λ∈ (0,1),
µ(λA1 + (1 − λ)A2) ≥ µ(A1)λµ(A2)1−λ,
where λA1 + (1 − λ)A2 = {λx+ (1 − λ)y: x∈ A1,y ∈ A2}.
Corollary 3.1 (High Dimensional CLT for Simple Convex Sets with Log–
concave Distributions) . Suppose that each Xi obeys a centered log-concave
distribution on Rp and that all the eigenvalues of E[XiX′
i] are bounded from
below by a constant k1 >0 and from above by a constant k2 ≥ k1 for every
i= 1 ,...,n . Then
ρn(Asi(a,d)) ≤ Cn−1/6 log7/6(pn),
where the constant C depends only on a,b,d,k 1, and k2. 

","High-dimensional Central Limit Theorem (CLT) for simple and sparsely convex sets refers to obtaining bounds similar to those in Section 2 but under certain conditions, specifically for larger classes of sets. In this context, it involves extending results to include simple convex sets and also considering sparse versions of these sets. These bounds are significant in high-dimensional spaces where the class of simple convex sets is large and allows deriving similar bounds for sparsely convex sets as well.",2.6792495250701904,"The high-dimensional Central Limit Theorem (CLT) for simple and sparsely convex sets refers to the analysis and approximation of the distributions of statistics arising from high-dimensional models, particularly in the context of simple convex sets, which can be characterized by their support functions and generated by half-spaces. Under certain conditions, the theorem establishes bounds on the convergence rates of these statistics, such as the probability that specific statistical measures fall within certain thresholds. 

Proposition 3.1 outlines that the results extend to larger classes of sets, including sparsely convex sets, where the number of non-zero elements of vector representations is limited. This finding is significant in statistics, especially for sparse models, as it allows practitioners to infer key properties from high-dimensional distributions, thus providing a framework for data analysis where traditional low-dimensional assumptions do not hold. 

Moreover, the theorem can also specifically address distributions of transformed variables satisfying log-concave properties, enhancing the application of high-dimensional CLT in various statistical inference contexts."
qwen2.5:3b,"What is multiplier bootstrap?
","Document 1: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 2: 
 search trajectory followed by the conﬁguration procedure. The key idea behind
this generalisation (dubbed aggressive capping) is to additionally bound the time
allowed for evaluating conﬁgurations based on the performance observed for the
current incumbent, i.e., the best-performing conﬁguration encountered since the
beginning of the ParamILS run. The additional bound is obtained by multiplying
the performance estimate of the incumbent by a constant, bm, called the bound
multiplier. Formally, for bm = ∞, the additional bound becomes inactive (assum-
ing the performance measure is to be minimised), and the behaviour of trajectory-
preserving capping is obtained. Forbm = 1, on the other hand, a very aggressive
heuristic is obtained, which limits the evaluation of any conﬁguration to the time
spent on evaluating the current incumbent. In practice,bm = 2 appears to result
in good performance and is used as a default setting in ParamILS. Despite its
heuristic nature, this modiﬁed capping mechanism preserves the PAC property of
FocusedILS.
Although Hutter et al. [ 37] spelled out their adaptive capping mechanisms for
the performance objective of minimising a target algorithm’s mean run-time only,
these mechanisms generalise to other objectives in a rather straightforward way (a
discussion of capping in the context of minimising quantiles of run-time is found
in Ch. 7 of [32]). We note, however, that – especially when several target algorithm 

Document 3: 
 with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of
power) as the ratio p/n grows. We also show that the jackknife resampling technique for
estimating the variance of ˆβ severely overestimates the variance in high dimensions.
We contribute alternative procedures based on our theoretical results that result in
dimensionality adaptive and robust bootstrap methods.
Keywords: Bootstrap, high-dimensional inference, random matrices, resampling
1. Introduction
The bootstrap (Efron, 1979) is a ubiquitous tool in applied statistics, allowing for infer-
ence when very little is known about the properties of the data-generating distribution.
The bootstrap is a powerful tool in applied settings because it does not make the strong
assumptions common to classical statistical theory regarding this data-generating distri-
bution. Instead, the bootstrap resamples the observed data to create an estimate, ˆF, of
the unknown data-generating distribution, F. The distribution ˆF then forms the basis of
further inference.
c⃝2018 Noureddine El Karoui and Elizabeth Purdom. 

Document 4: 
 ρMB
n(A) ≤ C{D(1)
n (α) + D(1)
n,q(α)},
where the constant C depends only on a,b,d,q , and s.
Remark 4.2 (Bootstrap theorems in a.s. sense) . Proposition 4.1 leads to
the following multiplier bootstrap theorem in the a.s. sens e. Suppose that
A is a subclass of Asi(a,d) as in Proposition 4.1 and that (M.2) is satisﬁed.
We allow p= pn → ∞ and Bn → ∞ as n→ ∞ but assume that a,b,d,q,s
are all ﬁxed. Then by applying Proposition 4.1 with α= αn = n−1(log n)−2,
together with the Borel-Cantelli lemma (note that ∑ ∞
n=4 n−1(log n)−2 <∞),
we have with probability one
ρMB
n(A) =
{
O{D(1)
n (αn)} under (E.1)
O{D(1)
n (αn) ∨ D(2)
n,q(αn)} under (E.2) ,
and it is routine to verify that D(1)
n (αn) = o(1) if B2 

Document 5: 
 10 CHERNOZHUKOV, CHETVERIKOV, AND KATO
4. Empirical and Multiplier Bootstrap Theorems
So far we have shown that the probabilities P( SX
n ∈ A) can be well
approximated by the probabilities P( SY
n ∈ A) under weak conditions for hy-
perrectangles A∈ A re, simple convex sets A∈ A si(a,d), or sparsely convex
sets A∈ A sp(s). In practice, however, the covariance matrix of SY
n is typi-
cally unknown, and direct computation of P( SY
n ∈ A) is infeasible. Hence,
in this section, we derive high dimensional bootstrap theor ems which allow
us to approximate the probabilities P( SY
n ∈ A), and hence P( SX
n ∈ A),
by data-dependent techniques. We consider here multiplier and empirical
bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn 

Document 6: 
 Buja, A. and Eyuboglu, N. (1992). Remarks on parallel analysis. Multivariate Behavioral Research,
27(4):509 – 540.
Campbell, J. Y. and Yogo, M. (2006). Efficient tests of stock return predictability.Journal of Financial
Economics, 81(1):27–60.
Chamberlain, G. and Rothschild, M. (1983). Arbitrage, factor structure, and mean-variance analysis
on large asset markets. Econometrica, 51(5):1281–1304.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multiplier
bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics ,
41(6):2786 – 2819.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2017). Central limit theorems and bootstrap in
high dimensions. The Annals of Probability , 45(4):2309 – 2352.
Chernozhukov, V., Chetverikov, D., Kato, K., and Koike, Y. (2023). High-dimensional data bootstrap.
Annual Review of Statistics and Its Application , 10(1):427–449. 

","Multiplier bootstrap is a method considered first in the context of bootstrap techniques discussed, where sequences of i.i.d. N(0,1) random variables are used independently of Xn to create an estimate ˆF of the unknown data-generating distribution F. This approach allows for approximating probabilities like P(SYn ∈ A) by using data-dependent methods in high-dimensional settings.",2.2026524543762207,"The multiplier bootstrap is a resampling method used to make statistical inferences about a population when very little is known about the underlying data-generating distribution. In this procedure, one generates a sequence of independent and identically distributed (i.i.d.) standard normal random variables and uses them to create a normalized sum that helps in approximating the distribution of interest. Specifically, it involves computing a standardized version of the data, leveraging the concept of a sample mean and variance to form a new sample distribution that can then be utilized to derive estimates and confidence intervals for parameters of interest, such as regression coefficients. This technique is particularly important in high-dimensional statistics where traditional inference approaches may fail due to the dimensionality of the data."
qwen2.5:3b,"What are the methods of examining impact of imputed values on XAI methods?
","Document 1: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 2: 
 k (e.g., Onatski, 2009, 2010; Alessi et al., 2010; Owen and Wang, 2016; Dobriban and Owen, 2018;
Dobriban, 2020), tests for changepoints in the factor loadings (e.g., Breitung and Eickmeier, 2011; Bai
et al., 2022), tests for whether observed proxies of the factor returns Xt are good proxies (Bai and Ng,
2006), bootstrap methods to debias OLS estimates of the exposures (Gon¸ calves and Perron, 2020),
covariance matrix estimation (e.g., Fan et al., 2008, 2011), estimation and inference for dynamic factor
models (e.g., Bates et al., 2013; Stock and Watson, 2016) and more—see Bai and Wang (2016) for a
review. Many of these techniques leverage key results from random matrix theory (Johnstone, 2001;
Paul, 2007; Bai and Silverstein, 2010), sometimes in combination with permutation-based methods
(Buja and Eyuboglu, 1992).
Factor models are also widely discussed in the asset pricing literature (see Giglio et al. (2022) for
review), including methods for explaining excess returns (Fama and French, 1992, 2008; Welch and
Goyal, 2007; Lewellen, 2015; Freyberger et al., 2020; Gu et al., 2020), estimating factors and exposures
7 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 Journal of Clinical Epidemiology 56 (2003) 968–976
Imputation of missing longitudinal
data: a comparison of methods
Jean Mundahl Engels *, Paula Diehr
Departments of Biostatistics and Health Services, University of Washington, 1959 Northeast Paciﬁc Avenue, Box 357232, Seattle, WA 98195, USA
Accepted 11 October 2002
Abstract
Background and Objective: Missing information is inevitable in longitudinal studies, and can result in biased estimates and a loss
of power. One approach to this problem is to impute the missing data to yield a more complete data set. Our goal was to compare the
performance of 14 methods of imputing missing data on depression, weight, cognitive functioning, and self-rated health in a longitudinal
cohort of older adults.
Methods: We identiﬁed situations where a person had a known value following one or more missing values, and treated the known
value as a “missing value.” This “missing value” was imputed using each method and compared to the observed value. Methods were
compared on the root mean square error, mean absolute deviation, bias, and relative variance of the estimates.
Results: Most imputation methods were biased toward estimating the “missing value” as too healthy, and most estimates had a variance
that was too low. Imputed values based on a person’s values before and after the “missing value” were superior to other methods, followed
by imputations based on a person’s values before the “missing value.” Imputations that used no information speciﬁc to the person, such 

Document 5: 
 follows a VAR of ﬁnite order,
/Phi1( L)Ht = ut ,
where /Phi1( L) = /Phi10 − ∑h
j=1 /Phi1j Lh , with /Phi10 being possibly not an identity matrix. Bernanke et al.
(2005) propose two ways to analyze FAVAR. The ﬁrst is based on a two-step principal components
method, in which in the ﬁrst step, the method of principal components is employed to form
estimates of the space spanned by bothFt and Wt . In the second step, various identiﬁcation
schemes, such as Cholesky ordering, can be applied to obtain estimates of latent factors ˆFt ,w h i c h
are treated as observed when conducting VAR analysis of [ ˆF′
t,W′
t]′. Under suitable identiﬁcation
conditions, Bai et al. (2016) show that inferential theory can be developed for such a two-step
estimator, which differs from a standard large factor model. Conﬁdence bands for the impulse
responses can be readily constructed using the theory therein. The second method involves a
one-step likelihood approach, implemented by Gibbs sampling, which leads to joint estimation of
both the latent factors and impulse responses. The two methods can complement each other, with
the ﬁrst one being computationally simple and the second providing possibly better inference in
ﬁnite samples although with increased computational cost. 

Document 6: 
 van Buuren, 2018), which is veryﬂexible and can be implemented
with different models. While some applications can beneﬁt from
this uncertainty information, integrating this uncertainty
information in data pipelines can be challenging. From a
practitioner’s point of view, point estimates are much simpler
to integrate into conventional data pipelines. This is why we
restrict our analysis to point estimate imputations. Note,
however, that all the experiments conducted in this work
could, in principle, also be evaluated with respect to their
uncertainty estimates in a MICE setting, using the examined
imputation methods as the model underlying the MICE
estimator.
More recently, also ML approaches have increasingly been
used for imputation. Popular methods include k-nearest
neighbors (k-NNs) ( Batista and Monard, 2003 ), matrix
factorization (Troyanskaya et al., 2001 ; Koren et al., 2009 ;
Mazumder et al., 2010 ), random-forest –based approaches
(Stekhoven and Bühlmann, 2012), discriminative deep learning
methods (Biessmann et al., 2018), and generative deep learning
methods (Shang et al., 2017; Yoon et al., 2018; Li et al., 2019;
Nazábal et al., 2020; Qiu et al., 2020).
Most imputation studies provide solid experimental
evidence that the respective proposed method in the
application setting investigated outperforms other 

","When evaluating the impact of imputed (estimated or substituted) values on Explainable AI (XAI) methods, several methods can be employed to ensure that the imputation process does not distort the performance and interpretation of models. Here are some key methods:

1. **Cross-Validation**: 
   - Use k-fold cross-validation to validate the robustness of your XAI method when using different imputed values.
   - This helps in understanding how well your model generalizes with varying degrees of missing data imputation.

2. **Bootstrapping**:
   - Resample the dataset with replacement and create multiple datasets (bootstrap samples).
   - Apply the XAI methods to these resampled datasets to see if the results are consistent across different bootstrapped samples.
   - This can help in understanding the variability and stability of your model's performance due to imputed values.

3. **Validation Set Approach**:
   - Use a separate validation set not used during training (or cross-validation) to check the final performance metrics.
   - Assess how imputation affects performance on this unseen dataset.

4. **Model Sensitivity Analysis**: 
   - Test multiple imputation models with different algorithms and parameters, including single imputations and chained equations.
   - Compare their results to see which ones are most robust and consistent across various settings.
   
5. **Simulation Studies**:
   - Simulate missing values under different scenarios (e.g., varying levels of noise in the data or specific patterns of missingness).
   - Apply XAI methods to these simulations to understand how imputation impacts various metrics, such as interpretability, accuracy, and bias.

6. **Inference on Imputed Data**:
   - Ensure that statistical inference is valid when dealing with imputed datasets.
   - Use appropriate techniques for handling incomplete data within your chosen XAI framework (e.g., Rubin's Rules for multiple imputation).

7. **Sensitivity Analysis**:
   - Examine the sensitivity of interpretability metrics to changes in missing value patterns or imputation methods.
   - Assess whether small differences in how values are estimated can lead to significantly different explanations.

8. **External Validation**: 
   - If possible, use external data sources for validation purposes if your current dataset is incomplete.
   - This provides an independent check on the validity and robustness of your findings based on imputed datasets.

9. **Domain Knowledge Integration**:
   - Use domain-specific knowledge to guide imputation processes or validate them post-hoc by assessing consistency with known values in real-world scenarios.

By employing these methods, you can systematically evaluate and mitigate potential biases arising from the use of imputed values in XAI methodologies, ensuring that your explanations are reliable and meaningful.",8.267764568328857,"The impact of imputed values on XAI (explainable artificial intelligence) methods can be examined using various strategies as outlined in the provided snippets. Specifically, the methods of examination include:

1. **Comparison of Imputation Methods**: Document 1 emphasizes exploring the effects of different imputation methods on the calculation of Shapley values. By systematically contrasting various imputation strategies, researchers can evaluate how these methods influence the interpretability and robustness of outcomes in machine learning models.

2. **Feature Relevance Evaluation**: Document 3 mentions approaches that evaluate features based on their average expected marginal contribution. This can be affected by the imputed values, as the way in which features are weighted during decision-making may change depending on the imputation method used.

3. **Visual Explanations**: As noted in Document 3, using visualization techniques may also help in interpreting predictions when imputed values are involved. Visual analytics can reveal how different imputed values alter the model's predictions and subsequently affect explanations.

4. **Local Explanations**: Local explanation methodologies can provide insights into how specific imputed values affect the model's behavior around certain instances, as discussed in Document 3. This involves examining the model's workings around inputs that have been altered by different imputation methods.

5. **Statistical Comparison of Estimates**: Document 4 details comparing various imputation methods, evaluating their performance based on metrics like root mean square error and bias. This approach can also prove useful for assessing how different imputed values interact with XAI methodologies.

6. **Integration of Uncertainty**: Document 6 mentions that while point estimates are often used in practice, incorporating uncertainty estimates from multiple imputation techniques (like MICE) can provide a more thorough understanding of how imputation influences model interpretability and insights.

Through these methodological approaches, researchers can gain a better understanding of how imputation methods affect XAI techniques and the interpretability of machine learning models."
qwen2.5:3b,"What is MSE SHAP? 
","Document 1: 
 The following section proposes a uniﬁed approach that improves previous methods, preventing them
from unintentionally violating Properties 1 and 3.
4 SHAP (SHapley Additive exPlanation) Values
We propose SHAP values as a uniﬁed measure of feature importance. These are the Shapley values
of a conditional expectation function of the original model; thus, they are the solution to Equation
4
Figure 1: SHAP (SHapley Additive exPlanation) values attribute to each feature the change in the
expected model prediction when conditioning on that feature. They explain how to get from the
base value E[f(z)] that would be predicted if we did not know any features to the current output
f(x). This diagram shows a single ordering. When the model is non-linear or the input features are
not independent, however, the order in which features are added to the expectation matters, and the
SHAP values arise from averaging the φi values across all possible orderings.
8, where fx(z′) = f(hx(z′)) = E[f(z) |zS], and Sis the set of non-zero indexes in z′(Figure 1).
Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values 

Document 2: 
 the MIT license and is available at:
https://bitbucket.org/pl-edu-pw-mini-optimization/basic-pso-de-hybrid/ .
3.2. Samples archive
In order to store and eﬃciently retrieve samples, M-GAPSO utilizes a
multi-dimensional R-Tree index [34]. Due to performance reasons capacity of
the R-Tree index is limited and set by the user. After reaching the maximum
capacity, the index is restarted from scratch.
Samples archive is utilized in two scenarios. Mainly, for eﬃcient retrieval
of the nearest samples as described in Section 2.3. Subsequently those sam-
ples are utilized to ﬁt a quadratic or polynomial function model. Secondly,
samples archive serves as a cache memory, so that in the event of the algo-
rithm trying to sample the same location (as it does happen when the swarm
has nearly collapsed), it retrieves the function value from memory, saving
some budget of the ﬁtness function computations.
3.3. Restart management
M-GAPSO uses an enhanced version of JADE [5] restart manager. In
M-GAPSO the RestartManager registers iteration count intervals between
global optimum updates, considers a spread of personal best locations of
particles (eq. (10)) and additionally a spread of personal best locations val-
ues (eq. (11)). The last feature was added in order to better handle step 

Document 3: 
 ⏐⏐
(19)MSE = 1
m
m∑
i=1
(yi −¯yi)2
(20)RMSE =
√1
m
m∑
i−1
(yi −¯yi)2
(21)TPR = TP
TP + FN
Page 24 of 37Emmanuel et al. J Big Data           (2021) 8:140 
The major advantages of the MSE and RMSE is that they provide a quadratic loss func -
tion. Also, uncertainty in forecasting is measured when they are used. However, MSE 
and RMSE are highly influenced by extreme values [145]. While MAE is not influenced 
by extreme values, also a more natural measure and unambiguous [146]. Most stud -
ies in research are found to mostly use the RMSE for missing value imputation evalu -
ation [147–149]. Although some studies have proposed valid evidence against the use 
of RMSE in favour of MAE due to its less sensitive to extreme values [150]. The authors 
further advised against the reporting of RMSE in literature and strongly recommended 
the use of MAE [146, 150]. However, Chai and Draxler [145] partially disputed the con -
clusions and introduced arguments against avoiding RMSE. They contended that RMSE 
was appropriate to represent model performance than the MAE. The AUC like other 

Document 4: 
 ELSHADE-SPACMA 1.52 1.40 1.38 1.45 1.72 1.83 2.13 
GSKA 4.10 4.20 4.17 4.17 4.17 4.17 4.20 
PSO 6.03 5.97 5.97 5.97 5.93 5.93 5.87 
SSA 5.27 5.40 5.43 5.47 5.60 5.63 5.67 
jSO 2.70 2.40 2.35 2.37 2.47 2.35 2.05 
(b) 40–100% Evaluations 
ﬁtness evaluations, including the previous winners of CEC compe- 
titions (EBOwithCMAR and jSO) and the most competitive recent 
algorithm (ELSHADE-SPACMA). 
• EBOwithCMAR is the algorithm with the best results for dimensions 
10 and 30 (very close to ELSHADE-SPACMA in those dimensions), 
whereas ELSHADE-SPACMA is the best performing algorithm for di- 
mensions 50 and 100, closely followed by EBOwithCMAR and jSO. 
• For dimensions 30, 50 and 100, ELSHADE-SPACMA is the best one 
since the 40% of the budget of evaluations. Although Table 3 indi- 
cates that EBOwithCMAR obtains the best ﬁnal results, ELSHADE- 
SPACMA achieves better results during most of the search, being 
only improved by the former at the end. 
• Deciding which algorithm should be applied to a speciﬁc problem 
strongly depends on the eﬀort that can be devoted to the search. In 
this benchmark, GSKA is better when less evaluations are allowed, 

Document 5: 
 open-source innvestigate package. In the case of expected gradients, we bench-marked our imple-
mentation against a public one in the SHAP python package.
Surrogate Approaches.We consider:
• LIME (Ribeiro et al., 2016) locally approximate F around xi with a simple function, g,
that is then interpreted. LIME corresponds to: arg ming∈GL(f,g, pert(xi)) + Ω(g), where
pert(xi) local perturbations of the inputxi, and Ω(g) is a regularizer. Overall, recent work
hasshownthat, inthetabularsetting, LIMEapproximatesthecoeﬃcientsofablack-boxlinear
model with high probability. In our empirical implementation we follow the open source lime-
image package. Here to account for high dimensions, the input image is ﬁrst segmented into
50 segments and the local approximationg is ﬁt around input perturbations with50. We
experimented with 5,10,15,&25 dimensions as well. Overall, we found the LIME with50
segments to be more stable for the input data sizes that we consider. We use1000 samples in
model ﬁtting.
• SHAP (Lundberg and Lee, 2017) Similar to LIME, SHAP provides a local approximation
around a single input. The local model is then interpreted as a form of explanation. SHAP
uniﬁes LIME and several under methods under the same umbrella and turns out to be a 

Document 6: 
 4.1 | Features-oriented methods
SHapley Additive exPlanation (SHAP) (Lundberg & Lee, 2017) is a game-theoretic approach to explain ML predictions.
SHAP seeks to deduce the amount each feature contributed to a decision by representing the features as players in a
ANGELOV ET AL. 5o f1 3
 19424795, 2021, 5, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1424 by Cochrane Poland, Wiley Online Library on [28/12/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 

","MSE in this context refers to Mean Squared Error, which is a common metric used to evaluate prediction errors. In the context of SHapley Additive exPlanation (SHAP), it doesn't refer to a specific variant or modification of the SHAP method itself; rather, it's used when evaluating how well a model’s predictions align with actual values by measuring the average squared difference between them.",2.450039386749268,"The term ""MSE SHAP"" does not appear in the provided documents specifically. However, MSE (Mean Squared Error) is a common metric used to measure the accuracy of predictions in a model, calculated as the average of the squared differences between predicted and actual values. SHAP (SHapley Additive exPlanation) values, on the other hand, are a measure of feature importance that explains the contribution of each feature to the model's predictions based on game-theoretic principles. 

While there is no direct mention of ""MSE SHAP"" in the snippets, it could imply a context where both MSE (or other error metrics) are being evaluated alongside SHAP values to provide insights into model performance and feature contributions. In practice, researchers may analyze how different features (using SHAP) influence the MSE or other error metrics, but a specific definition or methodology termed ""MSE SHAP"" is not delineated in the provided excerpts."
qwen2.5:3b,"What are observations on impact of different imputation methods on explainability of models?
","Document 1: 
 relationships between observed and missing variables.
In recent years, more attention has been paid to the explainability of
the imputation method under missing data. For example, [37] introduces
DIMV, an imputation method that provides explainability and uncertainty
of imputation via conditional distribution, or [1] analyzes the effects of vari-
ous imputation and direct parameter estimation methods to the correlation
plot. Next, Hans et al. [38] introduce an explainable imputation method
that leverages association constraints within the data. However, up to our
5
knowledge, so far, there is no work on the effects of missing data on Shapley
values.
2.3. Direct missing data handling techniques without imputation
Different from the imputation approaches, methods that directly handle
missing data can have clearer implications in terms of explainability.
Specifically, Nguyen et al. [35] introduced the EPEM algorithm to esti-
mate the maximum likelihood estimates (MLEs) for multiple class monotone
missing data when the covariance matrices of all classes are assumed to be
equal. Additionally, DPER [39] addresses a more general case where miss-
ing data can occur in any feature by using pairs of features to estimate the
entries in the mean and covariance matrices. The implication to model ex-
plainability of using direct parameter estimation methods, like EPEM and
DPER, includes improved transparency and interpretability of the model’s
behavior, as these methods provide clear estimates of means and covariances, 

Document 2: 
 details of how these methods works is described by White, Royston, and
Wood (2011).
The related work shows that there exists several literature describing different
methods of data imputation. From the implementation perspective, it is also
very important to understand and evaluate performance of different imputation
methods so that appropriate method can be used while performing data mining
task. Though there exist some literatures that analyzed performance of different
imputation methods, in this paper, we intend to analyze performance of differ-
ent imputation method for numeric datasets that uses single and multiple
imputation methods namely mean imputation, median imputation, kNN impu-
tation, predictive mean matching, Bayesian Linear Regression (norm), Linear
Regression, non-Bayesian (norm.nob), and random sample.
Research Methodology
This section describes procedure followed for analyzing performance of the
imputation methods. Objective of this study is to analyze performance of imputa-
tion methods that includes: (a) Single Imputation Methods: Mean Imputation,
Median Imputation, KNN Imputation and (b) Multiple Imputation Methods:
Predictive Mean Matching (pmm), Bayesian Linear Regression (norm), Linear
Regression non-Bayesian (norm.nob), and Sample method. All these imputation
methods are applied only on numeric datasets. The datasets used in this study are
obtained from UCI Machine Learning Repository (Lichman2013). The descrip- 

Document 3: 
 The main limitations of ou r work were as follows. 
We used a data set contained only four categorical 
variables. Therefore questions whether number and 
nature of variables affected our conclusions remains 
to be addressed. In regression imputation, E-M algo-
rithm, and the MICE methods regression modelling 
has been used to draw the imputation. It is clear that, 
generally speaking, the more the number of variables 
the better the prediction. In addition, in regression 
and E-M imputations, it is as sumed that the data fol-
lows a normal distribution. This was not the case in 
our data set. This might partly explain poorer perfor-
mance of these two methods. Another limitation of 
our work was that we di d not compare performance 
of the imputation methods under different missing 
rates. One last issue was the process of model build-
ing. We developed all four models using ENTER 
method. This method fits a model including all varia-
bles offered to. Therefore, the behavior of imputation 
models under different variable selection methods 
(i.e. Backward and Forward) should be investigated. 
All issues noted affect the conclusions 5,25-28 and their 
influence should be explored in future studies.  
 Although the MICE method does not provide 
unique estimates,29 and communication of results with 
clinical audiences is not simple, evidence from litera-
ture suggested that the MICE method is the best ap-
proach to impute missing data. However, our literature 

Document 4: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 5: 
 of 10–20% in more than 75% of our experiments. This holds
for most imputation methods; we did not observe a clear
advantage for an imputation method overall. Taking into
account the considerable differences in wall-clock run time,
our results indicate that also when choosing an imputation
method that is both fast and improves downstream predictive
performance random forests would be the preferred
imputation method.
The positive impact of imputation on downstream
performance is most pronounced when the imputation
methods were trained on fully observed data. When
imputation methods were trained on incomplete data, the
positive impact of imputing missing values in the test data was
substantially lower, sometimes even negative. While this might
seem a disadvantage, we emphasize that, in many application use
cases, we can ensure that the training data be fully observed, for
instance, by acquiring more data before training the imputation
and the downstream ML model.
6.3 Limitations
Because one of the main goals of this study is a comprehensive
comparison of imputation methods on a large number of datasets
and missingness conditions, we made some decisions that limit
our results.
First, we focus on point estimates of imputed values rather than
multiple imputations because it is 1) easier to handle in automated
pipelines and 2) can be considered a more relevant scenario in real-
world applications of imputation methods. Thus, we do not
consider the inherent uncertainty of the imputation process. We
decided to measure and compare the impact imputation methods 

Document 6: 
 that is not positive definite, which is can possibly prevent further analysis such as calcu -
lating coefficients estimates [36]. Finally, pairwise deletion also known to produce low 
bias results for MCAR or MAR data [34].
Imputation
The process of imputation involves replacing missing values by some predicted values. 
The non-missing values data set is normally used to predict the values used to replace 
the missing values [8]. In the following we cover some of the most used imputation 
methods in the literature.
Simple imputation
Simple imputation approach entails replacing missing values for each individual value by 
using a quantitative attribute or qualitative attribute of all the non-missing values [37]. 
With simple imputation, missing data is handled by different methods such as, mode, 
mean, or median of the available values. In most studies simple imputation methods are 
used because of their simplicity and that they can be used as an easy reference technique 
[38]. However, simple imputation methods may produce bias or unrealistic results on a 
high-dimensional data sets. Also, with the generation of big data emerging, this method 
seems to be performing poorly and therefore is inadequate to be implemented on such 
data sets [39].
Regression imputation
Regression is one of the preferred statistical technique for handling missing values. This 
method is also termed conditional mean imputation, here missing values are replaced 
with a predicted value created on a regression model if data is missing at random. The 
overall regression process is a two-phase method: the first step, uses all the complete 

","The study reveals that different imputation methods have varying impacts on the calculation of Shapley values, which attribute feature importance in machine learning models. While imputation can improve downstream predictive performance by reducing missing data, it can also introduce biases or distortions, affecting the accuracy and reliability of Shapley values. The research suggests that choosing an appropriate imputation method is crucial for maintaining robustness in the insights derived from Shapley’s values, especially in model interpretation contexts. Random forests were found to be a preferred imputation method when trained on fully observed data, showing positive impacts across most experiments. However, this preference diminishes or can even reverse under incomplete training data conditions.",3.2205848693847656,"Different imputation methods have significant implications for the explainability of models, especially regarding the calculation of Shapley values, which attribute the contribution of features to model predictions. The choice of imputation method can introduce biases or distortions that affect the accuracy and reliability of Shapley values. 

1. **Effect on Shapley Values**: The impact of imputation methods on Shapley values has not been extensively studied despite its importance. Different imputation strategies can influence the interpretability of machine learning models and the robustness of the insights derived from Shapley values.

2. **Comparison of Methods**: Research has shown that the performance of imputation methods varies, and while some methods like random forests can improve downstream predictive performance, their effectiveness can diminish if trained on incomplete data. This highlights the need for careful selection of imputation methods based on the nature of the dataset.

3. **Explainability versus Performance**: Methods that provide clearer estimates of means and covariances, like direct parameter estimation techniques (e.g., EPEM and DPER), tend to offer better transparency and interpretability. On the other hand, simpler imputation methods might lead to unrealistic results in high-dimensional datasets, which could impact the explainability in a negative manner.

4. **Limitations of Imputation Methods**: Specific techniques such as regression and multiple imputation methods can perform poorly if the data does not meet the underlying assumptions (e.g., normal distribution) or if the imputation method does not adequately capture the complexity of the dataset. This suggests that the imputation methods used have a direct correlation with the explainability and interpretation of the resulting models.

Overall, selecting the appropriate imputation method is crucial for maintaining the integrity and interpretability of machine learning models that rely on imputed data."
qwen2.5:3b,"Does usage of different imputation methods on missing values has impact on SHAP feature importance?
","Document 1: 
 especially at higher missing rates. MICE and DIMV often show similar
patterns, possibly due to the fact that MICE is based on regression and
DIMV is based on a conditional Gaussian formula. MissForest and SOFT-
IMPUTE sometimes preserve feature rankings better than simpler methods,
but this is not consistent across all scenarios.
The variability in results between methods and missing rates underscores
the need to evaluate imputation effects when using Shapley values for model
interpretation.
The following discussion is structured around our result and the specific
pitfalls that may arise due to incomplete understanding of the relationship
between missing data, imputation methods, and Shapley values. We high-
23
light how different approaches can lead to vastly different interpretations, how
dataset characteristics and missing rates affect results, and the importance
of considering both imputation accuracy and interpretability preservation.
Pitfall 1: Assume the neutrality of the imputation method.Our
study reveals that different imputation methods can significantly alter Shap-
ley values and, consequently, the interpretability of the model. For instance,
mean imputation tends to distort feature importances, especially at higher
missing rates, while methods like MICE and DIMV often show similar pat-
terns. This underscores the importance of carefully considering the imputa-
tion method when using Shapley values for model explanation, as the choice
can lead to vastly different interpretations of feature importance.
Pitfall 2: Overlooking data set dependency. We observed that 

Document 2: 
 Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values
for features not in the set S. Since most models cannot handle arbitrary patterns of missing input
values, we approximate f(zS) with E[f(z) |zS]. This deﬁnition of SHAP values is designed to
closely align with the Shapley regression, Shapley sampling, and quantitative input inﬂuence feature
attributions, while also allowing for connections with LIME, DeepLIFT, and layer-wise relevance
propagation.
The exact computation of SHAP values is challenging. However, by combining insights from current
additive feature attribution methods, we can approximate them. We describe two model-agnostic
approximation methods, one that is already known (Shapley sampling values) and another that is
novel (Kernel SHAP). We also describe four model-type-speciﬁc approximation methods, two of
which are novel (Max SHAP, Deep SHAP). When using these methods, feature independence and
model linearity are two optional assumptions simplifying the computation of the expected values
(note that ¯Sis the set of features not in S):
f(hx(z′)) = E[f(z) |zS] SHAP explanation model simpliﬁed input mapping (9) 

Document 3: 
 dependsonthesizeofthedatasetandmissingvaluespresent
inthem.Here,theauthorsanalyzedoftheperformanceoffour
imputationpackagesinRontwodatasetsusingthreepredictive
algorithms-LogisticRegression,SupportVectorMachineandAr-
tificialNeuralNetwork.Theauthorsmeasuretheperformanceof
thepackagesintermsofimputationtime,imputationefficiency
andeffectonthevariance.Theauthorsalsoshowthattheper-
formance of an imputation method depends on the predictive
algorithm which is a function of dataset characteristics. There
aredifferentwaysofhandlingmissingdata[25–27,29–32].The
mostsimpleapproachistouseonlythesubsetthatiscomplete;
inotherwords,suchamethodignoresallfeaturevectorswith
missing values and the remaining subset is used for analysis.
Althoughverysimple,suchanapproachwillbeusefulonlywhen
asmallfractiondatapointshavemissingvalues.Amoreuseful
and popular approach is to impute the missing values using
differenttechniquesandthenusethedatasetconsistingofboth
featurevectorswithoutmissingvaluesandthefeaturevectors
with imputed missing values. These techniques often depend
ontheproblemthatonelikestosolveusingthedataset.For
example,toclusteranincompletedataset,twotypesofframe-
works may be adopted: single view clustering and multi-view 

Document 4: 
 ley values, ranging from (−0.5) to around (+1.5) across all missing rates, em-
phasizing its strong influence on the model outputs. The imputation methods
show similar patterns with Shapley values for key features slightly reduced
compared to the XGBoost model, but they still highlight the importance of
Latitude, Longitude, and MedInc in the predictive modeling.
Considering the results at each missing rate, we observed similar distribu-
tions in the Shapley values for Latitude, Longitude, and MedInc on both the
15
(a) Original
 (b) Xgboost
 (c) Mean Imputation
(d) MICE
 (e) DIMV
 (f) missForest
(g) SOFT-IMPUTE
Figure 1: Global feature importance plot on the California dataset with the missing rate
r = 0.2
original and imputed data using various imputation methods, generally fluc-
tuating around zero. However, XGBoost shows a distribution that is more
skewed from zero. For example, at a missing rate of 20% in figure 5, the
Shapley values in the original data for Latitude and Longitude range from
(−0.75) to (+0 .75), and for MedInc from ( −0.5) to (+0 .7). Likewise, the
ranges for models with imputation methods are [ −0.6; 0.6] and [ −0.4; 0.6],
respectively, indicating that the imputation does not significantly alter the 

Document 5: 
 dataset. By displaying each data point as a dot and arranging these
dots to show the distribution of Shapley values for each feature, the
beeswarm plot offers a comprehensive overview of feature importance
and interaction effects. It allows for the identification of patterns and
outliers, helping to understand the behavior of the model with respect
to individual features. Additionally, the plot can reveal how specific
features interact with others, providing insights into complex depen-
dencies within the data.
3.2. Imputation techniques
In this section, we briefly summarize the missing data handling methods
that we will examine for the effects on Shapley values. The methods being
investigated consist of a method that can directly learn from missing data,
such as XGBoost, to a simple imputation method as Mean Imputation, as
well as the widely used or recently developed imputation techniques, such as
8
MICE, DIMV, missForest, and SOFT-IMPUTE. The details of the methods
are as follows:
• XGBoost (Extreme Gradient Boosting) [47] is a powerful and efficient
algorithm that belongs to the family of gradient boosting techniques.
It builds an ensemble of decision trees, where each tree corrects errors
made by the previous ones, enhancing predictive accuracy. XGBoost
stands out for its speed and performance, employing advanced features
like tree pruning, regularization, and parallel processing, which help
in reducing overfitting and handling large-scale data. XGBoost can
handle missing data directly. 

Document 6: 
 2018). However, in that work, the authors only considered text data
as an inputﬁeld to an imputation method, not as a column that
could be imputed. Generally, most modern ML applications that
involve text data are based on rather sophisticated natural language
models. Combinations of such models with tabular data are an
importantﬁeld of research (Yin et al., 2020) but beyond the scope of
most imputation research so far.
Third, to measure the imputation impact on the downstream
performance, we discarded and imputed values in only a single
column. Therefore, the impact depends heavily on the chosen
column’s importance (e.g., see the work ofSchelter et al. (2021)).
Generally, the impact when using an imputation model could
vary when multiple columns are affected by missing values.
7 CONCLUSION
In this study, we developed an experimental protocol and
conducted a comprehensive benchmark for imputation
methods comparing classical and modern approaches on a
large number of datasets under realistic missingness conditions
with respect to the imputation quality and the impact on the
predictive performance of a downstream ML model. We also
evaluated how the results changed when the imputation and
downstream model were trained on incomplete data.
Our results can be summarized in two mainﬁndings. First, we
demonstrate that imputation helps to increase the downstream
predictive performance substantially regardless of the
missingness conditions. When training data are fully observed,
our results demonstrate that, in more than 75% of our 

","Yes, the usage of different imputation methods can significantly alter SHAP (SHapley Additive exPlanations) feature importance. The study found that while SHAP values generally fluctuated around zero for key features like Latitude, Longitude, and MedInc across various imputation techniques at a missing rate of 20%, XGBoost showed more skewed distributions. This indicates different methods can highlight or modify the importance of certain features differently. Therefore, choosing an appropriate imputation method is crucial to accurately interpret feature importance using SHAP.",2.892460823059082,"Yes, the usage of different imputation methods on missing values does have an impact on SHAP feature importance. Various imputation techniques can significantly alter the calculated SHAP values, leading to different interpretations of feature importance. For instance, mean imputation can distort feature importances, especially at higher missing rates, while methods like MICE and DIMV often produce similar patterns. Therefore, it is crucial to consider the choice of imputation method when interpreting SHAP values for model explanations, as different methods can yield vastly different results."
qwen2.5:3b,"How sampling for LIME for single observation is done?
","Document 1: 
 πx. We sample instances around x′ by drawing nonzero
elements of x′ uniformly at random (where the number of
such draws is also uniformly sampled). Given a perturbed
sample z′∈{0, 1}d′
(which contains a fraction of the nonzero
elements of x′), we recover the sample in the original repre-
sentation z ∈Rd and obtain f(z), which is used as a label for
the explanation model. Given this dataset Zof perturbed
samples with the associated labels, we optimize Eq. (1) to
get an explanation ξ(x). The primary intuition behind LIME
is presented in Figure 3, where we sample instances both
in the vicinity of x (which have a high weight due to πx)
and far away from x (low weight from πx). Even though
the original model may be too complex to explain globally,
LIME presents an explanation that is locally faithful (linear
in this case), where the locality is captured by πx. It is worth
noting that our method is fairly robust to sampling noise
since the samples are weighted byπ
x in Eq. (1). We now
present a concrete instance of this general framework.
3.4 Sparse Linear Explanations
For the rest of this paper, we let G be the class of linear
models, such that g(z′) = wg ·z′. We use the locally weighted 

Document 2: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 3: 
 DATA REDUCTION
Data reduction is typically conducted in two directions, i.e., row-
wise for data sample reduction and column-wise for data variable
reduction. Various data sampling techniques can be applied for
row-wise data reduction, such as random and stratiﬁed sampling
(Fan et al., 2015b). Random sampling is typically used to simulate
a random process in data sample selection. By contrast, stratiﬁed
sampling is performed to maintain the proportions of data
samples corresponding to different categories. For instance,
assuming there are 100 and 50 data samples in the original
data set for category A and B respectively, a strati ﬁed
sampling process with a selection rate of 0.7 will randomly
select 70 out of 100, and 35 out of 50 data samples in
category A and B for further analysis. Such techniques can be
applied for sampling time series data as well. In such a case, it is
recommended to transform the original sequence data into
shorter subsequences before data sampling, since direct
sampling may lead to temporal information loss.
There are three main methods of column-wise data variable
reduction. Theﬁrst is to use domain knowledge to directly select
variables of interests. The second is to use statistical feature
selection methods to select important variables for further
analysis. The third is to adopt feature extraction methods to
construct useful features for data analysis. The details of the latter
two methods are shown inFeature Selection Techniques, Feature
Extraction Techniques.
Feature Selection Techniques
As shown inTable 2, feature selection methods can be broadly 

Document 4: 
 the change in the conditional expectation of all features given
a subset of features. In other words, this calculation ignores
the nodes of features that are not present in the sample subset
and calculate change on the conditional expectation, given
this subset sample.
5 Experiments
The main steps of the experiments consisted of data acqui-
sition, data pre-processing, model tuning, model evaluation,
and interpretation of models and their predictions.
All the experiments ran in a free tier Google Colab
environment, which consisted of an Intel Xeon 2.34Ghz
quad-core processor, 25Gb of RAM, and Linux Ubuntu 18.04
operational system. The algorithms were implemented in
Python 3.7 using PIP packages. Implementation details can
be found here.
5.1 Data Acquisition
The experiment was executed with data provided by the
COVID-19 Data Sharing/BR, which is an initiative of the São
Paulo Research Foundation (FAPESP) in collaboration with
the University of São Paulo. The dataset consisted of sus-
pected COVID-19 patients admitted to the Hospital Israelita
Albert Einstein from February 2020 to June 2020. Addition-
ally, we conﬁrmed that it is possible to have two samples from
the same patient but from different days. It happens because
a patient can be admitted more than once in the same unit,
and also, a doctor can request a COVID-19 for the patient
more than once while they are still under treatment. 

Document 5: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 6: 
 Method
Used for Data Imputation 10% 20% 30% 40% 50%
Mean Imputation 0.068072 0.106639 0.120149 0.142043 0.160996
Median Imputation 0.069302 0.108637 0.123685 0.141440 0.165698
KNN Imputation 0.013803 0.025372 0.035587 0.047773 0.066471
Predictive Mean Matching(pmm) 0.017844 0.035328 0.044072 0.057366 0.081049
Bayesian Linear Regression(norm) 0.017917 0.033387 0.046552 0.057673 0.077796
Linear Regression, non-Bayesian (norm.nob) 0.017933 0.033459 0.044446 0.055386 0.072130
Random sample from observed values(sample) 0.072683 0.119587 0.131316 0.153484 0.169020
Figure 1. Plot of imputation method versus mean NRMSE for wine dataset.
Figure 2. Plot of imputation method versus mean NRMSE for glass dataset.
924 A. JADHAV ET AL.
given in ascending order of Mean NRMSE. It means that lowest Mean
NRMSE value get first rank.
Tables 7– 11 provide rank of imputation method for varying percentage of
missing data for five different datasets. Each table indicates performance of
imputation method on different datasets for given percentage of imputed
data. The reason for doing this is to assess consistency in performance of the 

","Lime (Local Interpretable Model-agnostic Explanations) works by generating synthetic data points that are similar to the original data point but fall within the same feature space. Here's a simplified explanation of how Lime generates these synthetic samples for a single observation:

1. **Feature Space Sampling**: For each feature in your dataset, a random perturbation is generated. This perturbation ensures that the new sample remains close to the original one while deviating along at least one feature dimension.

2. **Uniform Distribution**: Typically, this uniform distribution ensures that the sampled data points are randomly and uniformly distributed within a certain range around the original feature values. The size of this range can be controlled by parameters in the Lime implementation.

3. **Synthetic Data Generation**: For each feature being perturbed, the original value is adjusted according to the randomly generated perturbation. This creates new synthetic samples that are close to the original but different along at least one feature dimension.

4. **Model Prediction**: After generating these synthetic samples, you would typically predict the model’s output for each of them (e.g., if it's a classification task) and calculate how much these predictions deviate from the original prediction.

5. **Weighting & Explanation Generation**: Finally, Lime uses a weighted sum based on the distances between the perturbed features to explain why the model made its prediction for the original data point by attributing importance to each feature according to how changing that feature influences the outcome of the model’s predictions.

In summary, for a single observation with multiple features (e.g., an image), Lime would generate several synthetic images that are very similar but differ in at least one pixel value compared to the original. This allows Lime to understand which specific feature or combination of features contributed most to the model's prediction without needing to rely on complex high-level models for explanation.",5.87231969833374,"In LIME (Local Interpretable Model-agnostic Explanations), sampling for a single observation, denoted as x, involves drawing instances around a perturbed version of that observation (x′). The process includes sampling nonzero elements of x′ uniformly at random, with the number of draws also sampled uniformly. A perturbed sample z′ is created, which includes a fraction of the nonzero elements from x′, and this is then rephrased back to the original representation as z. The output of the original model for this representation, f(z), serves as the label for the explanation model. The sampling process captures instances close to x (high weight from the locality function πx) and farther away (lower weight), ensuring that the explanation remains locally faithful even in complex models."
qwen2.5:3b,"How can LIME lead to improvement of model by non-experts?
","Document 1: 
 In this paper, we propose providing explanations for indi-
vidual predictions as a solution to the “trusting a prediction”
problem, and selecting multiple such predictions (and expla-
nations) as a solution to the “trusting the model” problem.
Our main contributions are summarized as follows.
•LIME, an algorithm that can explain the predictions ofany
classiﬁer or regressor in a faithful way, by approximating
it locally with an interpretable model.
•SP-LIME, a method that selects a set of representative
instances with explanations to address the “trusting the
model” problem, via submodular optimization.
•Comprehensive evaluation with simulated and human sub-
jects, where we measure the impact of explanations on
trust and associated tasks. In our experiments, non-experts
using LIME are able to pick which classiﬁer from a pair
generalizes better in the real world. Further, they are able
to greatly improve an untrustworthy classiﬁer trained on
20 newsgroups, by doing feature engineering using LIME.
We also show how understanding the predictions of a neu-
ral network on images helps practitioners know when and
why they should not trust a model.
2. THE CASE FOR EXPLANATIONS
By“explaining a prediction”, we mean presenting textual or
visual artifacts that provide qualitative understanding of the
relationship between the instance’s components (e.g. words 

Document 2: 
 that is available.
2.3 Improving existing data
In addition to searching and labeling datasets, one can
also improve the quality of existing data and models. This
approach is useful in several scenarios. Suppose the target
application is novel or non-trivial where there are no relevant
datasets outside, or collecting more data no longer beneﬁts
the model’s accuracy due to its low quality. Here, a better
option may be to improve the existing data. One effective
approach is to improve the labels through re-labeling. Sheng
et al. [ 146] demonstrates the importance of improving labels
by showing the model accuracy trends against more training
examples for datasets with different qualities. As the data
quality decreases, even if more data is used, the accuracy of
the model does not increase from some point and plateaus. In
this case, the only way to improve the model accuracy is to
improve the label quality, which can be done by re-labeling
and taking majority votes on multiple labels per example. In
fact, one could clean the entire data including labels, which
naturally leads to the next section where we cover data vali-
dation, cleaning, and integration.
3 Data validation, cleaning, and integration
It is common for the training data to contain various errors.
Machine learning platforms like TensorFlow Extended
(TFX) [13] have data validation [ 117] components to detect
such data errors in advance using data visualization and
schema generation techniques. Data cleaning can be used 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 commonly held misconception is that ML is a one-step training pro-
cedure that accepts data as input and generates a model as output.
Many research efforts, both within the DB community and in other
communities, focus on this simplified view and aim to improve the
effectiveness of ML training, e.g., by generating more powerful mod-
els through better training algorithms, or by reducing its resource
footprint through various software and hardware optimizations.
This view is reinforced by various leaderboard-style competitions
popular among practitioners (e.g., Kaggle), and academics (e.g., ML
benchmarks and KDD competitions).
At the same time, there is evidence from practitioners [16, 19, 48]
that ML deployments in production are significantly more compli-
cated. Specifically, ML in production involves pipelines with many
interlocking steps, only one of which is training. This has spurred
on the development of many end-to-end ML systems (e.g., TFX [16],
MLFlow [52], Microsoft Azure ML [8], AWS Sagemaker [34]) and
open-source ML libraries (e.g., MLlib [6], MetaFlow [2], and Scikit-
Learn [7]), all of which provide native support for data pre-processing,
data validation, model validation, and model deployment, in addi-
tion to model training, all within a single environment.
As an example, TFX [16] includes pipeline steps that perform 

Document 5: 
 that is, regardless of the model class of the original black-box. Usually,G is chosen to be a constrained class of intrinsi-
cally interpretable models (Section4.2), for example, linear models or GAMs. Put simply, LIME trains many interpret-
able surrogate models to approximate a black-box modelf /C1ðÞ locally. During training, instances are sampled around
each data pointxi weighted byπxi . In addition to local explanations given byξ /C1ðÞ , Ribeiro et al. (2016) introduce a pro-
cedure for obtaining a global understanding of the modelf /C1ðÞ : given a limited budget, their algorithm picks several
explanations based on greedy submodular optimization (Krause & Golovin,2014) and aggregates them into global vari-
able importances, similar to the random forest feature importance (Breiman,2001).
DeepLIFT
Shrikumar et al. (2017) introduce an efficient method for disentangling contributions of inputs in a neural network—
deep learning important features (DeepLIFT). Compared to LIME, DeepLIFT is not model-agnostic since it is explicitly
tailored to neural networks; it also requires a reference, or baseline, data point. While in natural images an all-black
image is typically used as a baseline input, the choice of a reference might not be so trivial for more specialized datasets 

Document 6: 
 SP-LIME
RP-LIME
No cleaning
Figure 10: Feature engineering experiment. Each
shaded line represents the average accuracy of sub-
jects in a path starting from one of the initial
10 sub-
jects. Each solid line represents the average across
all paths per round of interaction.
person inspects ( B) to 6. The position of each algorithm
and the order of the instances seen are randomized between
subjects. After examining the explanations, users are asked
to select which algorithm will perform best in the real world.
The explanations are produced by either greedy (chosen
as a baseline due to its performance in the simulated user
experiment) or LIME, and the instances are selected either
by random (RP) or submodular pick (SP). We modify the
greedy step in Algorithm 2 slightly so it alternates between
explanations of the two classiﬁers. For each setting, we repeat
the experiment with 100 users.
The results are presented in Figure 9. Note that all of
the methods are good at identifying the better classiﬁer,
demonstrating that the explanations are useful in determining
which classiﬁer to trust, while using test set accuracy would
result in the selection of the wrong classiﬁer. Further, we see
that the submodular pick (SP) greatly improves the user’s
ability to select the best classiﬁer when compared to random
pick (RP), with LIME outperforming greedy in both cases.
6.3 Can non-experts improve a classiﬁer? 

","LIME helps non-experts understand the relationships between input features and predictions by approximating black-box models locally with interpretable ones. This allows them to see why certain inputs are influencing a prediction, enabling users like non-experts to identify which part of a paired classifier might generalize better in real-world scenarios. By using LIME-generated explanations, they can also make informed decisions on which classifier is more trustworthy for deployment, improving model performance without needing advanced expertise.",2.5336034297943115,"LIME (Local Interpretable Model-agnostic Explanations) allows non-experts to understand and interpret the predictions of any classifier or regressor. By providing clear explanations for individual predictions, non-experts can identify which features are most influential in the model's decisions. This understanding enables them to engage in feature engineering by modifying the input features or improving the labeling of the data based on the insights gained from LIME. In experiments, it was shown that non-experts could effectively select better-performing classifiers by using LIME to analyze the models, ultimately leading to improved model accuracy and trustworthiness. The ability to explain predictions empowers users to make informed decisions about model improvement, even without deep expertise in machine learning."
qwen2.5:3b,"What is Explainable Boosting Machine model?
","Document 1: 
 feature x2 there is a weight β2 and β1 is ten times greater than
β2, it is reasonable to say that the feature x1 is considerably
more important to the decision of the model. That is, x1 will
have ten times more impact than x2. Therefore, it is possible
to make sense of a global feature importance by analyzing
the values of the β feature weights. One limitation of this
model is that due to its linear nature, local explanations can-
not be directly assessed, that is because the model adjusts the
feature importances as being the same for the entire features
distribution.
4.2 Explainable Boosting Machine
The explainable boosting machine (EBM) proposed in Nori
et al. ( 2019) is a glass-box-type model that leverages state-of-
the-art performing algorithms, such as boosting and bagging,
while maintaining intrinsic interpretability. The main idea
of this model is to build a generalized additive model with
interactions ( GA 2 M) by executing a round-robin training
procedure on one feature at a time, using a meager learning
rate. Its general is given in the form of:
g(E[y]) = β0 + /Sigma1f j (x j ) + /Sigma1fij (xi , x j ) (3)
where g is the link function and f j and fij are called smooth
functions. This approach brings two major improvements 

Document 2: 
 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
https://doi.org/10.1007/s40313-021-00858-y
Explainable Artiﬁcial Intelligence for COVID-19 Diagnosis Through
Blood Test Variables
Lucas M. Thimoteo 1 · Marley M. Vellasco 1 · Jorge Amaral 2 · Karla Figueiredo 3 · Cátia Lie Yokoyama 4 ·
Erito Marques 2
Received: 30 April 2021 / Revised: 1 October 2021 / Accepted: 20 October 2021 / Published online: 3 January 2022
© Brazilian Society for Automatics–SBA 2021
Abstract
This work proposes an explainable artiﬁcial intelligence approach to help diagnose COVID-19 patients based on blood test
and pathogen variables. Two glass-box models, logistic regression and explainable boosting machine, and two black-box
models, random forest and support vector machine, were used to assess the disease diagnosis. Shapley additive explanations
were used to explain predictions for the black-box models, while glass-box models feature importance brought insights into
the most relevant features. All global explanations show the eosinophils and leukocytes, white blood cells are among the
essential features to help diagnose the COVID-19. Moreover, the best model obtained an AUC of 0.87.
Keywords COVID-19 diagnosis · Machine learning · Explainability · Interpretability · Shapley additive explanations ·
Explainable boosting machine 

Document 3: 
 machine learning(Vol. 37, pp. 1718–1727). PMLR.
Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2020). Explainable AI: A review of machine learning interpretability methods.
Entropy, 23(1), 18.https://doi.org/10.3390/e23010018
Lipovetsky, S., & Conklin, M. (2001). Analysis of regression in game theory approach.Applied Stochastic Models in Business and Industry,
17(4), 319–330. https://doi.org/10.1002/asmb.446
Lipton, Z. C. (2018). The mythos of model interpretability.Queue, 16(3), 31–57. https://doi.org/10.1145/3236386.3241340
Liu, K., Sadoune, N., Rao, N., Greitemann, J., & Pollet, L. (2021). Revealing the phase diagram of Kitaev materials by machine learning:
Cooperation and competition between spin liquids.Physical Review Research, 3(2), 023016. https://doi.org/10.1103/physrevresearch.3.
023016
Liu, S., Kailkhura, B., Loveland, D., & Han, Y. (2019). Generative counterfactual introspection for explainable deep learning. In2019 IEEE 

Document 4: 
 atively to predict and impute each missing feature
from the other features until the imputed values con-
verge. MissForest is a popular imputation method
as it is capable of capturing non-linear and interac-
tion eﬀects between features to improve imputation
accuracy, and can be applied to mixed data types
(continuous and discrete). Note that, the framework
of MissForest is similar to that of MICE (Van Buuren
and Groothuis-Oudshoorn, 2011) — the only diﬀer-
ence is MissForest uses random forest while MICE
uses linear model as base model for imputation.
KNN imputation imputes the missing values by the
mean value of its K nearest neighbors in the training
set. The distance of two samples is measured on the
non-missing features in both samples. KNN imputa-
tion is fast and accurate but requires choosing a good
distance metric and tuning the hyperparameter K.
3.3. Explainable Boosting Machines
The methods proposed in this work are based on one
interpretable machine learning model, the Explain-
able Boosting Machine (EBM).
Suppose an input sample is denoted as (x,y), where
x is the p dimensional feature vector and y is the
target. Denote the jth dimension of the feature vector
as xj. Then a generalized additive model (GAM), 

Document 5: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 6: 
 show examples where all models correctly predicted the
123
634 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
Fig. 1 Global interpretation for logistic regression. Top: dataset with blood tests and pathogens; bottom: dataset with only blood tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 635
Fig. 2 Global interpretation for explainable boosting machine. Top: dataset with blood tests and pathogens; Bottom: dataset with only blood tests
outcome, some models failed, while others succeeded, all
models failed.
Thus, we will show the reader how one can interpret local
explanations for the COVID-19 diagnosis in a myriad of sit-
uations. In other words, we want to show that although the
overall results are really good, the models still make mistakes
and get confused with the features, and local explanations
can help with identify that. If the reader wants to check more
examples, we encourage seeing our GitHub, provided at the
beginning of the Experiments section.
Figure 5 shows explanations for a given patient that led to a
true-positive diagnosis and Fig. 6 for a true-negative patient.
Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis. 

","Explainable Boosting Machine (EBM) is a machine learning algorithm that aims to provide better interpretability while maintaining the predictive performance of traditional boosting methods. It was developed as an improvement over existing models like Gradient Boosted Decision Trees, by making it more interpretable.

Key features and benefits of Explainable Boosting Machines include:

1. **Interpretability:** EBM attempts to make complex machine learning models easier for humans to understand. This is achieved by creating a tree-like structure that helps explain the decision-making process behind predictions.

2. **Modularity:** The model can be split into smaller, more interpretable components (such as Decision Trees) which makes it easier to analyze and understand how different parts of the model contribute to final outcomes.

3. **Feature Importance:** EBM provides feature importance scores that help identify which features are most influential in making predictions.

4. **Out-of-the-box models:** There exist pre-trained Explainable Boosting Machine models for common tasks like classification, regression, etc., without requiring users to construct the model from scratch or tune hyperparameters extensively.

5. **Simplicity and Efficiency:** Despite its interpretability benefits, EBM still offers good performance, making it useful in practical applications where understanding of how predictions are made is important but not at the expense of computational efficiency and predictive accuracy.

EBM combines elements of both machine learning algorithms and model-agnostic methods for feature importance calculation. It uses techniques like tree boosting to create a flexible and powerful prediction function while incorporating interpretability features such as partial dependence plots, interaction trees, or global rule lists that describe complex interactions between input variables.",5.236999988555908,"The Explainable Boosting Machine (EBM) is a type of glass-box model that combines boosting and bagging techniques while maintaining interpretability. It builds a generalized additive model with interactions (GA2M) by training on one feature at a time using a small learning rate. The model is defined mathematically as g(E[y]) = β0 + Σ f_j(x_j) + Σ f_ij(x_i, x_j), where g represents the link function, and f_j and f_ij are smooth functions associated with the features. EBMs provide useful insights into feature importance, allowing for an understanding of the relevance of different features in model predictions, while being capable of achieving high predictive performance similar to that of black-box models."
qwen2.5:3b,"How can EBMs help with healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 icians tend to focus on abnormal ﬁndings.
Figure 3: EBM shape function of “heart rate” for
predicting pneumonia mortality risk. Blue curve is
the original shape function; red curve is the edited
shape function.
For example, this happens to a pneumonia mortal-
ity risk dataset (Cooper et al., 2005). The blue curve
in Figure 3 shows what an EBM model has learned
for predicting pneumonia mortality as a function of
heart rate. As expected, risk is elevated for patients
with abnormally low (10-30) or high heart rate (125-
200). The graph, however, shows a surprising region
of ﬂat risk between heart rate 38 and 125, which is a
normal heart rate for patients in a doctor’s oﬃce.
Moreover, the model surprisingly predicts patients
who have normal heart rate are at elevated risk: it
adds 0.22 to the risk for patients in this region.
On further inspection, it turns out that there are
no patients in the data set with heart rates between
38 and 125, and 91% of patients are missing their
heart rate which has then been coded as zero. In
other words, there are no data to support the model
in the normal range of heart rate 38-125, and instead
the patients who would be in this range are all coded
as zero in the data and on the graph. This explains
why the model predicts the lowest risk = -0.04 for
patients with heart rate = 0, because these are the
patients who actually have normal heart rates. 

Document 3: 
 indicate the existence of the disease.
Our results indicated that the proposed system could
indeed be used to detect COVID-19 patients using clinical
and laboratory data that is commonly used in clinical prac-
tice. In terms of healthcare, our approach could represent a
new method that can help by quickly screening patients with
COVID-19. This can be specially advantageous to expand
testing protocols to areas where there is not available tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 641
Fig. 8 Local explanations for
failed diagnosis on hemogram
dataset. Top: EBM; middle: RF;
bottom: SVM
for the local population. Additionally, situations where lab-
oratory workloads are heavy, and RT-PCR tests are scarce
could also beneﬁt from rapid blood sample tests. Therefore,
it would help reduce the burden in the healthcare system
and promote the optimal utilization of healthcare resources.
Besides, we think that is possible to improve the results with
the addition of the clinical signs such as the proportion of
cough, hyperthermia, myalgia, asthenia, diarrhea and confu-
sion.
We look forward to including more COVID-19 test types
as well as data from different geographical and socioeco-
nomic regions from Brazil. Due to the continental size of the
country, the epidemic dynamics changes drastically between
regions and our proposed approach might show different
performance and also different explanations. Moreover, we
would like to increase more blood test features in order 

Document 4: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 5: 
 Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis.
For the true positive, low levels of white blood cells and
elevated values of red cells greatly impact the positive out-
come for this patient, for all models.
For the true negative, the white blood cells are also respon-
sible for the majority impact on models output. Interestingly,
the SVM was the only model that took in consideration the
monocytes and sex of the patient as relevant features.
Figure 7 shows a negative patient where the EBM cor-
rectly identiﬁed and RF and SVM classiﬁed as positive. Even
though EBM got the classiﬁcation right, we notice that sev-
eral variables have positive contributions, similarly to the
SVM and RF models. Also, we see that the model output for
SVM and RF are nearly 0.5. Although not reported in the
ﬁgure, the model output for the EBM is 0.48. Therefore, this
patient lies in a low conﬁdence zone for all models.
Figure 8 shows a positive patient that all the models failed
to diagnose, for the hemogram dataset. We notice that all
models predicted a score near to 0 probability of a positive
case and most of the features actually contributed to a neg-
ative diagnosis in all cases. This implies that there might be 

Document 6: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

","EBMs help with healthcare data by providing interpretable explanations for predictions. They learn each feature function using modern machine learning techniques like bagging and gradient boosting, which allows them to automatically detect and include pairwise interaction terms. This increases accuracy while maintaining intelligibility. The smooth functions of EBMs can be visualized together on the same plot as observed vs imputed groups, making it easier to see how different imputation methods affect model predictions. This allows healthcare professionals to understand potential issues with models that might not be obvious otherwise.",2.862872362136841,"EBMs (Explainable Boosting Machines) can significantly assist with healthcare data by providing interpretable and editable models that detect potential issues arising from missing values or imputation. They capture subtle anomalies in data that may go unnoticed by simpler models like linear regression or decision trees. EBMs are particularly beneficial in high-stakes environments, such as medical care, where the costs of poor predictions can be severe.

One key advantage of EBMs is their ability to reveal the consequences of different imputation methods, thereby illuminating the influence of missing data on model predictions. For instance, when analyzing datasets related to pneumonia mortality risk, EBMs can highlight particular patterns, such as the abnormal prediction of risk for patients with normal heart rates due to missing data representation. This interpretability helps healthcare professionals make informed decisions based on model outputs.

Furthermore, EBMs can automatically include interaction terms among features, thereby enhancing accuracy while maintaining intelligibility. Their flexibility and enhancement over traditional generalized additive models mean they can adapt to various healthcare applications, improving patient screenings and enabling better resource allocation in testing protocols. Overall, the use of EBMs equips healthcare practitioners with insights that promote safer and more effective clinical decision-making."
qwen2.5:3b,"How can be EDMs used in imputing missing values in healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 and class median impute the missing value as the mean/
median of known Depression values at that time point, in the
appropriate class. Thehot deck [12] method, which is used
to impute labor force items in the Current Population Survey
[13], involves a recipient (the person with missing data) and
a donor (another person in the same imputation class, whose
value is known). We selected a donor by sampling without
replacement from the recipient’s class, and replaced the re-
cipient’s missing value with the donor’s value at the appro-
priate time point. These class imputations imply that persons
with missing data are a random sample of the persons in
their class[14].
Another baseline approach is regression imputation. The
variable of interest (known Depression value for the year in
which the “missing value” occurred) is regressed on baseline
covariates, and the resulting equation is used to estimate
the missing values for that year. We regressed Depression
on baseline Depression, age, gender, and health status. Data
for persons missing any covariate information were not im-
puted. Two different types of regression imputation were
performed.Regressionassigns the person’s predicted value to
the “missing value.” Persons with the same covariates will
have exactly the same imputed value. This can lead to the
variance of the imputed data set being too small, yielding
inappropriately small standard errors and P-values at the 

Document 3: 
 orange) when predicting whether HR is missing. In
the plot, the curve of the imputed group is signiﬁ-
cantly higher than that of the observed group, again
suggesting that when the blood pressure of the pa-
tients is missing, their heart rate is also more likely
to be missing. This eﬀect is strong, as the maximum
gap between the two curves is approximately 2.5 (1.5
in orange curve and -1.0 in blue curve) of predicted
log odds. Again the blue curve is constant.
92
Interpretable Missing Values in Healthcare
Figure 4(c) shows the shape function for tempera-
ture when predicting if urea is missing or not. There
is no missing value for temperature, so there is no
orange curve. The bump at temperature ∈[36,37.5)
indicates that urea is more likely to be missing, which
suggests when a patient has normal body tempera-
ture, doctors may be less likely to order a blood test
to measure urea.
To test how well can EBM predict the missing-
ness, we generate some semi-synthetic datasets with
ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out- 

Document 4: 
 column values. But this approach is appropriate only when there are a couple of missing values in the 
data set. Although as the quantity of missing values increase, significant data may be lost by deletion of 
the incomplete instances [13].
 – Mean/mode substitution: This is a simple way to impute the missing data. It replaces the missing values 
by the mean or mode of all the observations or a subgroup at the same variable. It consists of replacing 
the unknown value for a given attribute by the mean (quantitative attribute) or mode (qualitative attrib-
ute) of all known values of that attribute. But replacing all missing records with a single value distorts 
the input data distribution [21].
 – Maximum likelihood procedures [3]: These methods are used for estimation of parameters of model by 
making use of the expectation–maximization algorithm [13, 21]. These methods make assumption of 
model distribution for the variables, such as a multivariate normal model, which are very much sensitive 
to outliers [21].
 – Multiple imputation [5, 18]: This method imputes the value of a missing variable more than once. Then, 
analysis is done by averaging them [21].
 – Machine learning-based imputation [13]: There are a number of approaches for missing value imputation 
include choosing the most common value among the neighbors, weighted imputation based on k-nearest 
neighbors, decision tree based, association based, fuzzy logic, neural networks, SVM, and clustering- 

Document 5: 
 Meanwhile, the importance of explainable AI is increasingly recognized across
various sectors. Taking medical imaging is an example where physicians rely on
explainability to evaluate and potentially improve diagnoses based on machine
outputs. However, a significant challenge arises when these explanation methods
encounter missing data in the dataset, an aspect they often overlook. The
occurrence of missing data inherently introduces additional uncertainty into the
model. Depending on the imputation values used to fill in the missing data,
explanations and results can vary, potentially leading to inaccurate evaluations
of a patient’s health status. Such issues are not exclusive to healthcare but
extend to other domains, including banking, biology, and more.
Thus, being able to explain the imputation of missing values in the data
is an essential step toward more trustworthy data analysis. This motivates
us to introduceconditional Distribution based Imputation of Missing Values
(DIMV) algorithm, an algorithm that is able to find the conditional distribution
of the features with missing values based on fully observed features in a fashion
2
similar to the Expectation-Maximization algorithm. However, DIMV only
conditionsuponselectedrelevantfeatures, andmoreover, themeanandcovariance
matrix are estimated directly based on the DPER algorithm [2], which gives
DIMV an advantage in speed compared to EM. In addition, the imputation
step of DIMV can be interpreted as Ridge regression, with coefficients giving
direct explainability as regression coefficients. Furthermore, note that by the 

Document 6: 
 ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out-
put score is higher than the threshold. The diﬀer-
ence between MAR and MNAR is whether the target
feature value is considered as an input of the miss-
ing models. Table 2 compares EBM’s the test accu-
racy of predicting missingness with machine learning
models commonly used for missing value imputation.
EBM predicts missingness better than other methods
in cases of MAR and MNAR values generated from
linear and curvilinear models and is not far behind
Random Forest in case of quadratic model.
Summary: We use EBMs to predict the missing-
ness of features from other input features. EBM pre-
dicts the missingness accurately. The interpretability
of EBMs can help users understand the relationship
between the features and missingness and thus bring
more insight for the cause(s) of missingness.
5. Detecting and avoiding potential
risks of missing value imputations
5.1. Imputation With the Mean
Because many machine learning methods cannot na-
tively handle missing values, it is common for data
scientists to impute missing values before training
models. There are many diﬀerent ways to do this 

","The passage does not provide specific information on using EBM (Explainable Boosted Machines) for imputing missing values directly. It mainly discusses the importance of explainability in detecting and fixing issues with missing value imputations, rather than explaining how EBM can be used to impute missing values itself.",2.350893259048462,"EBMs (Explainable Boosting Machines) can be used in imputing missing values in healthcare data by effectively predicting the missingness of features based on the relationships with other input features. Given their interpretable nature, EBMs can uncover subtle anomalies and provide insights into the causes of missing data, whether they follow a Missing At Random (MAR) or Missing Not At Random (MNAR) mechanism. The ability of EBMs to model the shape functions allows them to capture complex interactions between features and missing values, which makes them more proficient than traditional methods like mean or regression imputation. Furthermore, since EBMs are editable, any detected issues related to missing values can be rectified using model editing tools, contributing to more reliable healthcare decisions and potentially mitigating the risks associated with erroneous predictions derived from incomplete data."
qwen2.5:3b,"How can EBMs help detect observations poorly imputed by other methods?
","Document 1: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

Document 2: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 3: 
 the mean value of the feature is the same before and
after mean imputation, we can directly ﬁnd the bin
(of EBM) covering the mean value, and detect if the
bin is a spike or not. This also works for median im-
putation — the median of a feature does not change
by imputing the missing values with the median.
To address the second problem, we need an algo-
rithm to distinguish spikes resulting from mean im-
putation and ﬂuctuations that naturally occur in the
EBM shape functions. We formulate this as an out-
lier detection problem. First, we calculate the second
order diﬀerences for all bins in all shape functions
(excluding ﬁrst and last bins), since spikes usually
have extreme second order diﬀerences. We denote
the function values of the kth bin and its neighbour-
ing bins as fk, fk−1, and fk+1. The corresponding
bin sizes are denoted as hk, hk−1, and hk+1. The
second order diﬀerence is
f′′
k (x) ≈
fk+1−fk
(hk+1+hk)/2 − fk−fk−1
(hk+hk−1)/2
hk + hk+1/2 + hk−1/2 . (5)
We then run an outlier detection algorithm (Isolation
Forest (Liu et al., 2008)) on these second order diﬀer- 

Document 4: 
 and generative models like deep generative models
(Yoon et al., 2018). Since most of these methods are
based on black-box machine learning methods and
the accuracy and behavior of the ﬁnal model depends
on the imputed values, it is diﬃcult for users to recog-
nize and understand the potential harms that might
be introduced by these imputation methods.
Recently developed interpretable machine learning
methods have been shown to be useful for debugging
models and detecting issues with datasets (Adebayo
et al., 2020; Koh and Liang, 2017). Interpretable ma-
chine learning methods provide a new opportunity to
study missing values and revisit some of the classi-
cal methods for handling missing values. In this pa-
per, we propose novel methods based on the Explain-
able Boosting Machine (EBM) (Lou et al., 2012, 2013;
Nori et al., 2019), a high-accuracy, fully-interpretable
glass-box machine learning method, to answer the fol-
lowing questions: (1) how interpretability can help
users gain insights on the causes of missingness, and
(2) how interpretability can help detect and avoid po-
tential risks introduced by diﬀerent imputation meth-
ods. We show that the glass-box models provide new
insights into missingness mechanisms, and in some
settings, suggest alternate ways of handling missing
values, as well as new tools that can alert users when 

Document 5: 
 a method to test MCAR based on EBM shape func-
tions. The testing process of the proposed method
88
Interpretable Missing Values in Healthcare
can be directly visualized on the shape function plots,
which is not achievable by Little’s test. We will also
show that EBM can bring additional insights beyond
simply testing for MCAR.
4.1.1. Testing for MCAR with EBM
To test for MCAR, we use the common trick of encod-
ing missing values with a unique value for the feature,
e.g., -1 for a feature with positive values or a sepa-
rate category for a categorical feature. After ﬁtting
an EBM that predicts the target, we get a shape func-
tion representing the contribution of diﬀerent feature
values for predicting the target, including the unique
value denoting missingness. Note that the leaf nodes
in EBM split the feature values into many bins, where
each bin has a prediction score. These bins and scores
together form the shape function. Therefore, the
EBM shape function fj(·) of feature j can be rewrit-
ten as a linear combination of a series of indicator
variables denoting if the feature values are within the
bins, and the coeﬃcients are the corresponding scores
of the bins, i.e.,
fj(xj) =
Bj−1∑
k=0 

Document 6: 
 be similar to uniformly imputing the heart rates in
the interval 38-125 and retraining the model.) This
approach has the following advantages:
1. Editing shape functions provides an opportunity
for experts to use their professional training to
correct and improve models in ways that may not
be adequately represented in the training data.
2. Editing the model may not only improve the ac-
curacy of the model in the real world where it
will be used (instead of just on held-aside test
data from the train set), but also make the shape
plots more “reasonable” and trusted by experts.
3. Editing an EBM shape function can be done
without retraining the model and potentially in-
troducing new problems.
Summary: We show that EBM shape function
can help identify the case when feature values are
missing because they are assumed to be normal. We
also show how editing the EBM graphs can help ad-
dress issues resulting from missing assumed normal.
4.3. Predicting the Missingness
Most missing values are not MCAR, but as mentioned
in Section 2, MNAR and MAR can be diﬃcult to dis-
tinguish (Van Buuren, 2018). For both cases, inter-
pretable models like EBM can still be useful in pro-
viding insights on possible missingness mechanisms.
(a) “Bilirubin” shape function when predicting missing-
ness of “Na”
(b) “Systolic blood pressure” shape function when pre- 

","EBM helps detect poorly imputed observations by using its shape functions to identify outliers. These shapes show differences in how features contribute to predictions between observed and imputed data groups. By distinguishing between spikes caused by imputation (like mean or median imputation) and natural fluctuations, EBM can highlight problematic models resulting from poor imputation methods like Random Forest (RF). This enables users to recognize and address potential risks introduced by such imputation techniques.",2.5618910789489746,"EBMs can help detect observations poorly imputed by other methods by providing a clear visualization of the contribution and behavior of feature values through their shape functions. When different imputation methods are applied, such as MissForest, EBMs can reveal discrepancies between the imputed groups and the observed groups, highlighting significant differences in predicted outcomes, such as ICU mortality risk. 

By analyzing the shape functions, researchers can identify spikes or unusual patterns that may indicate problematic imputation results. Additionally, EBMs allow for the calculation of outlier detection based on second-order differences in feature values, making it possible to distinguish between natural fluctuations and those caused by flawed imputation methods. This interpretability enables users to gain insights into the implications of missing data handling, allowing them to address issues arising from imputation and potentially leading to more accurate and reliable model performance."
qwen2.5:3b,"How can one distinct terms intepretabilitry and explainability in XAI?
","Document 1: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 2: 
 tant to distinguish between features and interpretable data
representations. As mentioned before, interpretable expla-
nations need to use a representation that is understandable
to humans, regardless of the actual features used by the
model. For example, a possible interpretable representation
for text classiﬁcation is a binary vector indicating the pres-
ence or absence of a word, even though the classiﬁer may
use more complex (and incomprehensible) features such as
word embeddings. Likewise for image classiﬁcation, an in-
terpretable representationmay be a binary vector indicating
the “presence” or “absence” of a contiguous patch of similar
pixels (a super-pixel), while the classiﬁer may represent the
image as a tensor with three color channels per pixel. We
denote x∈Rd be the original representation of an instance
being explained, and we use x′∈{0, 1}d′
to denote a binary
vector for its interpretable representation.
3.2 Fidelity-Interpretability Trade-off
Formally, we deﬁne an explanation as a model g ∈G,
where G is a class of potentially interpretable models, such
as linear models, decision trees, or falling rule lists [ 27], i.e. a
model g∈Gcan be readily presented to the user with visual
or textual artifacts. The domain of g is {0,1}d′
, i.e. g acts 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 9.8.1735
Hohman, F., Srinivasan, A., & Drucker, S. M. (2019). TeleGam: Combining visualization and verbalization for interpretable machine learn-
ing. InIEEE visualization conference (VIS). IEEE.https://doi.org/10.1109/visual.2019.8933695
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., & Müller, H. (2019). Causability and explainability of artificial intelligence in medicine.
WIREs Data Mining and Knowledge Discovery, 9(4), e1312.https://doi.org/10.1002/widm.1312
Jain, S., & Wallace, B. C. (2019). Attention is not explanation. InProceedings of the 2019 conference of the North American chapter of the Asso-
ciation for Computational Linguistics: Human language technologies(pp. 3543–3556). Association for Computational Linguistics.https://
doi.org/10.18653/v1/N19-1357
Jesus, S., Belém, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P., & Gama, J. (2021). How can I choose an explainer? An application-
grounded evaluation of post-hoc explanations. InProceedings of the 2021 ACM conference on fairness, accountability, and transparency 

Document 5: 
 In general, there is no agreement within the ML community on the definition ofinterpretability and thetask of inter-
pretation (Doshi-Velez & Kim,2017; Lipton,2018). For example, Doshi-Velez and Kim (2017) define interpretability of
ML systems as“the ability to explain or to present in understandable terms to a human.” This definition lacks mathemat-
ical rigor (Lipton, 2018). Nevertheless, the notion of interpretability often depends on the domain of application
(Rudin, 2019) and the targetexplainee (Carvalho et al.,2019), that is, the recipient of interpretations and explanations.
Therefore, an all-purpose definition might be infeasible (Rudin,2019) or unnecessary. Other terms that are synonymous
with interpretability and also appear in the ML literature are“intelligibility” (Caruana et al.,2015; Lou et al.,2012) and
“understandability” (Lipton, 2018). These concepts are often used interchangeably.
Yet another term prevalent in the literature is“explainability,” giving rise to the direction ofexplainable artificial
intelligence (XAI) (Gunning & Aha,2019). This concept is closely tied with interpretability; and many authors do not
differentiate between the two (Carvalho et al.,2019). Doshi-Velez and Kim (2017) provide a definition ofexplanation 

Document 6: 
 Kirrane, 2020)).
In recognition of the growing importance of this topic, NIST published in August 2020Four principles of XAI
(Phillips et al., 2020), which define the following fundamental principles which an AI must honor to be considered an
XAI as follows:
 Explanation: this principle states that an AI system must supply evidence, support; or reasoning for each decision
made by the system.
 Meaningful: this principle states that the explanation provided byt h eA Is y s t e mm u s tb eu n d e r s t a n d a b l eb y ,a n dm e a n i n g f u l
to, its users. As different groups of users may have different necessities and experiences, the explanation provided by the AI
system must be fine-tuned to meet the various characteristics and needs of each group.
 Accuracy: this principle states that the explanation provided by the AI system must reflect accurately the system's processes.
 Knowledge limits: this principle states that AI systems must identify cases that they were not designed to operate in
and, therefore, their answers may not be reliable.
Figure 3 depicts the ontology of the XAI taxonomy. Transparent models can easily achieve explainability, while
opaque models require post hoc approaches to make them explainable. The categories of post hoc approaches are illus-
trated accordingly. 

","In the context of Explainable AI (XAI), the concepts of ""interpretability"" and ""explainability"" are often used to evaluate how well an AI system's decisions or predictions can be understood by humans. Here’s a brief explanation of both terms:

### Interpretability:
**Interpretability** refers to the degree to which the internal workings of a machine learning model or algorithm can be understood, explained, and reasoned about by human users. A highly interpretable model allows one to understand why it made a specific prediction or decision.

- **Principles of Interpretability:**
  - **Clear Rules/Logic:** The system should have clear rules or logical flow that leads to the final decisions.
  - **Simplicity:** The underlying logic is simple and easy to comprehend.
  - **Transparency:** The model’s internal representations are transparent, allowing for inspection.

### Explainability:
**Explainability**, on the other hand, focuses more on providing justification for a specific prediction or decision. While interpretability deals with understanding how a model operates internally, explainability is concerned with explaining why that particular input led to a certain output.

- **Principles of Explainability:**
  - **Detailed Justification:** It should provide detailed reasons or explanations for the inputs leading up to the final decision.
  - **Consistency:** The explanations are consistent and align with the model’s outputs in all relevant scenarios.

### Key Differences:
1. **Interpretability** is about understanding how a system works; it focuses on the internal mechanisms of the AI.
2. **Explainability**, while closely related, emphasizes providing reasons for specific outcomes or decisions made by the AI.

In XAI frameworks and research, interpretability often serves as a foundation to build explainability upon. A model that is highly interpretable can be more easily explained because its decision process is clear and straightforward. However, achieving both interpretability and explainability simultaneously in complex AI systems remains challenging.

### Example:
- **Interpretable Model:** If you have a decision tree or a simple linear regression model, these are quite interpretable since their internal workings (the rules and coefficients) can be easily understood.
- **Explainable but Less Interpretable Models:** Neural networks or deep learning models can sometimes offer high prediction accuracy but may not be as straightforward to understand due to their complex architectures.

In practice, developing AI systems that strike a balance between interpretability and explainability is essential for ensuring trust, accountability, and robustness in applications where the reliability of decisions needs to be clearly understood by humans.",8.03618836402893,"Interpretability and explainability are terms that are often used interchangeably within the field of explainable artificial intelligence (XAI), but they can be distinguished based on their definitions and implications. Interpretability refers to the degree to which a user can understand the cause of a decision made by an AI system, typically implying that the model is transparent and its workings can be easily explained in human terms. Doshi-Velez and Kim (2017) define interpretability as “the ability to explain or present in understandable terms to a human.”

On the other hand, explainability encompasses a broader scope. It pertains to how an AI system provides reasoning or support for its decisions and aims to convey this information meaningfully to users. Explainability is necessary when dealing with complex or ""black-box"" models where the internal processes are not straightforward or easily understood. As Burkart and Huber (2020) note, while explainable models are interpretable by default, the opposite is not necessarily true, highlighting that interpretability alone may not address all aspects of understanding machine learning decisions.

In summary, while interpretability focuses on the clarity and transparency of the model itself, explainability relates more to the quality and relevance of the information provided to users regarding the model's decision-making process."
qwen2.5:3b,"What issues in machine learning can be categorized as 'model bugs'?
","Document 1: 
 2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n
i, wherex∈X and y∈Y, a classiﬁer’s goal is to learn a function,fθ : X→Y ,
that generalizes. fθ is then used to predict test examples,xtest ∈X, as ytest = fθ(xtest). Given a
loss functionL, and model parameter,θ, for a model family, we provide a categorization of bugs as
model, data and test-time contamination:
Learning: arg min
θ
Model Contamination
L(
Data Contamination  
(Xtrain,Ytrain) ; θ);
Prediction: ytest = fθ(
Test-Time Contaminationxtest ).
1We will provide code to replicate our ﬁndings at: https://github.com/adebayoj/explaindebug.git.
3
Bug Category Speciﬁc Examples tested Formalization
Data Contamination Spurious Correlation arg min
θ
L(Xspurious artifact,Ytrain;θ)
Labelling Errors arg min
θ 

Document 2: 
 lyzing all of this data together requires some form of data
integration. In machine learning, two relevant integration
techniques are alignment and co-learning. Alignment is to
ﬁnd relationships of sub-components of instances that have
multiple modalities. For example, if there are multi-view time
series, one can perform subsampling, forward or backward
ﬁlling, or aggregate in time windows so that the time series
can be better integrated. Co-learning is to train better on a
modality using a different modality. For example, if there
are embeddings from different modalities, one approach is
to concatenate them together for a multimodal representa-
tion. In general, data integration is by itself a large research
area that has been studied for decades [ 46,152], although not
all techniques are relevant to machine learning.
4 Robust model training
Even after collecting the right data and cleaning it, data
quality may still be an issue during model training. It is
widely agreed that real-world datasets are dirty and erro-
neous despite the data cleaning process. As summarized in
Table2, these ﬂaws in datasets can be categorized depending
on whether data values are noisy or missing and depending
on whether these ﬂaws exist in data features (attributes) or
labels.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 799
Table 2 Types of data poisoning covered in this survey
Noisy Missing 

Document 3: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 4: 
 attributions, and attributions for these samples on other models. As seen in Table 3, we observe
visual similarity, particularly for the VGG-16 model on ImageNet, but essentially no correlation in
feature ranking.
10
Insights from Human Subject Study: users use prediction labels, not the attributions.
For the domain shift study, we show participants attribution of dogs that were not used during
training, and whose breeds diﬀered from those that the model was trained to predict. We ﬁnd that
users do not recommend a model under this setting due to wrong prediction labels (Figure 5).
6 Discussion & Conclusion
Debugging machine learning models remains a challenging endeavor, and model explanations could
be a useful tool in that quest. Even though a practitioner or a researcher may have a large class of
explanation methods available, it is still unclear which methods are useful for what bug type. This
work aims to address this gap by ﬁrst, categorizing model bugs into: data, model, and test-time
contamination bugs, then testing feature attribution methods, a popular explanation approach for
DNNs trained on image data, against each bug type. Overall, we ﬁnd that feature attribution
methods are able to diagnose the spatial spurious correlation bug tested, but do not conclusively
help to distinguish mislabelled examples for normal ones. In the case of model contamination, we
ﬁnd that certain feature attributions that perform positive aggregation while computing feature
relevance with modiﬁed back-propagation produce attributions that are invariant to the parameters 

Document 5: 
 that the humans exploit label information and Feng and Boyd-Graber (2019) demonstrate how to
assess explanations in a natural language setting. Similarly, Alqaraawi et al. (2020) ﬁnd that the
LRP explanation method (see Section 2.2) improves participant understanding of model behavior
for an image classiﬁcation task, but provides limited utility to end-users when predicting the model’s
output on new inputs.
Feature attributions can be easily manipulated, providing evidence for a collective ‘weakness’ of
current approaches (Ghorbani et al., 2019; Heo et al., 2019; Lakkaraju and Bastani, 2020; Slack
et al., 2020). While susceptibility is an important issue, our work focuses on providing insights
when model bugs are ‘unintentionally’ created.
2 Bug Characterization, Explanation Methods, & User Study
We now present our characterization of model bugs, provide an overview of the explanation methods
assessed, and close with a background on the human subject study.1
2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n 

Document 6: 
 model as a way to provide a global understanding.
There are several ways a model or its evaluation can go
wrong. Data leakage, for example, deﬁned as the uninten-
tional leakage of signal into the training (and validation) data
that would not appear when deployed [ 14], potentially in-
creases accuracy. A challenging example cited by (author?)
[14] is one where the patient ID was found to be heavily corre-
lated with the target class in the training and validation data.
This issue would be incredibly challenging to identify just by
observing the predictions and the raw data, but much easier
if explanations such as the one in Figure 1 are provided, as
patient ID would be listed as an explanation for predictions.
Another particularly hard to detect problem is dataset shift
[5], where training data is diﬀerent than test data (we give
an example in the famous 20 newsgroups dataset later on).
The insights given by explanations are particularly helpful in
identifying what must be done to convert an untrustworthy
model into a trustworthy one – for example, removing leaked
data or changing the training data to avoid dataset shift.
Machine learning practitioners often have to select a model
from a number of alternatives, requiring them to assess
the relative trust between two or more models. In Figure
Figure 2: Explaining individual predictions of com-
peting classiﬁers trying to determine if a document
is about “Christianity” or “Atheism”. The bar chart
represents the importance given to the most rele- 

","Model bugs in machine learning can be characterized as contamination in the learning and/or prediction pipeline that causes a model to produce incorrect predictions or learn error-causing associations. These issues are specifically focused on problems originating from within the learning process, such as data leakage and dataset shift.",2.023444890975952,"Model bugs in machine learning can be categorized as contamination in the learning and/or prediction pipeline that leads to incorrect predictions or the learning of error-causing associations. These bugs can be classified into three primary categories based on their source:

1. **Model Contamination**: Issues arising from the model itself that can affect its learning process or predictions.
  
2. **Data Contamination**: Problems related to the dataset used for training, such as:
   - Spurious correlations in the training data.
   - Labeling errors, where incorrect or noisy labels are present due to manual annotation errors or software mistakes.

3. **Test-Time Contamination**: Problems that occur during the model's prediction phase, potentially due to issues like data leakage, where unintended signals from the training set could artificially inflate accuracy in the prediction phase, or dataset shift, where the training data differs from the test data.

These model bugs highlight the complexities and challenges in ensuring model reliability and accuracy in machine learning frameworks."
qwen2.5:3b,"What XAI techniques can be heloful in detecting mislabeled observation in image classification?
","Document 1: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 2: 
 In the ILSVRC2012 ImageNet competition, it surpassed the second-best entry by a huge
advantage. After AlexNet achieved excellent results in the ImageNet image classiﬁcation
competition, researchers began to study CNN more deeply, Zeiler and Fergus proposed
a visualization technique to understand CNNs and proposed ZFNet [ 20]. Min Lin et al.
proposed NIN network [21], which contributed to the control of the parameter amount
and the number of channels. Next, refs. [ 22–27] yielded high performance during the
ILSVRC2014-2017 classiﬁcation challenge, they all made great innovations on the original
basis. From 2017 to the present, more models with superior performance have appeared
one after another. CNNs have increasingly demonstrated irreplaceable superiority in image
classiﬁcation.
With the successful application of CNN to large-scale visual classiﬁcation tasks, around
2015, the application of CNNs has ﬁnally taken off in the remote sensing image analysis
ﬁeld [28,29]. A variety of CNN-based scene classiﬁcation methods have emerged by using
different strategies of exploiting CNNs [ 30–36]. Generally, CNN-based remote sensing
image scene classiﬁcation methods can be divided into three types: (i) The pre-trained
CNNs is used as a feature extractor [ 37–44]. (ii) Fine-tune the pre-trained CNNs on the
dataset [30,45–52]. (iii) Globally initialize the weights of CNNs for training [ 31,53–55].
As we all know, the CNN-based image classiﬁcation method was originally designed for 

Document 3: 
 ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 6021–6029. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/6064.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
17
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from
error visibility to structural similarity.IEEE transactions on image processing, 13(4):600–612,
2004.
Gezheng Wen, Brenda Rodriguez-Niño, Furkan Y Pecen, David J Vining, Naveen Garg, and Mia K
Markey. Comparative study of computational visual attention models on two-dimensional medical
images. Journal of Medical Imaging, 4(2):025503, 2017.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. InAdvances in neural information processing systems, pages
9291–9301, 2018. 

Document 4: 
 self-supervised learning and generative models, and we
will cover a self-supervised learning technique called Jig-
sawNet [108].
In Mean-Teacher [ 156], the teacher model is the average
of consecutive student models. Both the student and teacher
models evaluate the input in a training batch. The softmax
output of the student model is compared with the one-hot
label using a classiﬁcation cost. Additionally, the output is
compared with the teacher output using the consistency loss.
After the weights of the student models are updated via gra-
dient descent, the teacher model weights are updated as an
exponential moving average of the student model weights.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 803
Fig. 10 Example of the jigsaw puzzle task for a given unlabeled
image [108]
A training step with unlabeled examples is done without the
classiﬁcation cost.
In MixMatch [ 17], to exploit an unlabeled dataset, it per-
forms label guessing where stochastic data augmentation is
applied to an unlabeled imageK times; then, each augmented
image is fed through the classiﬁer. The average of these
K predictions is sharpened by adjusting the distribution’s
temperatures. The MixMatch algorithm mixes both labeled
examples and unlabeled example with label guesses. In more
detail, when mixing two images, the images are overlayed,
and the labels are averaged, following the MixUp augmen- 

Document 5: 
 which was veriﬁed via3 questions.
Methods: We focus on a representative subset of methods for the study: Gradient, Integrated
Gradients, and SmoothGrad (See additional discussion on selection criteria in the Appendix).
Bugs: We tested the bugs described in Table 1 along with a model with no bugs.
3 Debugging Data Contamination
Overview. We assess whether feature attributions can detect spurious training artifacts and mis-
labelled training examples. Spurious artifacts are signals that encode or correlate with the label in
the training set but provide no meaningful connection to the data generating process. We induce
a spurious correlation in the input background and test whether feature attributions are able di-
agnose this eﬀect. We ﬁnd that the methods considered indeed attribute importance to the image
background for inputs with spurious signals. However, despite visual evidence in the attributions,
5
participants in the human subject study were unsure about model reliability for the spurious model
condition; hence, did not out-rightly reject the model.
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
A
B
C
D
Figure 3: Feature Attributions for Spurious Correlation Bugs.Figure shows attributions 

Document 6: 
 3.2 Mislabelled Training Examples
Bug Implementation.We train a BVD-CNN model on a birds-vs-dogs dataset where10 percent
of training samples have their labels ﬂipped. The model achieves a93.2, 91.7, 88 percent accuracy
on the training, validation, and test sets.
Correct  
Label
Incorrect  
Label
Input
Correct  
Label
Incorrect  
Label
Figure 6: Diagnosing Mislabelled Training Examples.The Figure shows two training inputs
along with feature attributions for each method. The correct label row corresponds to feature
attributions derived from a model with the correct label in the training set. The incorrect-label row
shows feature attributions derived from a model with the wrong label in the training set. We see
that the attributions under both settings are visually similar.
Results. We ﬁnd that attributions from mislabelled examples for a defective model are visually
similar to attributions for these same examples but derived from a model with correct input labels
(examples in Figure 6). We ﬁnd that the SSIM between the attributions of a correctly labeled
instance, and the corresponding incorrectly labeled instance, are in the range0.73 −0.99 for all
methods tested. These results indicate that the attribution methods tested might be ineﬀective for
identifying mislabelled examples. We refer readers to Section I of the Appendix for visualizations
of additional examples.
Insights from Human Subject Study: users use prediction labels, not attribution meth- 

","When dealing with mislabeled observations in image classification, Explainable Artificial Intelligence (XAI) techniques can play a crucial role by providing insights into the model's decision-making process. Here are several XAI methods that could help detect and diagnose mislabeled observations:

1. **LIME (Local Interpretable Model-agnostic Explanations):**
   - LIME can generate explanations for predictions made by complex models like deep neural networks. By creating a simple linear model around the misclassified observation, it can highlight how the model is likely overfitting to specific patterns in that data point.

2. **SHAP (SHapley Additive exPlanations):**
   - SHAP values provide a unified measure of feature importance for each instance and prediction, helping you understand why a particular observation was classified as such. Mislabeled instances might have high absolute SHAP values or unusual patterns compared to correctly labeled images.

3. **Grad-CAM (Gradient-weighted Class Activation Mapping):**
   - Grad-CAM generates class-specific heatmaps that highlight the parts of an image contributing most to the model's decision. By analyzing these maps, you can identify if there are regions in mislabeled observations that could be indicative of the error.

4. **Attention Mechanisms:**
   - In models like Transformers and their variants used for image classification (like CLIP), attention mechanisms focus on different parts of an image during predictions. Misclassified instances might have a strong focus on irrelevant features or lack attention to key distinguishing characteristics.

5. **Confidence Intervals and Uncertainty Analysis:**
   - Techniques such as Bayesian Neural Networks or using dropout can provide uncertainty estimates for model predictions. Images with high prediction uncertainty could be candidates for further investigation, especially if they are mislabeled.

6. **Visualizing the Decision Boundary:**
   - By visualizing decision boundaries, you can understand how different features influence the model's decisions. Misclassified instances might reside on or near decision boundaries, making them more prone to errors.

7. **Confusion Matrix Analysis:**
   - Analyzing confusion matrices for each class and across classes can reveal patterns where mislabeled data is likely occurring. Instances that frequently appear in incorrect predictions could be flagged as potential candidates for review.

8. **Feature Importance Distributions:**
   - Examining how feature importance distributions change between correctly and incorrectly classified instances might highlight features that are often confused or not adequately distinguished by the model.

9. **Domain Adaptation Techniques:**
   - If mislabeled observations come from a different domain than training data, techniques like domain adaptation can help identify these anomalies more effectively by penalizing differences in feature distributions across domains.

10. **Label Smoothing:**
    - Applying label smoothing to your training process makes the model less confident about certain predictions. Misclassified instances might show higher confidence levels after applying this technique, making them easier to spot.

By integrating multiple XAI techniques, you can build a comprehensive understanding of why some observations are being misclassified and focus on refining your dataset or model accordingly. This approach not only helps in identifying problematic data points but also enhances the overall robustness and reliability of your image classification system.",9.823513269424438,"The context suggests that several explainable AI (XAI) techniques can be utilized to help detect mislabeled observations in image classification. Some of these methods include:

1. **Feature Attribution Methods**: Techniques like Gradient, Integrated Gradients, and SmoothGrad can be used to assess the importance of various features in the model's predictions. However, evidence from studies indicates that these feature attribution methods may struggle to effectively differentiate between correctly and incorrectly labeled instances, as the attributions derived from mislabeled data can be visually similar to those from correctly labeled instances.

2. **Diagnostic Models**: Training models on datasets with known proportions of mislabeled examples, such as a BVD-CNN on a birds-vs-dogs dataset with flipped labels, can help evaluate how models respond to mislabeling. This approach can provide insights into the model's performance and the impact of label noise.

3. **Spurious Correlation Detection**: Techniques that assess attributions for spurious signals (artifacts unrelated to true data generating processes) can be employed to identify misleading patterns in training data, including mislabeled examples.

These techniques highlight that while XAI methods can provide insights into model behavior, identifying mislabeled observations may require careful design and additional strategies beyond traditional attribution approaches."
qwen2.5:3b,"How can deep learning methods be used in imputation of missing data?
","Document 1: 
 least square methods [23] for imputation. For large datasets, deep learn-
ing imputation techniques have gained popularity due to their performance
4
[24, 25, 26]. It’s important to note that different imputation methods may
produce varying values for each missing entry, making the modeling of un-
certainty for each missing value crucial. Bayesian and multiple imputation
techniques, such as Bayesian principal component analysis-based imputa-
tion [27] and multiple imputations using Deep Denoising Autoencoders [28],
are particularly useful in these scenarios. Furthermore, certain tree-based
methods can naturally handle missing data through prediction, including
missForest [29], the DMI algorithm [30], and DIFC [31]. Methods that can
manage mixed data types have also emerged, such as SICE [32], FEMI [33],
and HCMM-LD [34].
Although various missing data methods have been developed [35], most
of them to date do not have a built-in explanation mechanism for imputed
values, although how a value is imputed can have a profound impact on the
performance and interpretability of downstream machine learning models.
If one uses a prediction model such as a regression or classification model,
then one can use Shapley values or some other XAI technique to explain the
imputation of missing values. However, many times, one may want to use
a more complicated imputation method that requires building regression or
classification models and looping through the data multiple times to improve
imputation accuracy. 

Document 2: 
 forinitializationofthemissingvalues.Moreimportantly,almost
allofthesemethodsuseabottlenecklayerasthelatentspace,
whileweuseanencodinglayerofsizemuchbiggerthanthatof
inputwithtwopurposes:Firsttoensureabetterreconstruction
oftheinputandsecondthedataintheprojectedspaceofhigher
dimensionarelikelytobeflattened[51].In[51]authorsremarked
that unfolding of class manifolds using deep networks results
inhigherdimensionofthedatawhichmakestheclassesmore
separable.Moreover,likethesemethods,ourobjectiveisnotjust
therestorationofmissingvalues,buthowwecanimputemissing
valuessothatwecanusedatawithmissingvaluesindesigning
moreeffectiveclassifiers.
Theselforganizingmap(SOM)[30,31]andmultilayerpercep-
tron(MLP)[32]aretheothertwomachinelearningtechniques
that have also been used to impute missing values. There are
several other machine learning techniques to impute missing
data[52–56].Aknowledge-basedmethodhasalsobeendevel-
opedthatcandealwithmissingvalueswhichisnon-numericin
nature[57].Chih-Fongetal.[58]proposedamodifiedformofthe
meanimputationmethod.First,thedatasetisdividedintocom- 

Document 3: 
 SN Computer Science
[175] 2021 Springer Data imputation in wireless 
sensor network using deep 
learning techniques
Data Analytics and Manage-
ment
[176] 2020 Sustainable and Resilient 
Infrastructure
Handling incomplete and 
missing data in water net-
work database using imputa-
tion methods
Taylor and Francis
Page 28 of 37Emmanuel et al. J Big Data           (2021) 8:140 
missing to 5%, 10% and 15% across all observations. RMSE performance measure was 
then used to help quantify the accuracy of the estimated values compared to the actual 
non-missing data.
After simulation of missing values, KNN imputation was implemented to replace the 
missing values. Firstly, when implementing the imputation method, the nearest neigh -
bors (K) must be chosen. The value of K was chosen based on experimental results start -
ing with K = 1 and stopped at K = 5, the best accurate estimation value of K was then 
used for the experiment which was K = 4. The Euclidean distance measure was used on 
the KNN imputation algorithm. The  RF missForest algorithm was then implemented, 
which is a nonparametric imputation method based on the random forest. For every 
variable missForest fits a random forest on the observed values and then predicts the 
missing variables. The process of training and predicting of missForest is repeated in an 
iterative process until a number of iterations are reached. The missForest ran for three 

Document 4: 
 this in a concise way. This is the area to employ meta-learning model to capture these high-level interactions. In future
work, we aim to extend this analysis with adding new data sets and training surrogate model to deeper understanding
the complex interactions between the structure of missing data in tasks, the assumption of imputation method and
ML algorithms (Wo´ znica and Biecek, 2020). Another extension may be considering hyperparameters optimization in
machine learning models as well as imputation methods.
References
B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. Vanschoren. OpenML
Benchmarking Suites. 8 2017. URL http://arxiv.org/abs/1708.03731.
M. L. Brown and J. F. Kros. Data mining and the impact of missing data. Industrial Management and Data Systems,
103(8-9):611–621, 2003. ISSN 02635577. doi: 10.1108/02635570310497657.
T. Hastie, R. Mazumder, J. D. Lee, and R. Zadeh. Matrix Completion and Low-Rank SVD via Fast Alternating Least
Squares. Technical report, 2014.
J. Honaker, G. King, and M. Blackwell. Amelia II: A program for missing data. Journal of Statistical Software, 45(7): 

Document 5: 
 missing rate is high and the samples are inadequate. Another
solution is to ﬁll in the missing values with substituted val-
ues, which is known asdata imputation . However, these
methods do not capture variable correlations and may not
capture complex patterns to perform imputation. Combin-
ing the imputation methods with prediction models often
results in a two-step process where imputation and prediction
models are separated; the missing patterns are not effectively
explored in the prediction model, thus leading to suboptimal
analysis results.
GRU-D [29] is a deep learning model based on the gated
recurrent unit (GRU) to effectively exploit two representa-
tions of informative missingness patterns—masking and time
interval. Masking informs the model of which inputs are
observed or missing, while time interval encapsulates the
input observation patterns. GRU-D captures the observa-
tions and their dependencies by applying masking and time
interval, which are implemented using a decay term, to the
inputs and network states of the GRU, and jointly train all
model components through back-propagation. GRU-D not
only captures the long-term temporal dependencies of time-
series observations, but also utilizes the missing patterns to
improve the prediction results.
We elaborate on the two components of GRU-D: making
and time interval. The value of a missing variable tends to be
close to some default value if its last observation happened 

Document 6: 
 bayesian ridge regression, Symmetry 12 (2020) 1594.
[23] P. Keerin, W. Kurutach, T. Boongoen, An improvement of missing value
imputation in dna microarray data using cluster-based lls method, in:
2013 13th International Symposium on Communications and Informa-
tion Technologies (ISCIT), IEEE, 2013, pp. 559–564.
[24] S. J. Choudhury, N. R. Pal, Imputation of missing data with neural net-
works for classification, Knowledge-Based Systems 182 (2019) 104838.
[25] A. Garg, D. Naryani, G. Aggarwal, S. Aggarwal, Dl-gsa: a deep learning
metaheuristic approach to missing data imputation, in: International
Conference on Sensing and Imaging, Springer, (2018), pp. 513–521.
[26] K. Mohan, J. Pearl, Graphical models for processing missing data,
Journal of the American Statistical Association (2021) 1–42.
[27] V. Audigier, F. Husson, J. Josse, Multiple imputation for continuous
variables using a bayesian principal component analysis, Journal of
statistical computation and simulation 86 (2016) 2140–2156.
[28] L. Gondara, K. Wang, Multiple imputation using deep denoising au-
toencoders, arXiv preprint arXiv:1705.02737 (2017). 

","Deep learning methods, particularly neural networks, can be effectively employed for the imputation of missing data. This approach is based on the idea that deeper and more complex models are capable of capturing intricate patterns and relationships within the data. Here’s a breakdown of how deep learning methods can be utilized:

1. **Data Embedding**: Deep learning models like Autoencoders, Convolutional Neural Networks (CNNs), or Recurrent Neural Networks (RNNs) can learn to embed the original feature space into a higher-dimensional latent space where missing data is more likely to fill in naturally.

2. **Conditional Generation**: Techniques such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) allow for generating new, realistic samples by training the network on available data points with known values. These generated synthetic observations can then be used to estimate missing entries.

3. **Multivariate Analysis**: Deep learning models are adept at handling multiple interdependent features simultaneously. This is particularly useful in scenarios where imputing one feature depends heavily on other related features. Models like CNNs and Transformers can model such dependencies effectively.

4. **Multi-output Prediction**: In cases with specific multi-label or multi-class prediction tasks, deep learning models (including those used for regression) can be trained to predict not just a single output but multiple outputs that collectively cover all missing values in the data set.

5. **Feature Learning and Fusion**: By leveraging hierarchical feature extraction capabilities of deep architectures, one can learn and fuse relevant features from different sources or levels to provide more accurate imputations.

6. **Interpretability for Adjustments**: In some cases, after training a model on complete datasets, it might be beneficial to adjust its outputs (after imputation) based on domain knowledge or additional constraints specific to the problem at hand.

7. **Ensemble Methods**: Combining multiple deep learning models can improve performance and robustness in handling missing data imputation problems by leveraging different strategies for prediction and regularization.

8. **Bayesian Approaches with Deep Learning**: Incorporating Bayesian methods within a deep learning framework allows for uncertainty quantification during the imputation process, which is particularly useful when dealing with highly uncertain or noisy datasets.

9. **Self-Supervised Learning**: Using self-supervised tasks (where labels are not required) can provide additional context and structure to help fill in missing data more accurately.

By integrating these strategies into deep learning models, one can enhance their ability to learn patterns from available data effectively, leading to better imputation of missing values. However, it's important to ensure that the underlying assumptions about the nature of missingness (MCAR, MAR, MNAR) are respected and accounted for appropriately during model development and evaluation.",8.254482984542847,"Deep learning methods can be utilized in the imputation of missing data through various techniques that leverage the capabilities of neural networks to model complex patterns and relationships within the data. For example, models such as Deep Denoising Autoencoders can be employed to perform multiple imputations by reconstructing missing values based on learned representations of the data. Additionally, Gated Recurrent Unit based models like GRU-D exploit informative missingness patterns by incorporating mechanisms for masking and time intervals, allowing them to capture long-term dependencies in time-series data and thus improve prediction results. Moreover, traditional deep learning architectures, such as multilayer perceptrons (MLP), and self-organizing maps (SOM) can also be adapted for the imputation task, alongside ensemble methods like random forest imputation (missForest), which iteratively trains on observed values to predict missing entries. Overall, deep learning approaches often allow for more effective handling of missing data by integrating the imputation process with classification or prediction models, facilitating better overall model performance."
qwen2.5:7b-instruct-q4_0,"What does the term ""learn to optimize"" mean?
","Document 1: 
 NatlSciRev ,2024,Vol.11,nwae132
Solver 4
Solver 1
Solver 2
Solver 3
Configuration space of algorithms
(solver class)
Facilitate the tedious trial-and-error process for
getting a good solver for an unseen instance
Solver 5
Unseen instances
Training set Solver class
Optimization problem class
Knowledge
Learning
Vehicle routing
problem
Machine learning
Traveling
salesman
problem
Problem Instance 1
Problem Instance 2
Problem Instance 3
Optimization problems
Figure 1 Illustration of the general idea of L2O. 
the same general paradigm that is very similar to 
the typical ML paradigm. That is, they leverage on 
a set of training instances from the target optimiza- 
tion problem class to gain something that would help 
alleviate the tedious algorithm configuration process 
on unseen problem instances, as i l lustrated in Fig. 1 . 
This article puts them under the umbrella term learn 
to optimize (L2O) and provides a brief overview on 
the general ideas as well as critical issues of L2O, in- 
cluding the training process, theoretical foundations 
as well as generalization issues. For more algorithmic 
details on NN-based solvers and heuristic solvers, 
readers are referred to [13 ,14 ] and [15 ,16 ], respec- 
tively. 

Document 2: 
 m∑
i=1
U(s, Xi),
where Xi is a random variable, representing an instance randomly drawn from X.
Notice that Um(s) is a random variable.
We deﬁne two properties of SPPAs, generalization and optimality. An SPPA
learning algorithm generalizes well, when the utility on all instances is close to the
utility on the training set. An SPPA learning algorithm is optimal, if the optimal
SPPA on the training set is close to the optimal result on the set of all instances.
These properties are formalized by the following deﬁnition.
Learning parallel portfolios of algorithms 99
Deﬁnition 6.1 We say that an SPPA learning algorithm mean-generalizes,i ff o ra n y
0 <ϵ and 0 <δ< 1 it outputs an SPPA s ∈ S,f o rw h i c h
P [Um(s) − E [U(s, X)] >ϵ ] ≤ δ.
Let the globally optimal algorithm be:
s∗ = arg max
s∈S
E [U(s, X)] .
We say that an SPPA learning algorithm ismean optimal,i ff o ra l l0 <ϵ and 0<δ< 1
it outputs a schedule s
P
[
E
[
U(s∗, X)
]
− E [U(s, X)] >ϵ 

Document 3: 
 model and select conﬁgurations, respectively.
Input :Target algorithm Awith parameter conﬁguration space Θ; instance set Π; cost
metric ˆc
Output :Optimized (incumbent) parameter conﬁguration, θinc
1 [R, θinc] ←Initialize(Θ, Π);
2 repeat
3 [M,tfit ] ←FitModel(R);
4 [ ⃗Θnew,tselect] ←SelectConﬁgurations(M, θinc, Θ);
5 [R,θinc] ←Intensify( ⃗Θnew, θinc, M, R, tfit + tselect, Π, ˆc);
6 until total time budget for conﬁguration exhausted;
7 return θinc;
parameter optimization (SPO) toolbox—which has received considerable attention in the
evolutionary algorithms community—provides many features that facilitate the manual
analysis and optimization of algorithm parameters; it also includes an automated SMBO
procedure for optimizing continuous parameters on single instances. We started our
own work in SMBO by comparing SKO vs SPO, studying their choices for the four
SMBO components [18]. We demonstrated that component Intensify mattered most, and
improved it in our SPO + algorithm [18]. Subsequently, we showed how to reduce the
overhead incurred by construction and use of response surface models via approximate
GP models. We also eliminated the need for a costly initial design by interleaving ran- 

Document 4: 
 this is to be taken as either the efficiency or the 
effectiveness of the solver.
r = n! Ts
Tns
The Challenges 
As presented in the introductory paragraphs, a 
divide-and-conquer approach can be lucky. A run 
can benefit from a good split, which brings a solu -
tion at the beginning of some subspace and allows 
for an early stop. In contrast, a different division 
can decrease performance. What is interesting here 
is that adding resources can decrease the perform -
ance since it can produce more demanding sub -
spaces. 
Even if portfolio-based approaches are less prone 
to this problem, extending the size of a portfolio 
can still be detrimental to its performance. In gen -
eral, this increases the overhead, due to more fre -
quent and broader clause sharing, and worsens 
cache-congestion issues. A priori, the question of 
deciding the most effective number of resources to 
use against a given formula is a difficult one. 
One possible direction of research is to extend 
automatic tuning techniques. These approaches 
use machine learning to craft a predictive function 
that relates the features of an instance and the 
parameters of a given solver to its expected run 
time. This function can be learned and tested 
offline against a large set of representative 
instances and used at run time to configure a solver 
and maximize its performance. This offline 
approach assumes that a large and representative 

Document 5: 
 OPT := inf
θ∈Θ
R(θ).
The search for the optimal conﬁguration is generally too ambitious, as the total runtime
required for the conﬁgurator must be extraordinarily large (possibly inﬁnite) to guarantee
that the best algorithm conﬁguration returned by the conﬁgurator is in fact the optimal
one with high probability.
As a workaround, one can leverage the idea underlying PAC learning (Valiant, 1984) to
the problem at hand. The basic idea is to relax the goal of ﬁnding the optimal conﬁguration
itself and, instead, ﬁnd a conﬁguration that is considered to be “good enough”. As there are
potentially several such “good enough” conﬁgurations 6, this relaxation of the goal allows
the search to be completed in less (and, thus, feasible) time. In this context, “good enough”
means that the expected runtime is only worse than the optimal expected runtime up to
a multiplicative factor of 1 + ε for some ﬁxed precision parameter ε > 0. Formally, a
conﬁguration is said to be ε-optimal (“good enough”) iﬀ
Ei∼P(c(i,θ)) ≤(1 + ε)OPT.
However, this relaxation of the target is problematic in the context of AC problems, since the
runtimes of conﬁgurations often exhibit a heavy-tailed distribution. Indeed, it is not diﬃcult
to construct an example based on such distributions in which any (sensible) conﬁgurator
would, in the worst case, take inﬁnitely long to ﬁnd an ε-optimal conﬁguration; see for 

Document 6: 
 tions of learning machines, which lead to objective 
functions that favor different optimization methods. 
As a result, many optimization techniques, including 
convex optimization methods [1 ], gradient descent 
[2 ], heuristic search [3 ] as well as evolutionary algo- 
rithms (E A s) [4 ], have been utilized by the ML com- 
munity in the past decades. 
Mainstream optimization algorithms are usu- 
ally highly configurable parameterized algorithms, 
i.e. they involve tunable parameters. Performance of 
such algorithms may vary significantly with different 
parameter settings. Hence, to solve the optimization 
problem induced by a learning task, tedious efforts 
are typically needed to configure the optimization al- 
gorithm. Consider the case of training an NN with 
stochastic gradient descent (SGD). It is widely ac- 
knowledged that the training performance is sensi- 
tive to the learning rate [5 ], a parameter of SGD. 
How to determine the optimal or at least appropri- 
ate learning rate has become a major hurdle for train- 
ing NNs, especially for large-scale NNs [6 ], or when 
the objective function is not an explicit differentiable 
function of the weights of an NN, as in the typical re- 
inforcement learning setting [7 ]. 
In order to reduce the human labor required, 
many studies have attempted to automate the config- 
uration of optimization algorithms [6 ,8 ]. However, 

","Learn to optimize (L2O) refers to leveraging a set of training instances from an optimization problem class to develop models that can select effective solver configurations for unseen problem instances, thereby alleviating the need for tedious manual configuration processes.",3.4167404174804688,"The term ""learn to optimize"" (L2O) refers to a paradigm in which machine learning techniques are utilized to assist in the optimization process for various optimization problems. This approach leverages a set of training instances from a specific class of optimization problems to develop a solution that can alleviate the often tedious and lengthy trial-and-error process commonly associated with configuring algorithms for unseen instances. The overarching goal is to enhance the efficiency and effectiveness of solving optimization problems by using learned knowledge to predict or determine better algorithm configurations for new instances that have not been encountered before."
qwen2.5:7b-instruct-q4_0,"Please give some examples of metaheuristics.
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 aside any metaphorical language. We utterly believe that eﬀorts in 
this direction should be intensiﬁed, building upon the initial postula- 
tions established in some recent works [105,106] . Speciﬁcally, meta- 
heuristics components (including search operators and algorithmic 
behavioral patterns) and interfaces between them should be stan- 
dardized towards objectively assessing similarities and diﬀerences 
between metaphor-based solvers [107] . A novel metaphor is by no 
means a suﬃcient guarantee for a signiﬁcant scientiﬁc contribution. 
• The third of these reasons is methodological, i.e., the relevance of 
some of the building blocks of the overall algorithm. A particular 
algorithm can include a given component (for example, a local op- 
timizer) that can be of relevance even if the algorithm as a whole 
is not completely competitive with respect to the prevailing liter- 
ature. A good example supporting this claim can be observed in 
co-evolutionary frameworks, which usually include a procedure to 
identify the subcomponents that will be individually co-evolved. In 
those cases, even if the subcomponent optimizer is not very sophis- 
ticated, the co-evolutionary framework can be relevant by itself. In 
this sense, it is important to select the appropriate framework to 
highlight the desired characteristic of the proposed algorithm, as dis- 
cussed in Section 3.1 . Following the same example of subcomponents 
identiﬁcation, a researcher focused on large-scale global optimiza- 

Document 3: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

Document 4: 
 erations Research54(1):99–114.
Bartz-Beielstein, T. 2006. Experimental Research in Evolution-
ary Computation. Springer V erlag.
Battiti, R., and Brunato, M. 2005. Reactiv e search: machine
learning for memory-based heuristics. Technical Report DIT-05-
058, Universit`a Degli Studi Di Trento, Trento, Italy.
Birattari, M.; St¨utzle, T.; Paquete, L.; and V arrentrapp, K. 2002.
A racing algorithm for conﬁguring metaheuristics. In Proc. of
GECCO-02, 11–18.
Birattari, M. 2004. The Problem of Tuning Metaheuristics as
Seen from a Machine Learning Perspective. Ph.D. Dissertation,
Universit´e Libre de Bruxelles, Brussels, Belgium.
Coy, S. P .; Golden, B. L.; Runger, G. C.; and Wasil, E. A. 2001.
Using experimental design to ﬁnd effective parameter settings for
heuristics. Journal of Heuristics7(1):77–97.
E´en, N., and S¨orensson, N. 2003. An extensible SA T solver. In
Proc. of SAT-03, 502–518. 

Document 5: 
 art algorithm, MOS, has been clearly surprised by SHADE-ILS and 
MLSHADE-SPA, hence becoming the most competitive algorithms 
(with a preference by SHADE-ILS, by its better performance and sim- 
plicity). 
8. Conclusions and outlook 
In this work we have stressed on the need for circumventing com- 
mon mistakes and ﬂaws observed in the ﬁeld of bio-inspired optimiza- 
tion, particularly when new meta-heuristic algorithms are proposed and 
experimentally validated over benchmarks designed to this end. Specif- 
ically, we have reviewed and critically analyzed contributions deal- 
ing with experimental recommendations and practices related to meta- 
heuristics. Following our literature study, we have prescribed a set of 
methodological recommendations for preparing a successful proposal 
of bio-inspired meta-heuristic algorithms, from the deﬁnition of the ex- 
perimentation to the presentation of the results. A number of useful 
techniques (graphically summarized in Fig. 9 ) have been suggested for 
prospective studies to implement our proposed methodological frame- 
work, in an attempt at ensuring fairness, coherence and soundness in 
future studies on the topic. Two diﬀerent case studies have been de- 
signed to exemplify the application of our prescribed methodology, dis- 
cussing on the results of the application of each guideline. Although both 
case studies deal with well-known benchmarks, we envision that our 
methodology can be a core part of the design process of meta-heuristic 

Document 6: 
 achieved using several heuristic mechanisms that interact in complex, non-intuitive
ways. For example, a DPLL-style complete solver for SAT (a prototypical NP-
complete problem with important applications in the design of reliable soft- and
hardware) may use different heuristics for selecting variables to be instantiated and
the values ﬁrst explored for these variables, as well as heuristic mechanisms for
managing and using logical constraints derived from failed solution attempts. The
activation, interaction and precise behaviour of those mechanisms is often controlled
by parameters, and the settings of such parameters have a substantial impact on the
Holger H. Hoos
Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver,
BC, V6T 1Z4, Canada, e-mail: hoos@cs.ubc.ca
1 We note that the use of heuristic techniques does not imply that the resulting algorithms are nec-
essarily incomplete or do not have provable performance guarantees, but often results in empirical
performance far better than the bounds guaranteed by rigorous theoretical analysis.
Y . Hamadi et al. (eds.),Autonomous Search,
DOI 10.1007/978-3-642-21434-9 3,
© Springer-Verlag Berlin Heidelberg 2011
37
38 Holger H. Hoos
efﬁcacy with which a heuristic algorithm solves a given problem instance or class of 

","Some examples of metaheuristics include Simulated Annealing, Genetic Algorithms, Particle Swarm Optimization, and Ant Colony Optimization.",2.243536710739136,"Examples of metaheuristics include Simulated Annealing (SA), Tabu Search (TS), Evolutionary Algorithms (EAs), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO)."
qwen2.5:7b-instruct-q4_0,"What is the ""no free lunch"" theorem about?
","Document 1: 
 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTA TION, VOL. 1, NO. 1, APRIL 1997 67
No Free Lunch Theorems for Optimization
David H. Wolpert and William G. Macready
Abstract— A framework is developed to explore the connection
between effective optimization algorithms and the problems they
are solving. A number of “no free lunch” (NFL) theorems are
presented which establish that for any algorithm, any elevated
performance over one class of problems is offset by perfor-
mance over another class. These theorems result in a geometric
interpretation of what it means for an algorithm to be well
suited to an optimization problem. Applications of the NFL
theorems to information-theoretic aspects of optimization and
benchmark measures of performance are also presented. Other
issues addressed include time-varying optimization problems and
a priori“head-to-head” minimax distinctions between optimiza-
tion algorithms, distinctions that result despite the NFL theorems’
enforcing of a type of uniformity over all algorithms.
Index T erms— Evolutionary algorithms, information theory,
optimization.
I. I NTRODUCTION
T
HE past few decades have seen an increased interest
in general-purpose “black-box” optimization algorithms
that exploit limited knowledge concerning the optimization
problem on which they are run. In large part these algorithms
have drawn inspiration from optimization processes that occur
in nature. In particular, the two most popular black-box 

Document 2: 
 function, and will produce .
The proof is completed if we show that there is no cost
function so that produces a sample containing and
and such that produces a sample containing and .
There are four possible pairs of samples to consider:
i) ;
ii) ;
iii) ;
iv) .
Since if its ﬁrst point is a , jumps to which is where
starts, when ’s ﬁrst point is a its second point must
equal ’s ﬁrst point. This rules out possibilities i) and ii).
For possibilities iii) and iv), by ’s sample we know that
must be of the form , for some variable . For
case iii), would need to equal , due to the ﬁrst point in
’s sample. For that case, however, the second point sees
would be the value at , which is , contrary to hypothesis.
For case iv), we know that the would have to equal , due
to the ﬁrst point in ’s sample. That would mean, however,
that jumps to for its second point and would therefore
see a , contrary to hypothesis.
Accordingly, none of the four cases is possible. This is
a case both where there is no symmetry under exchange of
’s between and , and no symmetry under exchange of
histograms.
A PPENDIX G 

Document 3: 
 The sum over can be accomplished in the same manner
is summed over. In fact, all the sums over all can
be done, leaving
(7)
In this last step, the statistical independence of and has
been used.
Further progress depends on whether represents or
. We begin with analysis of the case. For this case
, since only reﬂects cost
values from the last cost function, . Using this result gives
The ﬁnal sum over is a constant equal to the number of
ways of generating the sample from cost values drawn
from . The important point is that it is independent of
the particular . Because of this the sum over can be
evaluated eliminating the dependence
This completes the proof of Theorem 2 for the case of .
The proof of Theorem 2 is completed by turning to the
case. This is considerably more difﬁcult since
cannot be simpliﬁed so that the sums over cannot be
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 79
decoupled. Nevertheless, the NFL result still holds. This is
proven by expanding (7) over possible values
(8)
The innermost sum over only has an effect on the
, term so it contributes ,
. This is a constant, equal to . This
leaves
The sum over is now simple
The above equation is of the same form as (8), only with a
remaining sample of size rather than . Consequently, in
an analogous manner to the scheme used to evaluate the sums 

Document 4: 
 always use algorithm . As applied to this case, they
mean that for ﬁxed and ,i f does better (on average)
with the algorithms in some set , then does better (on
average) with the algorithms in the set of all other algorithms.
In particular, if for some favorite algorithms a certain “well-
behaved”results in better performance than does the random
, then that well-behaved gives worse than random behavior
on the set all remaining algorithms. In this sense, just as there
are no universally efﬁcacious search algorithms, there are no
universally benignwhich can be assured of resulting in better
than random performance regardless of one’s algorithm.
In fact, things may very well be worse than this. In super-
vised learning, there is a related result [11]. Translated into
the current context, that result suggests that if one restricts
sums to only be over those algorithms that are a good match
to , then it is often the case that “stupid” choosing
procedures—like the irrational procedure of choosing the
algorithm with the less desirable—outperform “intelligent”
ones. What the set of algorithms summed over must be in
order for a rational choosing procedure to be superior to an
irrational procedure is not currently known.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 77
VIII. C ONCLUSIONS
A framework has been presented in which to compare 

Document 5: 
 values after distinct cost evaluations produced by using a
particular instantiation of an evolutionary algorithm?”
At ﬁrst glance this seems to be an intractable question,
but the NFL theorem provides a way to answer it. This is
because—according to the NFL theorem—the answer must be
independent of the algorithm used to generate. Consequently,
3 In particular, one may want to impose restrictions on /80 /40 /102 /41 . For instance,
one may wish to only consider /80 /40 /102 /41 that are invariant under at least partial
relabeling of the elements in /88 , to preclude there being an algorithm that will
assuredly “luck out” and land on /109/105 /110 /120 /50/88 /102 /40 /120 /41 on its very ﬁrst query.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 73
we can choose an algorithm for which the calculation is
tractable.
Theorem 3: For any algorithm, the fraction of cost func-
tions that result in a particular histogram is
For large enough , this can be approximated as
where is the entropy of the distribution , and
is a constant that does not depend on .
This theorem is derived in Appendix C. If some of the are
zero, the approximation still holds, only with redeﬁned to 

Document 6: 
 to-head minimax distinctions. Such an analysis is beyond the
scope of this paper.)
On the other hand, having the sums be equal does carry ob-
vious implications for whether there are head-to-head minimax
distinctions. For example, if both algorithms are deterministic,
then for any particular
equals one for one pair and zero for all others. In such
a case, is just the number
of that result in the pair .S o
implies
that there are no head-to-head minimax distinctions between
and . The converse, however, does not appear to hold. 4
4 Consider the grid of all /40 /122/59 /122 /48 /41 pairs. Assign to each grid point the number
of /102 that result in that grid point’s /40 /122/59 /122 /48 /41 pair. Then our constraints are i)
by the hypothesis that there are no head-to-head minimax distinctions, if grid
point /40 /122 /49 /59/122 /50 /41 is assigned a nonzero number, then so is /40 /122 /50 /59/122 /49 /41 and ii) by
the no-free-lunch theorem, the sum of all numbers in row /122 equals the sum
of all numbers in column /122 . These two constraints do not appear to imply
that the distribution of numbers is symmetric under interchange of rows and 

","The No Free Lunch (NFL) theorem in optimization states that any algorithm's performance across all possible problems is equivalent to any other algorithm's, when averaged over all possible problems. This means there is no universally superior algorithm for optimization tasks.",4.0051429271698,"The ""no free lunch"" (NFL) theorem for optimization establishes that any optimization algorithm's elevated performance over a specific class of problems is counterbalanced by a corresponding decrease in performance over other classes of problems. This implies that no algorithm performs better than random search across all possible optimization problems; each algorithm may excel in particular scenarios but will underperform in others, resulting in an overall average performance that is uniform when considering all possible problems. Thus, the theorem highlights the limitations of universally effective optimization algorithms, emphasizing that there are no algorithms that can guarantee superior performance on all types of optimization tasks."
qwen2.5:7b-instruct-q4_0,"What is the concept behind Parallel Algorithm Portfolio?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 general-ized to denote a combination of different algorithms intended to outperform the search performance of any of its constituent algorithms [3, 6, 11-14]. Here we extend that formulation, so that an algorithm portfolio schedules its constituent algorithms to run concurrently on a set of processors. Let an algorithm be any CSP solver, as described in the previous section. Given a set A = {a1, a2, …, am} of m algorithms, a set P = {x1, x2, …, xn} of n problems, and a set of B consecutive time intervals T = {t1, t2, …, tB}, a simple schedule Sk for a prob-lem on a single processor specifies 

Document 4: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 5: 
 of processors [22], decomposition methods have shown their potential on many more processors (e.g., 64 cores or more in [19]). We will explore this in future work.  
9   Conclusions This paper presents WG, a constructor for non-parallel algorithm portfolios based on case-based reasoning and a greedy algorithm. It formulates parallel algorithm portfo-lio construction as an integer-programming problem, and generalizes WG to RSR-WG, a constructor for parallel algorithm portfolios based on a property of the optimal solution to the inherent integer-programming problem. To address a set of problems one at a time, RSR-WG creates portfolios of deterministic algorithms offline. Exper-iments show that the parallel algorithm portfolios produced by RSR-WG are statisti-cally significantly better than those produced by naïve parallel versions of popular portfolio constructors. Moreover, with only a few additional processors, RSR-WG portfolios are competitive with an oracle solver on a single processor.  Acknowledgements. This research was supported in part by the National Science Foundation under grants IIS-0811437, CNS-0958379 and CNS-0855217, and the City University of New York High Performance Computing Center. 

Document 6: 
 Ann Math Artif Intell (2006) 48:85–106
DOI 10.1007/s10472-007-9050-9
Learning parallel portfolios of algorithms
Marek Petrik ·Shlomo Zilberstein
Published online: 25 May 2007
© Springer Science + Business Media B.V. 2007
Abstract A wide range of combinatorial optimization algorithms have been devel-
oped for complex reasoning tasks. Frequently, no single algorithm outperforms all
the others. This has raised interest in leveraging the performance of a collection
of algorithms to improve performance. We show how to accomplish this using a
Parallel Portfolio of Algorithms (PPA). A PPA is a collection of diverse algorithms
for solving a single problem, all running concurrently on a single processor until
a solution is produced. The performance of the portfolio may be controlled by
assigning different shares of processor time to each algorithm. We present an
effective method for ﬁnding a PPA in which the share of processor time allocated
to each algorithm is ﬁxed. Finding the optimal static schedule is shown to be an
NP-complete problem for a general class of utility functions. We present bounds on
the performance of the PPA over random instances and evaluate the performance
empirically on a collection of 23 state-of-the-art SAT algorithms. The results show
signiﬁcant performance gains over the fastest individual algorithm in the collection.
Keywords Algorithm portfolios ·Resource bounded reasoning ·
Combinatorial optimization
Mathematics Subject Classiﬁcations (2000) 68T05 ·68T20 

","A parallel algorithm portfolio refers to a set of diverse algorithms that solve a single problem concurrently. The performance of the portfolio can be controlled by allocating different amounts of processor time to each algorithm, aiming to leverage their combined strengths to outperform any individual component.",2.9661900997161865,"The concept behind a Parallel Algorithm Portfolio (PAP) involves leveraging multiple algorithms simultaneously to solve problems more efficiently than any single algorithm could on its own. A PAP consists of a diverse set of algorithms that run concurrently on a processor, each contributing to the overall problem-solving process. The performance of the portfolio can be enhanced by dynamically allocating processor time shares to each algorithm, which allows for optimal utilization of their strengths. The goal is to improve performance in solving complex reasoning tasks and combinatorial optimization problems by combining the strengths of various algorithms, especially when no single algorithm consistently outperforms the others. This approach allows for greater efficiency and improved outcomes compared to relying on a single algorithm or a naive combination of them."
qwen2.5:7b-instruct-q4_0,"Please provide some approaches to how Parallel Algorithm Portfolio can be constructed.
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately
allocate computational resources to different solvers, while the
parallel portfolios do not necessarily require any extra resource
allocation since each solver is simply assigned with the same
amount of resources. Third, a parallel portfolio could be eas-
ily converted to a sequential portfolio by using the algorithm
selection methods [39] to build selectors on the solvers in the
portfolio, which means the portfolios generated by construc-
tion approaches (e.g., the approach proposed in this article)
could be further used for constructing sequential portfolios.
In this article, we propose a novel approach called the
generative adversarial solver trainer (GAST) for the auto-
matic construction of parallel portfolios. Unlike the existing
construction approaches, GAST would generate additional
training instances and construct a parallel portfolio with the
dynamically changing training set. More speciﬁcally, GAST
puts instance generation and portfolio construction in an adver-
sarial game. The instance generation aims to generate the hard
problem instances that could not be solved well by the cur-
rent portfolio, while the portfolio construction aims to ﬁnd a
new component solver for the portfolio to better solve these 

Document 3: 
 1 for j := 1 ... n do
2 obtain portfolio c(j)
1:k by running AC on conﬁguration space ∏k
l=1{(c) | c ∈ C} on I using m for time t/n
3 choose ˆc1:k ∈ arg minc(j)
1:k |j∈{1...n} m(c(j)
1:k, I) that achieved best performance on I according to m
4 return ˆc1:k
3.1.1. Simultaneous conﬁguration of all component solvers (Global)
Our ﬁrst portfolio conﬁguration method is the straightforward extension of standard algorithm conﬁguration to the 
construction of a parallel portfolio (see Algorithm 1). Speciﬁcally, if the given solver has ℓ parameters, we treat the portfolio 
c1:k as a single algorithm with ℓ · k parameters inducing a conﬁguration space of size |C|k, and conﬁgure it directly. As 
noted above, we identify a single conﬁguration as the best of n independent runs of AC . These runs can be performed in 
parallel, meaning that this procedure requires wall clock time t/n if n machines—one for each AC run—with k cores are 
available. The used CPU time will be the given time budget t for Lines 1 and 2 in Algorithm 1 and some small overhead 
ϵ to choose the best portfolio in Line 3. The scalability of this approach is limited by the fact that the global conﬁguration 

Document 4: 
 Learning Algorithm Portfolios for Parallel Execution Xi Yun1 and Susan L. Epstein1, 2  1 Department of Computer Science, The Graduate School of The City University of New York, New York, NY 10016, USA 2 Department of Computer Science, Hunter College of The City University of New York, New York, NY 10065, USA xyun@gc.cuny.edu, susan.epstein@hunter.cuny.edu 
Abstract. Portfolio-based solvers are both effective and robust, but their prom-ise for parallel execution with constraint satisfaction solvers has received rela-tively little attention. This paper proposes an approach that constructs algorithm portfolios intended for parallel execution based on a combination of case-based reasoning, a greedy algorithm, and three heuristics. Empirical results show that this method is efficient, and can significantly improve performance with only a few additional processors. On problems from solver competitions, the resultant algorithm portfolios perform nearly as well as an oracle. Keywords: constraint satisfaction, algorithm portfolio, parallel processing, ma-chine learning. 

Document 5: 
 sign of parallel algorithm portfolios—sets of solvers that are run in parallel on a given instance of a decision problem, such 
as SAT, until the ﬁrst of them ﬁnds a solution [39,27].
In this work,1 we study generic methods for solving a problem we call Automatic Construction of Parallel Portfolios 
(ACPP): automatically constructing a static2 parallel solver from a sequential solver or a set of sequential solvers. This 
task can be understood as falling within the programming by optimization paradigm [34] in that it involves the design of 
software in which many design decisions have been deliberately left open during the development process (here exposed as 
parameters of SAT solvers) to be made automatically later (here by means of an automated algorithm conﬁgurator) in order 
to obtain optimized performance for speciﬁc use cases. Hence, all that is required by our ACPP methods is a sequential 
solver whose conﬁguration space contains complementary conﬁgurations.
We study three variants of the ACPP problem. First, we consider building parallel portfolios starting from a single, 
highly parametric sequential solver design. However, for well-studied problems (e.g., SAT), there often exist a wide range of 
different solvers that contribute to the state of the art (see, e.g., [73]). Complementarities among such solvers can be ex-
ploited by algorithm portfolios, whether driven by algorithm selection (like SATzilla [72]) or by parallel execution (such as 

Document 6: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

","Parallel algorithm portfolios can be constructed using methods like simultaneous configuration of all component solvers, greedy algorithms combined with heuristics, and machine learning techniques such as the Generative Adversarial Solver Trainer (GAST). These approaches leverage parallel execution to enhance solver performance on complex problems.",2.7403786182403564,"Several approaches can be used for constructing Parallel Algorithm Portfolios, as highlighted in the provided context:

1. **Generative Adversarial Solver Trainer (GAST)**: This novel approach focuses on dynamic instance generation in conjunction with portfolio construction. GAST creates challenging problem instances that the current portfolio struggles to solve, then seeks to integrate new solvers into the portfolio to better address these difficulties.

2. **Case-Based Reasoning and Heuristic Combination**: An approach combining case-based reasoning with greedy algorithms and heuristics can efficiently construct parallel algorithm portfolios. This method has demonstrated substantial performance improvements, especially with the addition of a few processors.

3. **Automatic Construction of Parallel Portfolios (ACPP)**: This method involves using an existing sequential solver or multiple sequential solvers to build a static parallel portfolio. It exploits the complementary configurations of sequential solvers and applies automatic algorithm configuration to optimize performance for specific problems.

4. **Simultaneous Configuration**: A straightforward method extends the standard algorithm configuration approach to create a parallel portfolio. All component solvers are configured simultaneously, treating the portfolio as a single algorithm with multiple parameters, allowing the configuration to achieve enhanced performance.

5. **Parallel Execution of Solvers**: The fundamental concept of a parallel portfolio is to run multiple solvers in parallel on a given problem instance until one solver finds a solution. This allows for the leveraging of the strengths of various solvers collectively.

6. **Adaptive Mechanisms**: Incorporating adaptive strategies, like reactive search or hyper-heuristics, can help dynamically tailor the solver configurations during problem-solving, enabling the portfolio to adjust to the specific needs of different instances in real-time.

Overall, these methods emphasize the importance of parallelism and the strategic combination of different solving techniques to enhance computational efficiency."
qwen2.5:7b-instruct-q4_0,"Can co-evolution be used in Parallel Algorithm Portfolio construction?
","Document 1: 
 in a series of benchmark problems. Finally, the paper will end with
a brief conclusion and a discussion of future work in Section 5.
2 RELATED WORK
2.1 Portfolio of evolutionary algorithm
In the areas of evolutionary algorithms, algorithm portfolio is ap-
plied to increase the probability of finding a better solution by
allocating computational resources to several complementary algo-
rithms. The algorithm portfolio frameworks in the literature can
be classified into two categories as the parallel-based framework
and the sequential-based framework.
For the parallel-based framework, all candidates will run simul-
taneously in multiple sub-processes. Population-based algorithm
portfolio (PAP) is a typical example [16], which allocates compu-
tational resources before the optimization according to the prior
knowledge.Eachalgorithmhasitsownpopulationandevolveinde-
pendently,buttheinformationissharedamongdifferentalgorithms
bymigrationstrategy.Besides,otherparallel-basedportfolioframe-
works like AMALGAM-SO [21] and the UMOEAs [6] collect the
performance of algorithms during the optimisation process and
allocate more resources to the better algorithm.
On the other hand, the sequential based framework only runs
onealgorithmatmostofthetimeduringtheprocessofoptimisation.
Different from the parallel-based algorithm portfolio, this kind of
frameworktrytoselectthebestalgorithmindifferentoptimisation 

Document 2: 
 tic is likely to have the right properties for
this approach to be useful, thus making it
very general in terms of applications.
So far we have assumed that the com-
ponent algorithms are completely inde -
pendent of each other and do not com-
municate. They can be thought of as
“competing” with one another for ma-
chineresources.However,allowingforco -
operation or dependencies among the in-
dividualalgorithmswhiletheyarerunning
simultaneously can improve performance
(18, 19). This possibility raises the inter-
estingquestionoftheextenttowhichour
economics approach to portfolio algo-
rithms can also benefit from cooperation.
Basically, cooperation will introduce sta-
tistical correlations between the perfor -
mance of the individual algorithms, and
we will accordingly define the correlation
between them as
  
cov•t1,t2 
 var•t1 var•t2 
(3)
where cov(t1, t2) denotes the covariance of
theperformanceofthetwoalgorithms.The
effect of cooperation, when manifested in
negativecorrelations,istoincreasetheper-
formanceaswellasreducetherisk(Fig.5).
This change is easily understood in that
negative correlations mean that one algo-
rithmisparticularlygoodpreciselyonthose 

Document 3: 
 So far, we have shown that our procedures were able to construct effective parallel portfolios based on single solvers 
with rich design spaces. There is considerable evidence from the literature and from SAT competitions that strong portfolios 
can also be built by combining entirely different solvers in their default conﬁgurations (see, e.g., SATzilla [72], ppfolio [63]
and pfolioUZK [70]). For instance, ppfolio was obtained simply by combining the best solvers from the previous competition 
into a parallel portfolio. pfolioUZK included more state-of-the-art solvers from 2011 and relied on additional experiments 
to ﬁnd the best combination of solvers in a portfolio. Neither portfolio considers the conﬁguration space of the component 
solvers and therefore both can be seen as simple baselines for other parallelization approaches, including ours. However, 
ppfolio and pfolioUZK use Plingeling as a portfolio component. Since we aim to investigate the strength of our ACPP methods 
without additional human expert knowledge on parallel solving, we ﬁrst consider only sequential solvers as the basis for 
our ACPP approach. This section and the following section investigates the extension of our automatic techniques to the 
construction of portfolios based on the conﬁguration spaces spanned by such solver sets.
4.1. Approach
As long as all of our component solvers are sequential, we can simply use the ACPP procedures deﬁned in Section 3. 
We can accommodate the multi-solver setting by introducing a solver choice parameter for each portfolio component (see 

Document 4: 
 1
Few-shots Parallel Algorithm Portfolio Construction
via Co-evolution
Ke Tang, Senior Member, IEEE, Shengcai Liu, Member, IEEE, Peng Yang, Member, IEEE,
and Xin Yao, Fellow, IEEE
Abstract—Generalization, i.e., the ability of solving problem
instances that are not available during the system design and
development phase, is a critical goal for intelligent systems.
A typical way to achieve good generalization is to learn a
model from vast data. In the context of heuristic search, such a
paradigm could be implemented as conﬁguring the parameters
of a parallel algorithm portfolio (PAP) based on a set of
“training” problem instances, which is often referred to as PAP
construction. However, compared to traditional machine learning,
PAP construction often suffers from the lack of training instances,
and the obtained PAPs may fail to generalize well. This paper
proposes a novel competitive co-evolution scheme, named Co-
Evolution of Parameterized Search (CEPS), as a remedy to
this challenge. By co-evolving a conﬁguration population and an
instance population, CEPS is capable of obtaining generalizable
PAPs with few training instances. The advantage of CEPS in
improving generalization is analytically shown in this paper. Two
concrete algorithms, namely CEPS-TSP and CEPS-VRPSPDTW,
are presented for the Traveling Salesman Problem (TSP) and 

Document 5: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, co-evolution can be used in parallel algorithm portfolio construction. The paper describes using a competitive co-evolution scheme called CEPS to construct generalizable portfolios with few training instances, demonstrating its potential for this purpose.",2.8373162746429443,"Yes, co-evolution can be used in Parallel Algorithm Portfolio (PAP) construction. The context indicates that a competitive co-evolution scheme, specifically named Co-Evolution of Parameterized Search (CEPS), is proposed for this purpose. CEPS aims to enhance generalization in PAPs, which is particularly beneficial given the challenges of constructing effective portfolios from limited training instances. By co-evolving a population of configurations along with a population of problem instances, CEPS is capable of producing more generalizable PAPs, highlighting the advantage of this approach in constructing effective portfolios."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the PAP approach called ""GLOBAL""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 RDW 13.24 1.38 11.0 24.1 11.5–16.5 %
LEUKOCYTES (%) 7.61 3.16 0.76 33.82 %
LEUKOCYTES 7614.73 3190.62 760.0 33820.0 3500–10500µL
that have led to the production of a choice in a speciﬁc exam-
ple. The notion of explanation in this work is aligned with
Samek et al. ( 2019).
As stated previously, it is possible to analyze a model rela-
tion between input and output in two ways: looking at local
explanations and global interpretation/explanations (Molnar
2019). Here, we use these two terms interchangeably . While
global interpretations enlighten scientists about what features
may be more representative to the model given a data sam-
ple, local explanations show what led the model to the current
output.
In our scope, global interpretations can help healthcare
researchers to question data sample quality and also direct
the search for more relevant features. Single explanations of
the COVID-19 diagnosis can assist healthcare professionals
that are working directly with potentially infected patients in
their decision-making.
7.1 Global Interpretation
For the glass-box models, global interpretation can be quali-
tatively evaluated by analyzing its feature weights. For these
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 633
Table 2 Results of experiments 

Document 3: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 6: 
 (CEC2005) [12], denoted asfcec1 −fcec14. These 27 functions
span a diverse set of problem features, such as multimodality,
ruggedness, ill-conditioning, interdependency, etc. They pro-
vided an ideal platform for our investigation on reducing risk
on a large variety of problems. Short descriptions of these
functions are presented in Tables I and II. More details of these
functions can be found in [11] and [12]. In our experiments,
all the functions were solved in 30 dimensions.
B. Experimental Settings
All the results presented in this paper were obtained by
executing 30 independent runs for each experiment. Since
we expect the PAP framework to be general enough so that
alternative algorithms can be incorporated with little effort,
it should not rely much on the reﬁnement of the constituent
algorithms. Hence, we did not ﬁne-tune the parameters of
the constituent algorithms to ﬁt PAP. When implementing
SaNSDE, we used all the parameter settings suggested in
the original publication [7]. As suggested in [8], a linearly
decreasing inertia weight over the course of the search is
employed in our implementation of wPSO. The two coefﬁ-
cients of wPSO were both set to 1.49445. We assumed the
researchers who proposed G3PCX and CMA-ES are at the
best position to implement the two algorithms and ﬁne-tune
the parameters. Hence, we simply used the source code of 

","The term ""GLOBAL"" in the context of the PAP (Professional Adaptability Program) approach likely refers to a framework or methodology that aims to develop global competencies and skills among professionals. 

In the Professional Adaptability Program, GLOBAL is an acronym for:

- **G**lobal Perspective: Developing awareness and understanding of different cultures and contexts.
- **O**pen-Mindedness: Cultivating flexibility in thinking and being receptive to new ideas.
- **B**alanced Approach: Encouraging a holistic view that balances various aspects of professional life, such as personal and organizational goals.
- **L**earning Agility: Fostering the ability to learn quickly and apply new knowledge effectively in changing environments.
- **A**daptability: Enhancing one's capacity to adjust to new situations or changes in their work environment.
- **L**ifetime Development: Promoting a mindset of continuous learning and personal growth throughout one's career.

The GLOBAL framework serves as a comprehensive approach to help professionals develop the skills necessary for success in an increasingly interconnected and dynamic global landscape. It emphasizes not just technical proficiency but also the soft skills required to thrive in diverse, complex environments.",5.682362079620361,"The idea behind the PAP (Population Allocation Paradigm) approach called ""GLOBAL"" relates to investigating the performance of the algorithm across a wider set of problems as opposed to focusing solely on singular optimization issues. PAP seeks to reduce the risk by applying various population-based search algorithms to multiple problems simultaneously, rather than optimizing for just one. This framework allows these algorithms to interact and share insights through a multipopulation model where computation time (measured by fitness evaluations) is intelligently allocated among different constituent algorithms. This collaboration enables the algorithms to leverage diverse strengths and promote cooperation, thereby enhancing overall optimization performance. The goal is to foster a more robust algorithm performance that can adapt to various challenges presented by a broad spectrum of problems."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the PAP approach called ""PARHYDRA""?
","Document 1: 
 on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is
worthy of allocating the limited computation time to different
algorithms so that the overall performance can be enhanced.
In contrast, a speciﬁc topology/structure for the relationship
between subpopulations is usually explicitly deﬁned in existing
dEAs [26]–[29], [32], [33]. Third, we are more interested
in reducing the overall risk of an algorithm ona spectrum
of problems. This issue has never been investigated in the
literature of dEAs.
In addition to dEAs, another notable work that is related
to PAP is the isolation strategy employed by hierarchically
organized evolution strategies (HOES) [34], [35]. In HOES,
a population is partitioned into a number of subpopulations,
and ES is run on them with different control parameters (e.g.,
search step sizes). The isolation strategy works in such a
way that the subpopulations evolve separately for a predeﬁned
isolation period (e.g., a few generations), and then the control
parameters are updated based on the individuals generated in
the subpopulations during the isolation period. Unlike PAP,
the motivation of the isolation strategy is not to make use of 

Document 2: 
 SCENARIOS , THAT IS , SAT-SMALL-BIAS-1/2/3/4. T HE NAME OF THE
CONSTRUCTION APPROACH IS USED TO DENOTE THE PORTFOLIOS
CONSTRUCTED BY IT. “PARHYDRA-A” R EFERS TO PARHYDRA
CONFIGURING BASED ON AUGMENTED TRAINING SETS .AP A R - 1 0
SCORE IS SHOWN IN BOLDFACE IF IT WAS NOT SIGNIFICANTLY
DIFFERENT FROM THE BEST TEST PERFORMANCE IN THE SCENARIO
(ACCORDING TO A WILCOXON SIGNED -RANK TEST WITH p = 0.05)
To answer this question, in each of the eight SAT scenarios,
that is, SAT-SMALL/BIAS-1/2/3/4, we used the same instance
generation procedure as in GAST (lines 13–18 in Algorithm 1)
to generate a large set of instances based on the training set.
The size of the generated set is ﬁve times the size of the
training set. Recall that the training set contains 50 instances,
we thus obtained an augmented training set of 300 instances
in each SAT scenario, and then PARHYDRA was used to
construct a parallel portfolio on these augmented training sets,
and then the obtained portfolio was tested on the test sets. As
before, each portfolio was tested by running it on the test set 

Document 3: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 4: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 5: 
 be used to build parallel portfolios. (Building a parallel portfolio in this way has the added advantage that no instance 
features are required, since there is no need to select among algorithms.) We introduce some enhancements to this 
approach for the parallel portfolio setting (discussed in Section 3.1.2), and refer to our method as parHydra.
3. Some parallel solvers only achieve strong performance when running on more than one core; such solvers will not 
be found by a greedy approach like parHydra, which only adds one conﬁguration at a time and does not recognize 
interaction effects that arise between different threads of a parallel solver. To overcome this problem, we introduce a 
new method called parHydrab, which augments parHydra to train b solvers per iteration. This method trades off the 
computational beneﬁt of parHydra’s greedy approach with the greater coverage of Global.
We evaluated our ACPP methods on SAT. We chose this domain because it is highly relevant to academia and industry 
and has been widely studied. We thus had access to a wide range of strong, highly parametric solvers and were assured 
that the bar for demonstrating eﬃcacy of parallelization strategies was appropriately high. We note that our approach is not 
limited to SAT solvers and can be directly applied to other domains. To evaluate our methods in the single-solver setting, we 
studied both Lingeling and clasp: prominent, highly parametric state-of-the-art solvers for SAT. Lingeling won a gold medal in 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The term ""PARHYDRA"" does not appear to be a commonly recognized or standardized acronym in the context of the Public-Private Partnership (PAP) approach. However, based on the letters and their potential meanings within project management or organizational structures, we can break down what each letter might represent:

1. **P** - Planning
2. **A** - Analysis
3. **R** - Resource Management
4. **H** - Human Resources
5. **Y** - Yield/Youth (or another specific entity)
6. **D** - Development
7. **R** - Risk Management
8. **A** - Action

When combined, this could represent a structured approach to project management or organizational strategy that includes planning, analysis, resource and human resource management, yield/youth integration, development, risk management, and action.

Without more context, it's challenging to provide a definitive answer about ""PARHYDRA,"" but if you are referring to a specific organization or academic institution, they might have developed this approach for their particular needs. If you can provide additional context or the source of this term, I could offer a more precise explanation.",5.77008867263794,"The PAP approach called ""PARHYDRA"" refers to a method for constructing parallel portfolios using augmented training sets. It focuses on creating a parallel portfolio by utilizing a large augmented training set generated from a smaller training set, thereby optimizing the performance of the portfolio on various instances. This approach is designed to leverage multiple algorithms simultaneously without relying on specific instance features, allowing for more effective performance in SAT scenarios. PARHYDRA enhances algorithm selection by training multiple solvers concurrently, recognizing interaction effects, and maximizing computational efficiency."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the PAP approach called ""CLUSTERING""?
","Document 1: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 2: 
 proposed a big data k-means clustering, and a big data fuzzy k-means missing values 
approach that resulted in robust and efficient output for big data and offered reasonable 
execution times. The two imputation techniques surpassed in most cases mean imputa -
tion and elimination of the instances with lost values during classification. Offer robust 
and efficient results for Big Data datasets, offering reasonable execution times. The fuzzy 
k-means approach was proved to provide better results for high percentages of miss -
ing values in the data, while the k-means performed better with the dataset that had 
lower amounts of missing values. Zhang et al. [102], also proposed a multiple imputation 
clustering based approach that handled missing values in big longitudinal trial data in 
e-Health. The proposed concept proved that it could be easily adapted for different types 
of clustering for big incomplete longitudinal trial data in eHealth services.
Ensemble methods
Ensemble methods are strategies that make multiple models and then combine them to 
produce a single improved result. This method usually produces more precise results 
than a single model would. This has been the case in machine learning competitions, 
where the triumphant models used ensemble techniques [103]. Studies have confirmed 
that ensemble missing data handling algorithms outperform single base machine learn -
ing algorithms [104–108]. Also, ensemble methods can be implemented in parallel com -
puting environments, which are necessary to process missing data in big datasets. These 
ensemble algorithms are a group of techniques that their decisions are combined in a 

Document 3: 
 Built upon automatic algorithm conﬁguration, the automatic
construction of parallel algorithm portfolios (PAPs) [11]–[15]
seeks to identify a set of conﬁgurations to form a PAP. Each
conﬁguration in the PAP is called a component solver. To
solve a problem instance, all the component solvers are run
independently, typically in parallel, to get multiple solutions.
Then, the best solution will be taken as the output of the PAP.
Although a PAP would consume much more computational
resources than a single-conﬁguration solver, it has two im-
portant advantages. First, the performance of a PAP on any
given instance is the best performance achieved among its
component solvers on the instance. In other words, by ex-
ploiting the complementarity between the component solvers,
a PAP could achieve a much stronger overall performance than
any of its component solver. Second, considering the great
development of parallel computing architectures [16] (e.g.,
multi-core CPUs) over the last decade, exploiting parallelism
has become very important in designing efﬁcient solvers for
computationally hard problems. PAPs employ parallel solution
strategies, and thus allow exploiting modern high-performance
computing facilities in an extremely simple way.
From the practical point of view, a PAP construction method
is expected to identify a PAP that generalizes well, i.e.,
performs well not only on the instance set used during the 

Document 4: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 limited time budget. Although numerous algorithms are readily
applicable for this type of problems, their performance usually
varies signiﬁcantly from problem to problem. This implies that
there is an inherent risk associated with the selection of an
algorithm. Unfortunately, identifying a suitable (or optimal)
algorithm for a speciﬁc problem is a nontrivial task due to the
lack of prior knowledge. The limited time budget also prohibits
us from trying out different algorithms and then choosing the
best one. Instead of betting the entire time budget on a single
algorithm, we proposed that such a risk can be reduced by
distributing the time budget to multiple algorithms. Based on
this idea, a general framework called PAP has been proposed
in the context of population-based search algorithms. PAP
typically consists of a number of constituent algorithms, each
of which is allowed to run with a portion of the time budget.
Allocation of computation time is implemented by dividing the
whole population into a number of subpopulations, and main-
taining one for each constituent algorithm. To further boost
the performance, interaction among constituent algorithms is
carried out through regularly migrating individuals among the
subpopulations. We proposed a pairwise metric to compare
the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget. 

","The CLUSTERING approach for constructing a PAP first splits the training instance set into groups based on similarities, then constructs PAPs for each group, and finally combines them to form the final PAP.",2.7101378440856934,"The PAP approach called ""CLUSTERING"" is based on instance grouping. It involves splitting the training dataset into distinct groups or clusters before constructing the algorithm portfolio. This method focuses on organizing instances in a way that allows for more tailored and effective algorithm configurations, as it takes into account the similarities among the instances. By grouping similar instances together, the CLUSTERING approach aids in identifying which algorithms perform better on specific types of problems, thereby improving the overall performance of the PAP."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the PAP approach called ""PCIT""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 them.
PCIT conducts the instance transfer with the help of in-
cumbent configurations (i.e., the best configurations found
by the AC procedure). In each subset, the instances which
cannot be solved satisfactorily by the corresponding incum-
bent are identified as the ones that need to be transferred,
and the target subset of each transferred instance is deter-
mined according to how well the incumbent on the candi-
date subset could perform on the instance. In essence, the
incumbent on a subset can be seen as a common special
characteristic of those “similar” instances (in the sense they
share the same high-quality configurations) within the sub-
set, and PCIT uses it to identify those “dissimilar” instances
and find better subsets for them. In each subset, the perfor-
mance of the incumbent on each instance could be obtained
from the rundata collected from the AC procedure runs.
However, while determining the target subsets for the trans-
ferred instances, how well the incumbents on the candidate
subsets would perform on the transferred instances are un-
known. One way to obtain these performances is to actually
test these incumbents on the transferred instances, which
however would introduce considerable additional compu-
tational costs. To avoid this, PCIT builds empirical perfor-
mance models (EPM) (Hutter et al. 2014) based on the col-
lected rundata to predict these performances.
1562
Algorithm Framework 

Document 3: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 4: 
 of the more state-of-the-art Plingeling. Such results indicate
PCIT could identify powerful parallel portfolios, with little
human effort involved. Thus the portfolios constructed by
PCIT could conveniently provide at least two advantages.
That is, they are high-quality parallel solvers, and they could
be used as starting points for the development of more ad-
Table 4: Test results of parallel solvers on the test set of SAT-
Single and SAT-Multi. The performance of a solver is shown
in boldface if it was not significantly different from the best
performance (according to a permutation test with 100000
permutations and significance level p = 0.05).
SAT-Single SAT-Multi
#TOS PAR-10 PAR-1 #TOS PAR-10 PAR-1
PCIT 181 119 21 35 1164 219
Priss6 225 146 25 - - -
PfolioUZK - - - 36 1185 213
Plinegling-bbc452 276 32 33 1090 199
vanced parallel solvers.
Conclusion
In this paper we proposed a novel ACPP method, named
PCIT, which utilized an instance transfer mechanism to im-
prove the quality of the instance grouping. The experimen-
tal results on two widely studied problem domains, SAT and
TSP, have demonstrated the effectiveness of PCIT. Currently
PCIT relies on the instance features to build the EPM. Since 

Document 5: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The term you are referring to might be Parent-Child Interaction Therapy (PCIT). PCIT is a highly structured and evidence-based therapeutic intervention designed for families with young children, particularly those who have been maltreated or at risk for maltreatment. It focuses on improving the quality of parent-child interactions.

Here’s an overview of what PCIT involves:

1. **Enhancing Parent-Child Relationships**: PCIT aims to improve communication, attachment, and cooperation between parents and their children through structured play-based activities.

2. **Behavioral Management Skills**: Parents are taught specific skills for managing challenging behaviors in a positive way, such as using time-outs effectively and reinforcing good behavior with praise and rewards.

3. **Structured Environment**: The therapy takes place in a specially designed room that allows the therapist to observe and guide interactions between parents and children.

4. **Phased Approach**: PCIT is typically divided into two phases:
   - **Phase I (Observation Phase)**: Parents learn skills while being observed by the therapist.
   - **Phase II (Independent Practice Phase)**: Parents practice new skills in real-life situations with their child under the guidance of a therapist.

5. **Cultural Sensitivity**: PCIT is adaptable to different cultural and linguistic backgrounds, making it more inclusive for diverse families.

6. **Empowerment**: The goal is not just to manage behaviors but also to empower parents by providing them with tools and confidence to effectively interact with their children.

PCIT has been widely used in child welfare systems and early intervention programs to support the well-being of young children and their caregivers.",7.503355979919434,"The PAP approach known as ""PCIT"" (Performance-based Instance Transfer) leverages an instance transfer mechanism aimed at enhancing the quality of instance grouping. It identifies instances that are poorly solved by existing configurations (the incumbents) and transfers these instances to more suitable subsets based on performance predictions. This is achieved by utilizing empirical performance models (EPM) to predict how well incumbents would perform on these transferred instances without the need for extensive additional computational costs. The effectiveness of PCIT has been demonstrated through experimental results on various problem domains, indicating that it can construct high-quality parallel solvers efficiently with minimal human intervention."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the PAP approach called ""CEPS""?
","Document 1: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

Document 2: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 3: 
 this approach no longer guarantees the validity of the above
assumption. As a consequence, Θ′may generalize worse than
Θ. A remedy to prevent this as much as possible is to increase
redundancy in the evolution of the PAP. More speciﬁcally, one
could repeat the conﬁguration-removal procedure to Θ for n
times, leading to ntemporary PAPs, ¯Θ1,..., ¯Θn; then for each
temporary PAP ¯Θ, the new conﬁgurations are identiﬁed and
inserted, leading to n new PAPs, Θ′
1,..., Θ′
n, each of which
is of the same size as Θ; ﬁnally, the PAP among them that
performs best against T ∪T′ is retained.
III. C O-EVOLUTION OF PARAMETERIZED SEARCH
By incorporating the above-described procedure into the co-
evolution process, we arrive at the proposed CEPS framework,
as demonstrated in Algorithm 1. In general, CEPS consists
of two major phases, i.e., an initialization phase (lines 2-
7), and a co-evolution phase (lines 8-27) which could be
further subdivided into alternating between the evolution of
the conﬁguration population (representing the PAP) (lines 10-
15) and the evolution of the instance population (representing
the training instances) (lines 17-26) for MaxIte iterations in
total. These modules are detailed as follows. 

Document 4: 
 contributions of this paper include:
1) A novel PAP construction framework, namely Co-
Evolution of Parameterized Search (CEPS), is proposed.
It is also shown that CEPS approximates a process that
minimizes the upper bound, i.e., a tractable surrogate,
of the generalization performance.
2) To demonstrate the implementation details of CEPS as
well as to assess its potential, concrete instantiations are
also presented for two hard optimization problems, i.e.,
TSP and the Vehicle Routing Problem with Simultane-
ous Pickup–Delivery and Time Windows (VRPSPDTW)
[18]. Computational studies conﬁrm that CEPS is able
to obtain PAPs with better generalization performance.
3) The proposal of CEPS extends the realm of Co-
Evolution, for the ﬁrst time, to evolving algorithm
conﬁgurations and problem instances. Since CEPS does
not invoke domain-speciﬁc knowledge, its potential ap-
plications can go beyond optimization problems, even to
planning and learning problems.
The rest of the paper is organized as follows. Section II in-
troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 cores each (2.20 GHz, 30 MB Cache), running Centos 7.5.
D. Results and Analysis
We report the #TOs, PAR-10 and PANC achieved by the
PAPs on the testing set in Table II and also visualize their
medians and variance across all the testing instances by box-
plots in Figure 1. Note the mean value is also plotted in Figure
1 (indicated by “ ▲”) to show that for a PAP how its PAR-
10/PANC is affected by the outliers (the timeout cases) which
would be hidden by boxplots. In Table II the #TOs, PAR-
10/PANC of a PAP is highlighted in grey if it achieved the best
performance. One could make three important observations
from these results. First, the PAPs obtained by CEPS have
the smallest number of timeouts in all the six experiments,
which means they have the highest success rate for solving the
testing instances among all the tested PAPs. Recall that CEPS
actively searches in the instance space to identify the hard-
to-solve instances for further improving the generalization of
the PAPs. Such a mechanism makes CEPS the method that is
least affected by the hard testing instances which signiﬁcantly
differs from the given training instances. This could be further
veriﬁed by Figure 1, in which CEPS is the method that has
the least gap between the mean value (which takes timeouts
into account) and median value (which naturally ﬁlters out the 

","The term ""CEPS"" in the context of the Public Administration (PA) approach known as PAP (Policy Analysis Process) stands for **Cost-Effectiveness, Policy, and Strategy**. This framework is used to analyze and improve public policies and their implementation.

### Key Components of CEPS:

1. **Cost-Effectiveness**: 
   - Focuses on the relationship between the costs incurred in implementing a policy and its desired outcomes.
   - Evaluates whether the resources are being used efficiently to achieve the intended results.

2. **Policy**:
   - Involves defining, designing, and evaluating policies or programs.
   - Includes understanding the problem, setting objectives, developing strategies, and assessing the impacts of different policy options.

3. **Strategy**:
   - Relates to planning and managing how resources are allocated to achieve specific policy goals.
   - Focuses on identifying the most effective ways to implement a policy, including organizational structures, processes, and stakeholder engagement.

### Purpose of CEPS:

- **Systematic Approach**: Provides a structured method for analyzing policies from multiple perspectives—costs, effectiveness, and strategic implementation.
- **Decision-Making**: Helps policymakers make informed decisions by considering the financial and operational implications of different policy options.
- **Resource Allocation**: Ensures that resources are allocated in the most efficient manner to achieve desired outcomes.
- **Transparency**: Enhances transparency in the decision-making process by clearly outlining costs, expected benefits, and strategic approaches.

By integrating these three components—cost-effectiveness, policy analysis, and strategy—CEPS offers a comprehensive approach to public administration and policy analysis.",7.778733015060425,"The Co-Evolution of Parameterized Search (CEPS) is a framework designed for constructing Parameterized Algorithms (PAPs) that aims to improve their generalization performance across a set of problems rather than on a single one. The key idea behind CEPS is to co-evolve both the configurations of the algorithms and the problem instances, where the framework consists of two major phases: an initialization phase and a co-evolution phase. In this approach, the algorithm dynamically searches the instance space to identify challenging instances that can enhance the generalization capabilities of the obtained PAPs. CEPS utilizes historical data from the generated instances to assist in tuning parameters effectively, thereby enabling better performance even in a zero-shot learning setting. This allows CEPS to be applicable beyond traditional optimization problems, extending its potential to areas like planning and learning. Overall, CEPS seeks to minimize the upper bounds of generalization performance while reducing the risk across multiple problems."
qwen2.5:7b-instruct-q4_0,"Please give me the names of some state-of-the-art SAT solvers.
","Document 1: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 2: 
 Intelligence (AAAI’02), (pp. 674–681). AAAI Press / The MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building lo-
cal search sat solvers from components. In Proceedings of the Twenty-ﬁrst International Joint Conference
on Artiﬁcial Intelligence (IJCAI’09), (pp. 517–524).
Le Berre, D. & Simon, L. (2004). Fifty-ﬁve solvers in Vancouver: The SAT 2004 competition. In Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory and Applications of Satisﬁability Testing: Proceedings of the Seventh
International Conference (SAT’04), volume 3542 of Lecture Notes in Computer Science , (pp. 321–344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y . (2002). Learning the empirical hardness of optimization
problems: The case of combinatorial auctions. In Hentenryck, P. V . (Ed.), Principles and Practice of
Constraint Programming – CP 2002: Eighth International Conference , volume 2470 of Lecture Notes in
Computer Science, (pp. 556–572). Springer Verlag, Berlin, Germany. 

Document 3: 
 212
state-of-the-art SLS algorithms. SATenstein-LS has
41 parameters that control the selection and behavior of
its components, leading to a total conﬁguration space of
size 4.82 ×1012. Automatically-identiﬁed conﬁgurations of
SATenstein-LS have been shown to achieve better per-
formance than existing state-of-the-art SLS algorithms on
several well-known instance distributions.
Instances
We investigated the effectiveness of Hydra on four distri-
butions, drawing on well-known families of SAT instances.
Because no state-of-the-art SLS algorithms are able to prove
unsatisﬁability, we considered only satisﬁable instances. We
identiﬁed these by running all complete algorithms that won
a SAT competition category between 2002 and 2007 for
one hour. First, the BM data set is constructed from 500
instances taken from each of the six distributions used by
KhudaBukhsh et al. (2009) (
QCP, SWGCP, FACT, CBMC,
R3FIX, and HGEN), split evenly into training and test sets.
Second, the INDU data set is a mixture of 500 instances
from each of the CBMC and FACT distributions, again split
evenly into training and test sets. Third and fourth, the HAND
and RAND data sets include all satisﬁable instances from the
Random and Handmade categories of the SAT Competitions 

Document 4: 
 form (a conjunction of disjunctions of variables and their negations) and determine a satisﬁable assignment or prove that 
none exists. An incomplete SAT solver can ﬁnd satisfying assignments, but not prove unsatisﬁability.
Most state-of-the-art complete SAT solvers are based on conﬂict-driven clause learning (CDCL; [57]). Their parameters 
control variable selection for branching decisions, clause learning and restart techniques. State-of-the-art incomplete SAT 
solvers use stochastic local search (SLS; [38]), and their parameters control the selection of the variable whose value is 
modiﬁed in each local search step as well as the diversiﬁcation and additional intensiﬁcation strategies. Furthermore, there 
exist several preprocessing techniques (e.g., [21]) to simplify formulas and their parameters control how long and how 
aggressive preprocessing will be used – too much preprocessing can remove important structural information and hence, 
it can increase the hardness of formulas. The eﬃcacy of SAT solvers depends on multiple heuristic components whose 
basic functions and the interplay between them are controlled by parameters. Some parameters are categorical (e.g., choice 
between different search strategies in SLS), while many others are integer- or real-valued (e.g., the damping factor used in 
computing heuristic variable scores in CDCL).
Parallel SAT solvers have received increasing attention in recent years. ManySAT [30,31,29] was one of the ﬁrst parallel 

Document 5: 
 tial solver baselines, indicating the great benefit by combin-
ing complementary configurations obtained from a rich con-
figuration space.
Comparison with Hand-designed Parallel Solvers
To further evaluate the portfolios constructed by PCIT, we
compared them with the state-of-the-art manually designed
parallel solvers. Specifically, we considered the ones con-
structed for SAT. We chose Priss6 (Manthey, Stephan, and
Werner 2016) to compare with the one constructed in SAT-
Single, since Priss6 is the official parallel version of Riss6
(the base solver in SAT-Single). For the same reason, we
chose PfolioUZK (Wotzlaw et al. 2012) (the gold medal
winning solver of the parallel track of the SAT’12 Chal-
lenge) to compare with the one constructed in SAT-Multi.
Finally, we chose Plingeling (version bbc) (Biere 2016),
the gold medal winning solver of the parallel track of the
SAT’16 Competition, to compare with both. Note that all
the manually designed solvers considered here have imple-
mented far more advanced parallel solving strategies (e.g.,
clause sharing) than only independently running component
solvers in parallel. In the experiments the default settings of
these solvers were used and the same statistical tests as be-
fore were conducted. As shown in Table 4, on SAT-Single 

Document 6: 
 ACM 52(10):56–67.
Balyo, T.; Heule, M. J. H.; and J¨arvisalo, M., eds. 2016. Proceed-
ings of SAT Competition 2016: Solver and Benchmark Descrip-
tions, volume B-2016-1 of Department of Computer Science Se-
ries of Publications B. University of Helsinki.
Battiti, R.; Brunato, M.; and Mascia, F., eds. 2008. Reactive
Search and Intelligent Optimization. Springer.
Biere, A. 2016. Splatz, Lingeling, Plingeling, Treengeling, Yal-
SAT Entering the SAT Competition 2016. In Balyo et al. (2016),
44–45.
Burke, E. K.; Gendreau, M.; Hyde, M.; Kendall, G.; Ochoa, G.;
¨Ozcan, E.; and Qu, R. 2013. Hyper-heuristics: A Survey of the
State of the Art. Journal of the Operational Research Society
64(12):1695–1724.
Gomes, C. P., and Selman, B. 2001. Algorithm Portfolios. Arti-
ficial Intelligence 126(1-2):43–62.
Hamadi, Y ., and Wintersteiger, C. M. 2013. Seven Challenges in
Parallel SAT Solving. AI Magazine 34(2):99–106. 

","Some state-of-the-art SAT solvers include ManySAT, Priss6, PfolioUZK, and Plingeling.",2.447195291519165,"Some state-of-the-art SAT solvers mentioned in the context include Priss6, PfolioUZK, Plingeling, and SATenstein-LS. Other solvers are also referenced, such as Splatz, Lingeling, and Treengeling."
qwen2.5:7b-instruct-q4_0,"Please give me the names of some state-of-the-art TSP solvers.
","Document 1: 
 For surveys of the problem and its applications, the reader is referred to the excellent
volumes edited by Lawler et al. [24] and Gutin and Punnen [11].
Local search with k-exchange neighborhoods, k-opt, is the most widely used heu-
ristic method for the TSP. k-opt is a tour improvement algorithm, where in each step
k links of the current tour are replaced by k links in such a way that a shorter tour is
achieved.
It has been shown [7] that k-opt may take an exponential number of iterations and
that the ratio of the length of an optimal tour to the length of a tour constructed byk-opt
can be arbitrarily large when k ≤ n/2 − 5. Such undesirable cases, however, are very
rare when solving practical instances [31]. Usually, high-quality solutions are obtained
in polynomial time. This is, for example, the case for the Lin–Kernighan heuristic,
one of the most effective methods for generating optimal or near-optimal solutions
for the symmetric TSP. High-quality solutions are often obtained, even though only a
small part of the k-exchange neighborhood is searched.
In the original version of the Lin–Kernighan heuristic [ 25], the allowable
k-exchanges (or k-opt moves) are restricted to those that can be decomposed into a
2- or 3-exchange followed by a (possibly empty) sequence of 2-exchanges. This restric- 

Document 2: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 3: 
 10.0 
12.5 
15.0 
17.5 PAR-10 (c) TSP-3
Fig. 2: Visual comparison in boxplots of the medians and variance of the test performance of each TSP solver across the
testing instances. Note the mean value is also plotted, indicated by “ ▲”.
TABLE III: Comparison of the state-of-the-art TSP solvers
with the PAPs obtained by CEPS, on the testing set. #TOs
refers to number of total timeouts. PAR-10 is the penalized
average runtime-10. Performance of a solver is highlighted in
grey if it achieved the best testing performance.
TSP-1 TSP-2 TSP-3
#TOs PAR-10 #TOs PAR-10 #TOs PAR-10
LKH-default 131 30.84 137 31.98 150 34.73
LKH-tuned 29 8.23 34 9.40 27 7.67
EAX-default 69 17.98 73 18.91 69 17.95
EAX-tuned 33 10.97 30 10.38 29 10.12
VBS 6 3.82 7 4.26 6 4.13
CEPS 6 2.74 4 2.15 2 1.94
signiﬁcant performance improvement, though still obviously
falling behind of the PAPs obtained by CEPS. Second, the only
solver that could match the PAP’s performance level in one
of the three scenarios, is the VBS of the algorithm portfolio
considered by the algorithm selection approaches [29], [30].
However, in TSP-2 and TSP-3, the performance advantage of 

Document 4: 
 troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in
Section VI. Section VII concludes the paper with discussions.
II. P ARAMETERIZED SOLVERS MADE GENERALIZABLE
A. Notations and Problem Description
Assume a PAP is to be built for a problem class (e.g., TSP),
for which an instance of the problem class is denoted as s, and
the set of all possible sis denoted as Ω. Given a parameterized
algorithm, each component solver of the PAP is a conﬁguration
(full instantiation) of the algorithm. Generally speaking, the
parameterized algorithm can be any concrete computational
process, e.g., a traditional heuristic search process such as the
LK Heuristic for TSP or even a neural network [19]–[21] that
outputs a solution for a given instance of the target problem
class. Let θ denote a conﬁguration and let Θ denote a PAP
that contains K different conﬁgurations (component solvers),
i.e., Θ = {θ1,...,θ K}. The quality of a conﬁguration θ on a 

Document 5: 
 H. Trautmann, “Evolving diverse TSP instances by means of novel and
creative mutation operators,” in Proceedings of the 15th ACM/SIGEVO
Conference on Foundations of Genetic Algorithms, FOGA’2019. Pots-
dam, Germany: ACM, Aug 2019, pp. 58–71.
[38] Y . Nagata and S. Kobayashi, “A powerful genetic algorithm using edge
assembly crossover for the traveling salesman problem,” INFORMS
Journal on Computing , vol. 25, no. 2, pp. 346–363, 2013.
[39] X. Xie and J. Liu, “Multiagent optimization system for solving the
traveling salesman problem (tsp),” IEEE Transactions on Systems, Man,
and Cybernetics, Part B , vol. 39, no. 2, pp. 489–502, 2009.
[40] C. Wang, D. Mu, F. Zhao, and J. W. Sutherland, “A parallel simulated
annealing method for the vehicle routing problem with simultaneous
pickup-delivery and time windows,” Computers & Industrial Engineer-
ing, vol. 83, pp. 111–122, 2015.
[41] W. Huang and T. Zhang, “Vehicle routing problem with simultaneous
pick-up and delivery and time-windows based on improved global arti-
ﬁcial ﬁsh swarm algorithm,” Computer Engineering and Applications ,
vol. 52, no. 21, pp. 21–29, 2016. 

Document 6: 
 Portfolio-based algorithm selection for SAT,” Journal of Artiﬁcial In-
telligence Research, vol. 32, pp. 565–606, 2008.
[29] P. Kerschke, L. Kotthoff, J. Bossek, H. H. Hoos, and H. Trautmann,
“Leveraging TSP solver complementarity through machine learning,”
Evolutionary Computation, vol. 26, no. 4, pp. 597–620, 2018.
[30] K. Zhao, S. Liu, Y . Rong, and J. X. Yu, “Leveraging TSP solver
complementarity via deep learning,” arXiv preprint arXiv:2006.00715 ,
2020.
[31] L. Kotthoff, “Algorithm selection for combinatorial search problems: A
survey,” AI Magazine, vol. 35, no. 3, pp. 48–60, 2014.
[32] C. D. Rosin and R. K. Belew, “New methods for competitive coevolu-
tion,” Evolutionary Computation, vol. 5, no. 1, pp. 1–29, 1997.
[33] J. I. van Hemert, “Evolving combinatorial problem instances that are
difﬁcult to solve,” Evolutionary Computation , vol. 14, no. 4, pp. 433–
462, 2006.
[34] K. Helsgaun, “General k-opt submoves for the Lin-Kernighan TSP 

","Sure! Here are some state-of-the-art Traveling Salesman Problem (TSP) solvers:

1. **Concorde** - One of the most famous and widely used solvers for the TSP, especially for problems with up to a few thousand cities. It uses branch-and-cut algorithms and can be interfaced with other programs.

2. **LKH (Lin-Kernighan-Helsgaun)** - This is one of the most efficient TSP solvers available today. It's particularly known for its performance on large-scale problems, and it has been used to solve instances with hundreds or even thousands of cities.

3. **Google OR-Tools** - Part of Google’s open-source software suite for optimization, this includes a powerful TSP solver that can handle both symmetric and asymmetric TSPs. It supports various solvers like branch-and-bound, constraint programming, and linear programming.

4. **TSPLIB** - While not a solver itself, TSPLIB is a library of sample instances of the TSP, including benchmark problems used in testing other solvers. Many state-of-the-art solvers are tested against these instances to ensure their effectiveness.

5. **PILS (Parallel Iterated Local Search)** - This is another powerful solver that uses local search heuristics and parallel computing to solve large-scale TSPs efficiently.

6. **LinKernighanHeuristic** - A heuristic approach based on the Lin-Kernighan algorithm, which can provide very good solutions for large problems in a reasonable amount of time.

7. **Gurobi Optimizer** - An industrial-strength solver that supports various optimization models including TSPs. It uses advanced branch-and-bound techniques and is known for its efficiency and scalability.

8. **CVRPSP (Capacitated Vehicle Routing Problem Solver)** - While primarily designed for routing problems, it also includes a module for solving the TSP as part of more complex routing scenarios.

9. **Ant Colony Optimization (ACO) Solvers** - Various implementations of ACO algorithms are available and can be effective for certain types of TSP instances, especially when combined with other heuristics or exact methods.

10. **OpenTSP Solver** - An open-source solver that uses a combination of heuristic and exact methods to solve the TSP problem.

These solvers vary in their approach (exact algorithms vs. heuristics), efficiency on different types of problems, and scalability. Depending on your specific needs (e.g., problem size, computational resources, and desired solution quality), you might choose one or a combination of these tools.",12.375569343566896,"Some state-of-the-art TSP solvers mentioned in the context are LKH (Lin-Kernighan heuristic), EAX (Edge Assembly Crossover), and VBS (Variable Neighborhood Search)."
qwen2.5:7b-instruct-q4_0,"Please give me the names of some state-of-the-art Automatic Algorithm Configuration approaches.
","Document 1: 
 Journal of Artiﬁcial Intelligence Research 75 (2022) 425-487 Submitted 02/2022; published 10/2022
A Survey of Methods for Automated
Algorithm Conﬁguration
Elias Schede elias.schede@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Jasmin Brandt jasmin.brandt@upb.de
Alexander Tornede alexander.tornede@upb.de
Department of Computer Science,
Paderborn University, Paderborn, Germany
Marcel Wever marcel.wever@ifi.lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Viktor Bengs viktor.bengs@ifi.lmu.de
Institute of Informatics,
LMU Munich, Munich, Germany
Eyke H¨ ullermeier eyke@lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Kevin Tierney kevin.tierney@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews 

Document 2: 
 2 Related Work
2.1 Automatic Algorithm Conﬁguration
Several approaches exist in the literature for the automatic tuning
of algorithms. Some of these were created for a speciﬁc algorithm
or task. For example, [24] devises a modular algorithm for solving
constraint satisfaction problems (CSPs). Using a combination of ex-
haustive enumeration of all possible conﬁgurations and parallel hill-
climbing, the technique automatically conﬁgures the system for a
given set of training instances. Another approach, presented in [30],
focuses on the conﬁguration of adaptive algorithms, employing a se-
quential parameter optimization approach.
Other approaches automatically design and build an entire solver
to best tackle a set of example training instances. For example, [28]
uses genetic programming to create an evolutionary algorithm (EA).
Here the chromosome is an EA operation like the selection of par-
ents, mutation, or crossover, and the task is to ﬁnd a sequence of
the genetic programming operators that is best suited for the speci-
ﬁed problem. For SAT, [8] classiﬁes local search (LS) approaches by
means of context-free grammars. This approach then uses a genetic
programming approach to select a good LS algorithm for a given set
of instances.
There also exist approaches that are applicable to more general al-
gorithms. For example, in order to tune continuous parameters, [5]
suggests an approach that determines good parameters for individual
training instances. This approach ﬁrst evaluates the extreme param- 

Document 3: 
 470
A Survey of Methods for Automated Algorithm Configuration
General AC systems
D-SMAC https://github.com/tqichun/distributed-SMAC3
GPS https://github.com/YashaPushak/GPS
irace https://github.com/MLopez-Ibanez/irace
OAT (GGA) https://docs.optano.com/algorithm.tuner/current/
ParamILS https://www.cs.ubc.ca/labs/algorithms/Projects/ParamILS/
PyDGGA http://ulog.udl.cat/software/
REVAC https://github.com/ChrisTimperley/RubyREVAC
SMAC 3 https://github.com/automl/SMAC3
Benchmarks
AClib https://bitbucket.org/mlindauer/aclib2/src/master/
DAC https://github.com/automl/DAC
Table 7: List of available software in the realm of AC.
References
Belarmino Adenso-Diaz & Manuel Laguna. Fine-tuning of algorithms using fractional ex-
perimental designs and local search. Operations research, 54(1):99–114, 2006.
¨Ozg¨ ur Akg¨ un, Nguyen Dang, Ian Miguel, Andr´ as Z Salamon, & Christopher Stone. In- 

Document 4: 
 cal (e.g., choice of heuristic). Categorical parameters can be used to select and combine discrete
building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics); consequently,
our general view of algorithm conﬁguration includes the automated construction of a heuristic algo-
rithm from such building blocks. To the best of our knowledge, the methods discussed in this article
are yet the only general ones available for the conﬁguration of algorithms with many categorical
parameters.
We now give an overview of what follows and highlight our main contributions. After for-
mally stating the algorithm conﬁguration problem in Section 2, in Section 3 we describe ParamILS
(ﬁrst introduced by Hutter, Hoos & St ¨utzle, 2007), a versatile stochastic local search approach for
automated algorithm conﬁguration, and two of its instantiations, BasicILS and FocusedILS.
We then introduce adaptive capping of algorithm runs, a novel technique that can be used to
enhance search-based algorithm conﬁguration procedures independently of the underlying search
strategy (Section 4). Adaptive capping is based on the idea of avoiding unnecessary runs of the
algorithm to be conﬁgured by developing bounds on the performance measure to be optimized.
We present a trajectory-preserving variant and a heuristic extension of this technique. After dis-
cussing experimental preliminaries in Section 5, in Section 6 we present empirical evidence show-
ing that adaptive capping speeds up both BasicILS and FocusedILS. We also show that BasicILS
268 

Document 5: 
 rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 
Due to the separation between a tuning and a production phase, 
automatic algorithm conﬁguration is also known as oﬄine param- 
eter tuning to differentiate it from online approaches that adapt or 
control parameter settings while solving an instance [13,50] . Nev- 
ertheless, online approaches also contain parameters that need to 
be deﬁned oﬄine, for example, which and how parameters are 
adapted at run-time; such parameters and design choices can be 
conﬁgured by an oﬄine tuning method [59] . 
In our research on making the algorithm conﬁguration process 
more automatic , we have focused on racing approaches. Birattari 
et al. [19,20] proposed an automatic conﬁguration approach, F- 
Race, based on racing [64] and Friedman’s non-parametric two-way 
analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 

Document 6: 
 formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de-
veloped and used for more than a decade, and many of the fundamental techniques
date back even further. However, it has only recently become possible to effectively
solve complex conﬁguration problems involving target algorithms with dozens of
parameters, which are often categorical and conditional. This success is based in
3 Automated Algorithm Conﬁguration and Parameter Tuning 65
part on the increased availability of computational resources, but has mostly been
enabled by methodological advances underlying recent conﬁguration procedures.
Still, we see much room (and, indeed, need) for future work on automated algo-
rithm conﬁguration and parameter tuning methods. We believe that in developing
such methods, the fundamental features underlying all three types of methods dis-
cussed in this chapter can play an important role, and that the best methods will
employ combinations of these. We further believe that different conﬁguration pro-
cedures will likely be most effective for solving different types of conﬁguration
problems (depending, in particular, on the number and type of target algorithm pa-
rameters, but also on regularities in the parameter response). Therefore, we see a
need for research aiming to determine which conﬁgurator is most effective under 

","Some state-of-the-art Automatic Algorithm Configuration approaches include ParamILS, BasicILS, FocusedILS, and Iterated F-Race (I/F-Race).",2.652896881103516,"Some state-of-the-art Automatic Algorithm Configuration approaches include D-SMAC, GPS, irace, OAT (GGA), ParamILS, PyDGGA, REVAC, and SMAC 3."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the AAC approach called ""irace""?
","Document 1: 
 analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 
the I/F-Race procedure is given in those publications, an imple- 
mentation was not made publicly available. The irace package im- 
plements a general iterated racing procedure, which includes I/F- 
Race as a special case. It also implements several extensions al- 
ready described by Birattari [19] , such as the use of the paired 
t -test instead of Friedman’s test. Finally, irace incorporates sev- 
eral improvements not published before, such as sampling from a 
truncated normal distribution, a parallel implementation, a restart 
strategy that avoids premature convergence, and an elitist racing 
procedure to ensure that the best parameter conﬁgurations found 
are also evaluated on the highest number of training instances. 
The paper is structured as follows. Section 2 introduces the 
algorithm conﬁguration problem and gives an overview of ap- 
proaches to automatic algorithm conﬁguration. Section 3 describes 
the iterated racing procedure as implemented in the irace pack- 
age as well as several further extensions including the elitist irace . 
Section 4 illustrates the steps followed to apply irace to two con- 

Document 2: 
 5.2. Multi-objective optimization metaheuristics 
Besides the application to the MOACO framework described 
above [58] , irace has been applied to aid in the design of other 
multi-objective optimization algorithms. Dubois-Lacoste et al. 
[31] used irace to tune a hybrid of two-phase local search and 
Pareto local search (TP + PLS) to produce new state-of-the-art al- 
gorithms for various bi-objective permutation ﬂowshop problems. 
Fisset et al. [33] used irace to tune a framework of multi-objective 
optimization algorithms for clustering. When applied to a suﬃ- 
ciently ﬂexible algorithmic framework, irace has been used to de- 
sign new state-of-the-art multi-objective evolutionary algorithms 
[16,17] . 
5.3. Anytime algorithms (improve time-quality trade-offs) 
There is often a trade-off between solution quality and com- 
putation time: Algorithms that converge quickly tend to produce 
better solutions for shorter runtimes, whereas more exploratory al- 
gorithms tend to produce better solutions for longer runtimes. Im- 
proving the anytime behavior of an algorithm amounts to improv- 
ing the trade-off curve between solution quality and computation 
time such that an algorithm is able to produce as high quality so- 
lutions as possible at any moment during their execution. López- 

Document 3: 
 effect on the budget consumed is lower. The same experiment for 
the ACOTSP scenario showed that the best conﬁgurations become 
worse when T new or T ﬁrst are increased. This is due to the fact that 
ACOTSP has a homogeneous training set and, therefore, sampling 
new candidates is more important than executing a large number 
of instances. 
5. Other applications of irace 
Since the ﬁrst version of the irace package became publicly 
available in 2012, there have been many other applications of 
irace . In this section, we provide a list of the applications of the 
irace package of which we are aware at the time of writing. Some 
of these applications go beyond what is traditionally understood as 
algorithm conﬁguration, demonstrating the ﬂexibility of irace . 
54 M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 
5.1. Algorithm conﬁguration 
The traditional application of irace is the automatic conﬁgura- 
tion of algorithms. Several publications have used irace when eval- 
uating or designing algorithms for problems such as the traveling 
salesman with time windows [60] , slot allocation [77] , generalized 
hub location [68] , ﬂow shop [14] , virtual machine placement [86] , 
online bin packing [92] , graph coloring [23] , image binarization 
[69] , network pricing [90] , combined routing and packing prob- 

Document 4: 
 ming by optimization paradigm [40] . Moreover, the importance of 
properly tuning the parameters of algorithms before analyzing and 
comparing them is becoming widely recognized. We hope that the 
development of the irace package will help practitioners and re- 
searchers to put these ideas into practice. 
Acknowledgements 
This work received support from the COMEX project (P7/36) 
within the Interuniversity Attraction Poles Programme of the Bel- 
gian Science Policy Oﬃce, and the EU FP7 ICT Project COLOMBO, 
Fig. A.1 1. Scheme of the user-provided components required by irace . 
Table A1 
Parameters of irace corresponding to the description 
of iterated racing given in Section 3.2 . The full list of 
irace parameters is available in the user guide. 
Iterated racing parameter irace parameter 
B maxExperiments 
C (cost measure) targetRunner 
μ mu 
N min minNbSurvival 
T ﬁrst firstTest 
T each eachTest 
Statistical test testType 
Cooperative Self-Organizing System for Low Carbon Mobility at 
Low Penetration Rates (agreement no. 318622). Thomas Stützle and 
Mauro Birattari acknowledge support from the Belgian F.R.S.-FNRS, 
of which they are Senior Research Associates. Leslie Pérez Cáceres 
acknowledges support of CONICYT Becas Chile. 
Appendix A. The irace package 

Document 5: 
 man problem with time windows: adapting algorithms from travel-time to 
makespan optimization. Appl Soft Comput 2013;13(9):3806–15 . 
[61] López-Ibáñez M., Dubois-Lacoste J., Pérez Cáceres L., Stützle T., Birattari M.. 
2016a. The irace package: Iterated racing for automatic algorithm conﬁgura- 
tion. http://iridia.ulb.ac.be/supp/IridiaSupp2016-003/ . 
[62] López-Ibáñez M, Pérez Cáceres L, Dubois-Lacoste J, Stützle T, Birattari M. The 
irace package: user guide. Technical Report TR/IRIDIA/2016-004. IRIDIA, Uni- 
versité Libre de Bruxelles, Belgium; 2016b . http://iridia.ulb.ac.be/IridiaTrSeries/ 
IridiaTr2016-004.pdf . 
[63] Marmion M-E , Mascia F , López-Ibáñez M , Stützle T . Automatic design of hy- 
brid stochastic local search algorithms. In: Blesa MJ, Blum C, Festa P, Roli A, 
Sampels M, editors. Hybrid metaheuristics, volume 7919 of lecture notes in 

Document 6: 
 ent robot behaviors and the criteria to transition between behav- 
iors. Each module can be further customized by means of several 
parameters. A particular combination of behaviors and transitions 
represents one controller, that is, an instance of the software that 
controls the robots in the swarm. The performance of a particu- 
lar controller is evaluated by means of multiple simulations. The 
search for the best controller over multiple training simulations is 
carried out by means of irace . The authors report that this system 
is not only able to outperform a previous system that used F-race 
[34] , but also a human designer, under the scenarios studied by 
them. 
6. Conclusion 
This paper presented the irace package, which implements the 
iterated racing procedure for automatic algorithm conﬁguration. It- 
erated racing is a generalization of the iterated F-race procedure. 
M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 55 
The primary purpose of irace is to automatize the arduous task of 
conﬁguring the parameters of an optimization algorithm. However, 
it may also be used for determining good settings in other compu- 
tational systems such as robotics, traﬃc light controllers, compil- 
ers, etc. The irace package has been designed with simplicity and 
ease of use in mind. Despite being implemented in R , no previ- 
ous knowledge of R is required. We included two examples for the 

","Irace implements an iterated racing procedure for automatic algorithm configuration, which includes enhancements like using a paired t-test for statistical significance, and optimizing multiple algorithms' parameters to improve performance across various optimization problems.",3.1381373405456543,"The ""irace"" package implements an automatic algorithm configuration approach known as iterated racing. The key idea behind irace is to automate the process of tuning the parameters of optimization algorithms to improve their performance. It uses a statistical procedure to compare different parameter configurations, iteratively refining and sampling from the parameter space to identify the most effective settings. Irace incorporates various techniques such as parallel implementation, restart strategies to avoid premature convergence, and the ability to use different statistical tests for evaluating configurations. It has proven to be flexible and effective across various applications, including multi-objective optimization and robotics."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the AAC approach called ""SMAC""?
","Document 1: 
 intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods
SMAC Sequential model-based optimization for algorithm conﬁguration (SMAC) (Hutter
et al., 2011; Lindauer et al., 2021) can be seen as one of the ﬁrst fully-ﬂedged model-
based AC approaches, as it features solutions for many of the limitations of the previously
discussed SMBO techniques. SMAC generalizes TB-SPO to perform conﬁguration over
multiple problem instances so that it can support categorical parameters and handle tight
time constraints.
To support multiple problem instances, SMAC adapts the intensiﬁcation strategy of
TB-SPO to iteratively evaluate conﬁgurations on randomly sampled combinations of seeds
and problem instances. When doing so, it ensures that conﬁgurations are compared only
based on a performance estimate computed on the same randomly sampled set of problem
instances. Furthermore, SMAC’s surrogate model can generalize across problem instances
by incorporating problem instance features. To this end, a surrogate model is learned on
the joint problem instance and conﬁguration space to predict the performance of a given
conﬁguration on a given problem instance.
As a means to deal with a mixture of categorical and numerical parameters, SMAC 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 algorithm configuration framework. JArtifIntellRes 2009; 36 : 
267–306. 
9. Lindauer M, Eggensperger K, Feurer M et al. SMAC3: a ver- 
satile Bayesian optimization package for hyperparameter opti- 
mization. JMachLearnRes 2022; 23 : 54. 
10. Santu SKK, Hassan MM, Smith MJ et al. Automl to date and 
beyond: challenges and opportunities. ACMComputSurv 2022; 
54 : 175. 
Page7of9
Downloaded from https://academic.oup.com/nsr/article/11/8/nwae132/7639406 by guest on 18 September 2024 

Document 5: 
 more benchmark instances and compare its performance to
the best conﬁguration observed so far.
SMAC is an anytime algorithm that interleaves the ex-
ploration of new conﬁgurations with additional runs of the
current best conﬁguration to yield both better and more con-
ﬁdent results over time. As all anytime algorithms, SMAC
improves performance over time. While SMAC provably
converges for ﬁnite conﬁguration spaces, it often only ﬁnds
close-to-optimal conﬁgurations for realistic time budgets
and challenging conﬁguration problems.
Automated Portfolio Construction & Hydra
Our method for constructing sequential portfolios is closely
related to Hydra (Xu, Hoos, and Leyton-Brown 2010),
which automatically constructs selection-based portfolios.
Given a parameterized algorithm framework with a space
of algorithmsΘ, Hydra starts with an empty set of can-
didate algorithms C= ∅and iteratively calls an algorithm
conﬁguration method to add algorithms θ ∈Θ that comple-
ment Cbest. In each iteration, it constructs a portfolio from
the current set Cusing the portfolio-based algorithm selec-
tor SATzilla (Xu et al. 2008) and measures its performance
m(C,π) on each benchmark instance π ∈Π (in the ﬁrst it-
eration, m(∅,π) = −∞). The performance metric Hydra
maximizes via its algorithm conﬁguration method is then 

Document 6: 
 Anastacio and Hoos (2020) propose SMAC+PS, which integrates the idea of probabilis-
tic sampling known from irace into SMAC. This enhancement yields improvements over
both SMAC and irace in many cases. In particular, Anastacio and Hoos (2020) account for
the problem that many of the completely randomly sampled conﬁgurations by SMAC often
exhibit rather bad performance and thus, their evaluation yields only limited information.
To this end, the authors suggest to sample conﬁgurations according to a truncated normal
distribution centered around the default conﬁguration.
444
A Survey of Methods for Automated Algorithm Configuration
In (Lindauer & Hutter, 2018b) the authors suggest two diﬀerent strategies to warmstart
model-based AC approaches and apply their suggestions to SMAC, leading to signiﬁcant
speedups from days to hours of conﬁguration time. The idea underlying warmstarting is to
use the evaluations of conﬁgurations from previous runs, i.e., on diﬀerent problem instance
sets, to speed up the conﬁguration process in new runs of the conﬁgurator on a new set of
instances.
Distributed SMAC (Hutter et al., 2012) (D-SMAC) is an extension of SMAC leveraging
parallelization to speed up the conﬁguration process. The main idea behind D-SMAC is
to parallelize target algorithm runs onto available workers as much as possible. For this
purpose, it maintains a queue of target algorithm conﬁguration evaluations to be performed, 

","SMAC (Sequential Model-Based Algorithm Configuration) uses a model-based approach to optimize algorithm configurations by evaluating different settings and using surrogate models to predict performance. It can handle categorical parameters and tight time constraints, iteratively evaluating configurations on sampled problem instances to find close-to-optimal settings over time.",3.066528081893921,"The idea behind the AAC (automatic algorithm configuration) approach called SMAC (Sequential Model-based Optimization for Algorithm Configuration) is to optimize the performance of algorithms by systematically searching for optimal hyperparameter configurations. SMAC achieves this by using a surrogate model that predicts the performance of different configurations based on previously evaluated instances, allowing it to generalize across multiple problem instances. It interleaves the exploration of new configurations with additional runs of the current best configuration, ensuring iterative improvement over time. SMAC also accommodates a mixture of categorical and numerical parameters, making it versatile for a variety of algorithmic applications. Additionally, enhancements like SMAC+PS and D-SMAC have been proposed to improve its performance and speed through more efficient sampling and parallelization strategies."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the AAC approach called ""GGA""?
","Document 1: 
 to be evaluated based on k diﬀerently parameterized instantiations of the UCB criterion.
GGA++ Ans´ otegui et al. (2015) adapt the model-free AC approach GGA to include a
surrogate model. More precisely, the authors use a surrogate model to evaluate the quality
of new conﬁgurations. They integrate this within a crossover operator and call it genetic
engineering. Recall that GGA contains both a competitive and non-competitive popula-
tion in which winning conﬁgurations from the races between members of the competitive
population are recombined with individuals from the non-competitive population. To this
end, the crossover operator generates individuals according to the parameter tree crossover
of the original GGA method and evaluates them using the surrogate. Note that rather
than predicting the solution quality or runtime directly, the surrogate predicts the rank the
individual would have in a tournament. The individuals with the best ranks are accepted
into the population of the next generation in the same way as in GGA.
While the GGA++ surrogate is based on a random forest model, it diﬀers in a key
way. The premise of a random forest is to equally approximate the underlying function over
the complete input space. In the case of AC, this is undesirable as only the areas of the
input space that correspond to high-quality conﬁgurations are of interest. Thus, the authors
present specialized splitting criteria that focuses on only the best conﬁgurations to increase 

Document 2: 
 Our approach uses a similar insight as Bansal and Viswanathan (1993) and Chen and Ludvigson
(2009), who propose using a given set of conditional GMM equations to estimate the SDF with
neural networks, but restrict themselves to a small number of conditioning variables. In order to
deal with the inﬁnite number of moment conditions we extend the classical GMM setup of Hansen
(1982) and Chamberlain (1987) by an adversarial network to select the optimal moment conditions.
A similar idea has been proposed by Lewis and Syrgkanis (2018) for non-parametric instrumental
variable regressions. Our problem is also similar in spirit to the Wasserstein GAN in Arjosvky,
Chintala, and Leon (2017) that provides a robust ﬁt to moments. The Generative Adversarial
Network (GAN) approach was ﬁrst proposed by Goodfellow et al. (2014) for image recognition.
In order to ﬁnd the hidden states in macroeconomic time series we propose the use of Recurrent
Neural Networks with Long-Short-Term-Memory (LSTM). LSTMs are designed to ﬁnd patterns
4We have shared our data and estimated models with Avramov, Cheng, and Metzker (2020). In their comparison
study Avramov, Cheng, and Metzker (2020) also include a portfolio derived from our GAN model. However, they do
not consider our SDF portfolio based on ω but use the SDF loadings β to construct a long-short portfolio based on 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 instead of the fully predetermined lifetimes in the original
GA VaPS and nAMGA VaPS.
Another variation of the GA VaPS lifetime scheme was
described by Bäck et al .[ 18]. The adaptive population GA
(APGA) uses the same lifetime allocation but differs from
GA VaPS in the reproduction cycle and in that, when in-
crementing the ages of individuals in each cycle, the best
individual of that cycle remains unchanged. This adaptive
population strategy is part of an ensemble and is described
in more detail in Section V. This method was also applied to
cooperative coevolution by Iorio and Li [ 109]. An analysis
of APGA by Lobo and Lima [ 135] shows theoretical and
experimental results suggesting an upper bound and a con-
verging population to a constant size that is determined by
the minimum and maximum lifetime parameters. The authors
conclude that the population is not adapted by the APGA
but the size parameter is in fact replaced by the two lifetime
parameters.
Cook and Tauritz [36] suggested two strategies for removing
the population size parameter. FiScIS-EA removes individuals
according to a survival probability, derived by linear scaling
of the individual’s ﬁtness in the range between the minimum
and maximum ﬁtness values present in the population. GC-EA
simply evades choosing a population size by maintaining a
population as large as is allowed by memory limitations. This
requires a well-chosen parent selection mechanism that scales
well, while a survival selection operator is still needed in 

Document 5: 
 3.4.5.2 Generative Adversarial Network Imputation
GANs consist of two parts— a generator and a discriminator
(Goodfellow et al., 2014 ). In an adversarial process, the
generator learns to generate samples that are as close as
possible to the data distribution, and the discriminator learns
to distinguish whether an example is true or generated.
Imputation approaches based on GANs include those in the
work ofYoon et al. (2018); Shang et al. (2017);a n dLi et al.
(2019). Here, we employ one of the most popular approaches of
GAN-based imputation, Genera tive Adversarial Imputation
Nets (GAIN) (Yoon et al., 2018). GAIN adapts the original
GAN architecture as follows. The generator ’s input is the
concatenation of the input data and a binary matrix that
represents the missing values. The discriminator learns to
reconstruct the mask matrix. Its input is the concatenation of
the generator’s output and a hint matrix, which reveals partial
information about the missingness of the original data. The
computation of the hint matrix incorporates the introduced
hyperparameterhint rate. A second hyperparameter α that
GAIN introduces helps to balance the generator ’s
performance for observed and missing values. For training,
we use Adam optimizer with default hyperparameters except
for the learning rate for the generator and the discriminator, 

Document 6: 
 obtain promising initial results, the combined evolution of solutions and their genetic opera-
tors is comparatively challenging, and it remains an open question whether autoconstructive
evolution is indeed superior to non-autoconstructive evolution.
Multi-armed bandits DAC can be cast as a multi-armed bandit problem, where each
conﬁguration represents an arm with an associated reward distribution. The agent is then
tasked to maximize the cumulative reward, i.e., “pull the arm“ that maximizes the reward.
Put diﬀerently, the agent aims to select a conﬁguration that works well for the current time
step. This setting of the bandit problem is particularly challenging because the reward
distributions associated with the respective arms change over time. In Fialho et al. (2010),
various multi-armed bandit approaches are compared with an approach that only considers
rewards obtained within a certain time frame (i.e., a sliding window) to allow for smoother
transitions between conﬁgurations. Note that in this work, the authors assume rewards to
be retrievable during an algorithm run. While this assumption holds for many optimization
approaches, it does not necessarily hold for all of them (e.g., the pre-processing/root node
phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions 

","The term ""GGA"" in the context of Augmentative and Alternative Communication (AAC) likely refers to a specific method or strategy within AAC that focuses on Goal, Group, and Action. However, it's important to note that there isn't a widely recognized standard abbreviation called ""GGA"" for this exact approach.

If you're referring to a specific framework or methodology developed by a particular organization or researcher, here’s what the components might typically represent:

1. **Goal**: Setting clear communication goals that are meaningful and achievable for the individual using AAC.
2. **Group**: Considering the group of people who will be interacting with the user (e.g., family members, caregivers, educators) and tailoring the AAC system to meet their needs as well.
3. **Action**: Focusing on the actions or strategies needed to implement the AAC system effectively in real-life situations.

This approach likely aims to create a holistic and comprehensive plan for AAC implementation that addresses not just the individual user but also the broader communication context and goals.

If you have more specific details about ""GGA"" or if it's part of a particular AAC framework, providing those details would help in giving a more accurate explanation.",6.290748834609985,"The GGA (Genetic Algorithm with Genetic Engineering) approach, as described in the context, involves enhancing the traditional genetic algorithm by integrating a surrogate model to evaluate the quality of new configurations. The surrogate model predicts the rank an individual would achieve in a tournament rather than directly predicting solution quality or runtime, which is important in finding high-quality configurations. GGA employs a competitive and non-competitive population structure, where successful configurations from competitions are recombined through a crossover operator. This adaptation aims to focus on high-performing individuals, utilizing specialized splitting criteria within a random forest model to refine the search process in automatic algorithm configuration."
qwen2.5:7b-instruct-q4_0,"What is the idea behind the AAC approach called ""ISAC""?
","Document 1: 
 ISAC – Instance-Speciﬁc Algorithm Conﬁguration1
Serdar Kadioglu and Yuri Malitsky and Meinolf Sellmann and Kevin Tierney2
Abstract. We present a new method for instance-speciﬁc algorithm
conﬁguration (ISAC). It is based on the integration of the algorithm
conﬁguration system GGA and the recently proposed stochastic off-
line programming paradigm. ISAC is provided a solver with cate-
gorical, ordinal, and/or continuous parameters, a training benchmark
set of input instances for that solver, and an algorithm that com-
putes a feature vector that characterizes any given instance. ISAC
then provides high quality parameter settings for any new input in-
stance. Experiments on a variety of different constrained optimiza-
tion and constraint satisfaction solvers show that automatic algorithm
conﬁguration vastly outperforms manual tuning. Moreover, we show
that instance-speciﬁc tuning frequently leads to signiﬁcant speed-ups
over instance-oblivious conﬁgurations.
1 Introduction
When developing a new heuristic or complete algorithm for a con-
straint satisfaction or a constrained optimization problem, we fre-
quently face the problem of choice. There may be multiple branch-
ing heuristics that we can employ, different types of inference mech-
anisms, various restart strategies, or a multitude of neighborhoods to
choose from. Furthermore, the way in which the choices we make
affect one another is not readily known. The task of making these 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 in Ye et al. (2021) that performance gains can already be achieved when the algorithm
conﬁguration is adapted only once. Furthermore, the hyper-reactive approach of Ans´ otegui
et al. (2017) won several categories at the MaxSAT Evaluation 2016 (Argelich et al., 2016).
Thus, DAC oﬀers signiﬁcant potential for improving algorithms, however, it does require
algorithm designers to more deeply integrate their techniques with AC methods than was
performed in the past. In the following, we discuss the most frequently used approach to
DAC, reinforcement learning (RL). While it is the most popular choice, there also exist
other approaches such as policy portfolios, autoconstructive evolution, and multi-armed
bandits.
Reinforcement learning A challenge in DAC is to evaluate the quality of individual
parameter adjustments since feedback is obtained only at the end of processing a problem
instance. Due to this, it is natural to consider DAC within the realm of reinforcement
learning (we refer to this as DAC-RL). To this end, the DAC problem is modeled in terms of
a Markov decision process (MDP) (Puterman, 1994) based on which a policy π: I×Q→ Θ
is learned from data (Biedenkapp et al., 2020). As a reward signal, typically, the eventually
obtained performance value is propagated back to the agent. Using RL allows a wide variety
of researched mechanisms to be directly applied to DAC.
The roots of DAC extend to a variety of methods that use RL to control parameters of 

Document 4: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 5: 
 ACPP is closely related to the area of automatic algo-
rithm configuration, in which the task is to automatically
identify a high-quality configuration from a configuration
space. In the last few years, several high-performance AC
methods (which could handle considerable large configura-
tion spaces) such as ParamILS (Hutter et al. 2009), GGA
(Ans´otegui, Sellmann, and Tierney 2009), irace (L ´opez-
Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and Leyton-
Brown 2011) have been proposed. As a consequence, re-
cently there has been research interest in utilizing these
AC procedures to automatically identify useful portfolios
of configurations from large configuration spaces. Such at-
tempts were first done in constructing sequential portfolios.
The representative methods for solving this problem are Hy-
dra (Xu, Hoos, and Leyton-Brown 2010) and ISAC (Ka-
dioglu et al. 2010). The basic ideas of these methods were
then adapted to be used in constructing parallel portfolios
(i.e., ACPP), thus resulting in PARHYDRA and CLUSTER-
ING (Lindauer et al. 2017) (see the first section). The main
differences between the automatic construction of sequen-
tial portfolios and of parallel portfolios lie in two aspects: 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","ISAC, or Instance-Specific Algorithm Configuration, aims to automatically identify a high-quality configuration for a solver on unseen problem instances by leveraging previous training data. It uses a search-based method, specifically Genetic Guided Adaptation (GGA), to find the best configuration tailored to specific problems.",3.421088933944702,"ISAC, or Instance-Specific Algorithm Configuration, is an approach aimed at adapting algorithm parameters specifically for different instances of a problem. The fundamental idea is that ISAC leverages a training benchmark set of input instances and uses a feature vector to characterize any given instance. By doing so, ISAC provides high-quality parameter settings tailored to the specifics of new input instances. This method has been shown to outperform manual tuning and typically yields significant speed-ups compared to instance-oblivious configurations. In essence, ISAC aims to customize algorithm performance by aligning configurations closely with the characteristics of the problem instance at hand."
qwen2.5:7b-instruct-q4_0,"What is the difference between metaheuristics and heuristics?
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 principle difference between similarity and statistical learning (i.e., the two alternative approaches to evaluate the dif-
ference and divergence between two data items). Similarity can be defined over a pair of data items/samples while sta-
tistical measures require a large (theoretically infinite) number of independent data observations.
Another promising direction is to build Turing's type-B random machines (or unorganized machines) (Jiang &
Crookes, 2019; Webster, 2012), also random Boltzmann machines, which can possibly lead to a generalized AI. The
inclusion of new neuro-scientific findings into XAI models will make research on XAI more rationalized, and vice
versa: such a cross-disciplinary exploitation will make XAI not only meaningful for AI researchers but also help solve
century-old challenges on how to understand human intelligence, ultimately. Open research questions in this area
include: (i) how best to determine the network/model architecture?; (ii) how best to extract and represent features?;
(iii) what are the best distance metrics and what are the implications?; (iv) which is the best optimization method?; and
(v) how to determine the best set of prototypes that represent the data best (if a prototype-based method is being used)?
CONFLICT OF INTEREST
The authors have declared no conflicts of interest for this article.
DATA AVAILABILITY STATEMENT
Data sharing is not applicable to this article as no new data were created or analyzed in this study. 

Document 3: 
 the same standard. (In contrast, SAT solver competitions have compared solvers with a complex scoring function that includes the performance of all competitors [25].) As formulated here, the differences between two solvers may be simply in their choice of even a single technique, heuristic, or policy that sustains performance diver-sity. Thus an algorithm portfolio can be thought of as a mixture of experts [26], in-cluding variable-ordering and value-ordering heuristics, restart policies, and nogood learning methods. In particular, even if only one heuristic is available, the portfolio could consist of the heuristic and its opposite, or the heuristic and random selection. 

Document 4: 
 (Eds.), Handbook of Metaheuristics (pp. 321–353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search for classiﬁcation
and function approximation. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 59–66). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding the role of noise in stochastic local search: Analysis and experi-
ments. Artiﬁcial Intelligence, 172(8-9), 955–990.
Minton, S. (1993). An analytic learning system for specializing heuristics. In Bajcsy, R. (Ed.),Proceedings of
the Thirteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI’93), (pp. 922–929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically conﬁguring constraint satisfaction programs: A case study. Constraints,
1(1), 1–40.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conﬂicts: A heuristic repair 

Document 5: 
 PSO. 
For the methodological part of the comparisons, there are far more 
studies. Statistical tests, for instance, lay at the core of prior contribu- 
tions on this matter. However, such contributions are frequently writ- 
ten from a statistical point of view –l i k e the one by Dem š ar [39] –
making it diﬃcult for researchers in this ﬁeld to embrace their method- 
ological recommendations. More recently, some tutorials have tried to 
bring together the ﬁelds of meta-heuristics and inferential statistics [4] . 
Some examples can be found in [40] , in which a statistical treatment is 
suggested for distinguishing between measurements of performance in 
adaptive evolutionary algorithms. Another good example is [41] , which 
shows that in a popular real-parameter benchmark (CEC’2005), con- 
ditions needed for running parametric hypothesis tests did not hold, 
and non-parametric tests were thus recommended. More recently, in 
[42] , some recommendations for the comparison of evolutionary algo- 
rithms are provided, which can be even extrapolated to machine learn- 
ing benchmarks. 
Another important issue from a methodological point of view is the 
assessment of the performance of bio-inspired algorithms from the per- 
spective of the experimental design. Some studies [5] provide general 
recommendations to design experiments for the comparison of algo- 
rithms in a similar way to what we do in this contribution. However, 
these recommendations are far more general as it targets a broader scope 

Document 6: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

","Metaheuristics are high-level methodologies that do not deeply adapt to specific problems, allowing them to solve a wide range of problems. Heuristics, in contrast, are more problem-specific rules of thumb used to find solutions efficiently but may not always provide optimal results. Metaheuristics aim to escape local optima by introducing randomness and exploration mechanisms, whereas heuristics rely on predefined rules tailored to the problem at hand.",3.700871229171753,"The main difference between metaheuristics and heuristics lies in their scope and adaptability to specific problems. Heuristics are commonly problem-specific techniques that aim to produce solutions more quickly when classical methods are too slow. They typically focus on a specific problem domain and may produce sub-optimal solutions without addressing the possibility of escaping local optima.

On the other hand, metaheuristics are higher-level methodologies or algorithmic frameworks that can be applied to a wider range of problems. They do not adapt deeply to specific problems; instead, they provide a more generalized approach to problem-solving that allows for the exploration of the solution space and escaping local optima. Metaheuristics often incorporate stochastic components and are inspired by natural processes, making them versatile for tackling hard optimization problems across various domains."
qwen2.5:7b-instruct-q4_0,"What factors and challenges influenced the development of the Automatic Algorithm Configuration approaches?
","Document 1: 
 ods for algorithm development. The methods used in this context 
include experimental design techniques [2,29] , racing approaches 
[20] , and algorithmic methods for parameter conﬁguration, such 
as heuristic search techniques [3,10,41,73,81] , and statistical mod- 
eling approaches [11,43] . These methods have led to an increasing 
automatization of the algorithm design and parameter setting pro- 
cess. 
Automatic algorithm conﬁguration can be described, from a ma- 
chine learning perspective, as the problem of ﬁnding good param- 
eter settings for solving unseen problem instances by learning on 
a set of training problem instances [19] . Thus, there are two clearly 
delimited phases. In a primary tuning phase, an algorithm con- 
ﬁguration is chosen, given a set of training instances representa- 
tive of a particular problem. In a secondary production (or test- 
ing) phase, the chosen algorithm conﬁguration is used to solve un- 
seen instances of the same problem. The goal in automatic algo- 
rithm conﬁguration is to ﬁnd, during the tuning phase, an algo- 
rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 

Document 2: 
 diﬀerent problems (SAT, MIP, ASP, etc.) of varying complexity (number of variables and
problem instances) for tasks of runtime or quality conﬁguration. For DAC, the DACBench
has been proposed (Eimer et al., 2021), although this does not support DAC settings en-
visioned, e.g., by hyper-reactive search. As an alternative to such libraries, AC methods
can also be benchmarked by using surrogate models that are trained on test instances in
advance, resulting in cheaper evaluations when testing (Eggensperger et al., 2018). The ex-
isting benchmarks fail to cover other conﬁgurations settings like the realtime conﬁguration
setting or the conﬁguration of multi-objective target algorithms.
11.4 Novel AC Methodologies
AC methods have become extremely sophisticated and cover a wide range of methodologies
including evolutionary algorithms, statistical tests, and learned surrogate models. There
nonetheless remain opportunities to improve current methods and create better AC algo-
rithms. We note that our goal in this section is not necessarily to specify the methodologies
of the future, but rather to identify the key challenges that remain in the hopes that engaged
466
A Survey of Methods for Automated Algorithm Configuration
readers will ﬁll these gaps with their ideas. To this end, we discuss several challenges faced
by AC approaches: comparing/ranking conﬁgurations, generating new conﬁgurations, and
selecting instances.
Comparing/ranking conﬁgurations This challenge can be summarized as follows: 

Document 3: 
 available for detecting (approximate) independencies. Although there is some evi-
dence that GGA can solve some moderately difﬁcult conﬁguration problems more
effectively than FocusedILS without capping [2], it appears to be unable to reach
the performance of FocusedILS version 2.3 with aggressive capping on the most
challenging conﬁgurations problems [40]. Unfortunately, GGA also offers less ﬂex-
ibility than FocusedILS in terms of the performance metric to be optimised. More
algorithm conﬁguration procedures based on evolutionary algorithms are covered in
Chapter 2 of this book.
Finally, work originating from the Ph.D. project of Hutter [32] has recently over-
come two major limitations of the sequential model-based optimisation methods
discussed in Section 3.4 of this chapter by introducing a procedure that can han-
dle categorical parameters while explicitly exploiting the fact that performance is
evaluated on a set of problem instances. There is some evidence that this procedure,
dubbedSequential Model-based Algorithm Conﬁguration (SMAC) , can, at least on
some challenging conﬁguration benchmarks, reach and sometimes exceed the per-
formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de- 

Document 4: 
 Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews
do not take into account all derivatives of the AC problem, nor do they oﬀer a complete
classiﬁcation scheme. To this end, we introduce taxonomies to describe the AC problem and
features of conﬁguration methods, respectively. We review existing AC literature within
the lens of our taxonomies, outline relevant design choices of conﬁguration approaches,
contrast methods and problem variants against each other, and describe the state of AC in
industry. Finally, our review provides researchers and practitioners with a look at future
research directions in the ﬁeld of AC.
1. Introduction
Diﬃcult computational problems must be regularly solved in many areas of industry and
academia, such as constraint satisfaction problems, Boolean satisﬁability problems (SAT),
vehicle routing problems, ﬁnding a proper machine learning model for a given dataset,
or computing highly complex simulations. Algorithms that were developed to solve such
problems usually have parameters that strongly inﬂuence the behavior of the respective
algorithm and also, for example, the runtime that is required to solve problem instances
©2022 AI Access Foundation. All rights reserved.
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney 

Document 5: 
 better performance than the top-ranked solver from the competition.
9. Related Work
Many researchers before us have been dissatisﬁed with manual algorithm conﬁguration, and various
ﬁelds have developed their own approaches for automatic parameter tuning. We start this section
with the most closely-related work—approaches that employ direct search to ﬁnd good parameter
conﬁgurations—and then describe other methods. Finally, we discuss work on related problems,
such as ﬁnding the best parameter conﬁguration or algorithm on a per-instance basis, and approaches
that adapt their parameters during an algorithm’s execution (see also Hoos, 2008, for further related
work on automated algorithm design).
8. BasicILS was used, because FocusedILS had not yet been developed when that study was conducted.
297
HUTTER , HOOS , LEYTON -BROWN & ST ¨UTZLE
9.1 Direct Search Methods for Algorithm Conﬁguration
Approaches for automated algorithm conﬁguration go back to the early 1990s, when a number of
systems were developed for adaptive problem solving. One of these systems is Composer (Gratch
& Dejong, 1992), which performs a hill-climbing search in conﬁguration space, taking moves if
enough evidence has been gathered to render a neighbouring conﬁguration statistically signiﬁcantly
better than the current conﬁguration. Composer was successfully applied to improving the ﬁve
parameters of an algorithm for scheduling communication between a collection of ground-based
antennas and spacecrafts (Gratch & Chien, 1996). 

Document 6: 
 ration.
1 Introduction
We consider the problem of automatic solver conﬁguration. Practically all solvers have
parameters that are partly ﬁxed by the programmer and partly set by the user. In recent
years, systems have been devised which automate the task of tuning parameters for a
given set of training instances that are assu med to represent typical instances for the
target algorithm.
There are several motivations for such an automation, the ﬁrst being that it is of
course time consuming to tune parameters and it may lead to better results when leaving
the conﬁguration of solvers to a computer rather than doing it by hand.
Moreover, it is conceivable that the existence of an effective tuning environment
will cause algorithm developers to paramete rize more aspects of their algorithms and
thus leave more freedom for algorithmic solutions that are automatically tailored to the
problems of individual users. In particular, many of the SAT solvers that are available
today have parameters which cannot be set through the command line. These parameters
have been ﬁxed to values that the developers have found beneﬁcial without knowledge
about the particular instances a user may want to use the solver for. Automatic parameter
tuning allows solvers to adapt to the ﬁnal environment in which they need to perform.
After being shipped, rather than relying on default parameters, an algorithm can be
⋆ This work was partly supported by the projects TIN2007-68005-C04-04 and TIN2006-15662-
C02-02 funded by the MEC, and by the the National Science Foundation through the Ca-
reer: Cornﬂower Project (award number 0644113). 

","The development of automatic algorithm configuration (AAC) approaches has been shaped by a variety of factors and challenges, including:

1. **Complexity of Tuning Space**: The space of possible configurations can be extremely large and complex, making it difficult to find optimal settings through manual tuning. This complexity has driven the need for automated methods.

2. **Computational Resources**: Running algorithms with different parameter settings can be computationally expensive, especially when dealing with large datasets or high-dimensional problems. Efficient use of resources is crucial in developing effective AAC approaches.

3. **Algorithm Performance Variability**: The performance of an algorithm can vary significantly depending on the specific instance or problem at hand. AAC needs to account for this variability and adapt settings accordingly.

4. **Generalization Across Instances**: One challenge is to find configurations that work well across a wide range of instances rather than just optimizing for a single benchmark. This requires robustness and generalizability in the tuning process.

5. **Interactions Between Parameters**: Many parameters interact with each other, making it challenging to optimize them independently. AAC methods need to consider these interactions to avoid suboptimal configurations.

6. **Exploration vs Exploitation Trade-off**: AAC approaches must balance between exploring new parameter settings and exploiting known good configurations. This trade-off is crucial for efficient tuning.

7. **Algorithm-Specific Knowledge**: Some algorithms have unique characteristics that require specialized tuning techniques. For example, metaheuristics like genetic algorithms or simulated annealing may benefit from different strategies compared to traditional optimization methods.

8. **Instance-Specific Features**: The features of the problem instance can significantly affect which configurations are optimal. AAC approaches need to incorporate these features effectively.

9. **Scalability**: As problems become larger, both in terms of size and complexity, the scalability of AAC methods becomes a critical issue. Methods must be able to handle large-scale instances efficiently.

10. **Interpretability**: While automated tuning can lead to high performance, it is often desirable to have some level of interpretability about why certain configurations are better than others. This helps in understanding and improving algorithms.

11. **Hybrid Approaches**: Often, combining different techniques (e.g., evolutionary methods, random search, local search) leads to more effective AAC approaches. Hybrid strategies can leverage the strengths of multiple methods.

12. **Dynamic Environments**: In some applications, the environment in which an algorithm operates may change dynamically, requiring continuous or adaptive tuning. This adds another layer of complexity to AAC.

13. **Human Expertise and Feedback**: Integrating human expertise and feedback into the tuning process can improve the quality of configurations but also introduces challenges in ensuring that the system remains efficient and robust.

These factors have driven the development of various techniques such as Bayesian optimization, random search, evolutionary algorithms, local search, and hybrid methods, each addressing different aspects of the AAC problem. The goal is to balance exploration with exploitation, handle large and complex tuning spaces, and provide effective solutions for a wide range of problems and instances.",14.111491441726685,"Factors influencing the development of Automatic Algorithm Configuration (AC) approaches include the need for efficient tuning of algorithm parameters to enhance performance and runtime for diverse computational problems. These problems often exhibit complexity due to the variation in the number of variables and instances.

Challenges faced by AC include:

1. **Comparing and Ranking Configurations**: Ensuring that different configurations can be effectively evaluated and ranked based on performance.

2. **Generating New Configurations**: Developing methods to produce novel configurations that may outperform existing ones.

3. **Selecting Instances**: Identifying which problem instances to use for tuning the configurations so that they generalize well to unseen instances.

Additionally, the advancement of methodologies like evolutionary algorithms, surrogate models, and sequential model-based optimization has improved AC, but there are still opportunities for refinement and development in areas not comprehensively covered by existing benchmarks and methodologies."
qwen2.5:7b-instruct-q4_0,"What are the main benefits of AAC?
","Document 1: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 2: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 3: 
 phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions
with three themes in mind. First, we consider what is required for AC techniques to be
more widely adopted in industry. Second, we investigate algorithm conﬁguration settings
that have not yet been considered. Third, we consider what methodological advances could
move the ﬁeld forward.
11.1 Industry Adoption
Compared to AC, HPO has begun to be adopted in both industry and academia (Van der
Blom et al., 2021). Furthermore, HPO is the focus of a number of start-ups, such as
SigOpt (which was acquired by Intel (Intel Corporation, 2019)), MindsDB, pecan.ai, and
dotData, among many more. AC, however, has received comparatively little fanfare or
adoption. While some well-known AC tools have public releases, such as SMAC (Lindauer
et al., 2021), irace (L´ opez-Ib´ anez et al., 2016), and GGA (Ans´ otegui et al., 2009), we are
unaware of widespread use of these in industry. The company Optano GmbH released a 

Document 4: 
 a policy to adapt the conﬁguration dynamically. Note that even realtime AC commits to a
single conﬁguration when running a given instance, while DAC has the freedom to adjust
the conﬁguration according to target algorithm behavior during execution. Similar to oﬄine
AC, DAC can either focus on ﬁnding a policy for a set of problem instances or a policy that
is tailored towards a single problem instance (i.e., per-instance algorithm conﬁguration).
Two requirements must be met to implement DAC: (1) the algorithm in question needs
to support dynamic changes in its conﬁguration and (2) runtime information must be pro-
vided to describe the current state of the target algorithm.
DAC approaches consider two diﬀerent types of features: instance features I, which do
not change during target algorithm execution, and features encoding the internal state Q
of the algorithm. Examples of state features include the current iteration of a local search
algorithm, the current restart number of a SAT method, or the current solution quality for
optimization techniques.
461
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
Figure 7: Illustration of dynamic AC as presented in (Biedenkapp et al., 2020)
Biedenkapp et al. (2020) provide the ﬁrst formal deﬁnition of the DAC setting, however,
there is a signiﬁcant amount of earlier work for learning dynamic conﬁguration policies
(Lagoudakis & Littman, 2000, 2001; Pettinger & Everson, 2002). Such earlier works use 

Document 5: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 6: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

","Augmentative and alternative communication (AAC) systems provide numerous benefits for individuals with communication difficulties. Here are some of the key advantages:

1. **Enhanced Communication**: AAC helps people express themselves more effectively, leading to improved social interactions and relationships.

2. **Independence**: By giving individuals control over their means of communication, they can become more independent in various aspects of life, from school to work.

3. **Learning and Development**: AAC can support learning for children by providing a way for them to communicate what they want or need, which is crucial for developing language skills and literacy.

4. **Reduced Social Isolation**: Effective communication reduces feelings of loneliness and exclusion, helping individuals maintain social connections and form relationships.

5. **Improved Quality of Life**: With better ability to express needs and wants, individuals can manage their lives more effectively, leading to increased satisfaction and reduced stress.

6. **Expressing Emotions**: AAC helps individuals convey emotions and thoughts that may be challenging or impossible to express otherwise.

7. **Support for Medical Conditions**: For those with medical conditions that affect communication, such as severe burns or traumatic brain injury, AAC can provide a vital lifeline for expressing needs and desires.

8. **Assessment and Monitoring of Language Skills**: Therapists and caregivers can use AAC systems to monitor language development and progress in therapy sessions.

9. **Accessibility**: AAC technologies can be adapted to meet the specific needs of users with various abilities, ensuring inclusivity across different environments.

10. **Educational Benefits**: In educational settings, AAC supports inclusive education by enabling students with communication challenges to participate fully in classroom activities.

These benefits collectively contribute to a more fulfilling and functional life for individuals who use AAC systems.",8.148959636688232,"The main benefits of Automatic Algorithm Configuration (AAC) include:

1. **Performance Improvement**: AAC methods enhance the performance of algorithms across various domains, such as the boolean satisfiability problem (SAT), the traveling salesman problem (TSP), answer set programming (ASP), and machine learning applications. This improvement is achieved by automatically tuning algorithm parameters to better fit specific problem instances.

2. **Dynamic Adjustment**: AAC enables the use of dynamic configurations that can adapt during execution, allowing for better performance based on the current state of the algorithm and the problem. This adaptability is especially beneficial when dealing with different problem instances and conditions.

3. **Diversity of Methods**: There are several AAC methods available, such as ParamILS, GGA/GGA+, irace, and SMAC, which offer various strategies for parameter optimization. This diversity allows for flexibility in choosing the most suitable method based on the specific context or requirements of the project.

4. **Reduction in Need for Manual Tuning**: By automating the configuration process, AAC reduces the burden on domain experts who would otherwise need to manually optimize parameters, saving time and resources while also potentially achieving better configurations than manual tuning.

Overall, AAC supports the effective utilization of algorithms by streamlining the configuration process and improving their adaptability to diverse problem scenarios."
qwen2.5:7b-instruct-q4_0,"Please provide an overview of the types of AAC methods.
","Document 1: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 2: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 3: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

Document 4: 
 attempted. Nonetheless, the AC literature shows a surprising amount of hybridization of
local search, evolutionary and model-based methods. We hypothesize that there is still sig-
niﬁcant progress that can be made in the area of AC, despite the sophistication of current
methods, and are encouraged by the signiﬁcant increase in attention the ﬁeld has received,
in particular through the spread of HPO techniques. Finally, we especially encourage re-
searchers to address the real-world usability of AC techniques to ensure that the promising
performance gains the AC community is seeing can beneﬁt the world at large.
Acknowledgements
This work was partially supported by the German Research Foundation (DFG) within the
Collaborative Research Center “On-The-Fly Computing” (SFB 901/3 project no. 160364472)
and by the research training group “Dataninja” (Trustworthy AI for Seamless Problem
Solving: Next Generation Intelligence Joins Robust Data Analysis) funded by the Ger-
man federal state of North Rhine-Westphalia. We also would like to thank the anonymous
reviewers for their suggestions on the ﬁrst version of this manuscript.
13. Appendix
To help the reader navigate though the jungle of AC, we provide additional resources.
Table 6 contains a list of abbreviations with terms related to AC used within this work. In
addtion, we provide a list of software resources (Table 7) that contains currently available
tools for AC. We only include software that is widely used.
469 

Document 5: 
 algorithm conﬁgurators. To this end, we propose two classiﬁcation schemes: one for AC
problems, and one for algorithm conﬁgurators. Based on this, we structure and summarize
the available literature and classify existing problem variants as well as approaches to AC.
The remainder of the paper is structured as follows. First, in Section 2, we give a formal
introduction into the setting of algorithm conﬁguration, specify the scope of this survey, and
discuss the relation between AC, AS and HPO. In Section 3, we present the classiﬁcation
schemes for AC problems and approaches that are used, in turn, to describe and compare
existing algorithm conﬁgurators. In Sections 4 and 5, we survey algorithm conﬁguration
methods grouped by the property of whether these methods are model-free or leverage
a model respectively. Section 6 deals with theoretical guarantees that can be obtained.
Diﬀerent problem variants, such as realtime AC, instance-speciﬁc vs. feature-based, multi-
objective, and dynamic AC are discussed in Sections 7 to 10. Eventually, with the help of
our classiﬁcation schemes, we elaborate on appealing research directions in Section 11 and
conclude this survey in Section 12. A list of abbreviations used in this work can be found in
Table 6. In addition, we provide a list of useful software in Table 7. We note, however, that
this list is by no means exhaustive; it is meant to provide an idea about available software
at the time of publication.
2. Problem Formulation
2.1 Algorithm Conﬁguration
To describe the AC problem more formally, we introduce the following notation that is sim- 

Document 6: 
 signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement
learning problem by Sakurai et al.[ 173], Chen et al.[ 33] and
Pettinger and Everson [ 160]. Unlike the previous approaches,
these methods include the notion of state that is deﬁned
using feedback from the EA. For each distinct state, separate
preferences are learned for each operator and selection of the
operator to apply is based on the current state of the search.
An important issue with operator selection (as with all
adaptive parameter control) is the feedback used for control.
Veerapen et al .[ 198] presented and compared various utility
measures for variation operators. These measures are based on
a combination of exploration and exploitation measures and
use Pareto-dominance to evaluate operator utility. In another
paper, Whitacre et al .[ 204] make a distinction between
the source of feedback and any further statistical treatment
(a notion further elaborated in [112]). Several feedback sources
are suggested (including whether the offspring survives or
the number of generations it remains in the population).
The data received by these sources can be treated by av-
eraging them or by detecting the outliers in the sample. 

","Augmentative and Alternative Communication (AAC) refers to any communication method used to express thoughts, needs, wants, and ideas other than oral speech. There are several types of AAC methods that can be categorized into two main groups: non-electronic or low-tech AAC and electronic or high-tech AAC.

### Non-Electronic (Low-Tech) AAC Methods

1. **Sign Language**: Using hand gestures to form words and sentences, commonly used in deaf communities.
2. **Manual Communication Boards/Visual Symbols**:
   - **Line Drawing Systems**: Use line drawings to represent objects, concepts, or actions.
   - **Picture Exchange Communication System (PECS)**: Utilizes pictures of desired items to facilitate communication between caregivers and children with autism or other developmental disabilities.
   - **Natural Environment Teaching (NET) Boards**: Picture boards designed for natural environments where a person can request things they need in their daily life.
3. **Object Symbol Systems**:
   - **Objects of Reference (OOR)**: Using actual objects to represent concepts, actions, or items.
4. **Communication Folders/Books**: Organized collections of pictures, words, and symbols for common requests and interactions.
5. **Pointing Systems**: Using a single finger or a pointer to select symbols on a communication board.
6. **Facial Expressions and Gestures**: Non-verbal expressions and gestures that convey meaning.

### Electronic (High-Tech) AAC Methods

1. **AAC Apps**:
   - **Touch Screens/Appliances with Touch Screens**: Applications designed for tablets, smartphones, or dedicated devices such as DynaVox or LAMP.
2. **Speech Generating Devices (SGDs)**:
   - **Dedicated SGDs**: Devices specifically designed to be used by people who have difficulty speaking. Examples include the TiC-Tac from Nuance, the Accent from Tobii Dynavox, and the E-Chat 4 from Saltillo.
   - **Dynamic Display Systems**: Devices that provide a range of communication options including vocabulary selection, word prediction, phrase banking, voice output, picture symbols, and more. Examples include the Proloquo2Go app on iPads and the Communicator from Prentke-Romich Company (PRC).
3. **Voice Output Communication Aid (Voca)**: Devices that can record and playback spoken messages using a user's own voice or recorded message.
4. **Software-Based Systems**: Programs running on computers, tablets, or smartphones that provide communication capabilities.

### Combination Methods

1. **Hybrid Systems**: Combining both low-tech and high-tech methods to leverage the strengths of each approach. For example, using an iPad app with a line drawing system.
2. **Adaptive Technology**: Customized AAC solutions tailored to individual needs, often combining hardware and software.

Each type of AAC method has its own advantages and limitations, and the choice depends on factors such as user preferences, accessibility, technology availability, and communication goals.",14.055763959884644,"Automatic Algorithm Configuration (AAC) methods can be categorized into two main components: sampling in the solver class and evaluating candidate solvers. 

1. **Sampling Methods:**
   - **Search-based Methods:** These methods include techniques such as hill climbing, local search, and evolutionary algorithms (EAs). They typically make minimal assumptions about the solver structure and do not require gradient information, allowing for a broad exploration of the solver space.
   - **Model-based Methods:** These methods aim to capture performance dependencies between solvers and their configurations by training surrogate models based on performance data from the configuration process. This approach is akin to performance prediction techniques that can improve the effectiveness of the sampling process.

2. **Evaluation Strategies:**
   - The evaluation of a candidate solver often employs different performance estimators, as indicated by the estimation errors that decrease with larger sample sizes. The performance of the estimator can depend on various factors, including the number of training instances and the specific configuration scenarios at hand.

3. **Specialized Approaches:**
   - **Sequential Model-based Optimization (SMBO):** SMBO approaches are significant in the AAC landscape and involve techniques like sequential kriging and parameter optimization, which are tailored to optimize black-box functions, especially those affected by noise.
   - **Hybrid Approaches:** There is a noted hybridization of local search, evolutionary techniques, and model-based methods in AAC literature, suggesting that combining these strategies can lead to advancements in performance.

4. **Adaptive Parameter Control:** Techniques that incorporate feedback mechanisms, often guided by reinforcement learning principles, are also part of AAC methods. These methods adjust parameters dynamically based on the observed performance of solutions during the search process.

The landscape of AAC methods continues to evolve, with researchers encouraged to explore new directions and enhance the usability of these techniques in real-world applications."
qwen2.5:7b-instruct-q4_0,"Are Parallel Algorithm Portfolios connected with financial portfolios?
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 automatically producing effective parallel portfolios from multiple solvers, such as ppfolio and pfolioUZK, while exploiting 
the rich design spaces of these component solvers.
Katsirelos et al. [46] showed that an effective parallelization of a CDCL SAT solver does not merely hinge on picking a 
good clause sharing strategy, since it is not straightforward to obtain shorter resolution proofs by parallelization without 
essential changes of the underlying sequential reasoning mechanism. Our ACPP does not aim at parallelizing the resolution 
proof, but rather runs multiple algorithms and algorithm conﬁgurations in parallel, in order to maximize the probability 
that at least one of them ﬁnds a short proof quickly.
2.2. Related work
Well before there was widespread interest in multi-core computing, the potential beneﬁts offered by parallel algorithm 
portfolios were identiﬁed in seminal work by Huberman et al. [39]. Their notion of an algorithm portfolio is inspired by the 
use of portfolios of assets for risk management in ﬁnance and amounts to running multiple algorithms concurrently and 
independently on the same instance of a given problem, until one of them produces a solution. Gomes et al. [27] further 
investigated conditions under which such portfolios outperform their component solvers. Both lines of work considered 
prominent constraint programming problems (graph coloring and quasigroup completion), but neither presented methods 
for automatically constructing portfolio solvers. Parallel portfolios have since made practical impact, both in cases where 
the allocation of computational resources to algorithms in the portfolio is static [62,76] and where the component solvers 

Document 4: 
 is crucial for the effectiveness of any parallel portfolio.
According to (1), the performance of a parallel portfolio on
788 IEEE TRANSACTIONS ON CYBERNETICS, VOL. 52, NO. 2, FEBRUARY 2022
Algorithm 1 GAST
Input: base solvers B with conﬁguration space C; number of
component solvers k; instance set I; performance metric m;
algorithm conﬁgurator AC; independent conﬁgurator runs n; time
budgets tC, tV , tI for conﬁguration, validation and instance
generation respectively
Output:parallel portfolio c1:k
1: for i ← 1: k do
2: /*———— conﬁguration phase ———————*/
3: for j ← 1: n do
4: obtain a portfolio cj
1:i by running AC on conﬁguration space
{c1:i−1}×{ c|c ∈ C} using m for time tC
5: end for
6: validate c1
1:i, ...,cn
1:i on I using m for time tV
7: let c1:i ← arg mincj
1:i|j∈{1,...,n} P(cj
1:i, I) be the portfolio with
the best validation performance
8: /*———— instance-generation phase ————–*/ 

Document 5: 
 work relevant to parallel algorithm portfolios in-cludes online learning [2, 16] and methods that split problems [29, 30].) Effectively, the construction of a parallel algorithm portfolio to solve as many training problems as possible on K processors is an integer-programming (IP) prob-lem. The goal is to find the schedule S that specifies the time allotments to all algo-rithms on all processors, such that no problem can receive more than B time from all the processors together, and the total number of problems solved is a maximum. The expression (1 - )ij(tkj)) is 1 if problem xi is unsolved by algorithm aj after time tkj allo-cated to it 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, the concept of parallel algorithm portfolios is inspired by the use of portfolios of assets for risk management in finance, as identified in seminal work by Huberman et al. [39].",2.5844168663024902,"Yes, Parallel Algorithm Portfolios are conceptually connected with financial portfolios. The notion of an algorithm portfolio is inspired by the use of portfolios of assets in finance for risk management. In both cases, the idea is to run multiple independent strategies (algorithms or financial assets) concurrently to maximize the chances of achieving a favorable outcome (finding a solution in the case of algorithms, or achieving financial returns in the case of investments). This parallel processing allows for greater resilience and effectiveness, as it leverages the strengths of diverse approaches to address a problem or market condition."
qwen2.5:7b-instruct-q4_0,"What factors and challenges influenced the development of the Parallel Algorithm Portfolios?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 Historically, the parallel computing community quickly 
adopted combinatorial search as a playground for applications.
Search algorithms have the advantage of being conceptually 
simple (think of the most basic backtrack-style algorithm) and 
computationally demanding due to the (usually) exponential
size of the search space. In contrast, the search community did 
not really focus its research on parallelizing. The lack of proper 
Articles 
SUMMER 2013   99 Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 
Seven Challenges in 
Parallel SAT Solving 
Youssef Hamadi and Christoph M. Wintersteiger 
n This article provides a broad overview of the 
state of the parallel SAT solving field. A set of 
challenges to researchers is presented that, we 
believe, must be met to ensure the practical
applicability of parallel SAT solvers in the 
future. All these challenges are described infor -
mally but put into perspective with related 
research results, and a (subjective) grading of 
difficulty for each of them is provided.
infrastructure and, for many, the feeling that 
sequential algorithms were still full of research 
opportunities can go toward explaining that. In 
that community, parallelism was often only put in 
the perspectives of papers with no real perspec -
tives. This led to a situation where parallel search 
algorithms were designed by people with only one 
part of the required skills. 
Most computational problems solved on a com - 

Document 3: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 4: 
 rithm except some important parameter settings. Therefore, the
portfolio framework in PPTA framework can be parallel-based
framework as well as sequential-based framework.
The algorithm portfolio requires algorithm constituents to
be complementary so that the portfolio algorithm can handle
different types of problems. But it is not easy to select the com-
plementary algorithms which need to test algorithm candidates
in benchmark functions. In PPTA framework, the parameter
conﬁguration obtained from different kinds of problems are
naturally complementary thanks to the classiﬁcation of train-
ing instances. Therefore, we can obtain the complementary
algorithm much easier to construct the algorithm portfolio.
3) Robustness: For each algorithm constituent in the port-
folio, it is the best parameter conﬁguration found for the
speciﬁc kind of problems. Each algorithm is suitable for a
kind of problem so that the portfolio algorithm can handle
several kinds of problems. As a result, the optimization risk
is obviously reduced.
4) Efﬁciency: The time complexity of parameter tuning
algorithm in PPTA framework can be reduced signiﬁcantly
compared with commonly used parameter tuning methods. As
mentioned before, the performance metric only evaluates the
parameter candidates on one speciﬁc problem instance.
In the algorithm portfolio phase, the time complexity does-
n’t increase a lot compared with the single algorithm because
the operators of the algorithm in PPTA are the same with
single algorithm and the total computational resources (i.e.
max FEs) are also the same. The only difference is the
strategy allocating computational resources which varies from
algorithm to algorithm. 

Document 5: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

Document 6: 
 the former is to generate instances that are challenging for the
current portfolio, while the aim of the latter is to ﬁnd a new
component solver for the portfolio to better solve the newly gen-
erated instances. Applied to two widely studied problem domains,
that is, the Boolean satisﬁability problems (SAT) and the travel-
ing salesman problems (TSPs), the proposed approach identiﬁed
parallel portfolios with much better generalization than the ones
generated by the existing approaches when the training data
were scarce and biased. Moreover, it was further demonstrated
that the generated portfolios could even rival the state-of-the-art
manually designed parallel solvers.
Index Terms—Automatic portfolio construction (APC), genera-
tive adversarial approach, parallel algorithm portfolio, parameter
tuning.
I. I NTRODUCTION
M
ANY high-performance algorithms for solving com-
putationally hard problems, ranging from the exact
Manuscript received May 17, 2019; revised October 5, 2019 and March
4, 2020; accepted March 23, 2020. Date of publication April 29, 2020; date
of current version February 16, 2022. This work was supported in part by
the National Key Research and Development Program of China under Grant
2017YFB1003102, in part by the Guangdong Provincial Key Laboratory under
Grant 2020B121201001, in part by the Natural Science Foundation of China
under Grant 61672478, in part by the Program for Guangdong Introducing
Innovative and Enterpreneurial Teams under Grant 2017ZT07X386, in part 

","The development of parallel algorithm portfolios was significantly shaped by several factors and challenges. Here are some key influences:

1. **Computational Complexity and Resource Requirements**: As problems become more complex, traditional algorithms often struggle to find solutions in a reasonable amount of time on single-threaded processors. This necessitated the exploration of parallel computing techniques.

2. **Scalability Issues**: Many optimization and search-based algorithms suffer from scalability issues when dealing with large datasets or high-dimensional spaces. Parallelizing these algorithms can help manage this complexity more effectively.

3. **Algorithm Diversity**: Different problems may require different types of algorithms to solve them efficiently. A portfolio approach allows for the use of multiple algorithms, increasing the likelihood of finding a good solution quickly by leveraging the strengths of each algorithm.

4. **Problem-Specific Challenges**: Certain problem domains (e.g., combinatorial optimization, machine learning) have unique challenges that require specialized parallelization strategies. For example, in machine learning, parallelizing gradient descent or other optimization algorithms can significantly improve training speed and efficiency.

5. **Resource Management**: Efficiently managing resources across multiple threads or processes is crucial for achieving good performance. This includes load balancing, communication overhead between processes, and ensuring that the system does not become a bottleneck due to resource contention.

6. **Algorithm Design and Tuning**: Designing algorithms that can be effectively parallelized while maintaining their theoretical guarantees (e.g., convergence properties) is challenging. Additionally, tuning these algorithms for specific problem instances requires careful consideration of various parameters.

7. **Performance Evaluation and Comparison**: Comparing the performance of different algorithms in a parallel setting involves not only measuring time efficiency but also considering factors like resource utilization, scalability with increasing problem size, and solution quality. This often requires sophisticated benchmarking frameworks and tools.

8. **Integration with Existing Systems**: Parallel algorithm portfolios need to integrate well with existing computational environments (e.g., distributed systems, cloud computing platforms) to be practical for real-world applications.

9. **Dynamic Adaptation**: In some cases, the problem characteristics may change dynamically during computation. The ability of a parallel portfolio to adapt by dynamically selecting or modifying algorithms can be crucial in such scenarios.

10. **Communication Overhead and Synchronization Costs**: Parallel computing introduces additional overhead due to communication between processes and synchronization requirements. Efficient algorithms must minimize these costs while still achieving good performance.

These factors and challenges have driven the evolution of parallel algorithm portfolios, leading to more robust and adaptable solutions for solving complex problems across various domains.",12.07438850402832,"The development of Parallel Algorithm Portfolios has been influenced by several factors and challenges. Key aspects include:

1. **Complementarity of Algorithms**: The success of parallel portfolios relies on the ability to select algorithm constituents that are complementary. This allows the portfolio to handle a variety of problems effectively, thus reducing optimization risk (Document 4).

2. **Efficient Use of Resources**: With the rise of parallel computing architectures, effective resource allocation that does not significantly increase time complexity compared to single algorithms is crucial (Document 4).

3. **Dynamic Load Balancing**: For parallel search methods, particularly in SAT solving, efficient dynamic load balancing and worker cooperation through conflict clause exchange are vital for performance (Document 5).

4. **Generalization in Learning**: The generation of instances that challenge current portfolios and the identification of new component solvers for better performance in learning are important (Document 6). These factors are critical when existing training data is scarce and biased.

5. **Research Perspectives**: The historical focus of the parallel computing community on combinatorial search, combined with the lack of dedicated research into parallelization from the search community, has created challenges in effectively designing parallel search algorithms (Document 2).

6. **Algorithm Structure Homogeneity**: The homogeneity of the underlying structure of individual-based algorithms allows for easier integration into parallel frameworks without the need for significant modifications (Document 3).

Overall, the interplay of these factors shapes the design, implementation, and performance of parallel algorithm portfolios, addressing challenges related to efficiency, generalization, and collaboration among different solvers."
qwen2.5:7b-instruct-q4_0,"What are the main benefits of PAPs?
","Document 1: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 2: 
 on PAP with more than two constituent algorithms.
Two conclusions regarding the migration scheme of PAP
can be drawn from the superiority of PAP over the compared
algorithms. First, the migration scheme is of great impor-
tance to the success of PAP. Second, 1 andMAX GEN/20
are two appropriate and robust values for the parame-
tersmigration size and migration interval. To further
investigate the inﬂuence of these parameters on PAP, we
carried out a sensitivity analysis to check whether the per-
formance of PAP will change signiﬁcantly with other param-
eter settings. As stated in Section VI-B, 16 different pairs
ofmigration interval and migration size were tested for
every instantiation of PAP. For each pair, 30 independent
runs were executed on all 27 benchmark functions. Then, for
each instantiation on each function, Kruskal–Wallis one-way
analysis of variance by ranks was employed to test whether
the 16 pairs of parameters had led to signiﬁcantly different
performance. After that, for each instantiation of PAP, we
counted the number of the benchmark functions on which
all 16 pairs of parameters made no difference. The larger
the number, the more insensitive an instantiation is to the
parameters. For the sake of brevity, we only summarize in
Table IX these numbers for the 11 PAP instantiations, while
omit the full details. It can be observed that, in the worst case 

Document 3: 
 described as follows:
Ω(P, z) =max{max
θ∈P
Metric (θ, z), Metric ( ¯θ, z)}, (14)
where ¯θ represents the Restructure procedure. Note Equation (14) is slightly different from
Equation (11) (i.e., the performance of conventional PAPs) due to theRestructure procedure.
Initialization
member 
algorithm 
𝜃
1
𝑆
1
member 
algorithm 
𝜃
𝑖
𝑆
𝑖
member 
algorithm 
𝜃
𝑘
𝑆
𝑘
…
…
…
…
𝑆
𝑘
+
1
𝑆
𝑜𝑢𝑡𝑝𝑢𝑡
Comparison
Restructure
Improvement
Figure 1. Illustrations of MOEAs/PAP , the variant form of PAP for MOPs. Si represents the so-
lution set found by member algorithm θi, and Soutput represents the solution set ﬁnally returned
by MOEAs/PAP . The main difference of MOEAs/PAP from conventional PAPs lies in the way of
determining the ﬁnal output, as indicated by the dash box.
Electronics 2023, 12, 4639 7 of 18
4. Automatic Construction of MOEAs/PAP
As aforementioned, the member algorithms of a PAP could be automatically deter-
mined by solving the problem deﬁned in Equation (13). Below, we ﬁrst introduce the 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget.
To evaluate the effectiveness of PAP, 11 instantiations of
PAP were implemented based on four existing constituent
algorithms, including SaNSDE, wPSO, G3PCX, and CMA-
ES. The performance of each instantiation was compared to
PENG et al.: POPULATION-BASED ALGORITHM PORTFOLIOS FOR NUMERICAL OPTIMIZATION 799
its constituent algorithms on 27 benchmark functions. Our
experimental results showed that seven out of the 11 PAP in-
stantiations outperformed their constituent algorithms in terms
of solution quality and the proposed risk metric. Furthermore,
7 out of the 11 instantiations even achieved superior or compa-
rable performance in comparison with G-CMA-ES, which was
known to be superior to any of the four constituent algorithms.
Our empirical studies also revealed that PAP is capable of
increasing the probability of ﬁnding the global optimum and
is insensitive to control parameters of the migration scheme.
Further analyses have been conducted to investigate in what
circumstance PAP may outperform its constituent algorithms.
Complementarity was identiﬁed as a key issue.
Though PAP has been shown to be a promising framework, 

Document 6: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

","PAPs (Positive Airway Pressure devices) have several key benefits for people with sleep-related breathing disorders, such as Obstructive Sleep Apnea (OSA). Here are some of the main advantages:

1. **Improved Breathing**: The primary benefit is that it helps to keep the airways open during sleep, preventing apneas and hypopneas.

2. **Better Quality of Life**: By improving sleep quality, PAP therapy can lead to better overall health and well-being, reducing fatigue and daytime sleepiness.

3. **Reduced Health Risks**: Long-term use of PAP therapy has been linked with a reduced risk of cardiovascular diseases such as hypertension, heart failure, and stroke.

4. **Enhanced Cognitive Function**: Better sleep quality often leads to improved cognitive function, including better memory, concentration, and overall mental health.

5. **Decreased Snoring**: Many people who use PAPs find that their snoring decreases or even stops entirely.

6. **Reduced Risk of Accidents**: By reducing the risk of falls and other accidents caused by daytime sleepiness, PAP therapy can improve safety both at home and on the road.

7. **Weight Management**: Some studies suggest that treating OSA with PAPs may help manage weight better due to improved energy levels and reduced nighttime eating tendencies.

8. **Improved Mood and Emotional Health**: Better sleep often leads to improved mood, reduced anxiety, and depression symptoms.

9. **Increased Physical Performance**: For athletes or those engaged in physically demanding activities, better sleep can lead to improved physical performance and recovery.

10. **Reduced Risk of Diabetes Complications**: Managing OSA with PAPs may also help improve blood sugar control for people with diabetes.

It's important to note that while PAP therapy is generally very effective, it requires regular use and proper adherence to achieve these benefits.",9.333353996276855,"The main benefits of Population-Based Algorithms (PAPs) are:

1. **Ease of Implementation**: PAPs are straightforward to implement as they do not require complex resource allocation; each member algorithm is assigned an equal amount of resources.

2. **Enhanced Performance**: The performance of a PAP is determined by the best performance achieved among its member algorithms, allowing it to potentially outperform any single algorithm included within it.

3. **Utilization of Parallelism**: With the growth of parallel computing architectures, PAPs can leverage parallel solution strategies effectively, making them suitable for modern computing facilities.

4. **Diversity and Complementarity**: The effectiveness of a PAP relies on the diversity and complementarity among its member algorithms, which can improve the overall solution quality.

5. **Sensitivity to Parameters**: Certain PAP configurations have shown to be insensitive to control parameters, contributing to more robust performance across different scenarios.

6. **Empirical Success**: Experimental results indicate that many PAP instantiations can outperform their individual constituent algorithms and improve the likelihood of finding optimal solutions."
qwen2.5:7b-instruct-q4_0,"What is algorithm for mosaic permutation test?
","Document 1: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

Document 2: 
 algorithm and their default values, (i.e. the original values) are
presented in Table II.
The gamma and mu in GA represent the crossover rate
and mutation rate [20]. In DE algorithm, policy represents the
choice of mutation operator andF, CR are two important pa-
rameters in mutation and crossover operators [21]. The policy
candidates contain/rand/1, /best/1, /current−to−best/1,
/best/2 and /rand/2. In PSO algorithm, c1, c2 are two ﬂying
factors for each particle [22].
1852
Authorized licensed use limited to: University College London. Downloaded on July 03,2020 at 02:38:54 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: The tuned parameters for test algorithms with
Dimension d = 30.
Types GA DE PSO
gamma mu policy F CR c1 c2
TB1 0.4294 0.1728 4 0.4155 0.5441 1.7848 0.5908
TB2 0.5972 0.0617 3 0.6553 0.5350 0.1467 3.0000
TB3 0.5493 0.9737 3 0.7791 0.9308 0.7161 1.9731
TB4 0.5329 0.6145 1 0.9584 0.9273 1.7811 1.7866
TB5 0.9964 0.0839 1 0.4453 0.0581 0.4452 3.0000
TC1 0.5972 0.0617 1 0.7791 0.9308 1.2194 2.3496 

Document 3: 
 The mosaic permutation test: an exact and nonparametric
goodness-of-fit test for factor models
Asher Spector ∗ Rina Foygel Barber † Trevor Hastie ∗ Ronald N. Kahn ‡
Emmanuel Cand` es∗§
September 30, 2024
Abstract
Financial firms often rely on fundamental factor models to explain correlations among asset
returns and manage risk. Yet after major events, e.g., COVID-19, analysts may reassess whether
existing risk models continue to fit well: specifically, after accounting for a set of known factor ex-
posures, are the residuals of the asset returns independent? With this motivation, we introduce the
mosaic permutation test, a nonparametric goodness-of-fit test for preexisting factor models. Our
method can leverage modern machine learning techniques to detect model violations while provably
controlling the false positive rate, i.e., the probability of rejecting a well-fitting model, without mak-
ing asymptotic approximations or parametric assumptions. This property helps prevent analysts
from unnecessarily rebuilding accurate models, which can waste resources and increase risk. To
illustrate our methodology, we apply the mosaic permutation test to the BlackRock Fundamental
Equity Risk (BFRE) model. Although the BFRE model generally explains the most significant
correlations among assets, we find evidence of unexplained correlations among certain real estate
stocks, and we show that adding new factors improves model fit. We implement our methods in 

Document 4: 
 the maximum absolute estimated correlation between asset j⋆ and any asset j ∈ Gd.
• Reset Gd⋆ = Gd⋆ ∪ {j⋆}.
Return: Partition G1, . . . , GD.
In particular, let \MaxCorrj := maxj′̸=j | ˆCj,j′| be the maximum absolute estimated correlation between
asset j and asset j′. Let S ⊂ [p] be the subset of indices of [ p] corresponding to the assets with the
ℓ largest values of \MaxCorrj, so |S| = ℓ. Finally, let ˆν denote the top eigenvector of ˆCS,S. We then
define ˆv as follows:
ˆvj =
(
ˆνj j ∈ S
0 else.
In other words, S is the support of ˆ v, and on S, ˆv equals the maximum eigenvalue of ˆCS,S. We
picked this algorithm because it is conceptually simple and computationally cheap, but we have not
explored other algorithms. Of course, the mosaic permutation test could be used in combination with
a maximum bi-cross validation R2 statistic based on any sparse PCA algorithm.
42 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
",The mosaic permutation test involves constructing a residual estimate ˆϵ by splitting the data into rectangular tiles and separately estimating residuals within each tile. The significance of a given test statistic \(S(\hat{\epsilon})\) quantifying column correlations in \(\hat{\epsilon}\) is determined by permuting observations within each tile and computing p-values through this process.,3.611797571182251,"The mosaic permutation test is a nonparametric goodness-of-fit test designed for factor models in finance. The algorithm consists of the following steps:

1. **Inputs**: It takes asset returns \( Y \in \mathbb{R}^{T \times p} \), exposures \( L_t \in \mathbb{R}^{p \times k} \) for \( t \in [T] \), and a set of tiles \( \{(B_m, G_m)\}^M_{m=1} \) along with a test statistic \( S : \mathbb{R}^{T \times p} \to \mathbb{R} \).

2. **OLS Estimate**: For each tile \( m = 1, \ldots, M \), the algorithm computes \( \hat{\epsilon}(m) \), which is the ordinary least squares (OLS) estimate of the residuals using only the data in that tile \( Y(m) \). The exposures for the assets in the \( m \)-th tile are denoted as \( L(m) \).

3. **Residual Estimate**: Using these estimates, the algorithm creates a mosaic residual estimate \( \hat{\epsilon} \in \mathbb{R}^{T \times p} \).

4. **Permutation**: The algorithm permutes the order of the residuals multiple times to generate permuted variants \( \tilde{\epsilon}^{(r)} \) for \( r = 1, \ldots, R \) and checks if permuting decreases the value of a chosen meta test-statistic.

5. **P-value Calculation**: Finally, it computes a p-value based on the number of times the permuted residuals yield a test statistic value lower than the original.

Overall, this method allows analysts to check for independence of residuals after accounting for known factor exposures without requiring strong parametric assumptions."
qwen2.5:7b-instruct-q4_0,"What test statistic should be used for mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 power compared to the oracle tests, suggesting that the adaptive QMC statistic effectively adapts to
the unknown sparsity level. Furthermore, the MPT oracle and OLS double oracle—which use the
same test statistic but are applied to different residual estimates—have similar power. Indeed, the
average power difference is 3%, and the maximum power difference is 10%. This result should not
be too surprising, since ˆϵols and ˆϵ are estimating the same residuals ϵ, and thus the OLS and mosaic
statistic should be highly correlated. Indeed, in our application (Figure 4), in all three sectors, the
mosaic statistics are empirically ≥ 85% correlated with the OLS statistics. Thus, in this simulation,
the mosaic test is competitive with an oracle method based on OLS residuals.
7 Discussion
This paper introduces the mosaic permutation test, an exact and nonparametric goodness-of-fit test
for factor models with known exposures. In an empirical application to the BlackRock Fundamental
Equity Risk model, we demonstrate how to use the mosaic permutation test to diagnose financial
factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing 

Document 4: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","For the mosaic permutation test, the test statistic must be a function of the mosaic estimator \(\hat{\epsilon}\) of the residuals, rather than a conventional OLS estimator \(\hat{\epsilon}_{ols}\).",2.737812757492065,"The mosaic permutation test employs a test statistic denoted as S(ˆϵ), which quantifies the correlations among the columns of the residual estimate ˆϵ. This statistic is central to the test as it allows for the computation of a significance threshold by permuting the observations within the rectangular tiles of ˆϵ. The test is designed to evaluate the independence properties of the residuals, making S(ˆϵ) a key component in assessing the goodness-of-fit in factor models."
qwen2.5:7b-instruct-q4_0,"What is restriction for test statistic in mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when
estimating ˆϵ.
6 Do the mosaic residual estimates cause a loss of power?
Our method requires the test statistic S(ˆϵ) to be a function of mosaic residual estimates instead
of a function S(ˆϵols) of OLS residual estimates. We hope that the mosaic statistic is a good proxy
23
for the OLS statistic (which indeed seems to be the case in Figure 4), so this section analyzes via
simulations whether the mosaic test has lower power than an oracle test based on the OLS statistic.
Our simulations also show the effectiveness of the adaptive test statistic introduced in Section 5.1.
We conduct semisynthetic simulations where the exposures Lt are constant over time and equal to
the BFRE exposures for the financial sector on May 21st, 2020. We sample the factor returns Xtk
as i.i.d. t4 variables. The residuals satisfy ϵt = γt + Ztv for γt,j
i.i.d.∼ t4, Zt
i.i.d.∼ t4 and v ∈ Rp. In 

Document 5: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 6: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

","The test statistic \( S(\hat{\epsilon}) \) in the mosaic permutation test must be a function of the mosaic residual estimates \(\hat{\epsilon}\) rather than OLS residual estimates. This ensures that the independence properties of the true residuals are approximately preserved, allowing for valid hypothesis testing under local exchangeability assumptions.",2.8782782554626465,"The restriction for the test statistic in the mosaic permutation test is that it must be a function of the mosaic residual estimates \( \hat{\epsilon} \) instead of a function of the ordinary least squares (OLS) residual estimates \( \hat{\epsilon}_{ols} \). This means that the test statistic \( S(\hat{\epsilon}) \) is calculated using the newly introduced estimator \( \hat{\epsilon} \) which preserves some independence properties of the true residuals, rather than relying on the properties of the OLS residuals which do not meet the same independence criteria."
qwen2.5:7b-instruct-q4_0,"What is the trick for allowiing the exposures to change with each observation in Mosaic permutation test?
","Document 1: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 2: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","To allow exposures to change with each observation, the method introduces an augmented exposure matrix \( L^{\star}_t \) that changes every two observations. Specifically, \( L^{\star}_t \) is defined differently for odd and even time points, ensuring it only changes every two observations, thus maintaining the necessary conditions for the mosaic permutation test while accommodating changing exposures.",3.5134973526000977,"The trick for allowing the exposures to change with each observation in the Mosaic permutation test is to augment the exposure matrix. Specifically, instead of simply using the original exposure matrix \( L_t \), an augmented exposure matrix \( L^\star_t \) is defined, which is constructed to vary every two observations. The form of this augmented matrix is as follows:

\[
L^\star_t := 
\begin{cases}
\begin{pmatrix}
L_t \\ L_{t+1}
\end{pmatrix} & \text{if } t \text{ is odd} \\
\begin{pmatrix}
L_{t-1} \\ L_t
\end{pmatrix} & \text{if } t \text{ is even}
\end{cases}
\]

By this construction, \( L^\star_t \) only changes every two observations, and it still maintains that \( L_t \) is a submatrix of \( L^\star_t \). Therefore, the null hypothesis retains its validity for the augmented model, which allows the application of the mosaic permutation test without the need for \( L_t \) to be constant across all observations but instead ensures that it is constant within the tiles created for the permutation test."
qwen2.5:7b-instruct-q4_0,"Is there any way of choice of tiling in Mosaic permutation test?
","Document 1: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 2: 
 setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining
provable false positive control. For example, it allows analysts to use regularized estimates of the
covariance matrix of the residuals ϵ, for example, via a graphical lasso (Friedman et al., 2007), and
it also permits the use of cross-validation to choose the regularization strength. The only restriction
5
Figure 3: Semisynthetic simulation with Xtk, ϵtj
i.i.d.
∼ N(0, 1), and the exposures are taken from the BFRE
model for the energy sector on May 21st, 2020. Note T = 350, p = 183, k = 18, and we use the test statistic
from Figure 1. Panel 2(a) shows that a naive residual permutation test (discussed in Section 2.1) inaccurately
simulates the null distribution of the test statistic S(ˆϵols)—in fact, the true null distribution and the estimated
one do not overlap. Panel 2(b) shows that naive bootstrap Z-statistics (discussed in Section 2.2) are not
approximately mean zero, nor do they have unit variance. All p-values based on these two naive methods are 

Document 3: 
 The proof is simple, but the notation is subtle. To ease comprehension, recall that by definition
ϵ(m) = ϵBm,Gm where Bm and Gm are random. In this proof, we will use the notation ϵBm,Gm instead
of ϵ(m) to make the dependence on Bm and Gm explicit.
Let T = {(Bm, Gm)}m∈[M] denote the choice of tiles and let τ = {(βm, γm)}m∈[M] denote an arbitrary
fixed tiling in the support of T . It suffices to show that for any fixed permutation matrices P1 ∈
R|β1|×|β1|, . . . , PM ∈ R|βM |×|βM |,
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ) | T= τ. (A.8)
To show this, we note that Assumption A.1 yields the marginal result that
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ). (A.9)
To convert this to a conditional result, note that sinceT is a function of ˆϵ (which itself is a deterministic 

Document 4: 
 In future work, we plan to extend the mosaic permutation test to apply to this new setting (among
others).
• Using regularization: Our methods currently require the use of unregularized OLS regressions within
each tile to estimate the residuals (see Remark 3). However, to increase power, it might be valuable
to develop methods that can use regularization.
• Robustness: It might be valuable to develop tests that are robust to slight inaccuracies in the expo-
sures Lt. Indeed, this could also help relax the assumption that Lt is locally constant (Assumption
3.1), since small within-tile changes in Lt could be viewed as small “inaccuracies.” Similarly, it
would be useful to develop theory quantifying the robustness of the existing test, i.e., by bounding
the excess error in some interpretable way.
25
8 Code and data availability
We implemented our methods in the python package mosaicperm. All other code used in the paper is
available at https://github.com/amspector100/mosaic_factor_paper/. Although we are not able
to make the BFRE model data available, we have provided a publicly available sample dataset that
allows one to obtain qualitatively similar results (see the GitHub repository for more details).
9 Acknowledgements
The authors would like to thank John Cherian, Kevin Guo, Guido Imbens, Lihua Lei, and Bobby Luo 

Document 5: 
 highly computationally efficient procedure compared to one which (e.g.) repeatedly cross-validates an
expensive machine learning algorithm.
5.2 Adaptively choosing the tiling
Although Section 3.3 gives a good default choice of tiling, another option is to learn a good choice
of tiles that “separate” assets whose idiosyncratic returns are correlated. However, in general, if the
tiling is chosen using Y, ˆϵ will not necessarily be invariant to any permutations under the null because
of the dependence between {(Bm, Gm)}M
m=1 and Y. In other words, naive “double dipping” leads to
inflated false positives.
However, we can sequentially choose the mth tile (Bm, Gm) based on the estimated residuals from the
previous m−1 tiles as long as our choice of (Bm, Gm) does not depend on the order of the rows within
each of the previous m − 1 tiles. Precisely, suppose that we can write Bm, Gm as functions bm, gm of
the previous tiles as well as auxiliary randomness Um
i.i.d.∼ Unif(0, 1):
Bm = bm(ˆϵ(1), . . . ,ˆϵ(m−1), Um) and Gm = gm(ˆϵ(1), . . . ,ˆϵ(m−1), Um). (5.5) 

Document 6: 
 returns ˆϵ·,1, ˆϵ·,2 will never be “separated” by different permutations. We address this problem in the
next section.
3.2 The mosaic permutation test
We now introduce the general mosaic permutation test, which is more powerful and stable than
the simple method in Section 3.1. As an added benefit, we will also make the test more robust to
autocorrelation and nonstationarity among the residuals ϵ1, . . . , ϵT .
In Section 3.1, we separated the asset returns Y ∈ RT×p into two disjoint groups, computed residual
estimates ˆϵ separately for each group, and then permuted within each group. Now, we partition the
data Y into an arbitrary number M of rectangles along both axes. Formally, for m = 1, . . . , M, let
Bm ⊂ [T] denote a subset or “batch” of observations and Gm ⊂ [p] denote a subset or “group” of
assets. We say {(Bm, Gm)}M
m=1 is a tiling if for every timepoint t and asset j, there is exactly one pair
(Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we 

","Yes, the tiling can be chosen adaptively based on estimated residuals from previous tiles, as long as the choice does not depend on the order within each previous tile. This adaptive method helps reduce inflated false positives.",3.074666976928711,"Yes, the choice of tiling in the mosaic permutation test can be adaptively learned to separate assets whose idiosyncratic returns are correlated. Although a default choice of tiling is provided in Section 3.3, analysts can sequentially choose each tile based on the estimated residuals from previous tiles, ensuring that the choice does not depend on the order of rows within the previous tiles. This process involves writing the pair of sets (Bm, Gm) as functions of previous tiles along with auxiliary randomness, allowing for an adaptive and potentially more effective selection of tiling."
qwen2.5:7b-instruct-q4_0,"What is RNN with Long-Short-Term-Memory estimate in deep learning pricing asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper.
Hansen, L. P.(1982): “Large sample properties of generalized method of moments estimators,” Econo-
metrica, 50(4), 1029–1054.
Hansen, L. P., and R. Jagannathan(1997): “Assessing Speciﬁcation Errors in Stochastic Discount
Factor Models,” The Journal of Finance , 52(2), 557–590.
Heaton, J., N. Polson, and J. H. Witte(2017): “Deep learning for ﬁnance: Deep portfolios,” Applied
Stochastic Models in Business and Industry , 33(1), 3–12.
Hochreiter, S., and J. Schmidhuber(1997): “Long short-term memory,” Neural Computation, 9(8),
1735–1780.
Horel, E., and K. Giesecke(2020): “Towards Explainable AI: Signiﬁcance Tests for Neural Networks,”
Journal of Machine Learning Research, forthcoming . 

Document 3: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 4: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 5: 
 Rapach et al. 2013) and lasso (Chinco, Clark-Joseph & Ye 2019; Freyberger, Neuhierl & Weber
2020).
More recently, Gu, Kelly & Xiu (2020) conduct a wide-ranging analysis of machine learning
methods for return prediction, considering not only regularized linear methods but also more
cutting-edge nonlinear methods including random forest, boosted regression trees, and deep
learning.Their research illustrates the substantial gains of incorporating machine learning when
estimating expected returns. This translates into improvements in out-of-sample predictive R2
as well as large gains for investment strategies that leverage machine learning predictions. The
empirical analysis also identifies the most informative predictor variables, which helps facilitate
deeper investigation into economic mechanisms of asset pricing.
Machine learning also makes it possible to improve expected return estimates using predic-
tive information in complex and unstructured data sets. For example, Ke, Kelly & Xiu (2019)
propose a new supervised topic model for constructing return predictions from raw news text
and demonstrate its prowess for out-of-sample forecasting.Jiang,Kelly & Xiu (2021) and Obaid
& Pukthuanthong (2022) demonstrate how to tap return predictive information in image data
using machine learning models from the computer vision literature. Both text and image data
confer particularly strong return forecasting gains at short horizons of days and weeks and are 

Document 6: 
 characteristics. Feng, Polson, and Xu (2019) impose a no-arbitrage constraint by using a set of
pre-speciﬁed linear asset pricing factors and estimate the risk loadings with a deep neural network.
Rossi (2018) uses Boosted Regression Trees to form conditional mean-variance eﬃcient portfolios
based on the market portfolio and the risk-free asset. Our approach also yields the conditional
mean-variance eﬃcient portfolio, but based on all stocks. Gu, Kelly, and Xiu (2019) extend the
linear conditional factor model of Kelly, Pruitt, and Su (2019) to a non-linear factor model using an
autoencoder neural network. 3 We conﬁrm their crucial insight that imposing economic structure
2Other related work includes Sirignano, Sadhwani, and Giesecke (2020) who estimate mortgage prepayments,
delinquencies, and foreclosures with deep neural networks, Moritz and Zimmerman (2016) who apply tree-based
models to portfolio sorting and Heaton, Polson, and Witte (2017) who automate portfolio selection with a deep
neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the 

","Luyang Chen's work on using a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) units for asset pricing models involves applying these advanced neural network techniques to predict the future prices of financial assets. Here’s a breakdown of what this entails:

### 1. **Recurrent Neural Networks (RNNs):**
   - RNNs are designed to handle sequential data, where the order of observations is important.
   - They process inputs in a temporal sequence and maintain an internal state that captures information from previous time steps.

### 2. **Long Short-Term Memory (LSTM) Units:**
   - LSTMs are a type of RNN designed to solve the vanishing gradient problem, which can occur when training traditional RNNs on long sequences.
   - They use memory cells and gates (input, forget, output gates) to control how information flows through the network. This allows them to remember or forget relevant past inputs over longer periods.

### 3. **Asset Pricing Models:**
   - Asset pricing models aim to predict future asset prices based on historical price data.
   - These models can include various features such as previous stock prices, trading volumes, market indices, news sentiment, and other economic indicators.

### Chen's Approach:
- **Data Input:** Chen likely feeds historical time series data (such as daily or intraday stock prices) into the model.
- **Feature Engineering:** Features might include moving averages, technical indicators, volume ratios, and possibly additional external factors like macroeconomic indicators or news sentiment scores.
- **Model Architecture:**
  - The RNN with LSTM units processes these features sequentially to capture temporal dependencies in the data.
  - Chen may have experimented with different architectures, such as stacking multiple LSTMs, using bidirectional LSTMs (which process both past and future information), or incorporating attention mechanisms to focus on relevant parts of the time series.

- **Training:**
  - The model is trained using historical asset prices as labels.
  - Loss functions like mean squared error are commonly used for regression tasks in pricing models.

- **Forecasting:**
  - After training, the model predicts future stock prices based on current and past data.
  - These predictions can be used for trading strategies or risk management purposes.

### Potential Advantages:
- **Handling Temporal Dependencies:** LSTMs excel at capturing long-term dependencies in time series data, which is crucial for asset pricing.
- **Feature Learning:** The model learns to recognize patterns and features that are important for predicting asset prices.
- **Dynamic Feature Importance:** LSTMs can adapt their focus on different features over time, making them flexible for varying market conditions.

### Challenges:
- **Overfitting:** Financial data often contains noise or irrelevant signals, which could lead to overfitting if not properly regularized.
- **Data Quality:** The quality and availability of historical data are crucial. Poor quality data can degrade model performance.
- **Market Complexity:** Financial markets are highly complex and influenced by many factors that might not be fully captured in the training data.

### Conclusion:
Luyang Chen's work demonstrates how deep learning techniques, specifically RNNs with LSTM units, can be effectively applied to financial forecasting. This approach leverages the ability of LSTMs to handle sequential data and capture long-term dependencies, making them a powerful tool for asset pricing models in finance.",16.149023056030273,"The context does not provide specific information about the use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) in the asset pricing models developed by Luyang Chen. However, it mentions the use of deep learning techniques for estimating an asset pricing model that accounts for various conditioning information and time-variations. LSTMs are typically utilized in scenarios where data has sequential characteristics, making them suitable for time-series forecasting in finance. They are particularly effective in capturing long-term dependencies in data, which is crucial for modeling the dynamic behavior of asset returns. Therefore, while the specific model by Chen does not detail RNN or LSTM use, these architectures can enhance the processing of temporal data in asset pricing models."
qwen2.5:7b-instruct-q4_0,"What is loss function in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 In addition to the various layer-types of CNN architecture introduced in the previous
section, the ﬁnal classiﬁcation is achieved from the output layer that usually the last layer
of the FC layer, as shown in Figure 2. Different loss functions also affect the performance of
the CNN architecture and are applied to different visual tasks (e.g., image classiﬁcation,
face recognition, and object recognition). Here are some commonly used loss functions in
CNN-based image classiﬁcation methods (inherit the content of Section 2.1.2), as shown
in Table 1.
All in all, Softmax+Cross-Entropy has become the usual loss function of the CNN
model. There are also many improved versions based on it, such as center-loss [ 82], L-
Softmax [83], A-Softmax [ 84], AM-Softmax [ 85], PEDCC-loss [ 86], etc., which play an
important role in different visual tasks.
Remote Sens. 2021, 13, 4712 8 of 51
Table 1. Common loss functions for CNN models.
Loss Function Equation Characteristic
L1 (MAE) Loss(y, y∗) = 1
m ×
m
∑
i=1
⏐⏐⏐⏐y∗
i −yi
⏐⏐⏐⏐
This function is widely used in
regression problems. L1 Loss is called
mean absolute error (MAE)
L2 (MSE) Loss(y, y∗) = 1
m ×
m
∑
i=1
( 

Document 3: 
 We choose this speciﬁc linear version of the model as it maps directly into the linear approaches that
have already been successfully used in the literature. This linear framework essentially captures
the class of linear factor models. Appendix C provides a detailed overview of the various models
for conditional SDFs and their relationship to our framework.
II. Estimation
A. Loss Function and Model Architecture
The empirical loss function of our model minimizes the weighted sample moments which can
be interpreted as weighted sample mean pricing errors:
L(ω|ˆg,It,It,i) = 1
N
N∑
i=1
Ti
T

1
Ti
∑
t∈Ti
Mt+1Re
t+1,iˆg(It,It,i)

2
. (4)
13Kozak, Nagel, and Santosh (2020) consider also cross-products of the characteristics. They show that the PCA
rotation of the factors improves the pricing performance. Lettau and Pelger (2020) extend this important insight
to RP-PCA rotated factors. We consider PCA based factors in III.J. Our main analysis focuses on conventional
long-short factors as these are the most commonly used models in the literature.
14As before we deﬁne as tangency portfolio one of the portfolios on the global mean-variance eﬃcient frontier.
12
Electronic copy available at: https://ssrn.com/abstract=3350138 

Document 4: 
 part of network learning. The ﬁrst thing that backpropagation needs to determine is the
loss function of the model. There are many loss functions, see Section 2.2.5. The loss
function in the Figure 2 is L2 Loss:
Loss(y, y∗) = 1
m ×
m
∑
i=1
(y∗
i −yi)2 (2)
By calculating the mean square error between y∗ and y, the network weights w
and bias b can be updated by obtaining the partial derivative of the loss function: w′ =
w −η ×(∂Loss/∂w) and b′= b −η ×(∂Loss/∂b). With a suitable learning rate η, the loss
of the y and y∗can be gradually minimized. That is to make the y closer to y∗, so as to
achieve the effect of network training.
Remote Sens. 2021, 13, 4712 4 of 51
Figure 2. The structure of the MLP . It hasn input values and m output values, including k hidden
units. xn is the input value. The direction of the arrow is the direction in which the input value is
transmitted. The hidden unit is hk, it receives the input value of the previous layer. ym is the output
unit, and the real value is y∗m.
2.2. CNN Architecture
The main structure of CNN is the convolutional layer, pooling layer, nonlinear acti- 

Document 5: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 6: 
 The most common way is to translate the problem into an unconditional asset pricing model
on sorted portfolios. Under additional assumptions one could obtain a valid SDF Mt+1 conditional
on a set of asset-speciﬁc characteristics It,i by its projection on the return space:
Mt+1 = 1 −ω⊤
t Re
t with ωt,i = f(It,i),
where It,i is a vector of q characteristics observed for N stocks and f(·) is a general, potentially
nonlinear and non-separable function. Most of the reduced-form asset pricing models approximate
this function by a (potentially very large) set of simple managed portfolios fj(·), such that f(It,i) ≈∑˜Nbasis
j=1 fj (It,i) ˜wj. The SDF then becomes a linear combination of these managed portfolios with
constant weights ˜ωj:
Mt+1 = 1 −
˜Nbasis
∑
j=1
˜wj ˜Rt+1,j with ˜Rt+1,j =
N∑
i=1
fj (It,i) Re
t+1,i, (A.1)
where ˜Rt+1 are the returns of ˜Nbasis managed portfolios that correspond to diﬀerent basis functions
in the characteristic space. The number of basis portfolios increases by the complexity of the
basis functions and the number of characteristics. The most common managed portfolios are 

","The loss function in the deep learning asset pricing model by Luyang Chen et al. is likely based on mean square error (MSE), as indicated by the use of L2 Loss, which calculates the mean square error between predicted and actual values.",3.0858094692230225,"In the deep learning asset pricing model developed by Luyang Chen and co-authors, the empirical loss function minimizes the weighted sample moments, interpreted as weighted sample mean pricing errors. The loss function is mathematically represented as:

\[ L(\omega|\hat{g}, I_t, I_{t,i}) = \frac{1}{N} \sum_{i=1}^{N} T_i \left\| \frac{1}{T_i} \sum_{t \in T_i} M_{t+1} R_{e, t+1, i} \hat{g}(I_t, I_{t,i}) \right\|^2. \]

This function aims to optimize the model's performance in estimating asset prices based on conditioning information and the dynamic states of the economy."
qwen2.5:7b-instruct-q4_0,"What is the model archtecture in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 the most promising direction for future empirical asset pricing research is developing a genuine
fusion of economic theory and machine learning. It is a natural marriage, as asset pricing theory
revolves around price formation through the aggregation of investor beliefs, which undoubtedly
enterpricesinsubtle,complex,andsometimessurprisingways.Atthesametime,machinelearning
constitutes a sophisticated collection of statistical models that flexibly adapt to settings with rich
and complex information sets.
Machine learning factor models are one such example of this fusion. Almost all leading theo-
reticalassetpricingmodelspredictalow-dimensionalfactorstructureinassetprices.Wherethese
models differ is in their predictions regarding the identity of the common factors. Much of the
frontier work in empirical asset pricing can be viewed as using the (widely agreed upon) factor
structure skeleton as a theory-based construct within which various machine learning schemes
are injected to conduct an open-minded investigation into the economic nature of the common
factors.
Our survey is inevitably selective and disproportionally influenced by our own research on
thesetopics.Wehavemainlyfocusedonmethodologicalcontributions,leavingadetailedreviewof
empirical discoveries via these methodologies for future work.A frequently discussed dichotomy
in the literature is observable factor versus latent factor models. While some of the methods we
discuss apply to observable factor settings (or hybrid settings),we have also skewed our coverage 

Document 3: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 4: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 5: 
 paper.
Feng, G., N. G. Polson, and J. Xu(2019): “Deep Learning in Characteristics-Sorted Factor Models,”
Working paper.
Freyberger, J., A. Neuhierl, and M. Weber(2020): “Dissecting characteristics nonparametrically,”
Review of Financial Studies, forthcoming , 33(5), 2326–2377.
Goodfellow, I., Y. Bengio, and A. Courville(2016): Deep Learning. MIT Press.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio (2014): “Generative adversarial nets,” in Advances in neural information processing
systems, pp. 2672–2680.
Gu, S., B. Kelly, and D. Xiu(2019): “Autoencoder Asset Pricing Models,” Journal of Econometrics,
forthcoming.
Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper. 

Document 6: 
 neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the
additional elements of ﬁnding the macroeconomic states and identifying the most robust conditioning instruments.
Fundamentally, our object of interest is the pricing kernel. Kelly, Pruitt, and Su (2019) obtain a multi-factor factor
model that maximizes the explained variation. The linear special case applies PCA to a set of characteristic based
factors to obtain a linear lower dimensional factor model, while their more general autoencoder obtains the loadings
to characteristic based factors that can depend non-linearly on the characteristics. We show in Section III.J how our
SDF framework and their conditional multi-factor framework can be combined to obtain an even better asset pricing
model.
5
Electronic copy available at: https://ssrn.com/abstract=3350138
on a machine learning algorithm can substantially improve the estimation. Bryzgalova, Pelger,
and Zhu (2020) use decision trees to build a cross-section of asset returns, that is, a small set of
basis assets that capture the complex information contained in a given set of stock characteristics.
Their asset pricing trees generalize the concept of conventional sorting and are pruned by a novel 

","Luyang Chen, along with co-authors Markus Pelger and Jason Zhu, developed an autoencoder-based asset pricing model. This model replaces the linear beta specification with a more flexible beta function, explicitly accounting for the risk-return trade-off in equity returns.",3.5339949131011963,"The deep learning asset pricing model constructed by Luyang Chen and colleagues utilizes a deep neural network architecture that incorporates a flexible non-linear factor model. This model aims to estimate stock returns and takes into account various conditioning information while accounting for time-variation. The innovations of their approach include the integration of the fundamental no-arbitrage condition in the criterion function, the use of an adversarial approach to construct informative test assets, and the extraction of economic states from macroeconomic time series. The model is designed to outperform traditional methods in terms of Sharpe ratio, explained variation, and pricing errors, ultimately identifying key factors that drive asset prices."
qwen2.5:7b-instruct-q4_0,"What is done instead of comparing the variance of the factors before and after the break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 assumption violations.
In the current work, we extend the theory of generalized residuals to common factor
models with continuous MVs. This extension allows us to identify various sources of misfit
in common factor models, such as the nonlinear relationship between MVs and LVs,
heterogeneous variance of MVs conditional on LVs, and nonnormality of LVs. In addition,
we suggest a summary GOF statistic that complements the local statistics conditional on
LV values, which can facilitate an overall judgment on misfit. For all the proposed GOF
statistics, we derive asymptotic reference distributions to construct formal statistical tests.
Ultimately, our proposed framework is expected to complement conventional GOF
diagnostics for common factor models.
The rest of the paper is organized as follows. In Section 2, we introduce the
common factor model and present our extended theory of generalized residuals, along with
example GOF test statistics suitable for testing various model assumptions. In Section 3,
the performance of the proposed test statistics are evaluated by Monte Carlo studies. In
Section 4, our GOF testing methods are illustrated with a real data example. The paper is
concluded with a discussion of the main findings and future directions in Section 5.
2. Theory
2.1. Common Factor Model
2.1.1. Latent Variable Measurement Model
Let Yij ∈ Rbe the individuali’s response for manifest variable (MV)j, and
Xi = (Xi1, . . . , Xid)
⊤ 

Document 5: 
 spectively; and et = [ e1t,· · · ,eN t ]
′
is the N × 1 idiosyncratic error that may have serial and
cross-sectional dependence along with heteroskedasticit y . W e deﬁne π1 = T1/T ∈ (0,1) as the
break fraction, which is assumed to be a ﬁxed constant. This i mplies that T1 is a sequence that
depends on T. F or notational simplicity , we suppress the dependence of T1 on T.
W e are interested in testing the null hypothesis of no struct ural break in the factor loadings,
i.e.,
H0 : λi1 = λi2 ∀i. (2.2)
against the alternative hypothesis that a non-negligible p ortion of the cross sections have a break
in their loadings at a common time, i.e.,
H1 : λi1 ̸= λi2 for i∈ J (2.3)
where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally 

Document 6: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

","In lieu of directly comparing variances before and after the break, the likelihood ratio (LR) test implicitly explores the special feature where pre-break and post-break variances can be singular matrices under the alternative hypothesis. This makes the LR test more powerful than Wald-type tests by diverging faster.",3.673972845077514,"Instead of directly comparing the variance of the factors before and after the break in the likelihood ratio (LR) test for structural changes in factor models, the approach transforms the structural change problem into a low-dimensional problem by exploring a special feature of the estimated factors. Specifically, the test takes into account that the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, which enhances the power of the LR test compared to Wald-type tests. This method allows for more powerful testing of changes in the estimated factors' variances without requiring a direct comparison of those variances."
qwen2.5:7b-instruct-q4_0,"How to determine the number of break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 3: 
 100,500 0.154 0.081 0.017 0.106 0.053 0.017 0.108 0.054 0.01 3 0.063 0.031 0.010
200,200 0.080 0.032 0.005 0.072 0.036 0.013 0.084 0.030 0.00 7 0.052 0.023 0.006
200,500 0.156 0.080 0.022 0.108 0.056 0.018 0.102 0.043 0.01 1 0.066 0.027 0.007
500,500 0.158 0.084 0.020 0.121 0.069 0.020 0.111 0.059 0.01 4 0.066 0.039 0.014
23
T able 4: Size of structural break tests with unknown break da te in a factor model with t-distributed
factors. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.004 0.002 0.000 0.015 0.008 0.003 0.028 0.007 0.00 0 0.128 0.090 0.061
100,200 0.038 0.018 0.002 0.023 0.008 0.001 0.048 0.016 0.00 2 0.032 0.012 0.006 

Document 4: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

Document 5: 
 In this section, we estimate a factor model for the US industrial employment rates and apply
the proposed tests to check whether the factor loadings have undergone a structural change in
the past decade. Monthly data from January 2010 through Apri l 2022 are available from the US
Department of Labor for 84 industries. The data are a balance d panel with T = 147 ,N = 84.
W e use the information criteria ICp1 and ICp2 of Bai and Ng (2002), the ER (eigenvalue ratio)
and GR (growth ratio) of Ahn and Horenstein (2013), the empirical d istribution estimator of
Onatski (2010), and the bridge estimator of Caner and Han (20 14) to determine the number of
common factors in the data. The maximum number of factors is s et to 10. The estimated number
of factors by ICp1 and ICp2 is three; the methods of Onatski (2010) and Caner and Han (201 4)
detect two factors; and ER and GR of Ahn and Horenstein (2013) choose one common factor.
Therefore, we study the test results for the cases of one, two , and three common factors.
W e apply our LR and LR m tests to examine whether there exists a structural break in f actor
loadings and estimate the break date if the null hypothesis i s rejected. All of the settings for the
two tests are the same as those used in the simulation studies . 

Document 6: 
 References
Ahn, S. and Horenstein, A. 2013. Eigenvalue ratio test for the number of factors. Econometrica 81 ,
pp. 1203–1227.
Andrews, D.W.K., 1993. T ests for parameter instability and structural change with unknown
change point. Econometrica 61 , pp. 821–856.
Bai, J., 1997. Estimaing multiple breaks one at a time. Econometric Theory 13 , pp. 315–352.
Bai, J., 2000. V ector autoregressive models with structura l changes in regression coeﬃcients and
in variance-covariance matrices. Annals of Economics and Finance 1 , pp. 303–339.
43
Bai, J., 2010. Common breaks in means and variances for panel data. Journal of Econometrics 157 ,
pp. 78–92.
Bai, J. and Ng, S. 2002. Determining the number of factors in a pproximate factor models. Econo-
metrica 70 , pp. 191–221.
Bai, J., 2003. Inferential theory for factor models of large dimensions. Econometrica 71 , pp.
135–171.
Bai, J., Han, X., Shi, Y., 2020. Estimation and inference of c hange points in high-dimensional 

","Determining the number of breaks in a likelihood ratio (LR) test for structural changes in factor models involves several steps. Here's a general outline:

1. **Understand the Factor Model:**
   - First, ensure you understand the structure of your factor model. This could be an exploratory factor analysis (EFA), confirmatory factor analysis (CFA), or another type of factor model.
   
2. **Specify Potential Break Points:**
   - Identify potential break points based on external information such as known changes in economic conditions, policy shifts, or other exogenous events.

3. **Formulate the Null and Alternative Hypotheses:**
   - The null hypothesis (H0) typically states that there are no breaks in the factor model parameters.
   - The alternative hypothesis (Ha) suggests that structural changes have occurred at one or more points.

4. **Apply LR Test for Structural Changes:**
   - Perform an LR test to compare a model with breaks against a model without breaks. This involves estimating both models and comparing their likelihoods.

5. **Likelihood Ratio Statistic:**
   - Compute the LR statistic, which is typically given by:
     \[
     \text{LR} = -2 (\ln L_{\text{unrestricted}} - \ln L_{\text{restricted}})
     \]
     where \(L_{\text{unrestricted}}\) is the likelihood of the unrestricted model (with breaks) and \(L_{\text{restricted}}\) is the likelihood of the restricted model (without breaks).

6. **Determine Critical Values:**
   - Compare the LR statistic to critical values from a chi-square distribution with degrees of freedom equal to the number of parameters affected by the breaks.

7. **Decision Rule:**
   - If the LR statistic exceeds the critical value, reject the null hypothesis and conclude that there are structural changes.
   - Otherwise, fail to reject the null hypothesis and assume no breaks.

8. **Model Selection Criteria:**
   - Use information criteria such as AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to balance model fit with complexity. These criteria can help in selecting the number of breaks that minimizes overfitting.

9. **Bootstrap Methods:**
   - For more robust results, consider using bootstrapping techniques to estimate the distribution of the LR statistic under the null hypothesis and adjust for any potential bias or misspecification.

10. **Visual Inspection and Diagnostics:**
    - Perform visual inspection of factor loadings over time or use diagnostic plots (e.g., CUSUM statistics) to detect structural changes more intuitively.

11. **Iterative Refinement:**
    - Start with a small number of breaks and iteratively add or remove them based on the LR test results and model diagnostics until an optimal set is identified.

By following these steps, you can systematically determine the appropriate number of breaks in your factor models using likelihood ratio tests and related statistical techniques.",14.549753665924072,"To determine the number of breaks in the likelihood ratio (LR) test for structural changes in factor models, the procedure involves the following steps:

1. **Initial Break Point**: Identify the initial break point, denoted as \( \hat{k}_1 \), using a method such as the Quasi-Maximum Likelihood (QML) method.

2. **Subsample Division**: Split the entire sample into two subsamples: one from the start to \( \hat{k}_1 \) and the other from \( \hat{k}_1 \) to the end of the sample.

3. **Conduct Sup-LR Tests**: For each subsample, conduct a sup-LR test with \( m = 1 \). If the null hypothesis is rejected at a specified significance level \( \alpha \), this suggests the presence of additional breaks.

4. **Estimate Further Break Points**: For the subsample where the null is rejected, estimate a new break point, and repeat the process by further dividing the subsample at this newly estimated break point. 

5. **Continue Until Non-Rejection**: Continue this splitting and testing process until the LR test does not reject the null hypothesis for all subsamples. 

6. **Count Breaks**: The number of break points is determined by counting the number of subsamples minus one. This gives a consistent estimate of the number of breaks when the significance level \( \alpha \) converges to zero slowly.

This method effectively tracks structural changes and their locations by continuously refining the sample based on the results of the statistical tests at each step."
qwen2.5:7b-instruct-q4_0,"What is equivalent of a factor model with structural brak in factor loading?
","Document 1: 
 where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally
equivalent to a model with time-invariant loadings and pote ntially more pseudo-factors (e.g., HI,
2015; Baltagi et al., 2017). T o capture the factor dimension augmentation caused by the break,
we follow the framework of DBH (2022) and set r as the number of pseudo-factors in (2.1). W e
set
Λ1 = Λ B, Λ2 = Λ C,
5
where Λ is an N× rmatrix with full column rank r, B and C are r× rmatrices, rank( B) = r1 ≤ r,
and rank( C) = r2 ≤ r.
F or a given split point k, deﬁne
X(1)
k = [ x1 ,...,x k ]′, X(2)
k = [ xk+1,...,x T ]′, F(1)
k = [ f1,...,f k ]′ , F(2)
k = [ fk+1,...,f T ]′ ,
e(1) 

Document 2: 
 loadings βSDF as a function of characteristics on diﬀerent stocks for the estimation and evaluation.
Therefore, we can evaluate the model on stocks that are not available in the ﬁrst part of the data.
Importantly, the conditional model also allows us to directly study the economic sources of risk in
terms of ﬁrm-characteristics.
Appendix C.D. Conditional Factor Models
A conditional factor model assumes that the SDF is spanned by a linear combination of con-
ditional risk factors, and hence it restricts the basis assets that span the SDF. In contrast to the
60
Electronic copy available at: https://ssrn.com/abstract=3350138
unconditional model, the SDF weights and loadings of the conditional risk factors are a function
of the characteristics and hence time-varying. We use the Instrumented Principal Component
Analysis (IPCA) of Kelly, Pruitt, and Su (2019) to illustrate this setup. This conditional factor
model directly models individual stock returns as a function of characteristics given by Re
t+1,i =
b⊤
t,ifIPCA
t+1 + ϵt+1,i with bt,i = I⊤
i,tΓb. Instead of allowing the SDF to be Mt+1 = 1 −∑N
i=1 ωt,iRe
t+1,i,
it is restricted to Mt+1 = 1 −∑K
k=1 wIPCA
t,k fIPCA 

Document 3: 
 EC08CH03-Bai ARI 17 October 2016 11:32
where λ = vec
(
/Lambda1′)
. Consider the following restriction on the factor loadings:
λ = Bδ + C, (19)
where δ is a vector of free parameters with dim( δ) ≤ dim( λ). In general, B and C are known ma-
trices and vectors deﬁned by either identifying restrictions or other structural model restrictions.
In view of Equation 19, we may rewrite the restricted factor model in Equation 18 as
y = Zδ + vec( E),
where y = vec( X) − (IN ⊗ F)C and Z = [( IN ⊗ F)B]. If we impose some distributional assump-
tions on the error terms, for example, vec (E|Z) ∼ N(0 ,R ⊗ IT −s )f o rs o m e N × N positive
deﬁnite matrix R, such models can be estimated using the Bayesian algorithm from Bai & Wang
(2015).
11.3. Structural Vector Autoregression and Restricted Dynamic Factor Models
The dynamic factor models also bring new insight into the estimation of structural vector autore-
gression (SVAR) models with measurement errors. Consider a traditional SVAR given by
A( L)Zt = at , 

Document 4: 
 100,200 0.998 0.984 0.792 0.861 0.704 0.424 1.000 1.000 1.00 0 1.000 1.000 0.997
100,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
200,200 1.000 0.980 0.784 0.860 0.702 0.419 1.000 1.000 1.00 0 1.000 1.000 0.999
200,500 1.000 1.000 1.000 1.000 1.000 0.998 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
24
T able 6: Power of structural break tests with unknown break d ate as the magnitude of the break in factor
loadings increases. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
λi2 = λi1 + εi, εi
i.i.d.∼ N (0, b · Ir0 )
N = 100 , T = 100 , ρ = 0
b
0.0 0.015 0.005 0.000 0.026 0.007 0.002 0.055 0.018 0.003 0.158 0.111 0.074 

Document 5: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 6: 
 model that specifies the relationship between LVs and MVs.
A widely used LV measurement model is the common factor model (e.g., Bollen,
1989; Jöreskog, 1969; Kaplan, 2008; Kline, 2023; Lawley and Maxwell, 1971), particularly
when analyzing continuous data. The common factor model requires MVs to be linearly
dependent on LVs, aiming to attribute the dependencies among MVs to LVs. This model
has been widely used for testing theories about the number of LVs and the pattern of
MV-LV dependency (i.e., confirmatory factor analysis), or for learning these structures
from data (i.e., exploratory factor analysis).
Most of the commonly used measurement models, however, rely on strong
parametric assumptions. For example, in the common factor model, the conditional mean
of an MV given LVs is a linear function of LVs, and the conditional variance of an MV is
constant. In addition, it is often assumed that the MVs and LVs are normally distributed.
These restrictive assumptions may be violated when analyzing real-world data, resulting in
poor model-data fit and potentially erroneous inferences.
To prevent fallacious inferences, various goodness-of-fit (GOF) assessment tools
have been developed for common factor models. A majority of existing GOF diagnostics
rely on residual means, residual covariances, or deviances to reflect the discrepancies 

","A factor model with a structural break in the factor loadings can be understood as a scenario where the relationship between observed variables and latent factors changes at some point in time. This can be modeled using several approaches, depending on your specific needs and data characteristics. Here are some equivalent or related modeling techniques:

1. **Switching Regressions**: These models allow for different relationships (or regimes) to hold under different conditions. You could use a switching regression model where the factor loadings change at a structural break point.

2. **Markov-Switching Factor Models**: This approach extends traditional factor models by allowing the factor loadings to switch between different states based on a Markov process. Each state can have its own set of factor loadings, and transitions between states are governed by transition probabilities.

3. **Threshold Autoregressive (TAR) Models**: These models allow for regime changes in autoregressive relationships but could be adapted to include factor loadings as well. TAR models split the data into different regimes based on threshold variables or time points.

4. **Piecewise Linear Factor Models**: This approach involves fitting separate factor models over different segments of time, where each segment may have its own set of factor loadings. You would identify structural breaks and fit a model for each segment separately.

5. **Change Point Analysis**: You can use change point analysis to detect the exact time points (breaks) at which the relationships between factors and observed variables change. Once detected, you could then estimate separate models before and after these breakpoints.

6. **Bayesian Structural Time Series Models**: These models allow for parameter changes over time using a Bayesian framework, where factor loadings can vary across different time periods.

7. **Dynamic Factor Models with Regime-Switching**: A combination of dynamic factor models (which capture the evolution of factors over time) and regime-switching elements allows you to model how factor loadings change based on underlying regimes or states.

In practice, these approaches require careful specification of the structural breaks, identification of appropriate threshold variables or regimes, and robust estimation techniques. You may need to use statistical software packages like R (using packages such as `DynRegime`, `MSwM`, etc.) or Python (using libraries such as `statsmodels` for change point analysis) to implement these models effectively.

Choosing the right approach depends on your specific data characteristics, the nature of the structural breaks you expect, and the underlying theory guiding your model.",11.961141586303713,"A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This transformation simplifies the analysis of structural changes by reducing it from a high-dimensional problem to a low-dimensional problem, allowing for the likelihood ratio (LR) test to be more powerful in detecting variance changes in the estimated factors."
qwen2.5:7b-instruct-q4_0,"What is null hypothesis is likehood ratio test for structural changes in factor model?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 LQ(k) = klog(| ˜Σ1 (k)|) + ( T − k) log( | ˜Σ2(k)|)
= klog(|Q′ | · | ˆΣ1 (k)| · | Q|) + ( T − k) log( |Q′| · | ˆΣ2(k)| · | Q|)
= 2 Tlog(|Q|) + klog(| ˆΣ1 (k)|) + ( T − k) log( | ˆΣ2 (k)|)
The log-likelihood of no change for the entire sample become s
LQ0 = Tlog(|T−1
T∑
t=1
˜gt,Q ˜g′
t,Q|) = Tlog(|Q′ | · | T−1
T∑
t=1
ˆgt ˆg′
t| · | Q|) = 2 Tlog(|Q|)
because T−1 ∑ T
t=1 ˆgt ˆg′
t = Ir under (3.1). Thus, the likelihood ratio test under this alte rnative
identiﬁcation condition can be expressed as
LRQ(k) ≡ − klog(| ˜Σ1(k)|) − (T − k) log( | ˜Σ2 (k)|) + LQ0 

Document 3: 
 in Section 3.
2 Performance of default bootstrap and permutation methods
To review from Section 1, the problem statement is to test the following factor model:
Yt = LtXt + ϵt for t = 1, . . . , T, (2.1)
for outcomes Yt ∈ Rp, fixed and known exposures Lt ∈ Rp×k, unobserved factor returns Xt ∈ Rk and
residuals ϵt ∈ Rp. We seek to test the null hypothesis H0 that the time series of residuals for each
asset are independent:
H0 : ϵ·,1, ϵ·,2, . . . , ϵ·,p ∈ RT are jointly independent. (2.2)
Sections 2.1-2.2 now explain why naive permutation and bootstrap tests can yield false positive rates
of up to 100%, as in Figure 3. The main challenge is that the estimated OLS residuals ˆ ϵols do not
satisfy the same independence properties as the true residuals.
2.1 Naive residual permutation tests are invalid
For simplicity, we assume for this section that the vectors of residuals ϵ1, . . . , ϵT
i.i.d.∼ Pϵ are i.i.d. This
assumption plus H0 together imply that separately permuting the residuals of each asset does not
change the joint law of all of the residuals:
ϵ :=

 

Document 4: 
 aresettozero.
• Scenario2:Changeinonecolumn.
Let1−1 = (ρ(−)
ij ) with
ρ(−)
1j = ρ(−)
j1 =



sign(ρj−1)|ρ|
1+ 3(p− 1)ρ2/2
for j> 1,
1 for j= 1.
Theremainingcorrelationcoefficientsarezero.
• Scenario3:Allcorrelationcoefficientsarechanged.
Here,weput ρij = ρ|j−i| fori, j∈ {1, . . . ,p}.
• Scenario4:Changeinthenumberoffactors.
Thenumberoffactorsinthetruemodelisincreasedto K+ ˜Kwith˜K∈ {1, . . . ,10}.
Thesefourscenariosleadtodifferenttypesoffactormodelsunderthealternativehypothesis.Forinstance,incaseof
Scenario1,asinglechangeinthecorrelationmatrixofresidualsisassumed,whereasScenario2leadstochangesinthefirst
column(row)of 11.Scenario3correspondstochangesinallelementsof 11althoughtheirmagnitudebecomessmaller 

Document 5: 
 However, it can be shown that H′
0gt = R′ ηt for some orthonormal matrix R. Because a pre-
multiplication by R does not change the distribution of an independent standard normal vector,
we can still replace Ω with ˆΩ when simulating the limiting null distribution.
Note that the presence of Ω in (3.5) is due to the potentially m isspeciﬁed likelihood function
(3.2), which assumes that gt,t = 1 ,· · · ,T are i.i.d. Gaussian under the null hypothesis. If (3.2)
correctly speciﬁes the likelihood, then the limiting distr ibution reduces to
sup
[ǫT ]≤k≤[(1−ǫ)T ]
LR(k) ⇒ sup
π∈[ǫ,1−ǫ]
[W1(π) − πW1(1)]′[W1(π) − πW1(1)]
π(1 − π) (3.7)
where W1(π) is an r(r+ 1) /2 vector of independent Brownian motions. The distribution in (3.7)
is the same as that used in conventional supreme type tests fo r a structural break, and the critical
values can be found in Andrews (1993).
If gt,t = 1 ,· · · ,T are i.i.d. Gaussian and the null hypothesis is true, the sup- LR statistic is 

Document 6: 
 200,200 0.844 0.677 0.333 0.736 0.498 0.187 1.000 1.000 0.99 9 1.000 0.998 0.956
200,500 0.999 0.999 0.984 0.999 0.997 0.951 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 0.999 0.985 0.998 0.995 0.934 1.000 1.000 1.00 0 1.000 1.000 1.000
T able 9: Power of structural break tests with unknown break d ate in the case of factor mean change.
ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.048 0.014 0.000 0.071 0.026 0.005 0.128 0.049 0.00 5 0.196 0.106 0.037
100,200 0.202 0.103 0.018 0.215 0.109 0.022 0.269 0.141 0.02 8 0.551 0.335 0.101
100,500 0.653 0.499 0.221 0.649 0.508 0.222 0.678 0.531 0.25 6 0.967 0.919 0.611
200,200 0.199 0.096 0.020 0.215 0.107 0.019 0.277 0.149 0.03 2 0.556 0.341 0.115 

","The concept of a ""null hypothesis"" and likelihood ratio tests can be applied to detecting structural changes (also known as breaks or shifts) in a factor model, but it's important to understand the specifics:

### Null Hypothesis:
In the context of testing for structural changes in a factor model, the null hypothesis (\(H_0\)) typically states that there is no structural change. In other words, the parameters of the factor model remain constant over time or across different periods.

For example, if you are testing whether there has been a shift in the factors affecting asset returns, the null hypothesis might be:
\[ H_0: \text{No structural change in the factor model} \]

### Likelihood Ratio Test (LRT):
A likelihood ratio test is a statistical test that compares the goodness of fit between two models: one with a simpler structure (the null hypothesis) and another with an additional complexity (the alternative hypothesis).

In the context of detecting structural changes, you would:
1. **Fit the model under the null hypothesis**: Assume no change in the factor model parameters.
2. **Fit the model under the alternative hypothesis**: Allow for breaks or shifts in the parameters at certain points in time.

The likelihood ratio test statistic is then calculated as:
\[ \text{LRT} = -2 \times (\ln(\text{likelihood of the null model}) - \ln(\text{likelihood of the alternative model})) \]

If this test statistic follows a chi-squared distribution under the null hypothesis, you can determine whether to reject the null hypothesis (i.e., whether there is evidence of structural change).

### Application in Factor Models:
In factor models, such as the Fama-French three-factor model or any multivariate regression with factors, detecting structural changes involves testing for shifts in the loadings or intercepts. For instance:

- **Fama-French Three-Factor Model**: You might test whether the market, size, and value factors have changed over time.
- **General Factor Models**: You could be interested in detecting shifts in multiple factors that influence returns.

### Steps:
1. **Estimate Parameters Under \(H_0\)**: Fit the factor model without allowing for breaks (i.e., assume no change).
2. **Estimate Parameters Under \(H_1\)**: Allow for breaks by estimating different parameters before and after a potential shift.
3. **Calculate LRT Statistic**: Compute the likelihood ratio statistic based on these estimates.
4. **Compare to Chi-Squared Distribution**: Compare the calculated LRT to critical values from a chi-squared distribution with degrees of freedom equal to the number of additional parameters you are testing for (i.e., number of breaks).

### Example:
Suppose you fit a factor model without allowing any breaks, and then fit it again by estimating different loadings before and after 2008. If the LRT statistic is large enough (i.e., exceeds the critical value from the chi-squared distribution), you would reject the null hypothesis and conclude that there was a structural change in the factor model around 2008.

In summary, applying the concept of a null hypothesis and likelihood ratio tests to detect structural changes in factor models involves formulating the null and alternative hypotheses appropriately, fitting the models under these hypotheses, calculating the LRT statistic, and comparing it against critical values.",16.216251373291016,"The null hypothesis (H0) for the likelihood ratio test for structural changes in a factor model typically posits that there are no structural changes in the factor loadings or variances across the observed period. In the context of the provided snippets, this means that the time series of residuals for each asset are jointly independent, indicating that any dependencies occurring in the data can be attributed to the specified factor model without any structural shifts. The alternative hypothesis posits that there is a structural change, which may include changes in the factor loadings or variances of the factors."
qwen2.5:7b-instruct-q4_0,"Is it possible to test mutliple change with sup-LR test?
","Document 1: 
 limit is similar to that of the usual W ald test,
Waldm(k) ⇒ ∥ W(π) − πW(1)∥ 2
π(1 − π)
where W(π) is an r+ r(r+ 1) /2 = r(r+ 3) /2 standard Brownian motion. This test is nuisance-
parameter free.
Consider the simple weighting by k(T − k)/T2, let WWald = k(T − k)/T2 · Waldm(k). T ake
16
the maximum over the range say k= r+ 1,r + 2,· · · ,T − r− 1. The limiting distribution is simply
sup
π∈[0,1]
∥ W(π) − πW(1)∥ 2 ,
and the critical values do not depend on trimming parameter ǫ.
4.2 T esting multiple changes
In this section, we extend the sup-LR test to multiple changes. W e consider testing the null
hypothesis of no change versus the alternative hypothesis o f a prespeciﬁed number of changes. T o
allow for mchanges under the alternative hypothesis, let 0 = k0 <k1 <k2 <· · · <km <km+1 = T
and deﬁne
LR(k1 ,...,k m) = −
m∑
j=0 

Document 2: 
 the null hypothesis. As the value of a increases, all of the tests become more powerful, except
for W ald(HAC) and LM(HAC) when N = T = 100. The LR test has the highest power under
this DGP . The power of the LR m test increases at a slower rate as a increases. Our Theorem
3 demonstrates that the sup-LR diverges at the same rate as th e conventional sup-W ald when
both pre-break and post-break pseudo-factors have nonsing ular variances. However, our simulation
results indicate that sup-LR remains more powerful than sup -W ald under this scenario, as reported
in T able 7. Speciﬁcally , when the loading matrix undergoes a rotational change, sup-LR is notably
more powerful than sup-W ald with HAC variance. While we ackn owledge that the theoretical
power comparison under this setup may require analysis unde r local alternatives, we leave this as
a future research topic.
T able 8 presents the power against changes in the number of fa ctors. The post-break loadings
are equal to pre-break loadings multiplied by an r × r matrix, i.e., Λ 2 = Λ 1C. W e set C = 

Document 3: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 LRm (k1,...,k m) ⇒ sup
(π1 ,··· ,πm)∈Θ π,ǫ
m∑
j=0
∥ ¯Ω 1/2 [B(πj+1 )−B(πj )]∥ 2
πj+1 −πj
, (4.3)
where B(π) = U(π) − πU(1) is an (r+ r2) Brownian bridge process, and ¯Ω is deﬁned in Theorem
4.
These results depend on m, but mdoes not have to be correctly speciﬁed. W e can also consider
the double max type of test and the conditional test for multi ple breaks as in Bai and Perron (1998),
but we leave these as future research topics.
4.3 Determining the number of breaks
In practice, the number of breaks in a factor model is often unknown. W e follow Bai’s (1997)
sequential testing procedure to provide a consistent estim ate for the true number of breaks. The
procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples: 

Document 6: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

","Yes, the sup-LR test can be extended to test for multiple changes. It considers testing the null hypothesis of no change versus a prespecified number of changes by defining LR(k1,...,km) and taking the maximum over a range of k values.",2.8749701976776123,"Yes, it is possible to test for multiple changes using the sup-LR test. The context indicates that the sup-LR test has been extended to allow for multiple changes under the alternative hypothesis, enabling the testing of the null hypothesis of no change against a prespecified number of changes, as outlined in Document 1 and Document 5."
qwen2.5:7b-instruct-q4_0,"What is misbehavior of the residual bootstrap in high dimension?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 ϵ = 2. Each simulation uses the standard estimate of σ2
ϵ from
the generated ϵi’s. rρ(κ; G) was computed using a ﬁrst run of simulations with
ϵi
iid∽ G. The Huber loss in this plot is Huber 1 and not the default Huber 1.345 of
the rlm function.
to bootstrapping from the residuals. In the case of random design, it makes also a lot
of intuitive sense to use the pairs bootstrap, since resampling the predictors might be
interpreted as mimicking the data generating process.
However, as in residual bootstrap, it is clear that the pairs bootstrap will have problems,
at least in quite high dimensions. In fact, when resampling the Xi’s from ˆF, the number
of times a certain vector Xi0 is picked has asymptotically Poisson(1) distribution. So the
expected number of diﬀerent vectors appearing in the bootstrapped design matrix X∗ is
n(1 −1/e). When p/n is large, with increasingly high probability the bootstrapped design
matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that 

Document 3: 
 accurate conﬁdence intervals in moderately high dimensions.
In Section 4, we discuss another resampling scheme, the jackknife. We focus on the
jackknife estimate of variance and show that it has similarly poor behavior in high dimen-
sions. In the case of L2 loss with Gaussian design matrices, we further prove that the
jackknife estimator over estimates the variance of our estimator by a factor of 1 /(1 −p/n);
we also provide corrections for other losses that improve the jackknife estimate of variance
in moderately high dimensions.
We rely on simulation results to demonstrate the practical impact of the failure of the
bootstrap. The settings for our simulations and corresponding theoretical analyses are ide-
alized, without many of the common settings of heteroskedasticity, dependency, outliers and
so forth that are known to be a problem for bootstrapping. This is intentional, since even
these idealized settings are suﬃcient to demonstrate that the standard bootstrap methods
have poor performance. For brevity, we give only brief descriptions of the simulations in
what follows; detailed descriptions can be found in AppendixD.1.
Similarly, we focus on the basic implementations of the bootstrap for linear models.
While there are many proposed alternatives (often for speciﬁc loss functions or types of
data), the standard methods we study are most commonly used and recommended in prac-
tice. Furthermore, to our knowledge none of the alternative bootstrap methods we have 

Document 4: 
 regression, by making assumptions on the diagonal entries of the hat matrix. In our context,
these assumptions would be satisﬁed only if p/n →0. Hence those interesting results do
not apply to the present study. We also note that Hall (1992, p. 167) contains cautionary
notes about using the bootstrap in high-dimension.
While there has not been much theoretical work on the bootstrap in the setting where
p/n →κ ∈(0,1), one early work of Bickel and Freedman (1983) considered bootstrap-
ping scaled residuals for least-squares regression when κ > 0. They show that when
p/n → κ ∈ (0,1), there exists a data-dependent direction c, such that c′ˆβ∗
LS does not
have the correct asymptotic distribution (Bickel and Freedman, 1983, Theorem 3.1, p.39),
i.e., its distribution is not conditionally in probability close to the sampling distribution of
c′ˆβLS. Furthermore, they show that when the errors in the model are Gaussian, under the
assumption that the diagonal entries of the hat matrix are not all close to a constant, the
empirical distribution of the residuals is a scaled-mixture of Gaussian, which is not close to
the original error distribution.
As we previously explained, in this work we instead only consider inference forpredeﬁned
contrasts υ′β. The important and interesting problems pointed out in Bickel and Freedman 

Document 5: 
 Journal of Machine Learning Research 19 (2018) 1-66 Submitted 1/17; Revised 12/17; Published 08/18
Can We Trust the Bootstrap in High-dimensions?
The Case of Linear Models
Noureddine El Karoui nkaroui@berkeley.edu, n.elkaroui@criteo.com
Criteo AI Lab
32 Rue Blanche
75009 Paris, France
and
Department of Statistics
University of California
Berkeley, CA 94270, USA
Elizabeth Purdom epurdom@stat.berkeley.edu
Department of Statistics
University of California
Berkeley, CA 94270, USA
Editor: Guy Lebanon
Abstract
We consider the performance of the bootstrap in high-dimensions for the setting of linear
regression, where p<n but p/nis not close to zero. We consider ordinary least-squares as
well as robust regression methods and adopt a minimalist performance requirement: can
the bootstrap give us good conﬁdence intervals for a single coordinate of β (where β is the
true regression vector)?
We show through a mix of numerical and theoretical work that the bootstrap is fraught
with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","In high dimensions, the residuals from regression models tend to have a very different distribution from the true errors, differing not only in simple properties like variance but also in more general aspects such as marginal distributions. This leads to the residual bootstrap giving anti-conservative estimates with inflated Type I error rates.",3.1291263103485107,"The misbehavior of the residual bootstrap in high dimensions is primarily due to the fact that the residuals tend to have a very different distribution compared to the true errors. Specifically, their distributions differ not only in basic properties such as variances but also in more general aspects, including their marginal distributions. This discrepancy can lead to anti-conservative estimates, resulting in inflated Type I error rates. When the ratio of predictors to observations (p/n) is large, the residual bootstrap fails to provide accurate inferences, highlighting the challenges of using standard bootstrap methods in high-dimensional regression settings."
qwen2.5:7b-instruct-q4_0,"How misbehavior of the residual in high dimension can be explained?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 decent fit of the linear normal one-factor model, our method detected misfit in various
model assumptions: misfit in the LV density, the MV-level mean function, and the
MV-level variance function.
In factor analysis, it is common to assume the joint multivariate normality of MVs
and LVs in order to perform ML estimation. Various well-known methods exist for
assessing normality at the level of observed variables (e.g., Mardia, 1970; Mardia, 1980);
however, to our knowledge, assessing the normality assumption for LVs is less common.
This LV-level misfit can be conveniently assessed by generalized residuals, which sometimes
GENERALIZED RESIDUALS 29
can explain non-normality at the MV level, as is the case in our empirical example.
Furthermore, graphical plots such as the right panel of Figure 8 provide intuitive guidance
for model modification.
In the MV-level fit assessment, our method helps to identify curvilinear
relationships in the mean and/or variance functions. In the case of item 7 in our empirical
example, the misfitting linear normal factor model tended to overestimate the conditional
mean of log-RT while underestimating the conditional variance at the high end of the LV.
Such misfits can be particularly problematic if researchers’ focus is on investigating
extremely slow responders. Similar discussions can also be found in Liu and Wang (2022),
where similar patterns of curvatures in the conditional moment functions have been
identified using a semiparametric approach. 

Document 3: 
 P
→λ⋆. This gives that 2Λj is a
multiple of aχ2
1 variable with multiplicative factor given byκσ2
⋆/λ⋆.
This rough analysis shows that the distribution of the LLR in high dimensions deviates from aχ2
1 due
to the coupled eﬀects of two high-dimensional phenomena. The ﬁrst is the inﬂated variance of the MLE,
which is larger than classically predicted. The second comes from the termλ⋆, which is approximately equal
to Tr
(
H−1( ˆβ)
)
/n, where H( ˆβ) = ∇2ℓ( ˆβ) is the Hessian of the negative log-likelihood function. In the
classical setting, this Hessian converges to a population limit. This is not the case in higher dimensions and
the greater spread in the eigenvalues also contributes to the magnitude of the LRT.
5 Broader Implications and Future Directions
This paper shows that in high-dimensions, classical ML theory is unacceptable. Among other things, classical
theory predicts that the MLE is approximately unbiased when in reality it seriously overestimates eﬀect
magnitudes. Since the purpose of logistic modeling is to estimate the risk of a speciﬁc disease given a patient’s
observed characteristics, say, the bias of the MLE is extremely problematic. As we have seen, an immediate
consequence of the strong bias is that the MLE either dramatically overestimates, or underestimates, the
chance of being sick. The issue becomes increasingly severe as either the dimensionality or the signal 

Document 4: 
 data IRT (e.g., Wang, 2010).
Finally, exploring efficient ways to compute and visualize generalized residuals when
dealing with high-dimensional LVs is a potential topic for future research. When the latent
dimensionality is high, it is impossible to construct evaluation points using an outer
product grid as the number of evaluation points grows exponentially fast. For example, in
a five-dimensional case, examining residuals at just five different points per LV leads to
55 = 3, 125 evaluations, making it computationally intensive and posing a challenge for
generating graphical plots. Additionally, selecting which grids of LVs to summarize the
results would also be challenging. Formulating partially marginalized residuals conditioned
only on one or two LV(s) may yield more informative diagnostics.
GENERALIZED RESIDUALS 30
References
Alexander, P. A., Dumas, D., Grossnickle, E. M., List, A., & Firetto, C. M. (2016).
Measuring relational reasoning.The Journal of Experimental Education, 84(1),
119–151.
Bentler, P. M. (1990). Comparative fit indexes in structural models.Psychological bulletin,
107(2), 238.
Bentler, P. M. (1995).Eqs structural equations program manual(Vol. 6). Multivariate
software Encino, CA.
Bickel, P. J., & Doksum, K. A. (2015).Mathematical statistics: Basic ideas and selected 

Document 5: 
 pressed sensing. IEEE Transactions on Information Theory, 57(10):6920–6941, 2011.
[26] Noureddine El Karoui. Asymptotic behavior of unregularized and ridge-regularized high-dimensional
robust regression estimators: rigorous results.arXiv preprint arXiv:1311.2445, 2013.
[27] Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators.Probability Theory and Related Fields, pages
1–81, 2017.
23
[28] Noureddine El Karoui, Derek Bean, Peter J Bickel, Chinghway Lim, and Bin Yu. On robust regression
with high-dimensional predictors. Proceedings of the National Academy of Sciences, 110(36):14557–
14562, 2013.
[29] Jianqing Fan and Jinchi Lv. Nonconcave penalized likelihood with NP-dimensionality.IEEE Transac-
tions on Information Theory, 57(8):5467–5484, 2011.
[30] Yingying Fan, Emre Demirkaya, and Jinchi Lv. Nonuniformity of p-values can occur early in diverging
dimensions. https://arxiv.org/abs/1705.03604, May 2017.
[31] David Firth. Bias reduction of maximum likelihood estimates.Biometrika, 80(1):27–38, 1993. 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","In high-dimensional spaces, the behavior of residuals (the differences between observed and predicted values) can exhibit some unique characteristics that differ from lower dimensions. Here are a few ways to explain this:

1. **Curse of Dimensionality**: In high-dimensional spaces, data points tend to be sparse and far apart from each other. This means that even small residual errors in one dimension can become relatively larger when considering the overall Euclidean distance in a higher-dimensional space.

2. **Increased Variance**: As dimensions increase, the variance of residuals tends to increase because there are more ways for the data points to vary independently. In high dimensions, the variance of the residuals can grow at a faster rate than the mean squared error (MSE) decreases with the number of training examples.

3. **Randomness and Noise Amplification**: High-dimensional spaces can amplify noise due to the increased complexity and degrees of freedom available for deviations from the true model. This means that small random variations in the data can contribute more significantly to the residuals in higher dimensions.

4. **Non-Orthogonality Issues**: In high dimensions, it is harder to find orthogonal directions where the residual error would be minimized. This makes it difficult to isolate specific sources of error and can lead to a more scattered distribution of residuals.

5. **Overfitting**: High-dimensional models are prone to overfitting, which means they may fit the noise in the training data rather than the underlying signal. Overfitting results in high residual errors on unseen data, even if the model fits well within the training set.

6. **Dimensionality and Geometry**: The geometry of high-dimensional spaces can lead to counterintuitive phenomena like the concentration of distances (where most points are equidistant from each other) and the fact that high-dimensional spheres have more ""surface area"" than their volumes, which affects how residuals behave in these spaces.

7. **Statistical Independence**: In lower dimensions, it is easier for residual errors to be correlated or dependent on each other due to fewer variables. However, as dimensionality increases, it becomes harder for such dependencies to persist, leading to more random and less structured residual behavior.

8. **Regularization Effects**: Techniques like L1 and L2 regularization can mitigate some of these issues by constraining the model complexity. However, in very high dimensions, they may not be sufficient to prevent overfitting or maintain good generalization performance.

Understanding these aspects helps in designing models that are robust to the challenges posed by high-dimensional data and residuals. Techniques such as dimensionality reduction (e.g., PCA), regularization, and using more sophisticated models like neural networks with appropriate architectures can help manage these issues effectively.",13.039504289627075,"The misbehavior of the residual bootstrap in high dimensions can be explained by the fact that the residuals tend to have a very different distribution compared to that of the true errors. This discrepancy is evident in both simple properties, such as variances, and more general aspects, including the marginal distributions. The work of El Karoui et al. (2013) characterizes the asymptotic relationship between the residuals and true errors in high dimensions, which reveals that as the dimensionality increases, the classical assumptions of the bootstrap no longer hold, leading to a poor approximation of confidence intervals and other statistical inference derived from bootstrap methods."
qwen2.5:7b-instruct-q4_0,"What is a alternative residual bootstrap procedure called deconvolution?
","Document 1: 
 This explains why the scaled ˜ ri(i) is probably a good error distribution ϵ∗ to use in the
residual bootstrap when κis close to 0 or 1. We note that when κis close to 1, ˜ri(i) gives an
error distribution that is in general very diﬀerent from the distribution of ϵ. Our numerical
work of Section 2.3 shows that it is nonetheless a good error distribution from the point of
view of the residual bootstraps we consider.
Appendix B. Deconvolution Bootstrap
In the main text, we considered situations where our predictorsXi are i.i.d with an elliptical
distribution and assume for instance that Xi = λiξi, where ξi ∼N (0,Σ) and λi are i.i.d
scalar random variables with E
(
λ2
i
)
= 1. As described in the main text, if X is elliptical,
˜ei(i) is a convolution of the correct G distribution and a Normal distribution,
˜ei(i) ≃ϵi + ˜Zi,
where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the 

Document 2: 
 problem, and experiments demonstrate that increasing cardinality C is more effective
than going deeper or wider when we increase the capacity. The authors compared the
completely equivalent structures of the three mathematical calculations in Figure 18.
The experimental results show that block Figure 18c with grouped convolution is
more succinct and faster than the other two forms, and ResNeXt uses this structure as
a basic block.
5. Dilated Residual Networks (DRN). To solve the decrease in the resolution of the
feature map and the loss of feature information caused by downsampling. However,
simply removing subsampling steps in the network will reduce the receptive ﬁeld. So,
Yu et al. [120] introduced dilated convolutions that are used to increase the receptive
ﬁeld of the higher layers and replaced a subset of the internal downsampling layer
Remote Sens. 2021, 13, 4712 19 of 51
based on the residual network, compensating for the reduction in receptive ﬁeld
induced by removing subsampling. Compared to ResNet with the same parameter
amount, the accuracy of DRN is signiﬁcantly improved in image classiﬁcation.
6. Other models. Veit et al. [ 121] drops some of the layers of a trained ResNet and
still have comparable performance. Resnet in Resnet (RiR) [ 122] proposed a deep
dual-stream architecture that generalizes ResNets and standard CNNs and is easily
implemented with no computational overhead. DropBlock [123] technique discards 

Document 3: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 4: 
 and mild moment requirements. In implementing the two alternative residual-bootstrap
methods described above, we use ˆσϵ,LS as our estimate of σϵ, including for bootstrapping
robust regression where ρ(x) ̸= x2.
Performance in bootstrap inferenceIn Figure 2 we show the error rate of conﬁdence intervals
based on the two residual-bootstrap methods we proposed above. We see that both methods
control the Type I error, unlike bootstrapping directly from the residuals, and that both
methods are conservative. There is little diﬀerence between the two methods with this
sample size (n= 500), though with n= 100, we observe the deconvolution performance to
be worse in L1 (data not shown).
The deconvolution strategy, however, depends on the distribution of the design matrix,
which in these simulations we assumed was Gaussian (so we did not have to estimate λi’s).
For elliptical designs (λi ̸= 1), the error rate of the deconvolution method described above,
with no adaptation for the design, was similar to that of uncorrected residuals in high
dimensions (i.e., > 0.25 for p/n = 0 .5). Individual estimates of λi might improve the
deconvolution strategy, but this problem points to the general reliance of the deconvolution
method on precise knowledge about the design matrix. The bootstrap using standardized
predicted errors, on the other hand, had a Type I error for an elliptical design only slightly 

Document 5: 
 where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the
following bootstrap procedure:
1. Calculate the predicted errors, ˜ei(i)
2. Estimate |λi|∥ˆβρ(i) −β∥2 (the standard deviation of the ˜Zi)
3. Deconvolve in ˜ei(i) the error term ϵi from the ˜Zi term ;
4. Use the resulting estimates of G as the estimate of ˆG in residual bootstrapping.
34
Can We Trust the Bootstrap in High-dimensions?
B.1 Estimating ∥ˆβρ −β∥and the Variance of the Zi
Deconvolution methods that deconvolve ϵ from the ˜Zi require an estimate of the variance
of the ˜Zi. Equation (3) gives the variance as λ2
i∥ˆβρ(i) −β∥2
2, and we need to estimate this
quantity from the data. We use the approximation
∥ˆβρ(i) −β∥2 ≃∥ˆβρ −β∥2.
See AppendixA and references therein for justiﬁcation of this approximation.
Furthermore, as we note in the main text, in our implementation of this deconvolution in 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","An alternative residual bootstrap procedure called deconvolution involves estimating the variance of the ˜Zi term, deconvolving the error term ϵi from the predicted errors ˜ei(i), and using the resulting estimates of G as the estimate of ˆG in residual bootstrapping.",3.182284116744995,"The alternative residual bootstrap procedure called deconvolution involves several steps to estimate the distribution of the error term while accounting for the influence of the predictors. Specifically, the procedure includes:

1. Calculating the predicted errors \(\tilde{e}_i(i)\).
2. Estimating \(|\lambda_i|\| \hat{\beta}_{\rho}(i) - \beta \|_2\), which represents the standard deviation of the \(\tilde{Z}_i\) term.
3. Deconvolving in \(\tilde{e}_i(i)\) the error term \(\epsilon_i\) from the \(\tilde{Z}_i\) term.
4. Using the resulting estimates of \(G\) as the estimate of \(\hat{G}\) in residual bootstrapping.

This procedure is particularly useful in situations where the predictors have an elliptical distribution and is intended to improve the accuracy of bootstrap inference in high-dimensional settings."
qwen2.5:7b-instruct-q4_0,"What is a alternative residual bootstrapinf from sttandarized residual?
","Document 1: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 2: 
 samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con-
ditions, our results allow the factor returns {Xt}T
t=1 to be arbitrarily non-stationary and the residuals
{ϵt}T
t=1 to be non-stationary across tiles. For instance, in our empirical application, we require that
the residuals are stationary within each week, but their distributions can change arbitrarily between
weeks.
To illustrate this contribution, we conduct semisynthetic simulations using the exposure matrix Lt
from the BFRE model for energy stocks on May 21st, 2020. For simplicity, we generate data Y from
Eq. 1.1 after sampling the residuals and factor returns as i.i.d. standard Gaussians, with T = 350
observations. We use the test statistic from Section 1.2. Figure 3 shows that in this simple Gaussian
setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining 

Document 3: 
 15
El Karoui and Purdom
Implications for the Bootstrap For the purposes of the residual-bootstrap, Theorem 1 im-
plies that diﬀerent methods of estimating the residual distribution ˆG will result in similar
residual-bootstrap conﬁdence intervals as p/n→1, if ˆGhas the same variance. This agrees
with our simulations, where both of our proposed bootstrap strategies set the variance of ˆG
equal to ˆσ2
ϵ,LS and both had similar performance in our simulations for large p/n. Further-
more, as we noted, for p/n closer to 1, they both had similar performance to a bootstrap
procedure that simply sets ˆG = N(0,ˆσ2
ϵ,LS) (Figure 2) (see also AppendixA.3 for further
discussion of residual bootstrap methods which draw from the “wrong” distribution, i.e.,
forms of wild bootstraps (Wu, 1986)).
We return speciﬁcally to the bootstrap based on ˜ri(i), the standardized predicted errors.
Equation (3) tells us that the marginal distribution of ˜ei(i) is a convolution of the distribution
of ϵi and a normal, with the variance of the normal governed by the term∥ˆβρ−β∥2. Theorem
1 makes rigorous our previous assertion that as p/n →1, the normal term will dominate
and the marginal distribution of ˜ei(i) will approach normality, regardless of the distribution 

Document 4: 
 well beyond the expected 5%: we observe error rates of 10-15% for p/n= 0.3 and approxi-
mately 20% for p/n= 0.5. We see similar error rates for other robust-regression methods,
such as L1 and Huber loss, and also for diﬀerent error distributions and distributions of X
(Supplementary Figures A-1 and A-2). We explain some of the reasons for these problems
in Subsection 2.2 below.
2.1 Bootstrapping from Corrected Residuals
While resampling directly from the uncorrected residuals is widespread and often given
as a standard bootstrap procedure (e.g., Koenker, 2005; Chernick, 1999), the discrepancy
between the distribution of ϵi and ei has spurred more reﬁned recommendations in the
8
Can We Trust the Bootstrap in High-dimensions?
case of least-squares: form corrected residuals ri = ei/√1 −hi and sample the ϵ∗
i from the
empirical distribution of the ri −¯r (see e.g., Davison and Hinkley, 1997).
This correction is known to exactly align the variance of ri with that of ϵi regardless
of the design vectors Xi or the true error distribution, using simply the fact that the hat
matrix is a rank min(n,p) orthogonal projection matrix. We see that for L2 loss it corrects 

Document 5: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","For standardizing the residuals, one can resample from the empirical distribution of the standardized predicted errors \( \tilde{r}_i(i) = \frac{\tilde{e}_i}{\sqrt{1 - h_{ii}}} \), where \( \tilde{e}_i \) is the residual and \( h_{ii} \) is the diagonal element of the hat matrix. This approach aligns the variance of the resampled residuals with that of the true errors, addressing issues in high-dimensional settings where uncorrected residuals have a different distribution from true errors.",4.522209167480469,"An alternative to the standard residual bootstrap is to form corrected residuals instead of resampling directly from the uncorrected residuals. For instance, the corrected residuals can be defined as \( r_i = \frac{e_i}{\sqrt{1 - h_i}} \), where \( e_i \) are the residuals from the model and \( h_i \) are the leverage values derived from the hat matrix. By sampling \( \epsilon^*_i \) from the empirical distribution of the corrected residuals \( r_i - \bar{r} \), the variance of the corrected residuals can be aligned with the variance of the true errors \( \epsilon_i \) regardless of the distribution of the design vectors \( X_i \) or the true error distribution. This method aims to address the discrepancy between the distribution of the residuals and the true errors, especially in high-dimensional settings."
qwen2.5:7b-instruct-q4_0,"What is pair bootstrap?
","Document 1: 
 i = X′
iˆβρ + ϵ∗
i, from which ˆβ∗
ρ is computed. This
method of bootstrapping assumes that the linear model is correct for the mean of y (i.e.,
that E (yi) = X′
iβ); it also assumes ﬁxed Xi design vectors because the sampling is con-
ditional on the Xi. In the second method, called pairs bootstrap, ˆF is an estimate of the
joint distribution of the vector ( yi,Xi) ∈Rp+1 given by the empirical joint distribution
of {(yi,Xi)}n
i=1; the corresponding resampling method resamples the pairs ( yi,Xi). This
method makes no assumption about the mean structure of yand, by resampling the Xi, also
does not condition on the values of Xi. For this reason, pairs resampling is often considered
to be more generally applicable than residuals resampling (see e.g., Davison and Hinkley,
1997).
1.3 Background: High-dimensional Inference of Linear Models
Recent research shows that ˆβρ has very diﬀerent asymptotic properties when p/nhas a limit
κ that is bounded away from zero than it does in the classical setting where p/n→0 (see 

Document 2: 
 B.4 Bootstrap Estimates ˆβ∗from ˆG . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.5 Estimation of λ2
i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C Alternative Weight Distributions for Pairs Bootstrap 38
C.1 Case Σ ̸= Idp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D Description of Numerics 39
D.1 Simulation Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

Document 3: 
 v′ˆβ∗over repeated resampling from the pairs ( yi,Xi).
18
Can We Trust the Bootstrap in High-dimensions?
1 1
1
1
Ratio ( κ )
% Increase in Average CI Width
2
2
2
2
3
3
3
3
0.01 0.10 0.30 0.50
1
10
50
100
350 1
2
3
Normal
Ellip. Normal
Ellip. Exp
Figure 4: Comparison of width of 95% conﬁdence intervals of β1 for L2 loss:
Here we demonstrate the increase in the width of the conﬁdence interval due to
pairs bootstrapping. Shown on the y-axis is the percent increase of the average
conﬁdence interval width based on simulation ( n = 500), as compared to the
average for the standard conﬁdence interval based on normal theory in L2; the
percent increase is plotted against the ratio κ = p/n (x-axis). Shown are three
diﬀerent choices in simulating the entries of the design matrix X: (1) Xij ∼
N(0,1) (2) elliptical Xij with λi ∼ N(0,1) and (3) elliptical Xij with λi ∼
Exp(
√
2). The methods of simulation are the same as described in Figure 1;
exact values are given in Table A-2 in AppendixI. 

Document 4: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 5: 
 bootstrap and other resampling plans perform well when p is comparable to n.
Deﬁning success: accurate inference on β1 The common theoretical deﬁnition of whether
the bootstrap “works” is that the bootstrap distribution of the entire bootstrap estimate
ˆβ∗
ρ converges conditionally almost surely to the sampling distribution of the estimator ˆβρ
(see e.g., van der Vaart, 1998). The work of Bickel and Freedman (1983) on the residual
bootstrap for least squares regression, which we discuss in the background section 1.2, shows
that this theoretical requirement is not fulﬁlled even for the simple problem of least squares
regression.
In this paper, we choose to focus only on accurate inference for the projection of our
parameter on a pre-speciﬁed direction υ. More speciﬁcally, we concentrate only on whether
the bootstrap gives accurate conﬁdence intervals for υ′β. We think that this is the absolute
minimal requirement we can ask of a bootstrap inferential method, as well as one that is
meaningful from the standpoint of applied statistics. This is of course a much less stringent
requirement than performing well on complicated functionals of the whole parameter vector,
which is the implicit demand of standard deﬁnitions of bootstrap success. For this reason,
we focus throughout the exposition on inference for β1 (the ﬁrst element of β) as an example
of a pre-deﬁned direction of interest (where β1 corresponds to choosing υ = e1, the ﬁrst
canonical basis vector). 

Document 6: 
 matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that
situation makes little statistical sense (El Karoui, 2010, Subsection 4.4; Zheng et al., 2014).
For smaller ratios of p/n, we evaluate the performance of pairs bootstrapping on sim-
ulated data. We see that the performance of the bootstrap for inference also declines
17
El Karoui and Purdom
dramatically as the dimension increases, becoming increasingly conservative (Figure 1). In
pairs bootstrapping, the error rates of 95%-conﬁdence-intervals drop far below the nominal
5%, and are essentially zero for the ratio of p/n= 0.5. Like residual bootstrap, this overall
trend is seen for all the settings we simulated under (Supplemental Figures A-1, A-2). For
L1 loss, even ratios as small as 0 .1 yield incredibly conservative bootstrap conﬁdence inter-
vals for ˆβ1, with the error rate dropping to less than 0.01. For Huber and L2 losses, the
severe loss of power in our simulations starts for ratios of 0 .3.
A minimal requirement for the distribution of the bootstrapped data to give reasonable
inferences is that the variance of the bootstrap estimator ˆβ∗ 

","Pairs bootstrap resamples the pairs \((y_i, X_i)\) and does not condition on the values of \(X_i\), making it more generally applicable than residual bootstrap. It estimates the joint distribution of \(y_i\) and \(X_i\) using the empirical distribution of the data pairs, without assuming anything about the mean structure of \(y\).",3.234963178634644,"Pairs bootstrap is a resampling method used in statistical inference that involves resampling pairs of observations from a dataset, specifically the pairs of outcome variables (yi) and design vectors (Xi). Unlike residuals resampling, which assumes a specific mean structure for the outcome, pairs bootstrap does not make such assumptions about the mean of the outcome variable and allows for the resampling of the design vectors as well. This method is often considered to be more generally applicable because it does not condition on the values of Xi. It aims to estimate the joint distribution of the pairs (yi, Xi) based on their empirical distribution from the original sample."
qwen2.5:7b-instruct-q4_0,"How central limit theorem for hyperrectangles work?
","Document 1: 
 using the empirical or multiplier bootstraps.
The results in this paper substantially extend those obtain ed in [17] where
we considered the class A = Am of sets of the form A = {w ∈ Rp :
maxj∈J wj ≤ a} for some a∈ R and J ⊂ { 1,...,p }, but in order to obtain
much better dependence on n, we employ new techniques. Most notably, as
the main ingredient in the new proof, we employ an argument in spired by
Bolthausen [10]. Our paper builds upon our previous work [17 ], which in
turn builds on a number of works listed in the bibliography (s ee [18] for a
detailed review and links to the literature).
The organization of this paper is as follows. In Section 2, we derive
a Central Limit Theorem (CLT) for hyperrectangles in high di mensions;
that is, we derive a bound on ρn(A) for A = Are being the class of all
hyperrectangles and show that the bound converges to zero un der certain
conditions even when pis potentially larger or much larger than n. In Section
3, we extend this result by showing that similar bounds apply for A ⊂
Asi(a,d) being a class of simple convex sets and for A = Asp(s) being the
class of all s-sparsely convex sets. In Section 4, we derive high dimensio nal 

Document 2: 
 Assumption F1. Assumption 6(ii) strengthens Bai’s (2003) A ssumption F3 and ensures the con-
sistency of the heteroskedasticity and autocorrelation co nsistent (HAC) covariance estimator of
ˆg′
t ˆgt − Ir . Assumption 7 follows from Assumption 10 of DBH (2022) or Ass umption 8 of HI (2015).
Assumption 8 states that a basic functional central limit th eorem holds for the sums of gt under
the null hypothesis of no break.
9
3.2 Limiting distribution of sup-LR under the null hypothes is
It is well known that ˆGis an estimator of GH, where H = (Λ
′
Λ/N)(G
′ ˆG/T)V−1
N T and VN T denotes
the eigenvalues of XX′/NT. Bai’s (2003) Proposition 1 shows that G
′ ˆG/T →p Σ−1/2
Λ ΥV1/2 , where
V is the probability limit of VN T and an r× r diagonal matrix of the eigenvalues of Σ 1/2
Λ ΣG Σ1/2
Λ ,
and Υ is the eigenvector of Σ 1/2
Λ ΣGΣ1/2
Λ . Thus, 

Document 3: 
 where the ﬁrst three lines follow from the properties of Fβ(w) and g(t) (recall
that m(w) = g(Fβ(w))), and the last inequality follows from Lemma A.1.
Here the constant C depends only on b. Likewise we have
P(Vn ≤ y− φ−1) ≥ P(SW
n ≤ y− φ−1) − Cφ−1 log1/2 p− |E[In]|.
The conclusion of the lemma follows from combining these ine qualities with
the bound on |E[In]| derived in Step 1. ■
Proof of Corollary 5.1. Pick any hyperrectangle
A= {w∈ Rp : wj ∈ [aj,bj] for all j = 1 ,...,p }.
For i = 1 ,...,n , consider the random vectors ˜Xi and ˜Yi in R2p deﬁned
by ˜Xij = Xij and ˜Yij = Yij for j = 1 ,...,p , and ˜Xij = −Xi,j−p and
˜Yij = −Yi,j−p for j = p+ 1,..., 2p. Then
P(SX
n ∈ A) = P( S
˜X
n ≤ y), P(SY
n ∈ A) = P( S
˜Y 

Document 4: 
 ) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely
convex sets are sets that can be represented as intersection s of many
convex sets whose indicator functions depend only on a small subset of
their arguments, with hyperrectangles being a special case .
1. Introduction
Let X1,...,X n be independent random vectors in Rp where p≥ 3 may be
large or even much larger than n. Denote by Xij the j-th coordinate of Xi,
so that Xi = ( Xi1,...,X ip)′. We assume that each Xi is centered, namely
E[Xij] = 0, and E[ X2
ij] <∞ for all i= 1 ,...,n and j = 1 ,...,p . Deﬁne the
normalized sum
SX
n := ( SX
n1,...,S X
np)′ := 1
√n
n∑
i=1
Xi.
We consider Gaussian approximation to SX
n , and to this end, let Y1,...,Y n 

Document 5: 
 by ψα(x) := exp( xα) − 1, and for a real-valued random variable ξ, we deﬁne
∥ξ∥ψα := inf {λ> 0 : E[ ψα(|ξ|/λ)] ≤ 1}.
For α∈ [1,∞), ∥ · ∥ψα is an Orlicz norm, while for α∈ (0,1), ∥ · ∥ψα is not a
norm but a quasi-norm, that is, there exists a constant Kα depending only
on α such that ∥ξ1 + ξ2∥ψα ≤ Kα(∥ξ1∥ψα + ∥ξ2∥ψα ). Throughout the paper,
we assume that n≥ 4 and p≥ 3.
2. High Dimensional CLT for Hyperrectangles
This section presents a high dimensional CLT for hyperrecta ngles. We
begin with presenting an abstract theorem (Theorem 2.1); th e bound in
Theorem 2.1 is general but depends on the tail properties of t he distri-
butions of the coordinates of Xi in a nontrivial way. Next we apply this
theorem under simple moment conditions and derive more expl icit bounds
(Proposition 2.1).
Let Are be the class of all hyperrectangles in Rp; that is, Are consists of
all sets A of the form 

Document 6: 
 arXiv:1412.3661v4  [math.ST]  8 Mar 2016
CENTRAL LIMIT THEOREMS AND BOOTSTRAP IN
HIGH DIMENSIONS
VICTOR CHERNOZHUKOV, DENIS CHETVERIKOV, AND KENGO KATO
Abstract. This paper derives central limit and bootstrap theorems for
probabilities that sums of centered high-dimensional rand om vectors hit
hyperrectangles and sparsely convex sets. Speciﬁcally, we derive Gauss-
ian and bootstrap approximations for probabilities P( n− 1/ 2 ∑ n
i=1 Xi ∈
A) where X1, . . . , Xn are independent random vectors in Rp and A is
a hyperrectangle, or, more generally, a sparsely convex set , and show
that the approximation error converges to zero even if p = pn → ∞ as
n → ∞ and p ≫ n; in particular, p can be as large as O(eCnc
) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely 

","The central limit theorem (CLT) for hyperrectangles in high dimensions shows that the probability of a normalized sum of centered random vectors falling within a hyperrectangle converges to a Gaussian distribution, even as the dimension \( p \) grows much faster than the sample size \( n \). This convergence holds uniformly over all hyperrectangles and does not require any restrictions on the correlation among the coordinates.",3.499922752380371,"The Central Limit Theorem (CLT) for hyperrectangles in high dimensions, as described in the provided snippets, involves the analysis of sums of independent random vectors and how these sums behave when they are constrained to lie within hyperrectangles or sparsely convex sets. The key points are as follows:

1. **Definition of Hyperrectangles**: A hyperrectangle in \( \mathbb{R}^p \) is defined as a set \( A = \{ w \in \mathbb{R}^p : w_j \in [a_j, b_j] \text{ for all } j = 1, \ldots, p \} \).

2. **Random Vectors**: The CLT applies to independent random vectors \( X_1, \ldots, X_n \) in \( \mathbb{R}^p \), where the number of dimensions \( p \) can be much larger than the number of observations \( n \).

3. **Normalized Sums**: The normalized sum \( S_n^X = \frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \) is considered. The paper derives Gaussian approximations for the probabilities that \( S_n^X \) lies in a specified hyperrectangle.

4. **Convergence of Approximation Error**: The approximation error (the difference between the true distribution of \( S_n^X \) and the Gaussian distribution) converges to zero under certain conditions, even when \( p \) grows significantly with \( n \).

5. **Generalization to Sparsely Convex Sets**: The results hold not just for hyperrectangles but also for more general sparsely convex sets, which are combinations of several convex sets.

6. **Independence of Correlation Structure**: The results are robust in that they do not require any specific assumptions about the correlation structure of the random variables involved.

In summary, the CLT for hyperrectangles establishes that even in high dimensions, under appropriate conditions, the sum of independent centered random vectors will converge in distribution to a multivariate normal distribution when constrained to fall within hyperrectangles or certain convex shapes."
qwen2.5:7b-instruct-q4_0,"What is high dimensional CLT for simple and sparsely conver sets?
","Document 1: 
 In addition, denote
D(1)
n =
(B2
nlog7(pn)
n
)1/6
, D(2)
n,q =
(B2
nlog3(pn)
n1−2/q
)1/3
. (9)
An application of Theorem 2.1 under these conditions leads t o the following
proposition.
Proposition 2.1(High Dimensional CLT for Hyperrectangles) . Suppose
that conditions (M.1) and (M.2) are satisﬁed. Then under (E.1), we have
ρn(Are) ≤ CD(1)
n ,
where the constant C depends only on b; while under (E.2), we have
ρn(Are) ≤ C{D(1)
n + D(2)
n,q},
where the constant C depends only on b and q.
3. High Dimensional CLT for Simple and Sparsely Convex Sets
In this section, we extend the results of Section 2 by conside ring larger
classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather 

Document 2: 
 mT = max i≤N
∑
j≤N
1
(
σij ̸= 0
)
.
The sparsity assumption puts an upper bound assumption on mT in the sense that
m2
T = o
( T
r2 log ( N )
)
.
In this formulation, the number of factors r is allowed to be large and grows with T .U s i n g
principal components estimators under the normalization 1
T
∑T
t=1 Ft F′
t = Ir , one can decompose
the sample covariance of Xt as
SX = ˆ/Lambda1ˆ/Lambda1+
N∑
i=r+1
ˆμi ˆξi ˆξ
′
i ,
where ˆμi and ˆξi are the i-th leading eigenvalues and eigenvectors of SX , respectively. In the high-
dimensional setup, the sample covariance might be singular and provides a poor estimator for the
population covariance. For example, when N > T , the rank of SX can never exceed T ,w h e r e a s
the theoretical covariance /Sigma1X always has rank N . To overcome this problem, we may apply the
thresholding technique to the component ∑N 

Document 3: 
 kw≤ ak for all k= 1 ,...,m }
for some unit vectors vk ∈ Sp−1 and coeﬃcients ak, k = 1 ,...,m . If the
number of non-zero elements of each vk does not exceed s, this A is an
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 9
s-sparsely convex set. Yet another example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj for all j = 1 ,...,p and w2
1 + w2
2 ≤ c}
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , j = 1 ,...,p , and 0 <c ≤ ∞ . This
A is a 2-sparsely convex set. A more complicated example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj,w2
k + w2
l ≤ ckl, for all j,k,l = 1 ,...,p }
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , 0 < ckl ≤ ∞ , j,k,l = 1 ,...,p .
This A is a 2-sparsely convex set. Finally, consider the set 

Document 4: 
 classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather
large. In addition, it allows us to derive similar bounds for classes of sparsely
convex sets. These classes in turn may be of interest in stati stics where
sparse models and techniques have been of canonical importa nce in the past
years.
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 7
3.1. Simple convex sets. Consider a closed convex set A⊂ Rp. This set
can be characterized by its support function:
SA : Sp−1 → R ∪ {∞}, v ↦→ SA(v) := sup {w′v: w∈ A},
where Sp−1 := {v ∈ Rp : ∥v∥ = 1 }; in particular, A = ∩v∈Sp− 1 {w ∈ Rp :
w′v≤ S A(v)}. We say that the set Ais m-generated if it is generated by the
intersection of m half-spaces (that is, A is a convex polytope with at most
m facets). The support function SA of such a set A can be characterized 

Document 5: 
 CLT AND BOOTSTRAP IN HIGH DIMENSIONS 3
because they allow us to approximate the distributions of va rious key sta-
tistics that arise in inference for high-dimensional model s. For example,
the probability that a collection of Kolmogorov-Smirnov ty pe statistics falls
below a collection of thresholds
P
(
max
j∈Jk
SX
nj ≤ tk for all k= 1 ,...,κ
)
= P
(
SX
n ∈ A
)
can be approximated by P( SY
n ∈ A) within the error margin ρn(Are); here
{Jk} are (non-intersecting) subsets of {1,...,p }, {tk} are thresholds in the
interval ( −∞,∞), κ ≥ 1 is an integer, and A ∈ A re is a hyperrectangle of
the form {w∈ Rp : max j∈Jk wj ≤ tk for all k= 1 ,...,κ }. Another example
is the probability that a collection of Pearson type statist ics falls below a
collection of thresholds
P
(
∥(SX
nj)j∈Jk ∥2 ≤ tk for all k= 1 ,...,κ
)
= P
(
SX 

Document 6: 
 It is worthwhile to mention that a notable example where the t ransformed
variables ˜Xi = ( v′Xi)v∈V(Am) satisfy condition (E.1 ′) is the case where each
Xi obeys a log-concave distribution. Recall that a Borel proba bility measure
µ on Rp is log-concave if for any compact sets A1,A2 in Rp and λ∈ (0,1),
µ(λA1 + (1 − λ)A2) ≥ µ(A1)λµ(A2)1−λ,
where λA1 + (1 − λ)A2 = {λx+ (1 − λ)y: x∈ A1,y ∈ A2}.
Corollary 3.1 (High Dimensional CLT for Simple Convex Sets with Log–
concave Distributions) . Suppose that each Xi obeys a centered log-concave
distribution on Rp and that all the eigenvalues of E[XiX′
i] are bounded from
below by a constant k1 >0 and from above by a constant k2 ≥ k1 for every
i= 1 ,...,n . Then
ρn(Asi(a,d)) ≤ Cn−1/6 log7/6(pn),
where the constant C depends only on a,b,d,k 1, and k2. 

","High-dimensional CLT for simple convex sets provides bounds similar to those in Proposition 2.1, while extending them to larger classes of sets, including simple convex sets. Additionally, it allows bounding the probability for classes of sparsely convex sets, which is useful in statistics where sparse models are important.",2.938624143600464,"The high-dimensional Central Limit Theorem (CLT) for simple and sparsely convex sets refers to the analysis and approximation of the distributions of statistics arising from high-dimensional models, particularly in the context of simple convex sets, which can be characterized by their support functions and generated by half-spaces. Under certain conditions, the theorem establishes bounds on the convergence rates of these statistics, such as the probability that specific statistical measures fall within certain thresholds. 

Proposition 3.1 outlines that the results extend to larger classes of sets, including sparsely convex sets, where the number of non-zero elements of vector representations is limited. This finding is significant in statistics, especially for sparse models, as it allows practitioners to infer key properties from high-dimensional distributions, thus providing a framework for data analysis where traditional low-dimensional assumptions do not hold. 

Moreover, the theorem can also specifically address distributions of transformed variables satisfying log-concave properties, enhancing the application of high-dimensional CLT in various statistical inference contexts."
qwen2.5:7b-instruct-q4_0,"What is multiplier bootstrap?
","Document 1: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 2: 
 search trajectory followed by the conﬁguration procedure. The key idea behind
this generalisation (dubbed aggressive capping) is to additionally bound the time
allowed for evaluating conﬁgurations based on the performance observed for the
current incumbent, i.e., the best-performing conﬁguration encountered since the
beginning of the ParamILS run. The additional bound is obtained by multiplying
the performance estimate of the incumbent by a constant, bm, called the bound
multiplier. Formally, for bm = ∞, the additional bound becomes inactive (assum-
ing the performance measure is to be minimised), and the behaviour of trajectory-
preserving capping is obtained. Forbm = 1, on the other hand, a very aggressive
heuristic is obtained, which limits the evaluation of any conﬁguration to the time
spent on evaluating the current incumbent. In practice,bm = 2 appears to result
in good performance and is used as a default setting in ParamILS. Despite its
heuristic nature, this modiﬁed capping mechanism preserves the PAC property of
FocusedILS.
Although Hutter et al. [ 37] spelled out their adaptive capping mechanisms for
the performance objective of minimising a target algorithm’s mean run-time only,
these mechanisms generalise to other objectives in a rather straightforward way (a
discussion of capping in the context of minimising quantiles of run-time is found
in Ch. 7 of [32]). We note, however, that – especially when several target algorithm 

Document 3: 
 with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of
power) as the ratio p/n grows. We also show that the jackknife resampling technique for
estimating the variance of ˆβ severely overestimates the variance in high dimensions.
We contribute alternative procedures based on our theoretical results that result in
dimensionality adaptive and robust bootstrap methods.
Keywords: Bootstrap, high-dimensional inference, random matrices, resampling
1. Introduction
The bootstrap (Efron, 1979) is a ubiquitous tool in applied statistics, allowing for infer-
ence when very little is known about the properties of the data-generating distribution.
The bootstrap is a powerful tool in applied settings because it does not make the strong
assumptions common to classical statistical theory regarding this data-generating distri-
bution. Instead, the bootstrap resamples the observed data to create an estimate, ˆF, of
the unknown data-generating distribution, F. The distribution ˆF then forms the basis of
further inference.
c⃝2018 Noureddine El Karoui and Elizabeth Purdom. 

Document 4: 
 ρMB
n(A) ≤ C{D(1)
n (α) + D(1)
n,q(α)},
where the constant C depends only on a,b,d,q , and s.
Remark 4.2 (Bootstrap theorems in a.s. sense) . Proposition 4.1 leads to
the following multiplier bootstrap theorem in the a.s. sens e. Suppose that
A is a subclass of Asi(a,d) as in Proposition 4.1 and that (M.2) is satisﬁed.
We allow p= pn → ∞ and Bn → ∞ as n→ ∞ but assume that a,b,d,q,s
are all ﬁxed. Then by applying Proposition 4.1 with α= αn = n−1(log n)−2,
together with the Borel-Cantelli lemma (note that ∑ ∞
n=4 n−1(log n)−2 <∞),
we have with probability one
ρMB
n(A) =
{
O{D(1)
n (αn)} under (E.1)
O{D(1)
n (αn) ∨ D(2)
n,q(αn)} under (E.2) ,
and it is routine to verify that D(1)
n (αn) = o(1) if B2 

Document 5: 
 10 CHERNOZHUKOV, CHETVERIKOV, AND KATO
4. Empirical and Multiplier Bootstrap Theorems
So far we have shown that the probabilities P( SX
n ∈ A) can be well
approximated by the probabilities P( SY
n ∈ A) under weak conditions for hy-
perrectangles A∈ A re, simple convex sets A∈ A si(a,d), or sparsely convex
sets A∈ A sp(s). In practice, however, the covariance matrix of SY
n is typi-
cally unknown, and direct computation of P( SY
n ∈ A) is infeasible. Hence,
in this section, we derive high dimensional bootstrap theor ems which allow
us to approximate the probabilities P( SY
n ∈ A), and hence P( SX
n ∈ A),
by data-dependent techniques. We consider here multiplier and empirical
bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn 

Document 6: 
 Buja, A. and Eyuboglu, N. (1992). Remarks on parallel analysis. Multivariate Behavioral Research,
27(4):509 – 540.
Campbell, J. Y. and Yogo, M. (2006). Efficient tests of stock return predictability.Journal of Financial
Economics, 81(1):27–60.
Chamberlain, G. and Rothschild, M. (1983). Arbitrage, factor structure, and mean-variance analysis
on large asset markets. Econometrica, 51(5):1281–1304.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multiplier
bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics ,
41(6):2786 – 2819.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2017). Central limit theorems and bootstrap in
high dimensions. The Annals of Probability , 45(4):2309 – 2352.
Chernozhukov, V., Chetverikov, D., Kato, K., and Koike, Y. (2023). High-dimensional data bootstrap.
Annual Review of Statistics and Its Application , 10(1):427–449. 

","Multiplier bootstrap involves generating a sequence of i.i.d. N(0,1) random variables independent of the data, and then constructing a normalized sum using these variables to approximate the distribution of the original estimator. This method helps in approximating the distribution of test statistics without needing the unknown covariance matrix.",3.4061367511749268,"The multiplier bootstrap is a resampling method used to make statistical inferences about a population when very little is known about the underlying data-generating distribution. In this procedure, one generates a sequence of independent and identically distributed (i.i.d.) standard normal random variables and uses them to create a normalized sum that helps in approximating the distribution of interest. Specifically, it involves computing a standardized version of the data, leveraging the concept of a sample mean and variance to form a new sample distribution that can then be utilized to derive estimates and confidence intervals for parameters of interest, such as regression coefficients. This technique is particularly important in high-dimensional statistics where traditional inference approaches may fail due to the dimensionality of the data."
qwen2.5:7b-instruct-q4_0,"What are the methods of examining impact of imputed values on XAI methods?
","Document 1: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 2: 
 k (e.g., Onatski, 2009, 2010; Alessi et al., 2010; Owen and Wang, 2016; Dobriban and Owen, 2018;
Dobriban, 2020), tests for changepoints in the factor loadings (e.g., Breitung and Eickmeier, 2011; Bai
et al., 2022), tests for whether observed proxies of the factor returns Xt are good proxies (Bai and Ng,
2006), bootstrap methods to debias OLS estimates of the exposures (Gon¸ calves and Perron, 2020),
covariance matrix estimation (e.g., Fan et al., 2008, 2011), estimation and inference for dynamic factor
models (e.g., Bates et al., 2013; Stock and Watson, 2016) and more—see Bai and Wang (2016) for a
review. Many of these techniques leverage key results from random matrix theory (Johnstone, 2001;
Paul, 2007; Bai and Silverstein, 2010), sometimes in combination with permutation-based methods
(Buja and Eyuboglu, 1992).
Factor models are also widely discussed in the asset pricing literature (see Giglio et al. (2022) for
review), including methods for explaining excess returns (Fama and French, 1992, 2008; Welch and
Goyal, 2007; Lewellen, 2015; Freyberger et al., 2020; Gu et al., 2020), estimating factors and exposures
7 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 Journal of Clinical Epidemiology 56 (2003) 968–976
Imputation of missing longitudinal
data: a comparison of methods
Jean Mundahl Engels *, Paula Diehr
Departments of Biostatistics and Health Services, University of Washington, 1959 Northeast Paciﬁc Avenue, Box 357232, Seattle, WA 98195, USA
Accepted 11 October 2002
Abstract
Background and Objective: Missing information is inevitable in longitudinal studies, and can result in biased estimates and a loss
of power. One approach to this problem is to impute the missing data to yield a more complete data set. Our goal was to compare the
performance of 14 methods of imputing missing data on depression, weight, cognitive functioning, and self-rated health in a longitudinal
cohort of older adults.
Methods: We identiﬁed situations where a person had a known value following one or more missing values, and treated the known
value as a “missing value.” This “missing value” was imputed using each method and compared to the observed value. Methods were
compared on the root mean square error, mean absolute deviation, bias, and relative variance of the estimates.
Results: Most imputation methods were biased toward estimating the “missing value” as too healthy, and most estimates had a variance
that was too low. Imputed values based on a person’s values before and after the “missing value” were superior to other methods, followed
by imputations based on a person’s values before the “missing value.” Imputations that used no information speciﬁc to the person, such 

Document 5: 
 follows a VAR of ﬁnite order,
/Phi1( L)Ht = ut ,
where /Phi1( L) = /Phi10 − ∑h
j=1 /Phi1j Lh , with /Phi10 being possibly not an identity matrix. Bernanke et al.
(2005) propose two ways to analyze FAVAR. The ﬁrst is based on a two-step principal components
method, in which in the ﬁrst step, the method of principal components is employed to form
estimates of the space spanned by bothFt and Wt . In the second step, various identiﬁcation
schemes, such as Cholesky ordering, can be applied to obtain estimates of latent factors ˆFt ,w h i c h
are treated as observed when conducting VAR analysis of [ ˆF′
t,W′
t]′. Under suitable identiﬁcation
conditions, Bai et al. (2016) show that inferential theory can be developed for such a two-step
estimator, which differs from a standard large factor model. Conﬁdence bands for the impulse
responses can be readily constructed using the theory therein. The second method involves a
one-step likelihood approach, implemented by Gibbs sampling, which leads to joint estimation of
both the latent factors and impulse responses. The two methods can complement each other, with
the ﬁrst one being computationally simple and the second providing possibly better inference in
ﬁnite samples although with increased computational cost. 

Document 6: 
 van Buuren, 2018), which is veryﬂexible and can be implemented
with different models. While some applications can beneﬁt from
this uncertainty information, integrating this uncertainty
information in data pipelines can be challenging. From a
practitioner’s point of view, point estimates are much simpler
to integrate into conventional data pipelines. This is why we
restrict our analysis to point estimate imputations. Note,
however, that all the experiments conducted in this work
could, in principle, also be evaluated with respect to their
uncertainty estimates in a MICE setting, using the examined
imputation methods as the model underlying the MICE
estimator.
More recently, also ML approaches have increasingly been
used for imputation. Popular methods include k-nearest
neighbors (k-NNs) ( Batista and Monard, 2003 ), matrix
factorization (Troyanskaya et al., 2001 ; Koren et al., 2009 ;
Mazumder et al., 2010 ), random-forest –based approaches
(Stekhoven and Bühlmann, 2012), discriminative deep learning
methods (Biessmann et al., 2018), and generative deep learning
methods (Shang et al., 2017; Yoon et al., 2018; Li et al., 2019;
Nazábal et al., 2020; Qiu et al., 2020).
Most imputation studies provide solid experimental
evidence that the respective proposed method in the
application setting investigated outperforms other 

","Imputing missing values is a common preprocessing step in machine learning, but it can potentially affect the performance and interpretability of Explainable AI (XAI) methods. Here are some methods to examine how imputed values may impact XAI methods:

### 1. **Sensitivity Analysis**
   - **Definition**: This method involves evaluating how changes in imputed values affect model predictions.
   - **Implementation**:
     - Impute the missing data using different methods (e.g., mean, median, KNN imputation).
     - Train your XAI model on these datasets and compare the feature importance or other explainability metrics.
     - Observe if there are significant differences in the explanations for predictions.

### 2. **Shapley Values**
   - **Definition**: Shapley values provide a way to decompose the prediction of a machine learning model into individual contributions from each feature.
   - **Implementation**:
     - Impute missing data using various methods and train your model.
     - Use SHAP (SHapley Additive exPlanations) or similar XAI techniques to compute Shapley values for both imputed and non-imputed datasets.
     - Compare the Shapley values between these datasets to assess the impact of different imputation strategies.

### 3. **Feature Importance Comparisons**
   - **Definition**: Evaluate how feature importance rankings change with different imputation methods.
   - **Implementation**:
     - Impute missing data using various techniques (e.g., mean, KNN).
     - Train your model and extract feature importances using an XAI technique like permutation feature importance or SHAP values.
     - Compare the importance rankings between datasets to see if there are consistent differences due to imputation.

### 4. **Confidence Intervals**
   - **Definition**: Assess the reliability of explanations by computing confidence intervals around the features’ contributions.
   - **Implementation**:
     - Perform multiple imputations (e.g., using a cross-validation approach) for each missing value.
     - Train your model and generate explanations for each imputation.
     - Compute confidence intervals around the feature importance values to see how stable they are across different imputations.

### 5. **Randomized Missingness**
   - **Definition**: Introduce random missingness into the data and evaluate the impact on XAI methods.
   - **Implementation**:
     - Randomly remove some features in a controlled manner (e.g., 10% of data).
     - Impute these values using various strategies and train your model.
     - Compare the explainability metrics before and after introducing random missingness to see how robust they are.

### 6. **Counterfactual Explanations**
   - **Definition**: Generate counterfactual explanations that show what changes would cause a prediction to change direction (e.g., from positive to negative).
   - **Implementation**:
     - Impute the data using different methods.
     - Use a counterfactual explainer like LIME or SHAP to generate counterfactuals for each dataset.
     - Compare the counterfactual explanations and observe if there are significant differences in how they are generated due to imputed values.

### 7. **Feature Interaction Analysis**
   - **Definition**: Examine interactions between features, which can be affected by different imputation methods.
   - **Implementation**:
     - Impute missing data using various techniques.
     - Use XAI techniques that capture feature interactions (e.g., SHAP interaction values).
     - Compare the interaction effects to see if they vary depending on the imputed values.

### 8. **Robustness Tests**
   - **Definition**: Test how well the model’s explanations hold up under varying conditions of missing data.
   - **Implementation**:
     - Perform robustness tests by randomly introducing noise or missingness into the dataset after imputation.
     - Compare the explainability metrics (e.g., SHapley values, feature importances) before and after adding noise.

### 9. **A/B Testing**
   - **Definition**: Set up an A/B testing scenario where different imputation methods are applied to separate subsets of data.
   - **Implementation**:
     - Split the dataset into two or more subsets.
     - Apply different imputation techniques to each subset and train your model separately.
     - Compare the explanations generated for predictions on these datasets to see if there are significant differences.

### 10. **Domain-Specific Validation**
   - **Definition**: Validate the impact of imputed values in a domain-specific context, such as medical or financial data.
   - **Implementation**:
     - Collaborate with domain experts to understand which imputation methods align best with the specific application.
     - Use expert knowledge to manually validate the explanations generated for critical predictions.

### Summary
By employing these methods, you can systematically evaluate how different imputation strategies impact your XAI model's interpretability and reliability. This helps ensure that the insights provided by your AI system are consistent and meaningful across various data preprocessing scenarios.",23.51137900352478,"The impact of imputed values on XAI (explainable artificial intelligence) methods can be examined using various strategies as outlined in the provided snippets. Specifically, the methods of examination include:

1. **Comparison of Imputation Methods**: Document 1 emphasizes exploring the effects of different imputation methods on the calculation of Shapley values. By systematically contrasting various imputation strategies, researchers can evaluate how these methods influence the interpretability and robustness of outcomes in machine learning models.

2. **Feature Relevance Evaluation**: Document 3 mentions approaches that evaluate features based on their average expected marginal contribution. This can be affected by the imputed values, as the way in which features are weighted during decision-making may change depending on the imputation method used.

3. **Visual Explanations**: As noted in Document 3, using visualization techniques may also help in interpreting predictions when imputed values are involved. Visual analytics can reveal how different imputed values alter the model's predictions and subsequently affect explanations.

4. **Local Explanations**: Local explanation methodologies can provide insights into how specific imputed values affect the model's behavior around certain instances, as discussed in Document 3. This involves examining the model's workings around inputs that have been altered by different imputation methods.

5. **Statistical Comparison of Estimates**: Document 4 details comparing various imputation methods, evaluating their performance based on metrics like root mean square error and bias. This approach can also prove useful for assessing how different imputed values interact with XAI methodologies.

6. **Integration of Uncertainty**: Document 6 mentions that while point estimates are often used in practice, incorporating uncertainty estimates from multiple imputation techniques (like MICE) can provide a more thorough understanding of how imputation influences model interpretability and insights.

Through these methodological approaches, researchers can gain a better understanding of how imputation methods affect XAI techniques and the interpretability of machine learning models."
qwen2.5:7b-instruct-q4_0,"What is MSE SHAP? 
","Document 1: 
 The following section proposes a uniﬁed approach that improves previous methods, preventing them
from unintentionally violating Properties 1 and 3.
4 SHAP (SHapley Additive exPlanation) Values
We propose SHAP values as a uniﬁed measure of feature importance. These are the Shapley values
of a conditional expectation function of the original model; thus, they are the solution to Equation
4
Figure 1: SHAP (SHapley Additive exPlanation) values attribute to each feature the change in the
expected model prediction when conditioning on that feature. They explain how to get from the
base value E[f(z)] that would be predicted if we did not know any features to the current output
f(x). This diagram shows a single ordering. When the model is non-linear or the input features are
not independent, however, the order in which features are added to the expectation matters, and the
SHAP values arise from averaging the φi values across all possible orderings.
8, where fx(z′) = f(hx(z′)) = E[f(z) |zS], and Sis the set of non-zero indexes in z′(Figure 1).
Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values 

Document 2: 
 the MIT license and is available at:
https://bitbucket.org/pl-edu-pw-mini-optimization/basic-pso-de-hybrid/ .
3.2. Samples archive
In order to store and eﬃciently retrieve samples, M-GAPSO utilizes a
multi-dimensional R-Tree index [34]. Due to performance reasons capacity of
the R-Tree index is limited and set by the user. After reaching the maximum
capacity, the index is restarted from scratch.
Samples archive is utilized in two scenarios. Mainly, for eﬃcient retrieval
of the nearest samples as described in Section 2.3. Subsequently those sam-
ples are utilized to ﬁt a quadratic or polynomial function model. Secondly,
samples archive serves as a cache memory, so that in the event of the algo-
rithm trying to sample the same location (as it does happen when the swarm
has nearly collapsed), it retrieves the function value from memory, saving
some budget of the ﬁtness function computations.
3.3. Restart management
M-GAPSO uses an enhanced version of JADE [5] restart manager. In
M-GAPSO the RestartManager registers iteration count intervals between
global optimum updates, considers a spread of personal best locations of
particles (eq. (10)) and additionally a spread of personal best locations val-
ues (eq. (11)). The last feature was added in order to better handle step 

Document 3: 
 ⏐⏐
(19)MSE = 1
m
m∑
i=1
(yi −¯yi)2
(20)RMSE =
√1
m
m∑
i−1
(yi −¯yi)2
(21)TPR = TP
TP + FN
Page 24 of 37Emmanuel et al. J Big Data           (2021) 8:140 
The major advantages of the MSE and RMSE is that they provide a quadratic loss func -
tion. Also, uncertainty in forecasting is measured when they are used. However, MSE 
and RMSE are highly influenced by extreme values [145]. While MAE is not influenced 
by extreme values, also a more natural measure and unambiguous [146]. Most stud -
ies in research are found to mostly use the RMSE for missing value imputation evalu -
ation [147–149]. Although some studies have proposed valid evidence against the use 
of RMSE in favour of MAE due to its less sensitive to extreme values [150]. The authors 
further advised against the reporting of RMSE in literature and strongly recommended 
the use of MAE [146, 150]. However, Chai and Draxler [145] partially disputed the con -
clusions and introduced arguments against avoiding RMSE. They contended that RMSE 
was appropriate to represent model performance than the MAE. The AUC like other 

Document 4: 
 ELSHADE-SPACMA 1.52 1.40 1.38 1.45 1.72 1.83 2.13 
GSKA 4.10 4.20 4.17 4.17 4.17 4.17 4.20 
PSO 6.03 5.97 5.97 5.97 5.93 5.93 5.87 
SSA 5.27 5.40 5.43 5.47 5.60 5.63 5.67 
jSO 2.70 2.40 2.35 2.37 2.47 2.35 2.05 
(b) 40–100% Evaluations 
ﬁtness evaluations, including the previous winners of CEC compe- 
titions (EBOwithCMAR and jSO) and the most competitive recent 
algorithm (ELSHADE-SPACMA). 
• EBOwithCMAR is the algorithm with the best results for dimensions 
10 and 30 (very close to ELSHADE-SPACMA in those dimensions), 
whereas ELSHADE-SPACMA is the best performing algorithm for di- 
mensions 50 and 100, closely followed by EBOwithCMAR and jSO. 
• For dimensions 30, 50 and 100, ELSHADE-SPACMA is the best one 
since the 40% of the budget of evaluations. Although Table 3 indi- 
cates that EBOwithCMAR obtains the best ﬁnal results, ELSHADE- 
SPACMA achieves better results during most of the search, being 
only improved by the former at the end. 
• Deciding which algorithm should be applied to a speciﬁc problem 
strongly depends on the eﬀort that can be devoted to the search. In 
this benchmark, GSKA is better when less evaluations are allowed, 

Document 5: 
 open-source innvestigate package. In the case of expected gradients, we bench-marked our imple-
mentation against a public one in the SHAP python package.
Surrogate Approaches.We consider:
• LIME (Ribeiro et al., 2016) locally approximate F around xi with a simple function, g,
that is then interpreted. LIME corresponds to: arg ming∈GL(f,g, pert(xi)) + Ω(g), where
pert(xi) local perturbations of the inputxi, and Ω(g) is a regularizer. Overall, recent work
hasshownthat, inthetabularsetting, LIMEapproximatesthecoeﬃcientsofablack-boxlinear
model with high probability. In our empirical implementation we follow the open source lime-
image package. Here to account for high dimensions, the input image is ﬁrst segmented into
50 segments and the local approximationg is ﬁt around input perturbations with50. We
experimented with 5,10,15,&25 dimensions as well. Overall, we found the LIME with50
segments to be more stable for the input data sizes that we consider. We use1000 samples in
model ﬁtting.
• SHAP (Lundberg and Lee, 2017) Similar to LIME, SHAP provides a local approximation
around a single input. The local model is then interpreted as a form of explanation. SHAP
uniﬁes LIME and several under methods under the same umbrella and turns out to be a 

Document 6: 
 4.1 | Features-oriented methods
SHapley Additive exPlanation (SHAP) (Lundberg & Lee, 2017) is a game-theoretic approach to explain ML predictions.
SHAP seeks to deduce the amount each feature contributed to a decision by representing the features as players in a
ANGELOV ET AL. 5o f1 3
 19424795, 2021, 5, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1424 by Cochrane Poland, Wiley Online Library on [28/12/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 

","MSE SHAP refers to using Mean Squared Error (MSE) as a measure within the SHAP (SHapley Additive exPlanation) framework to assess the contribution of each feature to a model's prediction. However, it's important to note that ""MSE SHAP"" is not a standard term; MSE is generally used to evaluate model performance, while SHAP values explain individual predictions. The combination implies evaluating features' impact using MSE as a metric within SHAP's framework.",4.052080154418945,"The term ""MSE SHAP"" does not appear in the provided documents specifically. However, MSE (Mean Squared Error) is a common metric used to measure the accuracy of predictions in a model, calculated as the average of the squared differences between predicted and actual values. SHAP (SHapley Additive exPlanation) values, on the other hand, are a measure of feature importance that explains the contribution of each feature to the model's predictions based on game-theoretic principles. 

While there is no direct mention of ""MSE SHAP"" in the snippets, it could imply a context where both MSE (or other error metrics) are being evaluated alongside SHAP values to provide insights into model performance and feature contributions. In practice, researchers may analyze how different features (using SHAP) influence the MSE or other error metrics, but a specific definition or methodology termed ""MSE SHAP"" is not delineated in the provided excerpts."
qwen2.5:7b-instruct-q4_0,"What are observations on impact of different imputation methods on explainability of models?
","Document 1: 
 relationships between observed and missing variables.
In recent years, more attention has been paid to the explainability of
the imputation method under missing data. For example, [37] introduces
DIMV, an imputation method that provides explainability and uncertainty
of imputation via conditional distribution, or [1] analyzes the effects of vari-
ous imputation and direct parameter estimation methods to the correlation
plot. Next, Hans et al. [38] introduce an explainable imputation method
that leverages association constraints within the data. However, up to our
5
knowledge, so far, there is no work on the effects of missing data on Shapley
values.
2.3. Direct missing data handling techniques without imputation
Different from the imputation approaches, methods that directly handle
missing data can have clearer implications in terms of explainability.
Specifically, Nguyen et al. [35] introduced the EPEM algorithm to esti-
mate the maximum likelihood estimates (MLEs) for multiple class monotone
missing data when the covariance matrices of all classes are assumed to be
equal. Additionally, DPER [39] addresses a more general case where miss-
ing data can occur in any feature by using pairs of features to estimate the
entries in the mean and covariance matrices. The implication to model ex-
plainability of using direct parameter estimation methods, like EPEM and
DPER, includes improved transparency and interpretability of the model’s
behavior, as these methods provide clear estimates of means and covariances, 

Document 2: 
 details of how these methods works is described by White, Royston, and
Wood (2011).
The related work shows that there exists several literature describing different
methods of data imputation. From the implementation perspective, it is also
very important to understand and evaluate performance of different imputation
methods so that appropriate method can be used while performing data mining
task. Though there exist some literatures that analyzed performance of different
imputation methods, in this paper, we intend to analyze performance of differ-
ent imputation method for numeric datasets that uses single and multiple
imputation methods namely mean imputation, median imputation, kNN impu-
tation, predictive mean matching, Bayesian Linear Regression (norm), Linear
Regression, non-Bayesian (norm.nob), and random sample.
Research Methodology
This section describes procedure followed for analyzing performance of the
imputation methods. Objective of this study is to analyze performance of imputa-
tion methods that includes: (a) Single Imputation Methods: Mean Imputation,
Median Imputation, KNN Imputation and (b) Multiple Imputation Methods:
Predictive Mean Matching (pmm), Bayesian Linear Regression (norm), Linear
Regression non-Bayesian (norm.nob), and Sample method. All these imputation
methods are applied only on numeric datasets. The datasets used in this study are
obtained from UCI Machine Learning Repository (Lichman2013). The descrip- 

Document 3: 
 The main limitations of ou r work were as follows. 
We used a data set contained only four categorical 
variables. Therefore questions whether number and 
nature of variables affected our conclusions remains 
to be addressed. In regression imputation, E-M algo-
rithm, and the MICE methods regression modelling 
has been used to draw the imputation. It is clear that, 
generally speaking, the more the number of variables 
the better the prediction. In addition, in regression 
and E-M imputations, it is as sumed that the data fol-
lows a normal distribution. This was not the case in 
our data set. This might partly explain poorer perfor-
mance of these two methods. Another limitation of 
our work was that we di d not compare performance 
of the imputation methods under different missing 
rates. One last issue was the process of model build-
ing. We developed all four models using ENTER 
method. This method fits a model including all varia-
bles offered to. Therefore, the behavior of imputation 
models under different variable selection methods 
(i.e. Backward and Forward) should be investigated. 
All issues noted affect the conclusions 5,25-28 and their 
influence should be explored in future studies.  
 Although the MICE method does not provide 
unique estimates,29 and communication of results with 
clinical audiences is not simple, evidence from litera-
ture suggested that the MICE method is the best ap-
proach to impute missing data. However, our literature 

Document 4: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 5: 
 of 10–20% in more than 75% of our experiments. This holds
for most imputation methods; we did not observe a clear
advantage for an imputation method overall. Taking into
account the considerable differences in wall-clock run time,
our results indicate that also when choosing an imputation
method that is both fast and improves downstream predictive
performance random forests would be the preferred
imputation method.
The positive impact of imputation on downstream
performance is most pronounced when the imputation
methods were trained on fully observed data. When
imputation methods were trained on incomplete data, the
positive impact of imputing missing values in the test data was
substantially lower, sometimes even negative. While this might
seem a disadvantage, we emphasize that, in many application use
cases, we can ensure that the training data be fully observed, for
instance, by acquiring more data before training the imputation
and the downstream ML model.
6.3 Limitations
Because one of the main goals of this study is a comprehensive
comparison of imputation methods on a large number of datasets
and missingness conditions, we made some decisions that limit
our results.
First, we focus on point estimates of imputed values rather than
multiple imputations because it is 1) easier to handle in automated
pipelines and 2) can be considered a more relevant scenario in real-
world applications of imputation methods. Thus, we do not
consider the inherent uncertainty of the imputation process. We
decided to measure and compare the impact imputation methods 

Document 6: 
 that is not positive definite, which is can possibly prevent further analysis such as calcu -
lating coefficients estimates [36]. Finally, pairwise deletion also known to produce low 
bias results for MCAR or MAR data [34].
Imputation
The process of imputation involves replacing missing values by some predicted values. 
The non-missing values data set is normally used to predict the values used to replace 
the missing values [8]. In the following we cover some of the most used imputation 
methods in the literature.
Simple imputation
Simple imputation approach entails replacing missing values for each individual value by 
using a quantitative attribute or qualitative attribute of all the non-missing values [37]. 
With simple imputation, missing data is handled by different methods such as, mode, 
mean, or median of the available values. In most studies simple imputation methods are 
used because of their simplicity and that they can be used as an easy reference technique 
[38]. However, simple imputation methods may produce bias or unrealistic results on a 
high-dimensional data sets. Also, with the generation of big data emerging, this method 
seems to be performing poorly and therefore is inadequate to be implemented on such 
data sets [39].
Regression imputation
Regression is one of the preferred statistical technique for handling missing values. This 
method is also termed conditional mean imputation, here missing values are replaced 
with a predicted value created on a regression model if data is missing at random. The 
overall regression process is a two-phase method: the first step, uses all the complete 

","Several studies have observed that direct handling of missing data can provide clearer implications for model explainability compared to imputation. For instance, methods like EPEM and DPER directly manage missing values without introducing potential biases from imputed values, thus potentially enhancing model interpretability. However, comprehensive evaluations show no clear overall advantage for any single imputation method in improving downstream predictive performance or model explainability. Random forests were noted as a fast and effective choice when considering both speed and predictive improvement.",4.034760236740112,"Different imputation methods have significant implications for the explainability of models, especially regarding the calculation of Shapley values, which attribute the contribution of features to model predictions. The choice of imputation method can introduce biases or distortions that affect the accuracy and reliability of Shapley values. 

1. **Effect on Shapley Values**: The impact of imputation methods on Shapley values has not been extensively studied despite its importance. Different imputation strategies can influence the interpretability of machine learning models and the robustness of the insights derived from Shapley values.

2. **Comparison of Methods**: Research has shown that the performance of imputation methods varies, and while some methods like random forests can improve downstream predictive performance, their effectiveness can diminish if trained on incomplete data. This highlights the need for careful selection of imputation methods based on the nature of the dataset.

3. **Explainability versus Performance**: Methods that provide clearer estimates of means and covariances, like direct parameter estimation techniques (e.g., EPEM and DPER), tend to offer better transparency and interpretability. On the other hand, simpler imputation methods might lead to unrealistic results in high-dimensional datasets, which could impact the explainability in a negative manner.

4. **Limitations of Imputation Methods**: Specific techniques such as regression and multiple imputation methods can perform poorly if the data does not meet the underlying assumptions (e.g., normal distribution) or if the imputation method does not adequately capture the complexity of the dataset. This suggests that the imputation methods used have a direct correlation with the explainability and interpretation of the resulting models.

Overall, selecting the appropriate imputation method is crucial for maintaining the integrity and interpretability of machine learning models that rely on imputed data."
qwen2.5:7b-instruct-q4_0,"Does usage of different imputation methods on missing values has impact on SHAP feature importance?
","Document 1: 
 especially at higher missing rates. MICE and DIMV often show similar
patterns, possibly due to the fact that MICE is based on regression and
DIMV is based on a conditional Gaussian formula. MissForest and SOFT-
IMPUTE sometimes preserve feature rankings better than simpler methods,
but this is not consistent across all scenarios.
The variability in results between methods and missing rates underscores
the need to evaluate imputation effects when using Shapley values for model
interpretation.
The following discussion is structured around our result and the specific
pitfalls that may arise due to incomplete understanding of the relationship
between missing data, imputation methods, and Shapley values. We high-
23
light how different approaches can lead to vastly different interpretations, how
dataset characteristics and missing rates affect results, and the importance
of considering both imputation accuracy and interpretability preservation.
Pitfall 1: Assume the neutrality of the imputation method.Our
study reveals that different imputation methods can significantly alter Shap-
ley values and, consequently, the interpretability of the model. For instance,
mean imputation tends to distort feature importances, especially at higher
missing rates, while methods like MICE and DIMV often show similar pat-
terns. This underscores the importance of carefully considering the imputa-
tion method when using Shapley values for model explanation, as the choice
can lead to vastly different interpretations of feature importance.
Pitfall 2: Overlooking data set dependency. We observed that 

Document 2: 
 Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values
for features not in the set S. Since most models cannot handle arbitrary patterns of missing input
values, we approximate f(zS) with E[f(z) |zS]. This deﬁnition of SHAP values is designed to
closely align with the Shapley regression, Shapley sampling, and quantitative input inﬂuence feature
attributions, while also allowing for connections with LIME, DeepLIFT, and layer-wise relevance
propagation.
The exact computation of SHAP values is challenging. However, by combining insights from current
additive feature attribution methods, we can approximate them. We describe two model-agnostic
approximation methods, one that is already known (Shapley sampling values) and another that is
novel (Kernel SHAP). We also describe four model-type-speciﬁc approximation methods, two of
which are novel (Max SHAP, Deep SHAP). When using these methods, feature independence and
model linearity are two optional assumptions simplifying the computation of the expected values
(note that ¯Sis the set of features not in S):
f(hx(z′)) = E[f(z) |zS] SHAP explanation model simpliﬁed input mapping (9) 

Document 3: 
 dependsonthesizeofthedatasetandmissingvaluespresent
inthem.Here,theauthorsanalyzedoftheperformanceoffour
imputationpackagesinRontwodatasetsusingthreepredictive
algorithms-LogisticRegression,SupportVectorMachineandAr-
tificialNeuralNetwork.Theauthorsmeasuretheperformanceof
thepackagesintermsofimputationtime,imputationefficiency
andeffectonthevariance.Theauthorsalsoshowthattheper-
formance of an imputation method depends on the predictive
algorithm which is a function of dataset characteristics. There
aredifferentwaysofhandlingmissingdata[25–27,29–32].The
mostsimpleapproachistouseonlythesubsetthatiscomplete;
inotherwords,suchamethodignoresallfeaturevectorswith
missing values and the remaining subset is used for analysis.
Althoughverysimple,suchanapproachwillbeusefulonlywhen
asmallfractiondatapointshavemissingvalues.Amoreuseful
and popular approach is to impute the missing values using
differenttechniquesandthenusethedatasetconsistingofboth
featurevectorswithoutmissingvaluesandthefeaturevectors
with imputed missing values. These techniques often depend
ontheproblemthatonelikestosolveusingthedataset.For
example,toclusteranincompletedataset,twotypesofframe-
works may be adopted: single view clustering and multi-view 

Document 4: 
 ley values, ranging from (−0.5) to around (+1.5) across all missing rates, em-
phasizing its strong influence on the model outputs. The imputation methods
show similar patterns with Shapley values for key features slightly reduced
compared to the XGBoost model, but they still highlight the importance of
Latitude, Longitude, and MedInc in the predictive modeling.
Considering the results at each missing rate, we observed similar distribu-
tions in the Shapley values for Latitude, Longitude, and MedInc on both the
15
(a) Original
 (b) Xgboost
 (c) Mean Imputation
(d) MICE
 (e) DIMV
 (f) missForest
(g) SOFT-IMPUTE
Figure 1: Global feature importance plot on the California dataset with the missing rate
r = 0.2
original and imputed data using various imputation methods, generally fluc-
tuating around zero. However, XGBoost shows a distribution that is more
skewed from zero. For example, at a missing rate of 20% in figure 5, the
Shapley values in the original data for Latitude and Longitude range from
(−0.75) to (+0 .75), and for MedInc from ( −0.5) to (+0 .7). Likewise, the
ranges for models with imputation methods are [ −0.6; 0.6] and [ −0.4; 0.6],
respectively, indicating that the imputation does not significantly alter the 

Document 5: 
 dataset. By displaying each data point as a dot and arranging these
dots to show the distribution of Shapley values for each feature, the
beeswarm plot offers a comprehensive overview of feature importance
and interaction effects. It allows for the identification of patterns and
outliers, helping to understand the behavior of the model with respect
to individual features. Additionally, the plot can reveal how specific
features interact with others, providing insights into complex depen-
dencies within the data.
3.2. Imputation techniques
In this section, we briefly summarize the missing data handling methods
that we will examine for the effects on Shapley values. The methods being
investigated consist of a method that can directly learn from missing data,
such as XGBoost, to a simple imputation method as Mean Imputation, as
well as the widely used or recently developed imputation techniques, such as
8
MICE, DIMV, missForest, and SOFT-IMPUTE. The details of the methods
are as follows:
• XGBoost (Extreme Gradient Boosting) [47] is a powerful and efficient
algorithm that belongs to the family of gradient boosting techniques.
It builds an ensemble of decision trees, where each tree corrects errors
made by the previous ones, enhancing predictive accuracy. XGBoost
stands out for its speed and performance, employing advanced features
like tree pruning, regularization, and parallel processing, which help
in reducing overfitting and handling large-scale data. XGBoost can
handle missing data directly. 

Document 6: 
 2018). However, in that work, the authors only considered text data
as an inputﬁeld to an imputation method, not as a column that
could be imputed. Generally, most modern ML applications that
involve text data are based on rather sophisticated natural language
models. Combinations of such models with tabular data are an
importantﬁeld of research (Yin et al., 2020) but beyond the scope of
most imputation research so far.
Third, to measure the imputation impact on the downstream
performance, we discarded and imputed values in only a single
column. Therefore, the impact depends heavily on the chosen
column’s importance (e.g., see the work ofSchelter et al. (2021)).
Generally, the impact when using an imputation model could
vary when multiple columns are affected by missing values.
7 CONCLUSION
In this study, we developed an experimental protocol and
conducted a comprehensive benchmark for imputation
methods comparing classical and modern approaches on a
large number of datasets under realistic missingness conditions
with respect to the imputation quality and the impact on the
predictive performance of a downstream ML model. We also
evaluated how the results changed when the imputation and
downstream model were trained on incomplete data.
Our results can be summarized in two mainﬁndings. First, we
demonstrate that imputation helps to increase the downstream
predictive performance substantially regardless of the
missingness conditions. When training data are fully observed,
our results demonstrate that, in more than 75% of our 

","Yes, different imputation methods can significantly alter SHAP feature importance values, especially at higher missing rates. For instance, mean imputation shows similar patterns with Shapley values compared to the original data, but more sophisticated methods like MICE and DIMV can still highlight the importance of certain features such as Latitude, Longitude, and MedInc.",3.4695911407470703,"Yes, the usage of different imputation methods on missing values does have an impact on SHAP feature importance. Various imputation techniques can significantly alter the calculated SHAP values, leading to different interpretations of feature importance. For instance, mean imputation can distort feature importances, especially at higher missing rates, while methods like MICE and DIMV often produce similar patterns. Therefore, it is crucial to consider the choice of imputation method when interpreting SHAP values for model explanations, as different methods can yield vastly different results."
qwen2.5:7b-instruct-q4_0,"How sampling for LIME for single observation is done?
","Document 1: 
 πx. We sample instances around x′ by drawing nonzero
elements of x′ uniformly at random (where the number of
such draws is also uniformly sampled). Given a perturbed
sample z′∈{0, 1}d′
(which contains a fraction of the nonzero
elements of x′), we recover the sample in the original repre-
sentation z ∈Rd and obtain f(z), which is used as a label for
the explanation model. Given this dataset Zof perturbed
samples with the associated labels, we optimize Eq. (1) to
get an explanation ξ(x). The primary intuition behind LIME
is presented in Figure 3, where we sample instances both
in the vicinity of x (which have a high weight due to πx)
and far away from x (low weight from πx). Even though
the original model may be too complex to explain globally,
LIME presents an explanation that is locally faithful (linear
in this case), where the locality is captured by πx. It is worth
noting that our method is fairly robust to sampling noise
since the samples are weighted byπ
x in Eq. (1). We now
present a concrete instance of this general framework.
3.4 Sparse Linear Explanations
For the rest of this paper, we let G be the class of linear
models, such that g(z′) = wg ·z′. We use the locally weighted 

Document 2: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 3: 
 DATA REDUCTION
Data reduction is typically conducted in two directions, i.e., row-
wise for data sample reduction and column-wise for data variable
reduction. Various data sampling techniques can be applied for
row-wise data reduction, such as random and stratiﬁed sampling
(Fan et al., 2015b). Random sampling is typically used to simulate
a random process in data sample selection. By contrast, stratiﬁed
sampling is performed to maintain the proportions of data
samples corresponding to different categories. For instance,
assuming there are 100 and 50 data samples in the original
data set for category A and B respectively, a strati ﬁed
sampling process with a selection rate of 0.7 will randomly
select 70 out of 100, and 35 out of 50 data samples in
category A and B for further analysis. Such techniques can be
applied for sampling time series data as well. In such a case, it is
recommended to transform the original sequence data into
shorter subsequences before data sampling, since direct
sampling may lead to temporal information loss.
There are three main methods of column-wise data variable
reduction. Theﬁrst is to use domain knowledge to directly select
variables of interests. The second is to use statistical feature
selection methods to select important variables for further
analysis. The third is to adopt feature extraction methods to
construct useful features for data analysis. The details of the latter
two methods are shown inFeature Selection Techniques, Feature
Extraction Techniques.
Feature Selection Techniques
As shown inTable 2, feature selection methods can be broadly 

Document 4: 
 the change in the conditional expectation of all features given
a subset of features. In other words, this calculation ignores
the nodes of features that are not present in the sample subset
and calculate change on the conditional expectation, given
this subset sample.
5 Experiments
The main steps of the experiments consisted of data acqui-
sition, data pre-processing, model tuning, model evaluation,
and interpretation of models and their predictions.
All the experiments ran in a free tier Google Colab
environment, which consisted of an Intel Xeon 2.34Ghz
quad-core processor, 25Gb of RAM, and Linux Ubuntu 18.04
operational system. The algorithms were implemented in
Python 3.7 using PIP packages. Implementation details can
be found here.
5.1 Data Acquisition
The experiment was executed with data provided by the
COVID-19 Data Sharing/BR, which is an initiative of the São
Paulo Research Foundation (FAPESP) in collaboration with
the University of São Paulo. The dataset consisted of sus-
pected COVID-19 patients admitted to the Hospital Israelita
Albert Einstein from February 2020 to June 2020. Addition-
ally, we conﬁrmed that it is possible to have two samples from
the same patient but from different days. It happens because
a patient can be admitted more than once in the same unit,
and also, a doctor can request a COVID-19 for the patient
more than once while they are still under treatment. 

Document 5: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 6: 
 Method
Used for Data Imputation 10% 20% 30% 40% 50%
Mean Imputation 0.068072 0.106639 0.120149 0.142043 0.160996
Median Imputation 0.069302 0.108637 0.123685 0.141440 0.165698
KNN Imputation 0.013803 0.025372 0.035587 0.047773 0.066471
Predictive Mean Matching(pmm) 0.017844 0.035328 0.044072 0.057366 0.081049
Bayesian Linear Regression(norm) 0.017917 0.033387 0.046552 0.057673 0.077796
Linear Regression, non-Bayesian (norm.nob) 0.017933 0.033459 0.044446 0.055386 0.072130
Random sample from observed values(sample) 0.072683 0.119587 0.131316 0.153484 0.169020
Figure 1. Plot of imputation method versus mean NRMSE for wine dataset.
Figure 2. Plot of imputation method versus mean NRMSE for glass dataset.
924 A. JADHAV ET AL.
given in ascending order of Mean NRMSE. It means that lowest Mean
NRMSE value get first rank.
Tables 7– 11 provide rank of imputation method for varying percentage of
missing data for five different datasets. Each table indicates performance of
imputation method on different datasets for given percentage of imputed
data. The reason for doing this is to assess consistency in performance of the 

","Local Interpretable Model-agnostic Explanations (LIME) is a popular technique used to explain the predictions of any model by approximating it locally with an interpretable model. Here's how you can sample for a single observation in LIME:

1. **Prepare the Data**: You need the original dataset, including the feature values and target labels.

2. **Isolate the Observation**: Pick the specific observation (instance) whose prediction you want to explain.

3. **Generate Perturbations**:
   - Create a perturbed version of this single observation.
   - Each perturbation is slightly different from the original observation but still within the feasible range for each feature.
   
4. **Sampling Method**:
   - You can use various sampling methods such as Gaussian noise, uniform noise, or other predefined distributions to generate these perturbations.
   - The choice of sampling method depends on the type of features (continuous, categorical).

5. **Create Perturbed Dataset**: For each perturbation, create a new row in your dataset with the modified feature values and keep the same label as the original observation.

6. **Train a Local Model**:
   - Train an interpretable model (e.g., logistic regression, decision tree) on this perturbed dataset.
   - The local model tries to approximate the behavior of the global model around the selected observation.

7. **Explain the Prediction**: Use the coefficients or rules from the trained local model to explain why the original prediction was made.

Here's a simplified example in Python using the ` lime` library:

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from lime.lime_tabular import LimeTabularExplainer

# Load dataset
data = load_iris()
X, y = data.data, data.target

# Train a global model (e.g., logistic regression)
model = LogisticRegression(max_iter=1000)
model.fit(X, y)

# Select an observation to explain (index 0 for the first observation)
observation_index = 0
explanation_observation = X[observation_index].reshape(1, -1)

# Create a perturbed dataset around the selected observation
def generate_perturbation(x, num_samples=50):
    # Add Gaussian noise to the features
    perturbations = x + np.random.normal(scale=0.1, size=(num_samples, len(x)))
    return perturbations

perturbations = generate_perturbation(explanation_observation[0], num_samples=50)
X_exp = np.vstack([explanation_observation] * 50)  # Stack the original observation as many times as samples
Y_exp = [model.predict_proba(X_exp)[i][y[i]] for i in range(len(Y_exp))]  # Predict probabilities

# Create an explainer and explain the prediction
explainer = LimeTabularExplainer(X, feature_names=data.feature_names, class_names=data.target_names)
exp = explainer.explain_instance(explanation_observation[0], model.predict_proba, num_features=3)

# Print the explanation
print(exp.as_list())
```

In this example:
- We load the Iris dataset and train a logistic regression model.
- We select an observation to explain (index 0).
- We generate perturbations by adding Gaussian noise around this selected observation.
- We use LIME to create a local interpretable model that approximates the global model's behavior near the selected observation.

This process helps in understanding why the model made a particular prediction for the given instance.",16.852238416671753,"In LIME (Local Interpretable Model-agnostic Explanations), sampling for a single observation, denoted as x, involves drawing instances around a perturbed version of that observation (x′). The process includes sampling nonzero elements of x′ uniformly at random, with the number of draws also sampled uniformly. A perturbed sample z′ is created, which includes a fraction of the nonzero elements from x′, and this is then rephrased back to the original representation as z. The output of the original model for this representation, f(z), serves as the label for the explanation model. The sampling process captures instances close to x (high weight from the locality function πx) and farther away (lower weight), ensuring that the explanation remains locally faithful even in complex models."
qwen2.5:7b-instruct-q4_0,"How can LIME lead to improvement of model by non-experts?
","Document 1: 
 In this paper, we propose providing explanations for indi-
vidual predictions as a solution to the “trusting a prediction”
problem, and selecting multiple such predictions (and expla-
nations) as a solution to the “trusting the model” problem.
Our main contributions are summarized as follows.
•LIME, an algorithm that can explain the predictions ofany
classiﬁer or regressor in a faithful way, by approximating
it locally with an interpretable model.
•SP-LIME, a method that selects a set of representative
instances with explanations to address the “trusting the
model” problem, via submodular optimization.
•Comprehensive evaluation with simulated and human sub-
jects, where we measure the impact of explanations on
trust and associated tasks. In our experiments, non-experts
using LIME are able to pick which classiﬁer from a pair
generalizes better in the real world. Further, they are able
to greatly improve an untrustworthy classiﬁer trained on
20 newsgroups, by doing feature engineering using LIME.
We also show how understanding the predictions of a neu-
ral network on images helps practitioners know when and
why they should not trust a model.
2. THE CASE FOR EXPLANATIONS
By“explaining a prediction”, we mean presenting textual or
visual artifacts that provide qualitative understanding of the
relationship between the instance’s components (e.g. words 

Document 2: 
 that is available.
2.3 Improving existing data
In addition to searching and labeling datasets, one can
also improve the quality of existing data and models. This
approach is useful in several scenarios. Suppose the target
application is novel or non-trivial where there are no relevant
datasets outside, or collecting more data no longer beneﬁts
the model’s accuracy due to its low quality. Here, a better
option may be to improve the existing data. One effective
approach is to improve the labels through re-labeling. Sheng
et al. [ 146] demonstrates the importance of improving labels
by showing the model accuracy trends against more training
examples for datasets with different qualities. As the data
quality decreases, even if more data is used, the accuracy of
the model does not increase from some point and plateaus. In
this case, the only way to improve the model accuracy is to
improve the label quality, which can be done by re-labeling
and taking majority votes on multiple labels per example. In
fact, one could clean the entire data including labels, which
naturally leads to the next section where we cover data vali-
dation, cleaning, and integration.
3 Data validation, cleaning, and integration
It is common for the training data to contain various errors.
Machine learning platforms like TensorFlow Extended
(TFX) [13] have data validation [ 117] components to detect
such data errors in advance using data visualization and
schema generation techniques. Data cleaning can be used 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 commonly held misconception is that ML is a one-step training pro-
cedure that accepts data as input and generates a model as output.
Many research efforts, both within the DB community and in other
communities, focus on this simplified view and aim to improve the
effectiveness of ML training, e.g., by generating more powerful mod-
els through better training algorithms, or by reducing its resource
footprint through various software and hardware optimizations.
This view is reinforced by various leaderboard-style competitions
popular among practitioners (e.g., Kaggle), and academics (e.g., ML
benchmarks and KDD competitions).
At the same time, there is evidence from practitioners [16, 19, 48]
that ML deployments in production are significantly more compli-
cated. Specifically, ML in production involves pipelines with many
interlocking steps, only one of which is training. This has spurred
on the development of many end-to-end ML systems (e.g., TFX [16],
MLFlow [52], Microsoft Azure ML [8], AWS Sagemaker [34]) and
open-source ML libraries (e.g., MLlib [6], MetaFlow [2], and Scikit-
Learn [7]), all of which provide native support for data pre-processing,
data validation, model validation, and model deployment, in addi-
tion to model training, all within a single environment.
As an example, TFX [16] includes pipeline steps that perform 

Document 5: 
 that is, regardless of the model class of the original black-box. Usually,G is chosen to be a constrained class of intrinsi-
cally interpretable models (Section4.2), for example, linear models or GAMs. Put simply, LIME trains many interpret-
able surrogate models to approximate a black-box modelf /C1ðÞ locally. During training, instances are sampled around
each data pointxi weighted byπxi . In addition to local explanations given byξ /C1ðÞ , Ribeiro et al. (2016) introduce a pro-
cedure for obtaining a global understanding of the modelf /C1ðÞ : given a limited budget, their algorithm picks several
explanations based on greedy submodular optimization (Krause & Golovin,2014) and aggregates them into global vari-
able importances, similar to the random forest feature importance (Breiman,2001).
DeepLIFT
Shrikumar et al. (2017) introduce an efficient method for disentangling contributions of inputs in a neural network—
deep learning important features (DeepLIFT). Compared to LIME, DeepLIFT is not model-agnostic since it is explicitly
tailored to neural networks; it also requires a reference, or baseline, data point. While in natural images an all-black
image is typically used as a baseline input, the choice of a reference might not be so trivial for more specialized datasets 

Document 6: 
 SP-LIME
RP-LIME
No cleaning
Figure 10: Feature engineering experiment. Each
shaded line represents the average accuracy of sub-
jects in a path starting from one of the initial
10 sub-
jects. Each solid line represents the average across
all paths per round of interaction.
person inspects ( B) to 6. The position of each algorithm
and the order of the instances seen are randomized between
subjects. After examining the explanations, users are asked
to select which algorithm will perform best in the real world.
The explanations are produced by either greedy (chosen
as a baseline due to its performance in the simulated user
experiment) or LIME, and the instances are selected either
by random (RP) or submodular pick (SP). We modify the
greedy step in Algorithm 2 slightly so it alternates between
explanations of the two classiﬁers. For each setting, we repeat
the experiment with 100 users.
The results are presented in Figure 9. Note that all of
the methods are good at identifying the better classiﬁer,
demonstrating that the explanations are useful in determining
which classiﬁer to trust, while using test set accuracy would
result in the selection of the wrong classiﬁer. Further, we see
that the submodular pick (SP) greatly improves the user’s
ability to select the best classiﬁer when compared to random
pick (RP), with LIME outperforming greedy in both cases.
6.3 Can non-experts improve a classiﬁer? 

",Non-experts using LIME are able to improve an untrustworthy classifier trained on 20 newsgroups by doing feature engineering based on the explanations provided by LIME.,2.680821657180786,"LIME (Local Interpretable Model-agnostic Explanations) allows non-experts to understand and interpret the predictions of any classifier or regressor. By providing clear explanations for individual predictions, non-experts can identify which features are most influential in the model's decisions. This understanding enables them to engage in feature engineering by modifying the input features or improving the labeling of the data based on the insights gained from LIME. In experiments, it was shown that non-experts could effectively select better-performing classifiers by using LIME to analyze the models, ultimately leading to improved model accuracy and trustworthiness. The ability to explain predictions empowers users to make informed decisions about model improvement, even without deep expertise in machine learning."
qwen2.5:7b-instruct-q4_0,"What is Explainable Boosting Machine model?
","Document 1: 
 feature x2 there is a weight β2 and β1 is ten times greater than
β2, it is reasonable to say that the feature x1 is considerably
more important to the decision of the model. That is, x1 will
have ten times more impact than x2. Therefore, it is possible
to make sense of a global feature importance by analyzing
the values of the β feature weights. One limitation of this
model is that due to its linear nature, local explanations can-
not be directly assessed, that is because the model adjusts the
feature importances as being the same for the entire features
distribution.
4.2 Explainable Boosting Machine
The explainable boosting machine (EBM) proposed in Nori
et al. ( 2019) is a glass-box-type model that leverages state-of-
the-art performing algorithms, such as boosting and bagging,
while maintaining intrinsic interpretability. The main idea
of this model is to build a generalized additive model with
interactions ( GA 2 M) by executing a round-robin training
procedure on one feature at a time, using a meager learning
rate. Its general is given in the form of:
g(E[y]) = β0 + /Sigma1f j (x j ) + /Sigma1fij (xi , x j ) (3)
where g is the link function and f j and fij are called smooth
functions. This approach brings two major improvements 

Document 2: 
 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
https://doi.org/10.1007/s40313-021-00858-y
Explainable Artiﬁcial Intelligence for COVID-19 Diagnosis Through
Blood Test Variables
Lucas M. Thimoteo 1 · Marley M. Vellasco 1 · Jorge Amaral 2 · Karla Figueiredo 3 · Cátia Lie Yokoyama 4 ·
Erito Marques 2
Received: 30 April 2021 / Revised: 1 October 2021 / Accepted: 20 October 2021 / Published online: 3 January 2022
© Brazilian Society for Automatics–SBA 2021
Abstract
This work proposes an explainable artiﬁcial intelligence approach to help diagnose COVID-19 patients based on blood test
and pathogen variables. Two glass-box models, logistic regression and explainable boosting machine, and two black-box
models, random forest and support vector machine, were used to assess the disease diagnosis. Shapley additive explanations
were used to explain predictions for the black-box models, while glass-box models feature importance brought insights into
the most relevant features. All global explanations show the eosinophils and leukocytes, white blood cells are among the
essential features to help diagnose the COVID-19. Moreover, the best model obtained an AUC of 0.87.
Keywords COVID-19 diagnosis · Machine learning · Explainability · Interpretability · Shapley additive explanations ·
Explainable boosting machine 

Document 3: 
 machine learning(Vol. 37, pp. 1718–1727). PMLR.
Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2020). Explainable AI: A review of machine learning interpretability methods.
Entropy, 23(1), 18.https://doi.org/10.3390/e23010018
Lipovetsky, S., & Conklin, M. (2001). Analysis of regression in game theory approach.Applied Stochastic Models in Business and Industry,
17(4), 319–330. https://doi.org/10.1002/asmb.446
Lipton, Z. C. (2018). The mythos of model interpretability.Queue, 16(3), 31–57. https://doi.org/10.1145/3236386.3241340
Liu, K., Sadoune, N., Rao, N., Greitemann, J., & Pollet, L. (2021). Revealing the phase diagram of Kitaev materials by machine learning:
Cooperation and competition between spin liquids.Physical Review Research, 3(2), 023016. https://doi.org/10.1103/physrevresearch.3.
023016
Liu, S., Kailkhura, B., Loveland, D., & Han, Y. (2019). Generative counterfactual introspection for explainable deep learning. In2019 IEEE 

Document 4: 
 atively to predict and impute each missing feature
from the other features until the imputed values con-
verge. MissForest is a popular imputation method
as it is capable of capturing non-linear and interac-
tion eﬀects between features to improve imputation
accuracy, and can be applied to mixed data types
(continuous and discrete). Note that, the framework
of MissForest is similar to that of MICE (Van Buuren
and Groothuis-Oudshoorn, 2011) — the only diﬀer-
ence is MissForest uses random forest while MICE
uses linear model as base model for imputation.
KNN imputation imputes the missing values by the
mean value of its K nearest neighbors in the training
set. The distance of two samples is measured on the
non-missing features in both samples. KNN imputa-
tion is fast and accurate but requires choosing a good
distance metric and tuning the hyperparameter K.
3.3. Explainable Boosting Machines
The methods proposed in this work are based on one
interpretable machine learning model, the Explain-
able Boosting Machine (EBM).
Suppose an input sample is denoted as (x,y), where
x is the p dimensional feature vector and y is the
target. Denote the jth dimension of the feature vector
as xj. Then a generalized additive model (GAM), 

Document 5: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 6: 
 show examples where all models correctly predicted the
123
634 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
Fig. 1 Global interpretation for logistic regression. Top: dataset with blood tests and pathogens; bottom: dataset with only blood tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 635
Fig. 2 Global interpretation for explainable boosting machine. Top: dataset with blood tests and pathogens; Bottom: dataset with only blood tests
outcome, some models failed, while others succeeded, all
models failed.
Thus, we will show the reader how one can interpret local
explanations for the COVID-19 diagnosis in a myriad of sit-
uations. In other words, we want to show that although the
overall results are really good, the models still make mistakes
and get confused with the features, and local explanations
can help with identify that. If the reader wants to check more
examples, we encourage seeing our GitHub, provided at the
beginning of the Experiments section.
Figure 5 shows explanations for a given patient that led to a
true-positive diagnosis and Fig. 6 for a true-negative patient.
Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis. 

","The Explainable Boosting Machine (EBM) model is a type of machine learning algorithm that combines the strengths of traditional boosting methods with the interpretability of logistic regression. Here are some key points about EBMs:

1. **Interpretability**: Unlike many modern deep learning models, EBMs provide clear and understandable explanations for their predictions. This makes them suitable for applications where transparency is crucial.

2. **Boosting Framework**: EBMs use a boosting framework similar to decision trees or gradient boosting machines (GBMs). They iteratively combine weak learners (typically logistic regression models) to form a strong model.

3. **Logistic Regression Core**: The core of each weak learner in an EBM is a logistic regression model, which makes the underlying logic easier to understand and interpret.

4. **Feature Interaction Handling**: EBMs can handle interactions between features directly within their model structure, allowing for more nuanced and interpretable predictions.

5. **Scalability**: While providing interpretability, EBMs are designed to scale well with large datasets and high-dimensional feature spaces.

6. **Training Process**: The training process involves optimizing a combination of accuracy and sparsity in the representation. This ensures that models not only predict well but also do so using fewer features.

7. **Applications**: EBMs can be used for various tasks such as classification, regression, and ranking. They are particularly useful in fields like healthcare, finance, and social sciences where understanding model decisions is crucial.

8. **Implementation**: Several frameworks and libraries provide implementations of EBMs, making them accessible to practitioners without needing to build the models from scratch.

In summary, Explainable Boosting Machines offer a balance between predictive power and interpretability, making them a valuable tool in modern machine learning applications where transparency in model decision-making is essential.",8.375577211380005,"The Explainable Boosting Machine (EBM) is a type of glass-box model that combines boosting and bagging techniques while maintaining interpretability. It builds a generalized additive model with interactions (GA2M) by training on one feature at a time using a small learning rate. The model is defined mathematically as g(E[y]) = β0 + Σ f_j(x_j) + Σ f_ij(x_i, x_j), where g represents the link function, and f_j and f_ij are smooth functions associated with the features. EBMs provide useful insights into feature importance, allowing for an understanding of the relevance of different features in model predictions, while being capable of achieving high predictive performance similar to that of black-box models."
qwen2.5:7b-instruct-q4_0,"How can EBMs help with healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 icians tend to focus on abnormal ﬁndings.
Figure 3: EBM shape function of “heart rate” for
predicting pneumonia mortality risk. Blue curve is
the original shape function; red curve is the edited
shape function.
For example, this happens to a pneumonia mortal-
ity risk dataset (Cooper et al., 2005). The blue curve
in Figure 3 shows what an EBM model has learned
for predicting pneumonia mortality as a function of
heart rate. As expected, risk is elevated for patients
with abnormally low (10-30) or high heart rate (125-
200). The graph, however, shows a surprising region
of ﬂat risk between heart rate 38 and 125, which is a
normal heart rate for patients in a doctor’s oﬃce.
Moreover, the model surprisingly predicts patients
who have normal heart rate are at elevated risk: it
adds 0.22 to the risk for patients in this region.
On further inspection, it turns out that there are
no patients in the data set with heart rates between
38 and 125, and 91% of patients are missing their
heart rate which has then been coded as zero. In
other words, there are no data to support the model
in the normal range of heart rate 38-125, and instead
the patients who would be in this range are all coded
as zero in the data and on the graph. This explains
why the model predicts the lowest risk = -0.04 for
patients with heart rate = 0, because these are the
patients who actually have normal heart rates. 

Document 3: 
 indicate the existence of the disease.
Our results indicated that the proposed system could
indeed be used to detect COVID-19 patients using clinical
and laboratory data that is commonly used in clinical prac-
tice. In terms of healthcare, our approach could represent a
new method that can help by quickly screening patients with
COVID-19. This can be specially advantageous to expand
testing protocols to areas where there is not available tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 641
Fig. 8 Local explanations for
failed diagnosis on hemogram
dataset. Top: EBM; middle: RF;
bottom: SVM
for the local population. Additionally, situations where lab-
oratory workloads are heavy, and RT-PCR tests are scarce
could also beneﬁt from rapid blood sample tests. Therefore,
it would help reduce the burden in the healthcare system
and promote the optimal utilization of healthcare resources.
Besides, we think that is possible to improve the results with
the addition of the clinical signs such as the proportion of
cough, hyperthermia, myalgia, asthenia, diarrhea and confu-
sion.
We look forward to including more COVID-19 test types
as well as data from different geographical and socioeco-
nomic regions from Brazil. Due to the continental size of the
country, the epidemic dynamics changes drastically between
regions and our proposed approach might show different
performance and also different explanations. Moreover, we
would like to increase more blood test features in order 

Document 4: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 5: 
 Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis.
For the true positive, low levels of white blood cells and
elevated values of red cells greatly impact the positive out-
come for this patient, for all models.
For the true negative, the white blood cells are also respon-
sible for the majority impact on models output. Interestingly,
the SVM was the only model that took in consideration the
monocytes and sex of the patient as relevant features.
Figure 7 shows a negative patient where the EBM cor-
rectly identiﬁed and RF and SVM classiﬁed as positive. Even
though EBM got the classiﬁcation right, we notice that sev-
eral variables have positive contributions, similarly to the
SVM and RF models. Also, we see that the model output for
SVM and RF are nearly 0.5. Although not reported in the
ﬁgure, the model output for the EBM is 0.48. Therefore, this
patient lies in a low conﬁdence zone for all models.
Figure 8 shows a positive patient that all the models failed
to diagnose, for the hemogram dataset. We notice that all
models predicted a score near to 0 probability of a positive
case and most of the features actually contributed to a neg-
ative diagnosis in all cases. This implies that there might be 

Document 6: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

","EBMs can help with healthcare data by providing interpretable and explainable predictions, allowing practitioners to understand how each feature contributes to the model's prediction. This is particularly useful for handling missing values and understanding the impact of different imputation methods on model outputs.",3.26899528503418,"EBMs (Explainable Boosting Machines) can significantly assist with healthcare data by providing interpretable and editable models that detect potential issues arising from missing values or imputation. They capture subtle anomalies in data that may go unnoticed by simpler models like linear regression or decision trees. EBMs are particularly beneficial in high-stakes environments, such as medical care, where the costs of poor predictions can be severe.

One key advantage of EBMs is their ability to reveal the consequences of different imputation methods, thereby illuminating the influence of missing data on model predictions. For instance, when analyzing datasets related to pneumonia mortality risk, EBMs can highlight particular patterns, such as the abnormal prediction of risk for patients with normal heart rates due to missing data representation. This interpretability helps healthcare professionals make informed decisions based on model outputs.

Furthermore, EBMs can automatically include interaction terms among features, thereby enhancing accuracy while maintaining intelligibility. Their flexibility and enhancement over traditional generalized additive models mean they can adapt to various healthcare applications, improving patient screenings and enabling better resource allocation in testing protocols. Overall, the use of EBMs equips healthcare practitioners with insights that promote safer and more effective clinical decision-making."
qwen2.5:7b-instruct-q4_0,"How can be EDMs used in imputing missing values in healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 and class median impute the missing value as the mean/
median of known Depression values at that time point, in the
appropriate class. Thehot deck [12] method, which is used
to impute labor force items in the Current Population Survey
[13], involves a recipient (the person with missing data) and
a donor (another person in the same imputation class, whose
value is known). We selected a donor by sampling without
replacement from the recipient’s class, and replaced the re-
cipient’s missing value with the donor’s value at the appro-
priate time point. These class imputations imply that persons
with missing data are a random sample of the persons in
their class[14].
Another baseline approach is regression imputation. The
variable of interest (known Depression value for the year in
which the “missing value” occurred) is regressed on baseline
covariates, and the resulting equation is used to estimate
the missing values for that year. We regressed Depression
on baseline Depression, age, gender, and health status. Data
for persons missing any covariate information were not im-
puted. Two different types of regression imputation were
performed.Regressionassigns the person’s predicted value to
the “missing value.” Persons with the same covariates will
have exactly the same imputed value. This can lead to the
variance of the imputed data set being too small, yielding
inappropriately small standard errors and P-values at the 

Document 3: 
 orange) when predicting whether HR is missing. In
the plot, the curve of the imputed group is signiﬁ-
cantly higher than that of the observed group, again
suggesting that when the blood pressure of the pa-
tients is missing, their heart rate is also more likely
to be missing. This eﬀect is strong, as the maximum
gap between the two curves is approximately 2.5 (1.5
in orange curve and -1.0 in blue curve) of predicted
log odds. Again the blue curve is constant.
92
Interpretable Missing Values in Healthcare
Figure 4(c) shows the shape function for tempera-
ture when predicting if urea is missing or not. There
is no missing value for temperature, so there is no
orange curve. The bump at temperature ∈[36,37.5)
indicates that urea is more likely to be missing, which
suggests when a patient has normal body tempera-
ture, doctors may be less likely to order a blood test
to measure urea.
To test how well can EBM predict the missing-
ness, we generate some semi-synthetic datasets with
ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out- 

Document 4: 
 column values. But this approach is appropriate only when there are a couple of missing values in the 
data set. Although as the quantity of missing values increase, significant data may be lost by deletion of 
the incomplete instances [13].
 – Mean/mode substitution: This is a simple way to impute the missing data. It replaces the missing values 
by the mean or mode of all the observations or a subgroup at the same variable. It consists of replacing 
the unknown value for a given attribute by the mean (quantitative attribute) or mode (qualitative attrib-
ute) of all known values of that attribute. But replacing all missing records with a single value distorts 
the input data distribution [21].
 – Maximum likelihood procedures [3]: These methods are used for estimation of parameters of model by 
making use of the expectation–maximization algorithm [13, 21]. These methods make assumption of 
model distribution for the variables, such as a multivariate normal model, which are very much sensitive 
to outliers [21].
 – Multiple imputation [5, 18]: This method imputes the value of a missing variable more than once. Then, 
analysis is done by averaging them [21].
 – Machine learning-based imputation [13]: There are a number of approaches for missing value imputation 
include choosing the most common value among the neighbors, weighted imputation based on k-nearest 
neighbors, decision tree based, association based, fuzzy logic, neural networks, SVM, and clustering- 

Document 5: 
 Meanwhile, the importance of explainable AI is increasingly recognized across
various sectors. Taking medical imaging is an example where physicians rely on
explainability to evaluate and potentially improve diagnoses based on machine
outputs. However, a significant challenge arises when these explanation methods
encounter missing data in the dataset, an aspect they often overlook. The
occurrence of missing data inherently introduces additional uncertainty into the
model. Depending on the imputation values used to fill in the missing data,
explanations and results can vary, potentially leading to inaccurate evaluations
of a patient’s health status. Such issues are not exclusive to healthcare but
extend to other domains, including banking, biology, and more.
Thus, being able to explain the imputation of missing values in the data
is an essential step toward more trustworthy data analysis. This motivates
us to introduceconditional Distribution based Imputation of Missing Values
(DIMV) algorithm, an algorithm that is able to find the conditional distribution
of the features with missing values based on fully observed features in a fashion
2
similar to the Expectation-Maximization algorithm. However, DIMV only
conditionsuponselectedrelevantfeatures, andmoreover, themeanandcovariance
matrix are estimated directly based on the DPER algorithm [2], which gives
DIMV an advantage in speed compared to EM. In addition, the imputation
step of DIMV can be interpreted as Ridge regression, with coefficients giving
direct explainability as regression coefficients. Furthermore, note that by the 

Document 6: 
 ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out-
put score is higher than the threshold. The diﬀer-
ence between MAR and MNAR is whether the target
feature value is considered as an input of the miss-
ing models. Table 2 compares EBM’s the test accu-
racy of predicting missingness with machine learning
models commonly used for missing value imputation.
EBM predicts missingness better than other methods
in cases of MAR and MNAR values generated from
linear and curvilinear models and is not far behind
Random Forest in case of quadratic model.
Summary: We use EBMs to predict the missing-
ness of features from other input features. EBM pre-
dicts the missingness accurately. The interpretability
of EBMs can help users understand the relationship
between the features and missingness and thus bring
more insight for the cause(s) of missingness.
5. Detecting and avoiding potential
risks of missing value imputations
5.1. Imputation With the Mean
Because many machine learning methods cannot na-
tively handle missing values, it is common for data
scientists to impute missing values before training
models. There are many diﬀerent ways to do this 

","EBMs can predict the missingness of features from other input features, providing interpretable results that help understand the relationship between features and missingness. This allows for detecting potential risks associated with missing values and guiding more informed decisions. However, mean imputation is not recommended as it does not capture these relationships effectively.",3.1916513442993164,"EBMs (Explainable Boosting Machines) can be used in imputing missing values in healthcare data by effectively predicting the missingness of features based on the relationships with other input features. Given their interpretable nature, EBMs can uncover subtle anomalies and provide insights into the causes of missing data, whether they follow a Missing At Random (MAR) or Missing Not At Random (MNAR) mechanism. The ability of EBMs to model the shape functions allows them to capture complex interactions between features and missing values, which makes them more proficient than traditional methods like mean or regression imputation. Furthermore, since EBMs are editable, any detected issues related to missing values can be rectified using model editing tools, contributing to more reliable healthcare decisions and potentially mitigating the risks associated with erroneous predictions derived from incomplete data."
qwen2.5:7b-instruct-q4_0,"How can EBMs help detect observations poorly imputed by other methods?
","Document 1: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

Document 2: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 3: 
 the mean value of the feature is the same before and
after mean imputation, we can directly ﬁnd the bin
(of EBM) covering the mean value, and detect if the
bin is a spike or not. This also works for median im-
putation — the median of a feature does not change
by imputing the missing values with the median.
To address the second problem, we need an algo-
rithm to distinguish spikes resulting from mean im-
putation and ﬂuctuations that naturally occur in the
EBM shape functions. We formulate this as an out-
lier detection problem. First, we calculate the second
order diﬀerences for all bins in all shape functions
(excluding ﬁrst and last bins), since spikes usually
have extreme second order diﬀerences. We denote
the function values of the kth bin and its neighbour-
ing bins as fk, fk−1, and fk+1. The corresponding
bin sizes are denoted as hk, hk−1, and hk+1. The
second order diﬀerence is
f′′
k (x) ≈
fk+1−fk
(hk+1+hk)/2 − fk−fk−1
(hk+hk−1)/2
hk + hk+1/2 + hk−1/2 . (5)
We then run an outlier detection algorithm (Isolation
Forest (Liu et al., 2008)) on these second order diﬀer- 

Document 4: 
 and generative models like deep generative models
(Yoon et al., 2018). Since most of these methods are
based on black-box machine learning methods and
the accuracy and behavior of the ﬁnal model depends
on the imputed values, it is diﬃcult for users to recog-
nize and understand the potential harms that might
be introduced by these imputation methods.
Recently developed interpretable machine learning
methods have been shown to be useful for debugging
models and detecting issues with datasets (Adebayo
et al., 2020; Koh and Liang, 2017). Interpretable ma-
chine learning methods provide a new opportunity to
study missing values and revisit some of the classi-
cal methods for handling missing values. In this pa-
per, we propose novel methods based on the Explain-
able Boosting Machine (EBM) (Lou et al., 2012, 2013;
Nori et al., 2019), a high-accuracy, fully-interpretable
glass-box machine learning method, to answer the fol-
lowing questions: (1) how interpretability can help
users gain insights on the causes of missingness, and
(2) how interpretability can help detect and avoid po-
tential risks introduced by diﬀerent imputation meth-
ods. We show that the glass-box models provide new
insights into missingness mechanisms, and in some
settings, suggest alternate ways of handling missing
values, as well as new tools that can alert users when 

Document 5: 
 a method to test MCAR based on EBM shape func-
tions. The testing process of the proposed method
88
Interpretable Missing Values in Healthcare
can be directly visualized on the shape function plots,
which is not achievable by Little’s test. We will also
show that EBM can bring additional insights beyond
simply testing for MCAR.
4.1.1. Testing for MCAR with EBM
To test for MCAR, we use the common trick of encod-
ing missing values with a unique value for the feature,
e.g., -1 for a feature with positive values or a sepa-
rate category for a categorical feature. After ﬁtting
an EBM that predicts the target, we get a shape func-
tion representing the contribution of diﬀerent feature
values for predicting the target, including the unique
value denoting missingness. Note that the leaf nodes
in EBM split the feature values into many bins, where
each bin has a prediction score. These bins and scores
together form the shape function. Therefore, the
EBM shape function fj(·) of feature j can be rewrit-
ten as a linear combination of a series of indicator
variables denoting if the feature values are within the
bins, and the coeﬃcients are the corresponding scores
of the bins, i.e.,
fj(xj) =
Bj−1∑
k=0 

Document 6: 
 be similar to uniformly imputing the heart rates in
the interval 38-125 and retraining the model.) This
approach has the following advantages:
1. Editing shape functions provides an opportunity
for experts to use their professional training to
correct and improve models in ways that may not
be adequately represented in the training data.
2. Editing the model may not only improve the ac-
curacy of the model in the real world where it
will be used (instead of just on held-aside test
data from the train set), but also make the shape
plots more “reasonable” and trusted by experts.
3. Editing an EBM shape function can be done
without retraining the model and potentially in-
troducing new problems.
Summary: We show that EBM shape function
can help identify the case when feature values are
missing because they are assumed to be normal. We
also show how editing the EBM graphs can help ad-
dress issues resulting from missing assumed normal.
4.3. Predicting the Missingness
Most missing values are not MCAR, but as mentioned
in Section 2, MNAR and MAR can be diﬃcult to dis-
tinguish (Van Buuren, 2018). For both cases, inter-
pretable models like EBM can still be useful in pro-
viding insights on possible missingness mechanisms.
(a) “Bilirubin” shape function when predicting missing-
ness of “Na”
(b) “Systolic blood pressure” shape function when pre- 

","EBMs can help detect poorly imputed observations by visualizing the EBM shape functions for observed and imputed groups separately. This allows identifying significant differences in how the model predicts risk or outcomes between these groups, indicating problematic imputations. For instance, Figure 9 illustrates that RF-imputed data shows notably lower risk compared to observed data, suggesting issues with the imputation method.",3.5483365058898926,"EBMs can help detect observations poorly imputed by other methods by providing a clear visualization of the contribution and behavior of feature values through their shape functions. When different imputation methods are applied, such as MissForest, EBMs can reveal discrepancies between the imputed groups and the observed groups, highlighting significant differences in predicted outcomes, such as ICU mortality risk. 

By analyzing the shape functions, researchers can identify spikes or unusual patterns that may indicate problematic imputation results. Additionally, EBMs allow for the calculation of outlier detection based on second-order differences in feature values, making it possible to distinguish between natural fluctuations and those caused by flawed imputation methods. This interpretability enables users to gain insights into the implications of missing data handling, allowing them to address issues arising from imputation and potentially leading to more accurate and reliable model performance."
qwen2.5:7b-instruct-q4_0,"How can one distinct terms intepretabilitry and explainability in XAI?
","Document 1: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 2: 
 tant to distinguish between features and interpretable data
representations. As mentioned before, interpretable expla-
nations need to use a representation that is understandable
to humans, regardless of the actual features used by the
model. For example, a possible interpretable representation
for text classiﬁcation is a binary vector indicating the pres-
ence or absence of a word, even though the classiﬁer may
use more complex (and incomprehensible) features such as
word embeddings. Likewise for image classiﬁcation, an in-
terpretable representationmay be a binary vector indicating
the “presence” or “absence” of a contiguous patch of similar
pixels (a super-pixel), while the classiﬁer may represent the
image as a tensor with three color channels per pixel. We
denote x∈Rd be the original representation of an instance
being explained, and we use x′∈{0, 1}d′
to denote a binary
vector for its interpretable representation.
3.2 Fidelity-Interpretability Trade-off
Formally, we deﬁne an explanation as a model g ∈G,
where G is a class of potentially interpretable models, such
as linear models, decision trees, or falling rule lists [ 27], i.e. a
model g∈Gcan be readily presented to the user with visual
or textual artifacts. The domain of g is {0,1}d′
, i.e. g acts 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 9.8.1735
Hohman, F., Srinivasan, A., & Drucker, S. M. (2019). TeleGam: Combining visualization and verbalization for interpretable machine learn-
ing. InIEEE visualization conference (VIS). IEEE.https://doi.org/10.1109/visual.2019.8933695
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., & Müller, H. (2019). Causability and explainability of artificial intelligence in medicine.
WIREs Data Mining and Knowledge Discovery, 9(4), e1312.https://doi.org/10.1002/widm.1312
Jain, S., & Wallace, B. C. (2019). Attention is not explanation. InProceedings of the 2019 conference of the North American chapter of the Asso-
ciation for Computational Linguistics: Human language technologies(pp. 3543–3556). Association for Computational Linguistics.https://
doi.org/10.18653/v1/N19-1357
Jesus, S., Belém, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P., & Gama, J. (2021). How can I choose an explainer? An application-
grounded evaluation of post-hoc explanations. InProceedings of the 2021 ACM conference on fairness, accountability, and transparency 

Document 5: 
 In general, there is no agreement within the ML community on the definition ofinterpretability and thetask of inter-
pretation (Doshi-Velez & Kim,2017; Lipton,2018). For example, Doshi-Velez and Kim (2017) define interpretability of
ML systems as“the ability to explain or to present in understandable terms to a human.” This definition lacks mathemat-
ical rigor (Lipton, 2018). Nevertheless, the notion of interpretability often depends on the domain of application
(Rudin, 2019) and the targetexplainee (Carvalho et al.,2019), that is, the recipient of interpretations and explanations.
Therefore, an all-purpose definition might be infeasible (Rudin,2019) or unnecessary. Other terms that are synonymous
with interpretability and also appear in the ML literature are“intelligibility” (Caruana et al.,2015; Lou et al.,2012) and
“understandability” (Lipton, 2018). These concepts are often used interchangeably.
Yet another term prevalent in the literature is“explainability,” giving rise to the direction ofexplainable artificial
intelligence (XAI) (Gunning & Aha,2019). This concept is closely tied with interpretability; and many authors do not
differentiate between the two (Carvalho et al.,2019). Doshi-Velez and Kim (2017) provide a definition ofexplanation 

Document 6: 
 Kirrane, 2020)).
In recognition of the growing importance of this topic, NIST published in August 2020Four principles of XAI
(Phillips et al., 2020), which define the following fundamental principles which an AI must honor to be considered an
XAI as follows:
 Explanation: this principle states that an AI system must supply evidence, support; or reasoning for each decision
made by the system.
 Meaningful: this principle states that the explanation provided byt h eA Is y s t e mm u s tb eu n d e r s t a n d a b l eb y ,a n dm e a n i n g f u l
to, its users. As different groups of users may have different necessities and experiences, the explanation provided by the AI
system must be fine-tuned to meet the various characteristics and needs of each group.
 Accuracy: this principle states that the explanation provided by the AI system must reflect accurately the system's processes.
 Knowledge limits: this principle states that AI systems must identify cases that they were not designed to operate in
and, therefore, their answers may not be reliable.
Figure 3 depicts the ontology of the XAI taxonomy. Transparent models can easily achieve explainability, while
opaque models require post hoc approaches to make them explainable. The categories of post hoc approaches are illus-
trated accordingly. 

","In the context of Explainable Artificial Intelligence (XAI), interpretability and explainability are two important but distinct concepts. Here's how they differ:

1. **Interpretability**:
   - **Definition**: Interpretable models are those where humans can understand the reasoning process, decisions, or predictions made by the model.
   - **Focus**: The primary focus is on making the inner workings of a model comprehensible to people without requiring extensive technical knowledge.
   - **Application**: Often applied to simpler models like decision trees, rule lists, and linear models. These models can be visualized and their rules can be easily understood by humans.

2. **Explainability**:
   - **Definition**: Explainable systems provide clear, concise, and understandable explanations for the decisions or predictions made by a model.
   - **Focus**: The primary focus is on providing human interpretable reasons for specific predictions or outcomes of a model, even if the underlying model itself might be complex (e.g., deep neural networks).
   - **Application**: Applied to both simple and complex models. Techniques like local interpretable model-agnostic explanations (LIME), SHapley Additive exPlanations (SHAP), and counterfactual explanations can provide detailed explanations for individual predictions.

### Key Differences:

1. **Scope**:
   - **Interpretability** is about understanding how the model works in general.
   - **Explainability** is about understanding why a specific prediction was made by the model.

2. **Complexity**:
   - **Interpretability** often focuses on simpler models that are easier to understand.
   - **Explainability** can be applied to any model, even those that are highly complex and difficult to interpret directly.

3. **User Interaction**:
   - **Interpretability** might require users to understand the entire model.
   - **Explainability** provides specific explanations for individual predictions or outcomes, allowing users to make sense of a model's decisions without needing to understand the full complexity of the model.

### Examples:

- **Interpretability Example**: A decision tree or rule-based system that humans can read and follow step-by-step. Each node in the tree or each rule explains why a particular action is taken.
  
- **Explainability Example**: Using SHAP values to explain why a specific customer was classified as low risk by a complex credit scoring model. This provides an explanation for this single prediction, even though the overall model might be too complex to interpret comprehensively.

Understanding these distinctions can help in selecting appropriate methods and techniques when developing and explaining AI models, ensuring that both the model itself and its predictions are understandable and trustworthy.",12.683237314224243,"Interpretability and explainability are terms that are often used interchangeably within the field of explainable artificial intelligence (XAI), but they can be distinguished based on their definitions and implications. Interpretability refers to the degree to which a user can understand the cause of a decision made by an AI system, typically implying that the model is transparent and its workings can be easily explained in human terms. Doshi-Velez and Kim (2017) define interpretability as “the ability to explain or present in understandable terms to a human.”

On the other hand, explainability encompasses a broader scope. It pertains to how an AI system provides reasoning or support for its decisions and aims to convey this information meaningfully to users. Explainability is necessary when dealing with complex or ""black-box"" models where the internal processes are not straightforward or easily understood. As Burkart and Huber (2020) note, while explainable models are interpretable by default, the opposite is not necessarily true, highlighting that interpretability alone may not address all aspects of understanding machine learning decisions.

In summary, while interpretability focuses on the clarity and transparency of the model itself, explainability relates more to the quality and relevance of the information provided to users regarding the model's decision-making process."
qwen2.5:7b-instruct-q4_0,"What issues in machine learning can be categorized as 'model bugs'?
","Document 1: 
 2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n
i, wherex∈X and y∈Y, a classiﬁer’s goal is to learn a function,fθ : X→Y ,
that generalizes. fθ is then used to predict test examples,xtest ∈X, as ytest = fθ(xtest). Given a
loss functionL, and model parameter,θ, for a model family, we provide a categorization of bugs as
model, data and test-time contamination:
Learning: arg min
θ
Model Contamination
L(
Data Contamination  
(Xtrain,Ytrain) ; θ);
Prediction: ytest = fθ(
Test-Time Contaminationxtest ).
1We will provide code to replicate our ﬁndings at: https://github.com/adebayoj/explaindebug.git.
3
Bug Category Speciﬁc Examples tested Formalization
Data Contamination Spurious Correlation arg min
θ
L(Xspurious artifact,Ytrain;θ)
Labelling Errors arg min
θ 

Document 2: 
 lyzing all of this data together requires some form of data
integration. In machine learning, two relevant integration
techniques are alignment and co-learning. Alignment is to
ﬁnd relationships of sub-components of instances that have
multiple modalities. For example, if there are multi-view time
series, one can perform subsampling, forward or backward
ﬁlling, or aggregate in time windows so that the time series
can be better integrated. Co-learning is to train better on a
modality using a different modality. For example, if there
are embeddings from different modalities, one approach is
to concatenate them together for a multimodal representa-
tion. In general, data integration is by itself a large research
area that has been studied for decades [ 46,152], although not
all techniques are relevant to machine learning.
4 Robust model training
Even after collecting the right data and cleaning it, data
quality may still be an issue during model training. It is
widely agreed that real-world datasets are dirty and erro-
neous despite the data cleaning process. As summarized in
Table2, these ﬂaws in datasets can be categorized depending
on whether data values are noisy or missing and depending
on whether these ﬂaws exist in data features (attributes) or
labels.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 799
Table 2 Types of data poisoning covered in this survey
Noisy Missing 

Document 3: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 4: 
 attributions, and attributions for these samples on other models. As seen in Table 3, we observe
visual similarity, particularly for the VGG-16 model on ImageNet, but essentially no correlation in
feature ranking.
10
Insights from Human Subject Study: users use prediction labels, not the attributions.
For the domain shift study, we show participants attribution of dogs that were not used during
training, and whose breeds diﬀered from those that the model was trained to predict. We ﬁnd that
users do not recommend a model under this setting due to wrong prediction labels (Figure 5).
6 Discussion & Conclusion
Debugging machine learning models remains a challenging endeavor, and model explanations could
be a useful tool in that quest. Even though a practitioner or a researcher may have a large class of
explanation methods available, it is still unclear which methods are useful for what bug type. This
work aims to address this gap by ﬁrst, categorizing model bugs into: data, model, and test-time
contamination bugs, then testing feature attribution methods, a popular explanation approach for
DNNs trained on image data, against each bug type. Overall, we ﬁnd that feature attribution
methods are able to diagnose the spatial spurious correlation bug tested, but do not conclusively
help to distinguish mislabelled examples for normal ones. In the case of model contamination, we
ﬁnd that certain feature attributions that perform positive aggregation while computing feature
relevance with modiﬁed back-propagation produce attributions that are invariant to the parameters 

Document 5: 
 that the humans exploit label information and Feng and Boyd-Graber (2019) demonstrate how to
assess explanations in a natural language setting. Similarly, Alqaraawi et al. (2020) ﬁnd that the
LRP explanation method (see Section 2.2) improves participant understanding of model behavior
for an image classiﬁcation task, but provides limited utility to end-users when predicting the model’s
output on new inputs.
Feature attributions can be easily manipulated, providing evidence for a collective ‘weakness’ of
current approaches (Ghorbani et al., 2019; Heo et al., 2019; Lakkaraju and Bastani, 2020; Slack
et al., 2020). While susceptibility is an important issue, our work focuses on providing insights
when model bugs are ‘unintentionally’ created.
2 Bug Characterization, Explanation Methods, & User Study
We now present our characterization of model bugs, provide an overview of the explanation methods
assessed, and close with a background on the human subject study.1
2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n 

Document 6: 
 model as a way to provide a global understanding.
There are several ways a model or its evaluation can go
wrong. Data leakage, for example, deﬁned as the uninten-
tional leakage of signal into the training (and validation) data
that would not appear when deployed [ 14], potentially in-
creases accuracy. A challenging example cited by (author?)
[14] is one where the patient ID was found to be heavily corre-
lated with the target class in the training and validation data.
This issue would be incredibly challenging to identify just by
observing the predictions and the raw data, but much easier
if explanations such as the one in Figure 1 are provided, as
patient ID would be listed as an explanation for predictions.
Another particularly hard to detect problem is dataset shift
[5], where training data is diﬀerent than test data (we give
an example in the famous 20 newsgroups dataset later on).
The insights given by explanations are particularly helpful in
identifying what must be done to convert an untrustworthy
model into a trustworthy one – for example, removing leaked
data or changing the training data to avoid dataset shift.
Machine learning practitioners often have to select a model
from a number of alternatives, requiring them to assess
the relative trust between two or more models. In Figure
Figure 2: Explaining individual predictions of com-
peting classiﬁers trying to determine if a document
is about “Christianity” or “Atheism”. The bar chart
represents the importance given to the most rele- 

","Model bugs in machine learning can be categorized into data contamination, model contamination, and test-time contamination.这些问题可以通过定义模型bug来分类，包括数据污染、模型污染和测试时污染。",2.731980800628662,"Model bugs in machine learning can be categorized as contamination in the learning and/or prediction pipeline that leads to incorrect predictions or the learning of error-causing associations. These bugs can be classified into three primary categories based on their source:

1. **Model Contamination**: Issues arising from the model itself that can affect its learning process or predictions.
  
2. **Data Contamination**: Problems related to the dataset used for training, such as:
   - Spurious correlations in the training data.
   - Labeling errors, where incorrect or noisy labels are present due to manual annotation errors or software mistakes.

3. **Test-Time Contamination**: Problems that occur during the model's prediction phase, potentially due to issues like data leakage, where unintended signals from the training set could artificially inflate accuracy in the prediction phase, or dataset shift, where the training data differs from the test data.

These model bugs highlight the complexities and challenges in ensuring model reliability and accuracy in machine learning frameworks."
qwen2.5:7b-instruct-q4_0,"What XAI techniques can be heloful in detecting mislabeled observation in image classification?
","Document 1: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 2: 
 In the ILSVRC2012 ImageNet competition, it surpassed the second-best entry by a huge
advantage. After AlexNet achieved excellent results in the ImageNet image classiﬁcation
competition, researchers began to study CNN more deeply, Zeiler and Fergus proposed
a visualization technique to understand CNNs and proposed ZFNet [ 20]. Min Lin et al.
proposed NIN network [21], which contributed to the control of the parameter amount
and the number of channels. Next, refs. [ 22–27] yielded high performance during the
ILSVRC2014-2017 classiﬁcation challenge, they all made great innovations on the original
basis. From 2017 to the present, more models with superior performance have appeared
one after another. CNNs have increasingly demonstrated irreplaceable superiority in image
classiﬁcation.
With the successful application of CNN to large-scale visual classiﬁcation tasks, around
2015, the application of CNNs has ﬁnally taken off in the remote sensing image analysis
ﬁeld [28,29]. A variety of CNN-based scene classiﬁcation methods have emerged by using
different strategies of exploiting CNNs [ 30–36]. Generally, CNN-based remote sensing
image scene classiﬁcation methods can be divided into three types: (i) The pre-trained
CNNs is used as a feature extractor [ 37–44]. (ii) Fine-tune the pre-trained CNNs on the
dataset [30,45–52]. (iii) Globally initialize the weights of CNNs for training [ 31,53–55].
As we all know, the CNN-based image classiﬁcation method was originally designed for 

Document 3: 
 ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 6021–6029. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/6064.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
17
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from
error visibility to structural similarity.IEEE transactions on image processing, 13(4):600–612,
2004.
Gezheng Wen, Brenda Rodriguez-Niño, Furkan Y Pecen, David J Vining, Naveen Garg, and Mia K
Markey. Comparative study of computational visual attention models on two-dimensional medical
images. Journal of Medical Imaging, 4(2):025503, 2017.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. InAdvances in neural information processing systems, pages
9291–9301, 2018. 

Document 4: 
 self-supervised learning and generative models, and we
will cover a self-supervised learning technique called Jig-
sawNet [108].
In Mean-Teacher [ 156], the teacher model is the average
of consecutive student models. Both the student and teacher
models evaluate the input in a training batch. The softmax
output of the student model is compared with the one-hot
label using a classiﬁcation cost. Additionally, the output is
compared with the teacher output using the consistency loss.
After the weights of the student models are updated via gra-
dient descent, the teacher model weights are updated as an
exponential moving average of the student model weights.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 803
Fig. 10 Example of the jigsaw puzzle task for a given unlabeled
image [108]
A training step with unlabeled examples is done without the
classiﬁcation cost.
In MixMatch [ 17], to exploit an unlabeled dataset, it per-
forms label guessing where stochastic data augmentation is
applied to an unlabeled imageK times; then, each augmented
image is fed through the classiﬁer. The average of these
K predictions is sharpened by adjusting the distribution’s
temperatures. The MixMatch algorithm mixes both labeled
examples and unlabeled example with label guesses. In more
detail, when mixing two images, the images are overlayed,
and the labels are averaged, following the MixUp augmen- 

Document 5: 
 which was veriﬁed via3 questions.
Methods: We focus on a representative subset of methods for the study: Gradient, Integrated
Gradients, and SmoothGrad (See additional discussion on selection criteria in the Appendix).
Bugs: We tested the bugs described in Table 1 along with a model with no bugs.
3 Debugging Data Contamination
Overview. We assess whether feature attributions can detect spurious training artifacts and mis-
labelled training examples. Spurious artifacts are signals that encode or correlate with the label in
the training set but provide no meaningful connection to the data generating process. We induce
a spurious correlation in the input background and test whether feature attributions are able di-
agnose this eﬀect. We ﬁnd that the methods considered indeed attribute importance to the image
background for inputs with spurious signals. However, despite visual evidence in the attributions,
5
participants in the human subject study were unsure about model reliability for the spurious model
condition; hence, did not out-rightly reject the model.
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
A
B
C
D
Figure 3: Feature Attributions for Spurious Correlation Bugs.Figure shows attributions 

Document 6: 
 3.2 Mislabelled Training Examples
Bug Implementation.We train a BVD-CNN model on a birds-vs-dogs dataset where10 percent
of training samples have their labels ﬂipped. The model achieves a93.2, 91.7, 88 percent accuracy
on the training, validation, and test sets.
Correct  
Label
Incorrect  
Label
Input
Correct  
Label
Incorrect  
Label
Figure 6: Diagnosing Mislabelled Training Examples.The Figure shows two training inputs
along with feature attributions for each method. The correct label row corresponds to feature
attributions derived from a model with the correct label in the training set. The incorrect-label row
shows feature attributions derived from a model with the wrong label in the training set. We see
that the attributions under both settings are visually similar.
Results. We ﬁnd that attributions from mislabelled examples for a defective model are visually
similar to attributions for these same examples but derived from a model with correct input labels
(examples in Figure 6). We ﬁnd that the SSIM between the attributions of a correctly labeled
instance, and the corresponding incorrectly labeled instance, are in the range0.73 −0.99 for all
methods tested. These results indicate that the attribution methods tested might be ineﬀective for
identifying mislabelled examples. We refer readers to Section I of the Appendix for visualizations
of additional examples.
Insights from Human Subject Study: users use prediction labels, not attribution meth- 

","XAI (explainable artificial intelligence) techniques can indeed be useful for detecting mislabeled observations in image classification tasks. Here are some techniques that can help:

1. **LIME (Local Interpretable Model-agnostic Explanations)**:
   - LIME generates local explanations by approximating the model’s behavior around a particular data point using simpler, interpretable models like linear regression.
   - By analyzing these explanations, you can identify if an image is being classified differently than expected. For mislabeled images, the explanation might show that certain features are not contributing to the correct classification.

2. **SHAP (SHapley Additive exPlanations)**:
   - SHAP provides a way to explain individual predictions by attributing a value for each feature in the model.
   - By examining these explanations, you can identify which parts of an image are driving the misclassification and determine if there might be a labeling issue.

3. **Integrated Gradients**:
   - Integrated gradients show how much the output changes as you change input features by integrating along the path from a baseline (e.g., all zeros) to the actual input.
   - This can help identify regions in an image that are contributing most to the model's prediction, allowing you to spot potential errors.

4. **Activation Atlases**:
   - Activation atlases show which parts of the image correspond to specific neurons or filters in a neural network.
   - By visualizing these activations, you can see if certain regions are consistently being misinterpreted by your model and thus might be causing mislabeling.

5. **Attention Mechanisms**:
   - In transformer-based models or attention layers in CNNs, the model assigns importance weights to different parts of an image.
   - Visualizing these attention maps can help identify if the model is focusing on irrelevant regions, which could indicate a labeling issue.

6. **Counterfactual Explanations**:
   - Counterfactual explanations generate modified versions of the input that would lead to a different prediction.
   - By creating counterfactuals for misclassified images and analyzing how small changes can make the model classify them correctly, you might identify areas where labels are incorrect.

7. **Pixel Attribution Methods**:
   - Techniques like Grad-CAM (Gradient-weighted Class Activation Mapping) highlight important regions in an image that contribute most to a specific prediction.
   - This can help spot if certain parts of the image are being misinterpreted and thus indicate potential labeling errors.

8. **Feature Importance Analysis**:
   - Analyzing which features or pixels are deemed most important by the model through feature importance analysis (e.g., using SHAP values, integrated gradients) can reveal patterns that align with human understanding or highlight areas where the model is struggling.
   
9. **Cluster-Based Techniques**:
   - By clustering similar images and analyzing clusters containing mislabeled instances, you can identify common characteristics that might suggest labeling errors.

10. **Adversarial Examples Analysis**:
    - Adversarial examples are inputs designed to cause a machine learning model to make incorrect predictions.
    - Analyzing adversarial examples can sometimes reveal that certain features or regions of an image are being misinterpreted, which may indicate mislabeling issues.

By leveraging these XAI techniques, you can gain insights into the decision-making process of your models and identify potential mislabeled images for re-evaluation.",15.994511604309082,"The context suggests that several explainable AI (XAI) techniques can be utilized to help detect mislabeled observations in image classification. Some of these methods include:

1. **Feature Attribution Methods**: Techniques like Gradient, Integrated Gradients, and SmoothGrad can be used to assess the importance of various features in the model's predictions. However, evidence from studies indicates that these feature attribution methods may struggle to effectively differentiate between correctly and incorrectly labeled instances, as the attributions derived from mislabeled data can be visually similar to those from correctly labeled instances.

2. **Diagnostic Models**: Training models on datasets with known proportions of mislabeled examples, such as a BVD-CNN on a birds-vs-dogs dataset with flipped labels, can help evaluate how models respond to mislabeling. This approach can provide insights into the model's performance and the impact of label noise.

3. **Spurious Correlation Detection**: Techniques that assess attributions for spurious signals (artifacts unrelated to true data generating processes) can be employed to identify misleading patterns in training data, including mislabeled examples.

These techniques highlight that while XAI methods can provide insights into model behavior, identifying mislabeled observations may require careful design and additional strategies beyond traditional attribution approaches."
qwen2.5:7b-instruct-q4_0,"How can deep learning methods be used in imputation of missing data?
","Document 1: 
 least square methods [23] for imputation. For large datasets, deep learn-
ing imputation techniques have gained popularity due to their performance
4
[24, 25, 26]. It’s important to note that different imputation methods may
produce varying values for each missing entry, making the modeling of un-
certainty for each missing value crucial. Bayesian and multiple imputation
techniques, such as Bayesian principal component analysis-based imputa-
tion [27] and multiple imputations using Deep Denoising Autoencoders [28],
are particularly useful in these scenarios. Furthermore, certain tree-based
methods can naturally handle missing data through prediction, including
missForest [29], the DMI algorithm [30], and DIFC [31]. Methods that can
manage mixed data types have also emerged, such as SICE [32], FEMI [33],
and HCMM-LD [34].
Although various missing data methods have been developed [35], most
of them to date do not have a built-in explanation mechanism for imputed
values, although how a value is imputed can have a profound impact on the
performance and interpretability of downstream machine learning models.
If one uses a prediction model such as a regression or classification model,
then one can use Shapley values or some other XAI technique to explain the
imputation of missing values. However, many times, one may want to use
a more complicated imputation method that requires building regression or
classification models and looping through the data multiple times to improve
imputation accuracy. 

Document 2: 
 forinitializationofthemissingvalues.Moreimportantly,almost
allofthesemethodsuseabottlenecklayerasthelatentspace,
whileweuseanencodinglayerofsizemuchbiggerthanthatof
inputwithtwopurposes:Firsttoensureabetterreconstruction
oftheinputandsecondthedataintheprojectedspaceofhigher
dimensionarelikelytobeflattened[51].In[51]authorsremarked
that unfolding of class manifolds using deep networks results
inhigherdimensionofthedatawhichmakestheclassesmore
separable.Moreover,likethesemethods,ourobjectiveisnotjust
therestorationofmissingvalues,buthowwecanimputemissing
valuessothatwecanusedatawithmissingvaluesindesigning
moreeffectiveclassifiers.
Theselforganizingmap(SOM)[30,31]andmultilayerpercep-
tron(MLP)[32]aretheothertwomachinelearningtechniques
that have also been used to impute missing values. There are
several other machine learning techniques to impute missing
data[52–56].Aknowledge-basedmethodhasalsobeendevel-
opedthatcandealwithmissingvalueswhichisnon-numericin
nature[57].Chih-Fongetal.[58]proposedamodifiedformofthe
meanimputationmethod.First,thedatasetisdividedintocom- 

Document 3: 
 SN Computer Science
[175] 2021 Springer Data imputation in wireless 
sensor network using deep 
learning techniques
Data Analytics and Manage-
ment
[176] 2020 Sustainable and Resilient 
Infrastructure
Handling incomplete and 
missing data in water net-
work database using imputa-
tion methods
Taylor and Francis
Page 28 of 37Emmanuel et al. J Big Data           (2021) 8:140 
missing to 5%, 10% and 15% across all observations. RMSE performance measure was 
then used to help quantify the accuracy of the estimated values compared to the actual 
non-missing data.
After simulation of missing values, KNN imputation was implemented to replace the 
missing values. Firstly, when implementing the imputation method, the nearest neigh -
bors (K) must be chosen. The value of K was chosen based on experimental results start -
ing with K = 1 and stopped at K = 5, the best accurate estimation value of K was then 
used for the experiment which was K = 4. The Euclidean distance measure was used on 
the KNN imputation algorithm. The  RF missForest algorithm was then implemented, 
which is a nonparametric imputation method based on the random forest. For every 
variable missForest fits a random forest on the observed values and then predicts the 
missing variables. The process of training and predicting of missForest is repeated in an 
iterative process until a number of iterations are reached. The missForest ran for three 

Document 4: 
 this in a concise way. This is the area to employ meta-learning model to capture these high-level interactions. In future
work, we aim to extend this analysis with adding new data sets and training surrogate model to deeper understanding
the complex interactions between the structure of missing data in tasks, the assumption of imputation method and
ML algorithms (Wo´ znica and Biecek, 2020). Another extension may be considering hyperparameters optimization in
machine learning models as well as imputation methods.
References
B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. Vanschoren. OpenML
Benchmarking Suites. 8 2017. URL http://arxiv.org/abs/1708.03731.
M. L. Brown and J. F. Kros. Data mining and the impact of missing data. Industrial Management and Data Systems,
103(8-9):611–621, 2003. ISSN 02635577. doi: 10.1108/02635570310497657.
T. Hastie, R. Mazumder, J. D. Lee, and R. Zadeh. Matrix Completion and Low-Rank SVD via Fast Alternating Least
Squares. Technical report, 2014.
J. Honaker, G. King, and M. Blackwell. Amelia II: A program for missing data. Journal of Statistical Software, 45(7): 

Document 5: 
 missing rate is high and the samples are inadequate. Another
solution is to ﬁll in the missing values with substituted val-
ues, which is known asdata imputation . However, these
methods do not capture variable correlations and may not
capture complex patterns to perform imputation. Combin-
ing the imputation methods with prediction models often
results in a two-step process where imputation and prediction
models are separated; the missing patterns are not effectively
explored in the prediction model, thus leading to suboptimal
analysis results.
GRU-D [29] is a deep learning model based on the gated
recurrent unit (GRU) to effectively exploit two representa-
tions of informative missingness patterns—masking and time
interval. Masking informs the model of which inputs are
observed or missing, while time interval encapsulates the
input observation patterns. GRU-D captures the observa-
tions and their dependencies by applying masking and time
interval, which are implemented using a decay term, to the
inputs and network states of the GRU, and jointly train all
model components through back-propagation. GRU-D not
only captures the long-term temporal dependencies of time-
series observations, but also utilizes the missing patterns to
improve the prediction results.
We elaborate on the two components of GRU-D: making
and time interval. The value of a missing variable tends to be
close to some default value if its last observation happened 

Document 6: 
 bayesian ridge regression, Symmetry 12 (2020) 1594.
[23] P. Keerin, W. Kurutach, T. Boongoen, An improvement of missing value
imputation in dna microarray data using cluster-based lls method, in:
2013 13th International Symposium on Communications and Informa-
tion Technologies (ISCIT), IEEE, 2013, pp. 559–564.
[24] S. J. Choudhury, N. R. Pal, Imputation of missing data with neural net-
works for classification, Knowledge-Based Systems 182 (2019) 104838.
[25] A. Garg, D. Naryani, G. Aggarwal, S. Aggarwal, Dl-gsa: a deep learning
metaheuristic approach to missing data imputation, in: International
Conference on Sensing and Imaging, Springer, (2018), pp. 513–521.
[26] K. Mohan, J. Pearl, Graphical models for processing missing data,
Journal of the American Statistical Association (2021) 1–42.
[27] V. Audigier, F. Husson, J. Josse, Multiple imputation for continuous
variables using a bayesian principal component analysis, Journal of
statistical computation and simulation 86 (2016) 2140–2156.
[28] L. Gondara, K. Wang, Multiple imputation using deep denoising au-
toencoders, arXiv preprint arXiv:1705.02737 (2017). 

","Deep learning methods can be effectively utilized for the imputation of missing data, especially when dealing with complex and high-dimensional datasets. Here are some ways to use deep learning techniques for this purpose:

1. Autoencoders:
   - Denoising Autoencoders: These models learn a compressed representation of the input data by reconstructing corrupted versions of it (where missing or noisy values have been replaced by zeros). They can then be used to predict and fill in missing values based on the learned latent space.
   - Variational Autoencoders: These are similar to traditional autoencoders but also capture the uncertainty in the representation through a probabilistic framework. This can help account for variations in data distribution during imputation.
2. Generative Adversarial Networks (GANs):
   - GANs consist of two components: a generator and a discriminator. The generator learns to create realistic synthetic samples that match the true data distribution, while the discriminator tries to distinguish between real and generated data. In the context of missing data imputation, you can train a GAN to generate missing values based on observed patterns in the dataset.
3. Recurrent Neural Networks (RNNs) / Long Short-Term Memory networks (LSTMs):
   - RNNs/LSTMs are particularly useful for sequential or time-series data where temporal dependencies play an important role. They can learn to predict missing values by considering context from previous and future observations in the sequence.
4. Convolutional Neural Networks (CNNs):
   - CNNs excel at analyzing spatial relationships, making them suitable for image-like structured data such as medical imaging or satellite imagery. For tabular data, you may need to reshape it into a grid format before applying CNNs.
5. Transformer-based models:
   - Transformers can be applied to text and sequential data by converting input features into embeddings. These embedding representations can then be used for predicting missing values.

6. Attention mechanisms:
   - Adding attention layers in deep learning architectures allows the model to focus on relevant parts of the input when making predictions, which can improve imputation accuracy in certain scenarios.

7. Multi-modal approaches:
   - Combining multiple types of models or techniques mentioned above can sometimes lead to better performance compared to using a single method. For example, combining an autoencoder with a GAN or using attention mechanisms alongside RNNs/LSTMs.

8. Handling high-dimensional data:
   - Deep learning models are well-suited for handling high-dimensional datasets and complex patterns present in such data. Techniques like Principal Component Analysis (PCA) can be used to reduce dimensionality before applying deep learning methods if necessary.
9. Handling categorical data:
   - For imputing missing values in categorical variables, one-hot encoding or embedding layers can be employed within the neural network architecture.

10. Handling time-series data with seasonality and trends:
    - Deep learning models like LSTMs can capture temporal dependencies and seasonal patterns when dealing with time-series datasets containing missing values.
11. Handling imbalanced missingness:
    - Techniques such as oversampling or using class weights during training can help address issues arising from imbalanced missingness in the dataset.

When applying deep learning techniques for missing data imputation, it's essential to:

- Prepare and preprocess your data appropriately (e.g., normalization, handling categorical variables)
- Validate model performance on separate validation/test sets
- Experiment with different architectures and configurations to find the best approach for your specific problem
- Be cautious about overfitting when dealing with small datasets or highly complex models

By leveraging these deep learning techniques, you can effectively handle missing data in various applications, leading to more accurate predictions and better decision-making.",17.146880388259888,"Deep learning methods can be utilized in the imputation of missing data through various techniques that leverage the capabilities of neural networks to model complex patterns and relationships within the data. For example, models such as Deep Denoising Autoencoders can be employed to perform multiple imputations by reconstructing missing values based on learned representations of the data. Additionally, Gated Recurrent Unit based models like GRU-D exploit informative missingness patterns by incorporating mechanisms for masking and time intervals, allowing them to capture long-term dependencies in time-series data and thus improve prediction results. Moreover, traditional deep learning architectures, such as multilayer perceptrons (MLP), and self-organizing maps (SOM) can also be adapted for the imputation task, alongside ensemble methods like random forest imputation (missForest), which iteratively trains on observed values to predict missing entries. Overall, deep learning approaches often allow for more effective handling of missing data by integrating the imputation process with classification or prediction models, facilitating better overall model performance."
llama3.1:latest,"What does the term ""learn to optimize"" mean?
","Document 1: 
 NatlSciRev ,2024,Vol.11,nwae132
Solver 4
Solver 1
Solver 2
Solver 3
Configuration space of algorithms
(solver class)
Facilitate the tedious trial-and-error process for
getting a good solver for an unseen instance
Solver 5
Unseen instances
Training set Solver class
Optimization problem class
Knowledge
Learning
Vehicle routing
problem
Machine learning
Traveling
salesman
problem
Problem Instance 1
Problem Instance 2
Problem Instance 3
Optimization problems
Figure 1 Illustration of the general idea of L2O. 
the same general paradigm that is very similar to 
the typical ML paradigm. That is, they leverage on 
a set of training instances from the target optimiza- 
tion problem class to gain something that would help 
alleviate the tedious algorithm configuration process 
on unseen problem instances, as i l lustrated in Fig. 1 . 
This article puts them under the umbrella term learn 
to optimize (L2O) and provides a brief overview on 
the general ideas as well as critical issues of L2O, in- 
cluding the training process, theoretical foundations 
as well as generalization issues. For more algorithmic 
details on NN-based solvers and heuristic solvers, 
readers are referred to [13 ,14 ] and [15 ,16 ], respec- 
tively. 

Document 2: 
 m∑
i=1
U(s, Xi),
where Xi is a random variable, representing an instance randomly drawn from X.
Notice that Um(s) is a random variable.
We deﬁne two properties of SPPAs, generalization and optimality. An SPPA
learning algorithm generalizes well, when the utility on all instances is close to the
utility on the training set. An SPPA learning algorithm is optimal, if the optimal
SPPA on the training set is close to the optimal result on the set of all instances.
These properties are formalized by the following deﬁnition.
Learning parallel portfolios of algorithms 99
Deﬁnition 6.1 We say that an SPPA learning algorithm mean-generalizes,i ff o ra n y
0 <ϵ and 0 <δ< 1 it outputs an SPPA s ∈ S,f o rw h i c h
P [Um(s) − E [U(s, X)] >ϵ ] ≤ δ.
Let the globally optimal algorithm be:
s∗ = arg max
s∈S
E [U(s, X)] .
We say that an SPPA learning algorithm ismean optimal,i ff o ra l l0 <ϵ and 0<δ< 1
it outputs a schedule s
P
[
E
[
U(s∗, X)
]
− E [U(s, X)] >ϵ 

Document 3: 
 model and select conﬁgurations, respectively.
Input :Target algorithm Awith parameter conﬁguration space Θ; instance set Π; cost
metric ˆc
Output :Optimized (incumbent) parameter conﬁguration, θinc
1 [R, θinc] ←Initialize(Θ, Π);
2 repeat
3 [M,tfit ] ←FitModel(R);
4 [ ⃗Θnew,tselect] ←SelectConﬁgurations(M, θinc, Θ);
5 [R,θinc] ←Intensify( ⃗Θnew, θinc, M, R, tfit + tselect, Π, ˆc);
6 until total time budget for conﬁguration exhausted;
7 return θinc;
parameter optimization (SPO) toolbox—which has received considerable attention in the
evolutionary algorithms community—provides many features that facilitate the manual
analysis and optimization of algorithm parameters; it also includes an automated SMBO
procedure for optimizing continuous parameters on single instances. We started our
own work in SMBO by comparing SKO vs SPO, studying their choices for the four
SMBO components [18]. We demonstrated that component Intensify mattered most, and
improved it in our SPO + algorithm [18]. Subsequently, we showed how to reduce the
overhead incurred by construction and use of response surface models via approximate
GP models. We also eliminated the need for a costly initial design by interleaving ran- 

Document 4: 
 this is to be taken as either the efficiency or the 
effectiveness of the solver.
r = n! Ts
Tns
The Challenges 
As presented in the introductory paragraphs, a 
divide-and-conquer approach can be lucky. A run 
can benefit from a good split, which brings a solu -
tion at the beginning of some subspace and allows 
for an early stop. In contrast, a different division 
can decrease performance. What is interesting here 
is that adding resources can decrease the perform -
ance since it can produce more demanding sub -
spaces. 
Even if portfolio-based approaches are less prone 
to this problem, extending the size of a portfolio 
can still be detrimental to its performance. In gen -
eral, this increases the overhead, due to more fre -
quent and broader clause sharing, and worsens 
cache-congestion issues. A priori, the question of 
deciding the most effective number of resources to 
use against a given formula is a difficult one. 
One possible direction of research is to extend 
automatic tuning techniques. These approaches 
use machine learning to craft a predictive function 
that relates the features of an instance and the 
parameters of a given solver to its expected run 
time. This function can be learned and tested 
offline against a large set of representative 
instances and used at run time to configure a solver 
and maximize its performance. This offline 
approach assumes that a large and representative 

Document 5: 
 OPT := inf
θ∈Θ
R(θ).
The search for the optimal conﬁguration is generally too ambitious, as the total runtime
required for the conﬁgurator must be extraordinarily large (possibly inﬁnite) to guarantee
that the best algorithm conﬁguration returned by the conﬁgurator is in fact the optimal
one with high probability.
As a workaround, one can leverage the idea underlying PAC learning (Valiant, 1984) to
the problem at hand. The basic idea is to relax the goal of ﬁnding the optimal conﬁguration
itself and, instead, ﬁnd a conﬁguration that is considered to be “good enough”. As there are
potentially several such “good enough” conﬁgurations 6, this relaxation of the goal allows
the search to be completed in less (and, thus, feasible) time. In this context, “good enough”
means that the expected runtime is only worse than the optimal expected runtime up to
a multiplicative factor of 1 + ε for some ﬁxed precision parameter ε > 0. Formally, a
conﬁguration is said to be ε-optimal (“good enough”) iﬀ
Ei∼P(c(i,θ)) ≤(1 + ε)OPT.
However, this relaxation of the target is problematic in the context of AC problems, since the
runtimes of conﬁgurations often exhibit a heavy-tailed distribution. Indeed, it is not diﬃcult
to construct an example based on such distributions in which any (sensible) conﬁgurator
would, in the worst case, take inﬁnitely long to ﬁnd an ε-optimal conﬁguration; see for 

Document 6: 
 tions of learning machines, which lead to objective 
functions that favor different optimization methods. 
As a result, many optimization techniques, including 
convex optimization methods [1 ], gradient descent 
[2 ], heuristic search [3 ] as well as evolutionary algo- 
rithms (E A s) [4 ], have been utilized by the ML com- 
munity in the past decades. 
Mainstream optimization algorithms are usu- 
ally highly configurable parameterized algorithms, 
i.e. they involve tunable parameters. Performance of 
such algorithms may vary significantly with different 
parameter settings. Hence, to solve the optimization 
problem induced by a learning task, tedious efforts 
are typically needed to configure the optimization al- 
gorithm. Consider the case of training an NN with 
stochastic gradient descent (SGD). It is widely ac- 
knowledged that the training performance is sensi- 
tive to the learning rate [5 ], a parameter of SGD. 
How to determine the optimal or at least appropri- 
ate learning rate has become a major hurdle for train- 
ing NNs, especially for large-scale NNs [6 ], or when 
the objective function is not an explicit differentiable 
function of the weights of an NN, as in the typical re- 
inforcement learning setting [7 ]. 
In order to reduce the human labor required, 
many studies have attempted to automate the config- 
uration of optimization algorithms [6 ,8 ]. However, 

","The term ""Learn to Optimize"" (L2O) refers to a general paradigm that leverages a set of training instances from an optimization problem class to gain knowledge that helps alleviate the tedious algorithm configuration process on unseen problem instances.",3.786114692687988,"The term ""learn to optimize"" (L2O) refers to a paradigm in which machine learning techniques are utilized to assist in the optimization process for various optimization problems. This approach leverages a set of training instances from a specific class of optimization problems to develop a solution that can alleviate the often tedious and lengthy trial-and-error process commonly associated with configuring algorithms for unseen instances. The overarching goal is to enhance the efficiency and effectiveness of solving optimization problems by using learned knowledge to predict or determine better algorithm configurations for new instances that have not been encountered before."
llama3.1:latest,"Please give some examples of metaheuristics.
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 aside any metaphorical language. We utterly believe that eﬀorts in 
this direction should be intensiﬁed, building upon the initial postula- 
tions established in some recent works [105,106] . Speciﬁcally, meta- 
heuristics components (including search operators and algorithmic 
behavioral patterns) and interfaces between them should be stan- 
dardized towards objectively assessing similarities and diﬀerences 
between metaphor-based solvers [107] . A novel metaphor is by no 
means a suﬃcient guarantee for a signiﬁcant scientiﬁc contribution. 
• The third of these reasons is methodological, i.e., the relevance of 
some of the building blocks of the overall algorithm. A particular 
algorithm can include a given component (for example, a local op- 
timizer) that can be of relevance even if the algorithm as a whole 
is not completely competitive with respect to the prevailing liter- 
ature. A good example supporting this claim can be observed in 
co-evolutionary frameworks, which usually include a procedure to 
identify the subcomponents that will be individually co-evolved. In 
those cases, even if the subcomponent optimizer is not very sophis- 
ticated, the co-evolutionary framework can be relevant by itself. In 
this sense, it is important to select the appropriate framework to 
highlight the desired characteristic of the proposed algorithm, as dis- 
cussed in Section 3.1 . Following the same example of subcomponents 
identiﬁcation, a researcher focused on large-scale global optimiza- 

Document 3: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

Document 4: 
 erations Research54(1):99–114.
Bartz-Beielstein, T. 2006. Experimental Research in Evolution-
ary Computation. Springer V erlag.
Battiti, R., and Brunato, M. 2005. Reactiv e search: machine
learning for memory-based heuristics. Technical Report DIT-05-
058, Universit`a Degli Studi Di Trento, Trento, Italy.
Birattari, M.; St¨utzle, T.; Paquete, L.; and V arrentrapp, K. 2002.
A racing algorithm for conﬁguring metaheuristics. In Proc. of
GECCO-02, 11–18.
Birattari, M. 2004. The Problem of Tuning Metaheuristics as
Seen from a Machine Learning Perspective. Ph.D. Dissertation,
Universit´e Libre de Bruxelles, Brussels, Belgium.
Coy, S. P .; Golden, B. L.; Runger, G. C.; and Wasil, E. A. 2001.
Using experimental design to ﬁnd effective parameter settings for
heuristics. Journal of Heuristics7(1):77–97.
E´en, N., and S¨orensson, N. 2003. An extensible SA T solver. In
Proc. of SAT-03, 502–518. 

Document 5: 
 art algorithm, MOS, has been clearly surprised by SHADE-ILS and 
MLSHADE-SPA, hence becoming the most competitive algorithms 
(with a preference by SHADE-ILS, by its better performance and sim- 
plicity). 
8. Conclusions and outlook 
In this work we have stressed on the need for circumventing com- 
mon mistakes and ﬂaws observed in the ﬁeld of bio-inspired optimiza- 
tion, particularly when new meta-heuristic algorithms are proposed and 
experimentally validated over benchmarks designed to this end. Specif- 
ically, we have reviewed and critically analyzed contributions deal- 
ing with experimental recommendations and practices related to meta- 
heuristics. Following our literature study, we have prescribed a set of 
methodological recommendations for preparing a successful proposal 
of bio-inspired meta-heuristic algorithms, from the deﬁnition of the ex- 
perimentation to the presentation of the results. A number of useful 
techniques (graphically summarized in Fig. 9 ) have been suggested for 
prospective studies to implement our proposed methodological frame- 
work, in an attempt at ensuring fairness, coherence and soundness in 
future studies on the topic. Two diﬀerent case studies have been de- 
signed to exemplify the application of our prescribed methodology, dis- 
cussing on the results of the application of each guideline. Although both 
case studies deal with well-known benchmarks, we envision that our 
methodology can be a core part of the design process of meta-heuristic 

Document 6: 
 achieved using several heuristic mechanisms that interact in complex, non-intuitive
ways. For example, a DPLL-style complete solver for SAT (a prototypical NP-
complete problem with important applications in the design of reliable soft- and
hardware) may use different heuristics for selecting variables to be instantiated and
the values ﬁrst explored for these variables, as well as heuristic mechanisms for
managing and using logical constraints derived from failed solution attempts. The
activation, interaction and precise behaviour of those mechanisms is often controlled
by parameters, and the settings of such parameters have a substantial impact on the
Holger H. Hoos
Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver,
BC, V6T 1Z4, Canada, e-mail: hoos@cs.ubc.ca
1 We note that the use of heuristic techniques does not imply that the resulting algorithms are nec-
essarily incomplete or do not have provable performance guarantees, but often results in empirical
performance far better than the bounds guaranteed by rigorous theoretical analysis.
Y . Hamadi et al. (eds.),Autonomous Search,
DOI 10.1007/978-3-642-21434-9 3,
© Springer-Verlag Berlin Heidelberg 2011
37
38 Holger H. Hoos
efﬁcacy with which a heuristic algorithm solves a given problem instance or class of 

","Examples of metaheuristics include Simulated Annealing (SA), Tabu Search (TS), Evolutionary Algorithms (EAs), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO).",3.147459030151367,"Examples of metaheuristics include Simulated Annealing (SA), Tabu Search (TS), Evolutionary Algorithms (EAs), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO)."
llama3.1:latest,"What is the ""no free lunch"" theorem about?
","Document 1: 
 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTA TION, VOL. 1, NO. 1, APRIL 1997 67
No Free Lunch Theorems for Optimization
David H. Wolpert and William G. Macready
Abstract— A framework is developed to explore the connection
between effective optimization algorithms and the problems they
are solving. A number of “no free lunch” (NFL) theorems are
presented which establish that for any algorithm, any elevated
performance over one class of problems is offset by perfor-
mance over another class. These theorems result in a geometric
interpretation of what it means for an algorithm to be well
suited to an optimization problem. Applications of the NFL
theorems to information-theoretic aspects of optimization and
benchmark measures of performance are also presented. Other
issues addressed include time-varying optimization problems and
a priori“head-to-head” minimax distinctions between optimiza-
tion algorithms, distinctions that result despite the NFL theorems’
enforcing of a type of uniformity over all algorithms.
Index T erms— Evolutionary algorithms, information theory,
optimization.
I. I NTRODUCTION
T
HE past few decades have seen an increased interest
in general-purpose “black-box” optimization algorithms
that exploit limited knowledge concerning the optimization
problem on which they are run. In large part these algorithms
have drawn inspiration from optimization processes that occur
in nature. In particular, the two most popular black-box 

Document 2: 
 function, and will produce .
The proof is completed if we show that there is no cost
function so that produces a sample containing and
and such that produces a sample containing and .
There are four possible pairs of samples to consider:
i) ;
ii) ;
iii) ;
iv) .
Since if its ﬁrst point is a , jumps to which is where
starts, when ’s ﬁrst point is a its second point must
equal ’s ﬁrst point. This rules out possibilities i) and ii).
For possibilities iii) and iv), by ’s sample we know that
must be of the form , for some variable . For
case iii), would need to equal , due to the ﬁrst point in
’s sample. For that case, however, the second point sees
would be the value at , which is , contrary to hypothesis.
For case iv), we know that the would have to equal , due
to the ﬁrst point in ’s sample. That would mean, however,
that jumps to for its second point and would therefore
see a , contrary to hypothesis.
Accordingly, none of the four cases is possible. This is
a case both where there is no symmetry under exchange of
’s between and , and no symmetry under exchange of
histograms.
A PPENDIX G 

Document 3: 
 The sum over can be accomplished in the same manner
is summed over. In fact, all the sums over all can
be done, leaving
(7)
In this last step, the statistical independence of and has
been used.
Further progress depends on whether represents or
. We begin with analysis of the case. For this case
, since only reﬂects cost
values from the last cost function, . Using this result gives
The ﬁnal sum over is a constant equal to the number of
ways of generating the sample from cost values drawn
from . The important point is that it is independent of
the particular . Because of this the sum over can be
evaluated eliminating the dependence
This completes the proof of Theorem 2 for the case of .
The proof of Theorem 2 is completed by turning to the
case. This is considerably more difﬁcult since
cannot be simpliﬁed so that the sums over cannot be
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 79
decoupled. Nevertheless, the NFL result still holds. This is
proven by expanding (7) over possible values
(8)
The innermost sum over only has an effect on the
, term so it contributes ,
. This is a constant, equal to . This
leaves
The sum over is now simple
The above equation is of the same form as (8), only with a
remaining sample of size rather than . Consequently, in
an analogous manner to the scheme used to evaluate the sums 

Document 4: 
 always use algorithm . As applied to this case, they
mean that for ﬁxed and ,i f does better (on average)
with the algorithms in some set , then does better (on
average) with the algorithms in the set of all other algorithms.
In particular, if for some favorite algorithms a certain “well-
behaved”results in better performance than does the random
, then that well-behaved gives worse than random behavior
on the set all remaining algorithms. In this sense, just as there
are no universally efﬁcacious search algorithms, there are no
universally benignwhich can be assured of resulting in better
than random performance regardless of one’s algorithm.
In fact, things may very well be worse than this. In super-
vised learning, there is a related result [11]. Translated into
the current context, that result suggests that if one restricts
sums to only be over those algorithms that are a good match
to , then it is often the case that “stupid” choosing
procedures—like the irrational procedure of choosing the
algorithm with the less desirable—outperform “intelligent”
ones. What the set of algorithms summed over must be in
order for a rational choosing procedure to be superior to an
irrational procedure is not currently known.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 77
VIII. C ONCLUSIONS
A framework has been presented in which to compare 

Document 5: 
 values after distinct cost evaluations produced by using a
particular instantiation of an evolutionary algorithm?”
At ﬁrst glance this seems to be an intractable question,
but the NFL theorem provides a way to answer it. This is
because—according to the NFL theorem—the answer must be
independent of the algorithm used to generate. Consequently,
3 In particular, one may want to impose restrictions on /80 /40 /102 /41 . For instance,
one may wish to only consider /80 /40 /102 /41 that are invariant under at least partial
relabeling of the elements in /88 , to preclude there being an algorithm that will
assuredly “luck out” and land on /109/105 /110 /120 /50/88 /102 /40 /120 /41 on its very ﬁrst query.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 73
we can choose an algorithm for which the calculation is
tractable.
Theorem 3: For any algorithm, the fraction of cost func-
tions that result in a particular histogram is
For large enough , this can be approximated as
where is the entropy of the distribution , and
is a constant that does not depend on .
This theorem is derived in Appendix C. If some of the are
zero, the approximation still holds, only with redeﬁned to 

Document 6: 
 to-head minimax distinctions. Such an analysis is beyond the
scope of this paper.)
On the other hand, having the sums be equal does carry ob-
vious implications for whether there are head-to-head minimax
distinctions. For example, if both algorithms are deterministic,
then for any particular
equals one for one pair and zero for all others. In such
a case, is just the number
of that result in the pair .S o
implies
that there are no head-to-head minimax distinctions between
and . The converse, however, does not appear to hold. 4
4 Consider the grid of all /40 /122/59 /122 /48 /41 pairs. Assign to each grid point the number
of /102 that result in that grid point’s /40 /122/59 /122 /48 /41 pair. Then our constraints are i)
by the hypothesis that there are no head-to-head minimax distinctions, if grid
point /40 /122 /49 /59/122 /50 /41 is assigned a nonzero number, then so is /40 /122 /50 /59/122 /49 /41 and ii) by
the no-free-lunch theorem, the sum of all numbers in row /122 equals the sum
of all numbers in column /122 . These two constraints do not appear to imply
that the distribution of numbers is symmetric under interchange of rows and 

","The ""no free lunch"" theorem, developed by David H. Wolpert and William G. Macready, states that for any optimization algorithm, its elevated performance over one class of problems is offset by poor performance over another class of problems. In other words, there is no single algorithm that consistently outperforms all others across all types of optimization problems.",3.897271156311035,"The ""no free lunch"" (NFL) theorem for optimization establishes that any optimization algorithm's elevated performance over a specific class of problems is counterbalanced by a corresponding decrease in performance over other classes of problems. This implies that no algorithm performs better than random search across all possible optimization problems; each algorithm may excel in particular scenarios but will underperform in others, resulting in an overall average performance that is uniform when considering all possible problems. Thus, the theorem highlights the limitations of universally effective optimization algorithms, emphasizing that there are no algorithms that can guarantee superior performance on all types of optimization tasks."
llama3.1:latest,"What is the concept behind Parallel Algorithm Portfolio?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 general-ized to denote a combination of different algorithms intended to outperform the search performance of any of its constituent algorithms [3, 6, 11-14]. Here we extend that formulation, so that an algorithm portfolio schedules its constituent algorithms to run concurrently on a set of processors. Let an algorithm be any CSP solver, as described in the previous section. Given a set A = {a1, a2, …, am} of m algorithms, a set P = {x1, x2, …, xn} of n problems, and a set of B consecutive time intervals T = {t1, t2, …, tB}, a simple schedule Sk for a prob-lem on a single processor specifies 

Document 4: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 5: 
 of processors [22], decomposition methods have shown their potential on many more processors (e.g., 64 cores or more in [19]). We will explore this in future work.  
9   Conclusions This paper presents WG, a constructor for non-parallel algorithm portfolios based on case-based reasoning and a greedy algorithm. It formulates parallel algorithm portfo-lio construction as an integer-programming problem, and generalizes WG to RSR-WG, a constructor for parallel algorithm portfolios based on a property of the optimal solution to the inherent integer-programming problem. To address a set of problems one at a time, RSR-WG creates portfolios of deterministic algorithms offline. Exper-iments show that the parallel algorithm portfolios produced by RSR-WG are statisti-cally significantly better than those produced by naïve parallel versions of popular portfolio constructors. Moreover, with only a few additional processors, RSR-WG portfolios are competitive with an oracle solver on a single processor.  Acknowledgements. This research was supported in part by the National Science Foundation under grants IIS-0811437, CNS-0958379 and CNS-0855217, and the City University of New York High Performance Computing Center. 

Document 6: 
 Ann Math Artif Intell (2006) 48:85–106
DOI 10.1007/s10472-007-9050-9
Learning parallel portfolios of algorithms
Marek Petrik ·Shlomo Zilberstein
Published online: 25 May 2007
© Springer Science + Business Media B.V. 2007
Abstract A wide range of combinatorial optimization algorithms have been devel-
oped for complex reasoning tasks. Frequently, no single algorithm outperforms all
the others. This has raised interest in leveraging the performance of a collection
of algorithms to improve performance. We show how to accomplish this using a
Parallel Portfolio of Algorithms (PPA). A PPA is a collection of diverse algorithms
for solving a single problem, all running concurrently on a single processor until
a solution is produced. The performance of the portfolio may be controlled by
assigning different shares of processor time to each algorithm. We present an
effective method for ﬁnding a PPA in which the share of processor time allocated
to each algorithm is ﬁxed. Finding the optimal static schedule is shown to be an
NP-complete problem for a general class of utility functions. We present bounds on
the performance of the PPA over random instances and evaluate the performance
empirically on a collection of 23 state-of-the-art SAT algorithms. The results show
signiﬁcant performance gains over the fastest individual algorithm in the collection.
Keywords Algorithm portfolios ·Resource bounded reasoning ·
Combinatorial optimization
Mathematics Subject Classiﬁcations (2000) 68T05 ·68T20 

","A Parallel Algorithm Portfolio is a collection of diverse algorithms that run concurrently on multiple processors to solve a single problem, with each algorithm receiving a fixed share of processor time. This approach aims to improve performance by leveraging the strengths of individual algorithms and combining them in parallel.",3.2058284282684326,"The concept behind a Parallel Algorithm Portfolio (PAP) involves leveraging multiple algorithms simultaneously to solve problems more efficiently than any single algorithm could on its own. A PAP consists of a diverse set of algorithms that run concurrently on a processor, each contributing to the overall problem-solving process. The performance of the portfolio can be enhanced by dynamically allocating processor time shares to each algorithm, which allows for optimal utilization of their strengths. The goal is to improve performance in solving complex reasoning tasks and combinatorial optimization problems by combining the strengths of various algorithms, especially when no single algorithm consistently outperforms the others. This approach allows for greater efficiency and improved outcomes compared to relying on a single algorithm or a naive combination of them."
llama3.1:latest,"Please provide some approaches to how Parallel Algorithm Portfolio can be constructed.
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately
allocate computational resources to different solvers, while the
parallel portfolios do not necessarily require any extra resource
allocation since each solver is simply assigned with the same
amount of resources. Third, a parallel portfolio could be eas-
ily converted to a sequential portfolio by using the algorithm
selection methods [39] to build selectors on the solvers in the
portfolio, which means the portfolios generated by construc-
tion approaches (e.g., the approach proposed in this article)
could be further used for constructing sequential portfolios.
In this article, we propose a novel approach called the
generative adversarial solver trainer (GAST) for the auto-
matic construction of parallel portfolios. Unlike the existing
construction approaches, GAST would generate additional
training instances and construct a parallel portfolio with the
dynamically changing training set. More speciﬁcally, GAST
puts instance generation and portfolio construction in an adver-
sarial game. The instance generation aims to generate the hard
problem instances that could not be solved well by the cur-
rent portfolio, while the portfolio construction aims to ﬁnd a
new component solver for the portfolio to better solve these 

Document 3: 
 1 for j := 1 ... n do
2 obtain portfolio c(j)
1:k by running AC on conﬁguration space ∏k
l=1{(c) | c ∈ C} on I using m for time t/n
3 choose ˆc1:k ∈ arg minc(j)
1:k |j∈{1...n} m(c(j)
1:k, I) that achieved best performance on I according to m
4 return ˆc1:k
3.1.1. Simultaneous conﬁguration of all component solvers (Global)
Our ﬁrst portfolio conﬁguration method is the straightforward extension of standard algorithm conﬁguration to the 
construction of a parallel portfolio (see Algorithm 1). Speciﬁcally, if the given solver has ℓ parameters, we treat the portfolio 
c1:k as a single algorithm with ℓ · k parameters inducing a conﬁguration space of size |C|k, and conﬁgure it directly. As 
noted above, we identify a single conﬁguration as the best of n independent runs of AC . These runs can be performed in 
parallel, meaning that this procedure requires wall clock time t/n if n machines—one for each AC run—with k cores are 
available. The used CPU time will be the given time budget t for Lines 1 and 2 in Algorithm 1 and some small overhead 
ϵ to choose the best portfolio in Line 3. The scalability of this approach is limited by the fact that the global conﬁguration 

Document 4: 
 Learning Algorithm Portfolios for Parallel Execution Xi Yun1 and Susan L. Epstein1, 2  1 Department of Computer Science, The Graduate School of The City University of New York, New York, NY 10016, USA 2 Department of Computer Science, Hunter College of The City University of New York, New York, NY 10065, USA xyun@gc.cuny.edu, susan.epstein@hunter.cuny.edu 
Abstract. Portfolio-based solvers are both effective and robust, but their prom-ise for parallel execution with constraint satisfaction solvers has received rela-tively little attention. This paper proposes an approach that constructs algorithm portfolios intended for parallel execution based on a combination of case-based reasoning, a greedy algorithm, and three heuristics. Empirical results show that this method is efficient, and can significantly improve performance with only a few additional processors. On problems from solver competitions, the resultant algorithm portfolios perform nearly as well as an oracle. Keywords: constraint satisfaction, algorithm portfolio, parallel processing, ma-chine learning. 

Document 5: 
 sign of parallel algorithm portfolios—sets of solvers that are run in parallel on a given instance of a decision problem, such 
as SAT, until the ﬁrst of them ﬁnds a solution [39,27].
In this work,1 we study generic methods for solving a problem we call Automatic Construction of Parallel Portfolios 
(ACPP): automatically constructing a static2 parallel solver from a sequential solver or a set of sequential solvers. This 
task can be understood as falling within the programming by optimization paradigm [34] in that it involves the design of 
software in which many design decisions have been deliberately left open during the development process (here exposed as 
parameters of SAT solvers) to be made automatically later (here by means of an automated algorithm conﬁgurator) in order 
to obtain optimized performance for speciﬁc use cases. Hence, all that is required by our ACPP methods is a sequential 
solver whose conﬁguration space contains complementary conﬁgurations.
We study three variants of the ACPP problem. First, we consider building parallel portfolios starting from a single, 
highly parametric sequential solver design. However, for well-studied problems (e.g., SAT), there often exist a wide range of 
different solvers that contribute to the state of the art (see, e.g., [73]). Complementarities among such solvers can be ex-
ploited by algorithm portfolios, whether driven by algorithm selection (like SATzilla [72]) or by parallel execution (such as 

Document 6: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

","Several approaches are mentioned in the text for constructing parallel algorithm portfolios. One approach is the Generative Adversarial Solver Trainer (GAST), which involves an adversarial game between instance generation and portfolio construction. Another approach is a straightforward extension of standard algorithm configuration to construct a parallel portfolio, where each component solver is conﬁgured independently using Algorithm Conﬁgurator (AC).",4.393380403518677,"Several approaches can be used for constructing Parallel Algorithm Portfolios, as highlighted in the provided context:

1. **Generative Adversarial Solver Trainer (GAST)**: This novel approach focuses on dynamic instance generation in conjunction with portfolio construction. GAST creates challenging problem instances that the current portfolio struggles to solve, then seeks to integrate new solvers into the portfolio to better address these difficulties.

2. **Case-Based Reasoning and Heuristic Combination**: An approach combining case-based reasoning with greedy algorithms and heuristics can efficiently construct parallel algorithm portfolios. This method has demonstrated substantial performance improvements, especially with the addition of a few processors.

3. **Automatic Construction of Parallel Portfolios (ACPP)**: This method involves using an existing sequential solver or multiple sequential solvers to build a static parallel portfolio. It exploits the complementary configurations of sequential solvers and applies automatic algorithm configuration to optimize performance for specific problems.

4. **Simultaneous Configuration**: A straightforward method extends the standard algorithm configuration approach to create a parallel portfolio. All component solvers are configured simultaneously, treating the portfolio as a single algorithm with multiple parameters, allowing the configuration to achieve enhanced performance.

5. **Parallel Execution of Solvers**: The fundamental concept of a parallel portfolio is to run multiple solvers in parallel on a given problem instance until one solver finds a solution. This allows for the leveraging of the strengths of various solvers collectively.

6. **Adaptive Mechanisms**: Incorporating adaptive strategies, like reactive search or hyper-heuristics, can help dynamically tailor the solver configurations during problem-solving, enabling the portfolio to adjust to the specific needs of different instances in real-time.

Overall, these methods emphasize the importance of parallelism and the strategic combination of different solving techniques to enhance computational efficiency."
llama3.1:latest,"Can co-evolution be used in Parallel Algorithm Portfolio construction?
","Document 1: 
 in a series of benchmark problems. Finally, the paper will end with
a brief conclusion and a discussion of future work in Section 5.
2 RELATED WORK
2.1 Portfolio of evolutionary algorithm
In the areas of evolutionary algorithms, algorithm portfolio is ap-
plied to increase the probability of finding a better solution by
allocating computational resources to several complementary algo-
rithms. The algorithm portfolio frameworks in the literature can
be classified into two categories as the parallel-based framework
and the sequential-based framework.
For the parallel-based framework, all candidates will run simul-
taneously in multiple sub-processes. Population-based algorithm
portfolio (PAP) is a typical example [16], which allocates compu-
tational resources before the optimization according to the prior
knowledge.Eachalgorithmhasitsownpopulationandevolveinde-
pendently,buttheinformationissharedamongdifferentalgorithms
bymigrationstrategy.Besides,otherparallel-basedportfolioframe-
works like AMALGAM-SO [21] and the UMOEAs [6] collect the
performance of algorithms during the optimisation process and
allocate more resources to the better algorithm.
On the other hand, the sequential based framework only runs
onealgorithmatmostofthetimeduringtheprocessofoptimisation.
Different from the parallel-based algorithm portfolio, this kind of
frameworktrytoselectthebestalgorithmindifferentoptimisation 

Document 2: 
 tic is likely to have the right properties for
this approach to be useful, thus making it
very general in terms of applications.
So far we have assumed that the com-
ponent algorithms are completely inde -
pendent of each other and do not com-
municate. They can be thought of as
“competing” with one another for ma-
chineresources.However,allowingforco -
operation or dependencies among the in-
dividualalgorithmswhiletheyarerunning
simultaneously can improve performance
(18, 19). This possibility raises the inter-
estingquestionoftheextenttowhichour
economics approach to portfolio algo-
rithms can also benefit from cooperation.
Basically, cooperation will introduce sta-
tistical correlations between the perfor -
mance of the individual algorithms, and
we will accordingly define the correlation
between them as
  
cov•t1,t2 
 var•t1 var•t2 
(3)
where cov(t1, t2) denotes the covariance of
theperformanceofthetwoalgorithms.The
effect of cooperation, when manifested in
negativecorrelations,istoincreasetheper-
formanceaswellasreducetherisk(Fig.5).
This change is easily understood in that
negative correlations mean that one algo-
rithmisparticularlygoodpreciselyonthose 

Document 3: 
 So far, we have shown that our procedures were able to construct effective parallel portfolios based on single solvers 
with rich design spaces. There is considerable evidence from the literature and from SAT competitions that strong portfolios 
can also be built by combining entirely different solvers in their default conﬁgurations (see, e.g., SATzilla [72], ppfolio [63]
and pfolioUZK [70]). For instance, ppfolio was obtained simply by combining the best solvers from the previous competition 
into a parallel portfolio. pfolioUZK included more state-of-the-art solvers from 2011 and relied on additional experiments 
to ﬁnd the best combination of solvers in a portfolio. Neither portfolio considers the conﬁguration space of the component 
solvers and therefore both can be seen as simple baselines for other parallelization approaches, including ours. However, 
ppfolio and pfolioUZK use Plingeling as a portfolio component. Since we aim to investigate the strength of our ACPP methods 
without additional human expert knowledge on parallel solving, we ﬁrst consider only sequential solvers as the basis for 
our ACPP approach. This section and the following section investigates the extension of our automatic techniques to the 
construction of portfolios based on the conﬁguration spaces spanned by such solver sets.
4.1. Approach
As long as all of our component solvers are sequential, we can simply use the ACPP procedures deﬁned in Section 3. 
We can accommodate the multi-solver setting by introducing a solver choice parameter for each portfolio component (see 

Document 4: 
 1
Few-shots Parallel Algorithm Portfolio Construction
via Co-evolution
Ke Tang, Senior Member, IEEE, Shengcai Liu, Member, IEEE, Peng Yang, Member, IEEE,
and Xin Yao, Fellow, IEEE
Abstract—Generalization, i.e., the ability of solving problem
instances that are not available during the system design and
development phase, is a critical goal for intelligent systems.
A typical way to achieve good generalization is to learn a
model from vast data. In the context of heuristic search, such a
paradigm could be implemented as conﬁguring the parameters
of a parallel algorithm portfolio (PAP) based on a set of
“training” problem instances, which is often referred to as PAP
construction. However, compared to traditional machine learning,
PAP construction often suffers from the lack of training instances,
and the obtained PAPs may fail to generalize well. This paper
proposes a novel competitive co-evolution scheme, named Co-
Evolution of Parameterized Search (CEPS), as a remedy to
this challenge. By co-evolving a conﬁguration population and an
instance population, CEPS is capable of obtaining generalizable
PAPs with few training instances. The advantage of CEPS in
improving generalization is analytically shown in this paper. Two
concrete algorithms, namely CEPS-TSP and CEPS-VRPSPDTW,
are presented for the Traveling Salesman Problem (TSP) and 

Document 5: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, co-evolution can be used in parallel algorithm portfolio construction. In fact, this paper proposes a novel competitive co-evolution scheme, named Co-Evolution of Parameterized Search (CEPS), specifically for this purpose. CEPS is capable of obtaining generalizable parallel algorithm portfolios with few training instances.",3.846722364425659,"Yes, co-evolution can be used in Parallel Algorithm Portfolio (PAP) construction. The context indicates that a competitive co-evolution scheme, specifically named Co-Evolution of Parameterized Search (CEPS), is proposed for this purpose. CEPS aims to enhance generalization in PAPs, which is particularly beneficial given the challenges of constructing effective portfolios from limited training instances. By co-evolving a population of configurations along with a population of problem instances, CEPS is capable of producing more generalizable PAPs, highlighting the advantage of this approach in constructing effective portfolios."
llama3.1:latest,"What is the idea behind the PAP approach called ""GLOBAL""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 RDW 13.24 1.38 11.0 24.1 11.5–16.5 %
LEUKOCYTES (%) 7.61 3.16 0.76 33.82 %
LEUKOCYTES 7614.73 3190.62 760.0 33820.0 3500–10500µL
that have led to the production of a choice in a speciﬁc exam-
ple. The notion of explanation in this work is aligned with
Samek et al. ( 2019).
As stated previously, it is possible to analyze a model rela-
tion between input and output in two ways: looking at local
explanations and global interpretation/explanations (Molnar
2019). Here, we use these two terms interchangeably . While
global interpretations enlighten scientists about what features
may be more representative to the model given a data sam-
ple, local explanations show what led the model to the current
output.
In our scope, global interpretations can help healthcare
researchers to question data sample quality and also direct
the search for more relevant features. Single explanations of
the COVID-19 diagnosis can assist healthcare professionals
that are working directly with potentially infected patients in
their decision-making.
7.1 Global Interpretation
For the glass-box models, global interpretation can be quali-
tatively evaluated by analyzing its feature weights. For these
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 633
Table 2 Results of experiments 

Document 3: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 6: 
 (CEC2005) [12], denoted asfcec1 −fcec14. These 27 functions
span a diverse set of problem features, such as multimodality,
ruggedness, ill-conditioning, interdependency, etc. They pro-
vided an ideal platform for our investigation on reducing risk
on a large variety of problems. Short descriptions of these
functions are presented in Tables I and II. More details of these
functions can be found in [11] and [12]. In our experiments,
all the functions were solved in 30 dimensions.
B. Experimental Settings
All the results presented in this paper were obtained by
executing 30 independent runs for each experiment. Since
we expect the PAP framework to be general enough so that
alternative algorithms can be incorporated with little effort,
it should not rely much on the reﬁnement of the constituent
algorithms. Hence, we did not ﬁne-tune the parameters of
the constituent algorithms to ﬁt PAP. When implementing
SaNSDE, we used all the parameter settings suggested in
the original publication [7]. As suggested in [8], a linearly
decreasing inertia weight over the course of the search is
employed in our implementation of wPSO. The two coefﬁ-
cients of wPSO were both set to 1.49445. We assumed the
researchers who proposed G3PCX and CMA-ES are at the
best position to implement the two algorithms and ﬁne-tune
the parameters. Hence, we simply used the source code of 

","I don't have information on a specific approach called ""GLOBAL"" related to the PAP approach. The PAP (Population and AIDS Programme) has been involved in various initiatives, but without more context or details about what you're referring to, it's challenging to provide a precise answer regarding the concept of ""GLOBAL"" within this framework.

If you have any more specific information or a different question, feel free to ask!",3.1005430221557617,"The idea behind the PAP (Population Allocation Paradigm) approach called ""GLOBAL"" relates to investigating the performance of the algorithm across a wider set of problems as opposed to focusing solely on singular optimization issues. PAP seeks to reduce the risk by applying various population-based search algorithms to multiple problems simultaneously, rather than optimizing for just one. This framework allows these algorithms to interact and share insights through a multipopulation model where computation time (measured by fitness evaluations) is intelligently allocated among different constituent algorithms. This collaboration enables the algorithms to leverage diverse strengths and promote cooperation, thereby enhancing overall optimization performance. The goal is to foster a more robust algorithm performance that can adapt to various challenges presented by a broad spectrum of problems."
llama3.1:latest,"What is the idea behind the PAP approach called ""PARHYDRA""?
","Document 1: 
 on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is
worthy of allocating the limited computation time to different
algorithms so that the overall performance can be enhanced.
In contrast, a speciﬁc topology/structure for the relationship
between subpopulations is usually explicitly deﬁned in existing
dEAs [26]–[29], [32], [33]. Third, we are more interested
in reducing the overall risk of an algorithm ona spectrum
of problems. This issue has never been investigated in the
literature of dEAs.
In addition to dEAs, another notable work that is related
to PAP is the isolation strategy employed by hierarchically
organized evolution strategies (HOES) [34], [35]. In HOES,
a population is partitioned into a number of subpopulations,
and ES is run on them with different control parameters (e.g.,
search step sizes). The isolation strategy works in such a
way that the subpopulations evolve separately for a predeﬁned
isolation period (e.g., a few generations), and then the control
parameters are updated based on the individuals generated in
the subpopulations during the isolation period. Unlike PAP,
the motivation of the isolation strategy is not to make use of 

Document 2: 
 SCENARIOS , THAT IS , SAT-SMALL-BIAS-1/2/3/4. T HE NAME OF THE
CONSTRUCTION APPROACH IS USED TO DENOTE THE PORTFOLIOS
CONSTRUCTED BY IT. “PARHYDRA-A” R EFERS TO PARHYDRA
CONFIGURING BASED ON AUGMENTED TRAINING SETS .AP A R - 1 0
SCORE IS SHOWN IN BOLDFACE IF IT WAS NOT SIGNIFICANTLY
DIFFERENT FROM THE BEST TEST PERFORMANCE IN THE SCENARIO
(ACCORDING TO A WILCOXON SIGNED -RANK TEST WITH p = 0.05)
To answer this question, in each of the eight SAT scenarios,
that is, SAT-SMALL/BIAS-1/2/3/4, we used the same instance
generation procedure as in GAST (lines 13–18 in Algorithm 1)
to generate a large set of instances based on the training set.
The size of the generated set is ﬁve times the size of the
training set. Recall that the training set contains 50 instances,
we thus obtained an augmented training set of 300 instances
in each SAT scenario, and then PARHYDRA was used to
construct a parallel portfolio on these augmented training sets,
and then the obtained portfolio was tested on the test sets. As
before, each portfolio was tested by running it on the test set 

Document 3: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 4: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 5: 
 be used to build parallel portfolios. (Building a parallel portfolio in this way has the added advantage that no instance 
features are required, since there is no need to select among algorithms.) We introduce some enhancements to this 
approach for the parallel portfolio setting (discussed in Section 3.1.2), and refer to our method as parHydra.
3. Some parallel solvers only achieve strong performance when running on more than one core; such solvers will not 
be found by a greedy approach like parHydra, which only adds one conﬁguration at a time and does not recognize 
interaction effects that arise between different threads of a parallel solver. To overcome this problem, we introduce a 
new method called parHydrab, which augments parHydra to train b solvers per iteration. This method trades off the 
computational beneﬁt of parHydra’s greedy approach with the greater coverage of Global.
We evaluated our ACPP methods on SAT. We chose this domain because it is highly relevant to academia and industry 
and has been widely studied. We thus had access to a wide range of strong, highly parametric solvers and were assured 
that the bar for demonstrating eﬃcacy of parallelization strategies was appropriately high. We note that our approach is not 
limited to SAT solvers and can be directly applied to other domains. To evaluate our methods in the single-solver setting, we 
studied both Lingeling and clasp: prominent, highly parametric state-of-the-art solvers for SAT. Lingeling won a gold medal in 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","I couldn't find any information on a concept or method specifically named ""PARHYDRA"" within the context of the PAP (Pareto-Optimal Approach, Parametric Analysis, and Problem-solving) methodology.

However, it is possible that PARHYDRA could be an acronym or name used in a specific application or domain outside of my current knowledge base. If you have more information about where you encountered this term or its context, I might be able to provide more insight or guidance on how to proceed with your inquiry.",3.44470763206482,"The PAP approach called ""PARHYDRA"" refers to a method for constructing parallel portfolios using augmented training sets. It focuses on creating a parallel portfolio by utilizing a large augmented training set generated from a smaller training set, thereby optimizing the performance of the portfolio on various instances. This approach is designed to leverage multiple algorithms simultaneously without relying on specific instance features, allowing for more effective performance in SAT scenarios. PARHYDRA enhances algorithm selection by training multiple solvers concurrently, recognizing interaction effects, and maximizing computational efficiency."
llama3.1:latest,"What is the idea behind the PAP approach called ""CLUSTERING""?
","Document 1: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 2: 
 proposed a big data k-means clustering, and a big data fuzzy k-means missing values 
approach that resulted in robust and efficient output for big data and offered reasonable 
execution times. The two imputation techniques surpassed in most cases mean imputa -
tion and elimination of the instances with lost values during classification. Offer robust 
and efficient results for Big Data datasets, offering reasonable execution times. The fuzzy 
k-means approach was proved to provide better results for high percentages of miss -
ing values in the data, while the k-means performed better with the dataset that had 
lower amounts of missing values. Zhang et al. [102], also proposed a multiple imputation 
clustering based approach that handled missing values in big longitudinal trial data in 
e-Health. The proposed concept proved that it could be easily adapted for different types 
of clustering for big incomplete longitudinal trial data in eHealth services.
Ensemble methods
Ensemble methods are strategies that make multiple models and then combine them to 
produce a single improved result. This method usually produces more precise results 
than a single model would. This has been the case in machine learning competitions, 
where the triumphant models used ensemble techniques [103]. Studies have confirmed 
that ensemble missing data handling algorithms outperform single base machine learn -
ing algorithms [104–108]. Also, ensemble methods can be implemented in parallel com -
puting environments, which are necessary to process missing data in big datasets. These 
ensemble algorithms are a group of techniques that their decisions are combined in a 

Document 3: 
 Built upon automatic algorithm conﬁguration, the automatic
construction of parallel algorithm portfolios (PAPs) [11]–[15]
seeks to identify a set of conﬁgurations to form a PAP. Each
conﬁguration in the PAP is called a component solver. To
solve a problem instance, all the component solvers are run
independently, typically in parallel, to get multiple solutions.
Then, the best solution will be taken as the output of the PAP.
Although a PAP would consume much more computational
resources than a single-conﬁguration solver, it has two im-
portant advantages. First, the performance of a PAP on any
given instance is the best performance achieved among its
component solvers on the instance. In other words, by ex-
ploiting the complementarity between the component solvers,
a PAP could achieve a much stronger overall performance than
any of its component solver. Second, considering the great
development of parallel computing architectures [16] (e.g.,
multi-core CPUs) over the last decade, exploiting parallelism
has become very important in designing efﬁcient solvers for
computationally hard problems. PAPs employ parallel solution
strategies, and thus allow exploiting modern high-performance
computing facilities in an extremely simple way.
From the practical point of view, a PAP construction method
is expected to identify a PAP that generalizes well, i.e.,
performs well not only on the instance set used during the 

Document 4: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 limited time budget. Although numerous algorithms are readily
applicable for this type of problems, their performance usually
varies signiﬁcantly from problem to problem. This implies that
there is an inherent risk associated with the selection of an
algorithm. Unfortunately, identifying a suitable (or optimal)
algorithm for a speciﬁc problem is a nontrivial task due to the
lack of prior knowledge. The limited time budget also prohibits
us from trying out different algorithms and then choosing the
best one. Instead of betting the entire time budget on a single
algorithm, we proposed that such a risk can be reduced by
distributing the time budget to multiple algorithms. Based on
this idea, a general framework called PAP has been proposed
in the context of population-based search algorithms. PAP
typically consists of a number of constituent algorithms, each
of which is allowed to run with a portion of the time budget.
Allocation of computation time is implemented by dividing the
whole population into a number of subpopulations, and main-
taining one for each constituent algorithm. To further boost
the performance, interaction among constituent algorithms is
carried out through regularly migrating individuals among the
subpopulations. We proposed a pairwise metric to compare
the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget. 

","The CLUSTERING approach groups the training instances into clusters, and then it constructs the PAP by identifying a component solver for each cluster. This allows the PAP to focus on solving specific sub-problems within the larger problem class.",3.4701037406921387,"The PAP approach called ""CLUSTERING"" is based on instance grouping. It involves splitting the training dataset into distinct groups or clusters before constructing the algorithm portfolio. This method focuses on organizing instances in a way that allows for more tailored and effective algorithm configurations, as it takes into account the similarities among the instances. By grouping similar instances together, the CLUSTERING approach aids in identifying which algorithms perform better on specific types of problems, thereby improving the overall performance of the PAP."
llama3.1:latest,"What is the idea behind the PAP approach called ""PCIT""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 them.
PCIT conducts the instance transfer with the help of in-
cumbent configurations (i.e., the best configurations found
by the AC procedure). In each subset, the instances which
cannot be solved satisfactorily by the corresponding incum-
bent are identified as the ones that need to be transferred,
and the target subset of each transferred instance is deter-
mined according to how well the incumbent on the candi-
date subset could perform on the instance. In essence, the
incumbent on a subset can be seen as a common special
characteristic of those “similar” instances (in the sense they
share the same high-quality configurations) within the sub-
set, and PCIT uses it to identify those “dissimilar” instances
and find better subsets for them. In each subset, the perfor-
mance of the incumbent on each instance could be obtained
from the rundata collected from the AC procedure runs.
However, while determining the target subsets for the trans-
ferred instances, how well the incumbents on the candidate
subsets would perform on the transferred instances are un-
known. One way to obtain these performances is to actually
test these incumbents on the transferred instances, which
however would introduce considerable additional compu-
tational costs. To avoid this, PCIT builds empirical perfor-
mance models (EPM) (Hutter et al. 2014) based on the col-
lected rundata to predict these performances.
1562
Algorithm Framework 

Document 3: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 4: 
 of the more state-of-the-art Plingeling. Such results indicate
PCIT could identify powerful parallel portfolios, with little
human effort involved. Thus the portfolios constructed by
PCIT could conveniently provide at least two advantages.
That is, they are high-quality parallel solvers, and they could
be used as starting points for the development of more ad-
Table 4: Test results of parallel solvers on the test set of SAT-
Single and SAT-Multi. The performance of a solver is shown
in boldface if it was not significantly different from the best
performance (according to a permutation test with 100000
permutations and significance level p = 0.05).
SAT-Single SAT-Multi
#TOS PAR-10 PAR-1 #TOS PAR-10 PAR-1
PCIT 181 119 21 35 1164 219
Priss6 225 146 25 - - -
PfolioUZK - - - 36 1185 213
Plinegling-bbc452 276 32 33 1090 199
vanced parallel solvers.
Conclusion
In this paper we proposed a novel ACPP method, named
PCIT, which utilized an instance transfer mechanism to im-
prove the quality of the instance grouping. The experimen-
tal results on two widely studied problem domains, SAT and
TSP, have demonstrated the effectiveness of PCIT. Currently
PCIT relies on the instance features to build the EPM. Since 

Document 5: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

",The PCIT approach is a method for constructing Parallel Algorithm Portfolios (PAPs) using an instance transfer mechanism to improve the quality of instance grouping. It builds empirical performance models based on instance features and uses these models to identify powerful parallel portfolios with little human effort involved.,3.896594285964966,"The PAP approach known as ""PCIT"" (Performance-based Instance Transfer) leverages an instance transfer mechanism aimed at enhancing the quality of instance grouping. It identifies instances that are poorly solved by existing configurations (the incumbents) and transfers these instances to more suitable subsets based on performance predictions. This is achieved by utilizing empirical performance models (EPM) to predict how well incumbents would perform on these transferred instances without the need for extensive additional computational costs. The effectiveness of PCIT has been demonstrated through experimental results on various problem domains, indicating that it can construct high-quality parallel solvers efficiently with minimal human intervention."
llama3.1:latest,"What is the idea behind the PAP approach called ""CEPS""?
","Document 1: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

Document 2: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 3: 
 this approach no longer guarantees the validity of the above
assumption. As a consequence, Θ′may generalize worse than
Θ. A remedy to prevent this as much as possible is to increase
redundancy in the evolution of the PAP. More speciﬁcally, one
could repeat the conﬁguration-removal procedure to Θ for n
times, leading to ntemporary PAPs, ¯Θ1,..., ¯Θn; then for each
temporary PAP ¯Θ, the new conﬁgurations are identiﬁed and
inserted, leading to n new PAPs, Θ′
1,..., Θ′
n, each of which
is of the same size as Θ; ﬁnally, the PAP among them that
performs best against T ∪T′ is retained.
III. C O-EVOLUTION OF PARAMETERIZED SEARCH
By incorporating the above-described procedure into the co-
evolution process, we arrive at the proposed CEPS framework,
as demonstrated in Algorithm 1. In general, CEPS consists
of two major phases, i.e., an initialization phase (lines 2-
7), and a co-evolution phase (lines 8-27) which could be
further subdivided into alternating between the evolution of
the conﬁguration population (representing the PAP) (lines 10-
15) and the evolution of the instance population (representing
the training instances) (lines 17-26) for MaxIte iterations in
total. These modules are detailed as follows. 

Document 4: 
 contributions of this paper include:
1) A novel PAP construction framework, namely Co-
Evolution of Parameterized Search (CEPS), is proposed.
It is also shown that CEPS approximates a process that
minimizes the upper bound, i.e., a tractable surrogate,
of the generalization performance.
2) To demonstrate the implementation details of CEPS as
well as to assess its potential, concrete instantiations are
also presented for two hard optimization problems, i.e.,
TSP and the Vehicle Routing Problem with Simultane-
ous Pickup–Delivery and Time Windows (VRPSPDTW)
[18]. Computational studies conﬁrm that CEPS is able
to obtain PAPs with better generalization performance.
3) The proposal of CEPS extends the realm of Co-
Evolution, for the ﬁrst time, to evolving algorithm
conﬁgurations and problem instances. Since CEPS does
not invoke domain-speciﬁc knowledge, its potential ap-
plications can go beyond optimization problems, even to
planning and learning problems.
The rest of the paper is organized as follows. Section II in-
troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 cores each (2.20 GHz, 30 MB Cache), running Centos 7.5.
D. Results and Analysis
We report the #TOs, PAR-10 and PANC achieved by the
PAPs on the testing set in Table II and also visualize their
medians and variance across all the testing instances by box-
plots in Figure 1. Note the mean value is also plotted in Figure
1 (indicated by “ ▲”) to show that for a PAP how its PAR-
10/PANC is affected by the outliers (the timeout cases) which
would be hidden by boxplots. In Table II the #TOs, PAR-
10/PANC of a PAP is highlighted in grey if it achieved the best
performance. One could make three important observations
from these results. First, the PAPs obtained by CEPS have
the smallest number of timeouts in all the six experiments,
which means they have the highest success rate for solving the
testing instances among all the tested PAPs. Recall that CEPS
actively searches in the instance space to identify the hard-
to-solve instances for further improving the generalization of
the PAPs. Such a mechanism makes CEPS the method that is
least affected by the hard testing instances which signiﬁcantly
differs from the given training instances. This could be further
veriﬁed by Figure 1, in which CEPS is the method that has
the least gap between the mean value (which takes timeouts
into account) and median value (which naturally ﬁlters out the 

","I don't have information on a specific concept or approach within the PAP (Public Assistance Program) framework known as ""CEPS."" It's possible that CEPS could refer to a specific program, method, or strategy within the context of public assistance, but I'm unable to provide details without more information. If you're looking for specifics about a particular aspect of public assistance or related programs, I'd be happy to try and help with general information or direct you to resources that might be helpful.",3.621979951858521,"The Co-Evolution of Parameterized Search (CEPS) is a framework designed for constructing Parameterized Algorithms (PAPs) that aims to improve their generalization performance across a set of problems rather than on a single one. The key idea behind CEPS is to co-evolve both the configurations of the algorithms and the problem instances, where the framework consists of two major phases: an initialization phase and a co-evolution phase. In this approach, the algorithm dynamically searches the instance space to identify challenging instances that can enhance the generalization capabilities of the obtained PAPs. CEPS utilizes historical data from the generated instances to assist in tuning parameters effectively, thereby enabling better performance even in a zero-shot learning setting. This allows CEPS to be applicable beyond traditional optimization problems, extending its potential to areas like planning and learning. Overall, CEPS seeks to minimize the upper bounds of generalization performance while reducing the risk across multiple problems."
llama3.1:latest,"Please give me the names of some state-of-the-art SAT solvers.
","Document 1: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 2: 
 Intelligence (AAAI’02), (pp. 674–681). AAAI Press / The MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building lo-
cal search sat solvers from components. In Proceedings of the Twenty-ﬁrst International Joint Conference
on Artiﬁcial Intelligence (IJCAI’09), (pp. 517–524).
Le Berre, D. & Simon, L. (2004). Fifty-ﬁve solvers in Vancouver: The SAT 2004 competition. In Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory and Applications of Satisﬁability Testing: Proceedings of the Seventh
International Conference (SAT’04), volume 3542 of Lecture Notes in Computer Science , (pp. 321–344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y . (2002). Learning the empirical hardness of optimization
problems: The case of combinatorial auctions. In Hentenryck, P. V . (Ed.), Principles and Practice of
Constraint Programming – CP 2002: Eighth International Conference , volume 2470 of Lecture Notes in
Computer Science, (pp. 556–572). Springer Verlag, Berlin, Germany. 

Document 3: 
 212
state-of-the-art SLS algorithms. SATenstein-LS has
41 parameters that control the selection and behavior of
its components, leading to a total conﬁguration space of
size 4.82 ×1012. Automatically-identiﬁed conﬁgurations of
SATenstein-LS have been shown to achieve better per-
formance than existing state-of-the-art SLS algorithms on
several well-known instance distributions.
Instances
We investigated the effectiveness of Hydra on four distri-
butions, drawing on well-known families of SAT instances.
Because no state-of-the-art SLS algorithms are able to prove
unsatisﬁability, we considered only satisﬁable instances. We
identiﬁed these by running all complete algorithms that won
a SAT competition category between 2002 and 2007 for
one hour. First, the BM data set is constructed from 500
instances taken from each of the six distributions used by
KhudaBukhsh et al. (2009) (
QCP, SWGCP, FACT, CBMC,
R3FIX, and HGEN), split evenly into training and test sets.
Second, the INDU data set is a mixture of 500 instances
from each of the CBMC and FACT distributions, again split
evenly into training and test sets. Third and fourth, the HAND
and RAND data sets include all satisﬁable instances from the
Random and Handmade categories of the SAT Competitions 

Document 4: 
 form (a conjunction of disjunctions of variables and their negations) and determine a satisﬁable assignment or prove that 
none exists. An incomplete SAT solver can ﬁnd satisfying assignments, but not prove unsatisﬁability.
Most state-of-the-art complete SAT solvers are based on conﬂict-driven clause learning (CDCL; [57]). Their parameters 
control variable selection for branching decisions, clause learning and restart techniques. State-of-the-art incomplete SAT 
solvers use stochastic local search (SLS; [38]), and their parameters control the selection of the variable whose value is 
modiﬁed in each local search step as well as the diversiﬁcation and additional intensiﬁcation strategies. Furthermore, there 
exist several preprocessing techniques (e.g., [21]) to simplify formulas and their parameters control how long and how 
aggressive preprocessing will be used – too much preprocessing can remove important structural information and hence, 
it can increase the hardness of formulas. The eﬃcacy of SAT solvers depends on multiple heuristic components whose 
basic functions and the interplay between them are controlled by parameters. Some parameters are categorical (e.g., choice 
between different search strategies in SLS), while many others are integer- or real-valued (e.g., the damping factor used in 
computing heuristic variable scores in CDCL).
Parallel SAT solvers have received increasing attention in recent years. ManySAT [30,31,29] was one of the ﬁrst parallel 

Document 5: 
 tial solver baselines, indicating the great benefit by combin-
ing complementary configurations obtained from a rich con-
figuration space.
Comparison with Hand-designed Parallel Solvers
To further evaluate the portfolios constructed by PCIT, we
compared them with the state-of-the-art manually designed
parallel solvers. Specifically, we considered the ones con-
structed for SAT. We chose Priss6 (Manthey, Stephan, and
Werner 2016) to compare with the one constructed in SAT-
Single, since Priss6 is the official parallel version of Riss6
(the base solver in SAT-Single). For the same reason, we
chose PfolioUZK (Wotzlaw et al. 2012) (the gold medal
winning solver of the parallel track of the SAT’12 Chal-
lenge) to compare with the one constructed in SAT-Multi.
Finally, we chose Plingeling (version bbc) (Biere 2016),
the gold medal winning solver of the parallel track of the
SAT’16 Competition, to compare with both. Note that all
the manually designed solvers considered here have imple-
mented far more advanced parallel solving strategies (e.g.,
clause sharing) than only independently running component
solvers in parallel. In the experiments the default settings of
these solvers were used and the same statistical tests as be-
fore were conducted. As shown in Table 4, on SAT-Single 

Document 6: 
 ACM 52(10):56–67.
Balyo, T.; Heule, M. J. H.; and J¨arvisalo, M., eds. 2016. Proceed-
ings of SAT Competition 2016: Solver and Benchmark Descrip-
tions, volume B-2016-1 of Department of Computer Science Se-
ries of Publications B. University of Helsinki.
Battiti, R.; Brunato, M.; and Mascia, F., eds. 2008. Reactive
Search and Intelligent Optimization. Springer.
Biere, A. 2016. Splatz, Lingeling, Plingeling, Treengeling, Yal-
SAT Entering the SAT Competition 2016. In Balyo et al. (2016),
44–45.
Burke, E. K.; Gendreau, M.; Hyde, M.; Kendall, G.; Ochoa, G.;
¨Ozcan, E.; and Qu, R. 2013. Hyper-heuristics: A Survey of the
State of the Art. Journal of the Operational Research Society
64(12):1695–1724.
Gomes, C. P., and Selman, B. 2001. Algorithm Portfolios. Arti-
ficial Intelligence 126(1-2):43–62.
Hamadi, Y ., and Wintersteiger, C. M. 2013. Seven Challenges in
Parallel SAT Solving. AI Magazine 34(2):99–106. 

","Some mentioned state-of-the-art SAT solvers include: 
Plingeling, Riss6, ManySAT, Priss6, PfolioUZK, and Lingeling.",3.112995147705078,"Some state-of-the-art SAT solvers mentioned in the context include Priss6, PfolioUZK, Plingeling, and SATenstein-LS. Other solvers are also referenced, such as Splatz, Lingeling, and Treengeling."
llama3.1:latest,"Please give me the names of some state-of-the-art TSP solvers.
","Document 1: 
 For surveys of the problem and its applications, the reader is referred to the excellent
volumes edited by Lawler et al. [24] and Gutin and Punnen [11].
Local search with k-exchange neighborhoods, k-opt, is the most widely used heu-
ristic method for the TSP. k-opt is a tour improvement algorithm, where in each step
k links of the current tour are replaced by k links in such a way that a shorter tour is
achieved.
It has been shown [7] that k-opt may take an exponential number of iterations and
that the ratio of the length of an optimal tour to the length of a tour constructed byk-opt
can be arbitrarily large when k ≤ n/2 − 5. Such undesirable cases, however, are very
rare when solving practical instances [31]. Usually, high-quality solutions are obtained
in polynomial time. This is, for example, the case for the Lin–Kernighan heuristic,
one of the most effective methods for generating optimal or near-optimal solutions
for the symmetric TSP. High-quality solutions are often obtained, even though only a
small part of the k-exchange neighborhood is searched.
In the original version of the Lin–Kernighan heuristic [ 25], the allowable
k-exchanges (or k-opt moves) are restricted to those that can be decomposed into a
2- or 3-exchange followed by a (possibly empty) sequence of 2-exchanges. This restric- 

Document 2: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 3: 
 10.0 
12.5 
15.0 
17.5 PAR-10 (c) TSP-3
Fig. 2: Visual comparison in boxplots of the medians and variance of the test performance of each TSP solver across the
testing instances. Note the mean value is also plotted, indicated by “ ▲”.
TABLE III: Comparison of the state-of-the-art TSP solvers
with the PAPs obtained by CEPS, on the testing set. #TOs
refers to number of total timeouts. PAR-10 is the penalized
average runtime-10. Performance of a solver is highlighted in
grey if it achieved the best testing performance.
TSP-1 TSP-2 TSP-3
#TOs PAR-10 #TOs PAR-10 #TOs PAR-10
LKH-default 131 30.84 137 31.98 150 34.73
LKH-tuned 29 8.23 34 9.40 27 7.67
EAX-default 69 17.98 73 18.91 69 17.95
EAX-tuned 33 10.97 30 10.38 29 10.12
VBS 6 3.82 7 4.26 6 4.13
CEPS 6 2.74 4 2.15 2 1.94
signiﬁcant performance improvement, though still obviously
falling behind of the PAPs obtained by CEPS. Second, the only
solver that could match the PAP’s performance level in one
of the three scenarios, is the VBS of the algorithm portfolio
considered by the algorithm selection approaches [29], [30].
However, in TSP-2 and TSP-3, the performance advantage of 

Document 4: 
 troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in
Section VI. Section VII concludes the paper with discussions.
II. P ARAMETERIZED SOLVERS MADE GENERALIZABLE
A. Notations and Problem Description
Assume a PAP is to be built for a problem class (e.g., TSP),
for which an instance of the problem class is denoted as s, and
the set of all possible sis denoted as Ω. Given a parameterized
algorithm, each component solver of the PAP is a conﬁguration
(full instantiation) of the algorithm. Generally speaking, the
parameterized algorithm can be any concrete computational
process, e.g., a traditional heuristic search process such as the
LK Heuristic for TSP or even a neural network [19]–[21] that
outputs a solution for a given instance of the target problem
class. Let θ denote a conﬁguration and let Θ denote a PAP
that contains K different conﬁgurations (component solvers),
i.e., Θ = {θ1,...,θ K}. The quality of a conﬁguration θ on a 

Document 5: 
 H. Trautmann, “Evolving diverse TSP instances by means of novel and
creative mutation operators,” in Proceedings of the 15th ACM/SIGEVO
Conference on Foundations of Genetic Algorithms, FOGA’2019. Pots-
dam, Germany: ACM, Aug 2019, pp. 58–71.
[38] Y . Nagata and S. Kobayashi, “A powerful genetic algorithm using edge
assembly crossover for the traveling salesman problem,” INFORMS
Journal on Computing , vol. 25, no. 2, pp. 346–363, 2013.
[39] X. Xie and J. Liu, “Multiagent optimization system for solving the
traveling salesman problem (tsp),” IEEE Transactions on Systems, Man,
and Cybernetics, Part B , vol. 39, no. 2, pp. 489–502, 2009.
[40] C. Wang, D. Mu, F. Zhao, and J. W. Sutherland, “A parallel simulated
annealing method for the vehicle routing problem with simultaneous
pickup-delivery and time windows,” Computers & Industrial Engineer-
ing, vol. 83, pp. 111–122, 2015.
[41] W. Huang and T. Zhang, “Vehicle routing problem with simultaneous
pick-up and delivery and time-windows based on improved global arti-
ﬁcial ﬁsh swarm algorithm,” Computer Engineering and Applications ,
vol. 52, no. 21, pp. 21–29, 2016. 

Document 6: 
 Portfolio-based algorithm selection for SAT,” Journal of Artiﬁcial In-
telligence Research, vol. 32, pp. 565–606, 2008.
[29] P. Kerschke, L. Kotthoff, J. Bossek, H. H. Hoos, and H. Trautmann,
“Leveraging TSP solver complementarity through machine learning,”
Evolutionary Computation, vol. 26, no. 4, pp. 597–620, 2018.
[30] K. Zhao, S. Liu, Y . Rong, and J. X. Yu, “Leveraging TSP solver
complementarity via deep learning,” arXiv preprint arXiv:2006.00715 ,
2020.
[31] L. Kotthoff, “Algorithm selection for combinatorial search problems: A
survey,” AI Magazine, vol. 35, no. 3, pp. 48–60, 2014.
[32] C. D. Rosin and R. K. Belew, “New methods for competitive coevolu-
tion,” Evolutionary Computation, vol. 5, no. 1, pp. 1–29, 1997.
[33] J. I. van Hemert, “Evolving combinatorial problem instances that are
difﬁcult to solve,” Evolutionary Computation , vol. 14, no. 4, pp. 433–
462, 2006.
[34] K. Helsgaun, “General k-opt submoves for the Lin-Kernighan TSP 

","Here are some state-of-the-art TSP (Traveling Salesman Problem) solvers:

1. **Concorde**: Developed by Applegate et al., Concorde is a widely-used TSP solver that uses branch and cut, a mathematical optimization technique to solve the problem exactly.
2. **LKH (Lin-Kernighan Heuristic)**: LKH is a popular heuristic algorithm for solving TSP efficiently. It's known for its high-quality solutions and fast running times.
3. **Chained Lin-Kernighan**: This is an improved version of LKH that uses a different move evaluation function to find better solutions.
4. **Genetic Algorithm (GA) solvers**: GA-based methods, such as the ones implemented in MATLAB's Global Optimization Toolbox or the GA library in Python, can be effective for solving TSP instances with large numbers of cities.
5. **Ant Colony Optimization (ACO)**: This metaheuristic algorithm is inspired by the behavior of ants searching for food and can be used to solve TSP efficiently.

Some specific software tools that implement these algorithms include:

* **Google OR-Tools**: An open-source operations research library that includes a TSP solver based on Concorde.
* **OSL (Optimization Software Library)**: A library developed by the University of California, Berkeley, that includes a TSP solver based on branch and cut.

Keep in mind that the performance of these solvers can depend on various factors, including the size and structure of the input instance, as well as the specific problem constraints.",9.108098268508911,"Some state-of-the-art TSP solvers mentioned in the context are LKH (Lin-Kernighan heuristic), EAX (Edge Assembly Crossover), and VBS (Variable Neighborhood Search)."
llama3.1:latest,"Please give me the names of some state-of-the-art Automatic Algorithm Configuration approaches.
","Document 1: 
 Journal of Artiﬁcial Intelligence Research 75 (2022) 425-487 Submitted 02/2022; published 10/2022
A Survey of Methods for Automated
Algorithm Conﬁguration
Elias Schede elias.schede@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Jasmin Brandt jasmin.brandt@upb.de
Alexander Tornede alexander.tornede@upb.de
Department of Computer Science,
Paderborn University, Paderborn, Germany
Marcel Wever marcel.wever@ifi.lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Viktor Bengs viktor.bengs@ifi.lmu.de
Institute of Informatics,
LMU Munich, Munich, Germany
Eyke H¨ ullermeier eyke@lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Kevin Tierney kevin.tierney@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews 

Document 2: 
 2 Related Work
2.1 Automatic Algorithm Conﬁguration
Several approaches exist in the literature for the automatic tuning
of algorithms. Some of these were created for a speciﬁc algorithm
or task. For example, [24] devises a modular algorithm for solving
constraint satisfaction problems (CSPs). Using a combination of ex-
haustive enumeration of all possible conﬁgurations and parallel hill-
climbing, the technique automatically conﬁgures the system for a
given set of training instances. Another approach, presented in [30],
focuses on the conﬁguration of adaptive algorithms, employing a se-
quential parameter optimization approach.
Other approaches automatically design and build an entire solver
to best tackle a set of example training instances. For example, [28]
uses genetic programming to create an evolutionary algorithm (EA).
Here the chromosome is an EA operation like the selection of par-
ents, mutation, or crossover, and the task is to ﬁnd a sequence of
the genetic programming operators that is best suited for the speci-
ﬁed problem. For SAT, [8] classiﬁes local search (LS) approaches by
means of context-free grammars. This approach then uses a genetic
programming approach to select a good LS algorithm for a given set
of instances.
There also exist approaches that are applicable to more general al-
gorithms. For example, in order to tune continuous parameters, [5]
suggests an approach that determines good parameters for individual
training instances. This approach ﬁrst evaluates the extreme param- 

Document 3: 
 470
A Survey of Methods for Automated Algorithm Configuration
General AC systems
D-SMAC https://github.com/tqichun/distributed-SMAC3
GPS https://github.com/YashaPushak/GPS
irace https://github.com/MLopez-Ibanez/irace
OAT (GGA) https://docs.optano.com/algorithm.tuner/current/
ParamILS https://www.cs.ubc.ca/labs/algorithms/Projects/ParamILS/
PyDGGA http://ulog.udl.cat/software/
REVAC https://github.com/ChrisTimperley/RubyREVAC
SMAC 3 https://github.com/automl/SMAC3
Benchmarks
AClib https://bitbucket.org/mlindauer/aclib2/src/master/
DAC https://github.com/automl/DAC
Table 7: List of available software in the realm of AC.
References
Belarmino Adenso-Diaz & Manuel Laguna. Fine-tuning of algorithms using fractional ex-
perimental designs and local search. Operations research, 54(1):99–114, 2006.
¨Ozg¨ ur Akg¨ un, Nguyen Dang, Ian Miguel, Andr´ as Z Salamon, & Christopher Stone. In- 

Document 4: 
 cal (e.g., choice of heuristic). Categorical parameters can be used to select and combine discrete
building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics); consequently,
our general view of algorithm conﬁguration includes the automated construction of a heuristic algo-
rithm from such building blocks. To the best of our knowledge, the methods discussed in this article
are yet the only general ones available for the conﬁguration of algorithms with many categorical
parameters.
We now give an overview of what follows and highlight our main contributions. After for-
mally stating the algorithm conﬁguration problem in Section 2, in Section 3 we describe ParamILS
(ﬁrst introduced by Hutter, Hoos & St ¨utzle, 2007), a versatile stochastic local search approach for
automated algorithm conﬁguration, and two of its instantiations, BasicILS and FocusedILS.
We then introduce adaptive capping of algorithm runs, a novel technique that can be used to
enhance search-based algorithm conﬁguration procedures independently of the underlying search
strategy (Section 4). Adaptive capping is based on the idea of avoiding unnecessary runs of the
algorithm to be conﬁgured by developing bounds on the performance measure to be optimized.
We present a trajectory-preserving variant and a heuristic extension of this technique. After dis-
cussing experimental preliminaries in Section 5, in Section 6 we present empirical evidence show-
ing that adaptive capping speeds up both BasicILS and FocusedILS. We also show that BasicILS
268 

Document 5: 
 rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 
Due to the separation between a tuning and a production phase, 
automatic algorithm conﬁguration is also known as oﬄine param- 
eter tuning to differentiate it from online approaches that adapt or 
control parameter settings while solving an instance [13,50] . Nev- 
ertheless, online approaches also contain parameters that need to 
be deﬁned oﬄine, for example, which and how parameters are 
adapted at run-time; such parameters and design choices can be 
conﬁgured by an oﬄine tuning method [59] . 
In our research on making the algorithm conﬁguration process 
more automatic , we have focused on racing approaches. Birattari 
et al. [19,20] proposed an automatic conﬁguration approach, F- 
Race, based on racing [64] and Friedman’s non-parametric two-way 
analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 

Document 6: 
 formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de-
veloped and used for more than a decade, and many of the fundamental techniques
date back even further. However, it has only recently become possible to effectively
solve complex conﬁguration problems involving target algorithms with dozens of
parameters, which are often categorical and conditional. This success is based in
3 Automated Algorithm Conﬁguration and Parameter Tuning 65
part on the increased availability of computational resources, but has mostly been
enabled by methodological advances underlying recent conﬁguration procedures.
Still, we see much room (and, indeed, need) for future work on automated algo-
rithm conﬁguration and parameter tuning methods. We believe that in developing
such methods, the fundamental features underlying all three types of methods dis-
cussed in this chapter can play an important role, and that the best methods will
employ combinations of these. We further believe that different conﬁguration pro-
cedures will likely be most effective for solving different types of conﬁguration
problems (depending, in particular, on the number and type of target algorithm pa-
rameters, but also on regularities in the parameter response). Therefore, we see a
need for research aiming to determine which conﬁgurator is most effective under 

","Some state-of-the-art Automatic Algorithm Configuration (AC) approaches mentioned in the text are:

1. ParamILS
2. BasicILS
3. FocusedILS
4. Iterated F-Race (I/F-Race)

These approaches are either variants of or based on the concept of racing, which is a popular method for automatic algorithm configuration.",4.063003778457642,"Some state-of-the-art Automatic Algorithm Configuration approaches include D-SMAC, GPS, irace, OAT (GGA), ParamILS, PyDGGA, REVAC, and SMAC 3."
llama3.1:latest,"What is the idea behind the AAC approach called ""irace""?
","Document 1: 
 analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 
the I/F-Race procedure is given in those publications, an imple- 
mentation was not made publicly available. The irace package im- 
plements a general iterated racing procedure, which includes I/F- 
Race as a special case. It also implements several extensions al- 
ready described by Birattari [19] , such as the use of the paired 
t -test instead of Friedman’s test. Finally, irace incorporates sev- 
eral improvements not published before, such as sampling from a 
truncated normal distribution, a parallel implementation, a restart 
strategy that avoids premature convergence, and an elitist racing 
procedure to ensure that the best parameter conﬁgurations found 
are also evaluated on the highest number of training instances. 
The paper is structured as follows. Section 2 introduces the 
algorithm conﬁguration problem and gives an overview of ap- 
proaches to automatic algorithm conﬁguration. Section 3 describes 
the iterated racing procedure as implemented in the irace pack- 
age as well as several further extensions including the elitist irace . 
Section 4 illustrates the steps followed to apply irace to two con- 

Document 2: 
 5.2. Multi-objective optimization metaheuristics 
Besides the application to the MOACO framework described 
above [58] , irace has been applied to aid in the design of other 
multi-objective optimization algorithms. Dubois-Lacoste et al. 
[31] used irace to tune a hybrid of two-phase local search and 
Pareto local search (TP + PLS) to produce new state-of-the-art al- 
gorithms for various bi-objective permutation ﬂowshop problems. 
Fisset et al. [33] used irace to tune a framework of multi-objective 
optimization algorithms for clustering. When applied to a suﬃ- 
ciently ﬂexible algorithmic framework, irace has been used to de- 
sign new state-of-the-art multi-objective evolutionary algorithms 
[16,17] . 
5.3. Anytime algorithms (improve time-quality trade-offs) 
There is often a trade-off between solution quality and com- 
putation time: Algorithms that converge quickly tend to produce 
better solutions for shorter runtimes, whereas more exploratory al- 
gorithms tend to produce better solutions for longer runtimes. Im- 
proving the anytime behavior of an algorithm amounts to improv- 
ing the trade-off curve between solution quality and computation 
time such that an algorithm is able to produce as high quality so- 
lutions as possible at any moment during their execution. López- 

Document 3: 
 effect on the budget consumed is lower. The same experiment for 
the ACOTSP scenario showed that the best conﬁgurations become 
worse when T new or T ﬁrst are increased. This is due to the fact that 
ACOTSP has a homogeneous training set and, therefore, sampling 
new candidates is more important than executing a large number 
of instances. 
5. Other applications of irace 
Since the ﬁrst version of the irace package became publicly 
available in 2012, there have been many other applications of 
irace . In this section, we provide a list of the applications of the 
irace package of which we are aware at the time of writing. Some 
of these applications go beyond what is traditionally understood as 
algorithm conﬁguration, demonstrating the ﬂexibility of irace . 
54 M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 
5.1. Algorithm conﬁguration 
The traditional application of irace is the automatic conﬁgura- 
tion of algorithms. Several publications have used irace when eval- 
uating or designing algorithms for problems such as the traveling 
salesman with time windows [60] , slot allocation [77] , generalized 
hub location [68] , ﬂow shop [14] , virtual machine placement [86] , 
online bin packing [92] , graph coloring [23] , image binarization 
[69] , network pricing [90] , combined routing and packing prob- 

Document 4: 
 ming by optimization paradigm [40] . Moreover, the importance of 
properly tuning the parameters of algorithms before analyzing and 
comparing them is becoming widely recognized. We hope that the 
development of the irace package will help practitioners and re- 
searchers to put these ideas into practice. 
Acknowledgements 
This work received support from the COMEX project (P7/36) 
within the Interuniversity Attraction Poles Programme of the Bel- 
gian Science Policy Oﬃce, and the EU FP7 ICT Project COLOMBO, 
Fig. A.1 1. Scheme of the user-provided components required by irace . 
Table A1 
Parameters of irace corresponding to the description 
of iterated racing given in Section 3.2 . The full list of 
irace parameters is available in the user guide. 
Iterated racing parameter irace parameter 
B maxExperiments 
C (cost measure) targetRunner 
μ mu 
N min minNbSurvival 
T ﬁrst firstTest 
T each eachTest 
Statistical test testType 
Cooperative Self-Organizing System for Low Carbon Mobility at 
Low Penetration Rates (agreement no. 318622). Thomas Stützle and 
Mauro Birattari acknowledge support from the Belgian F.R.S.-FNRS, 
of which they are Senior Research Associates. Leslie Pérez Cáceres 
acknowledges support of CONICYT Becas Chile. 
Appendix A. The irace package 

Document 5: 
 man problem with time windows: adapting algorithms from travel-time to 
makespan optimization. Appl Soft Comput 2013;13(9):3806–15 . 
[61] López-Ibáñez M., Dubois-Lacoste J., Pérez Cáceres L., Stützle T., Birattari M.. 
2016a. The irace package: Iterated racing for automatic algorithm conﬁgura- 
tion. http://iridia.ulb.ac.be/supp/IridiaSupp2016-003/ . 
[62] López-Ibáñez M, Pérez Cáceres L, Dubois-Lacoste J, Stützle T, Birattari M. The 
irace package: user guide. Technical Report TR/IRIDIA/2016-004. IRIDIA, Uni- 
versité Libre de Bruxelles, Belgium; 2016b . http://iridia.ulb.ac.be/IridiaTrSeries/ 
IridiaTr2016-004.pdf . 
[63] Marmion M-E , Mascia F , López-Ibáñez M , Stützle T . Automatic design of hy- 
brid stochastic local search algorithms. In: Blesa MJ, Blum C, Festa P, Roli A, 
Sampels M, editors. Hybrid metaheuristics, volume 7919 of lecture notes in 

Document 6: 
 ent robot behaviors and the criteria to transition between behav- 
iors. Each module can be further customized by means of several 
parameters. A particular combination of behaviors and transitions 
represents one controller, that is, an instance of the software that 
controls the robots in the swarm. The performance of a particu- 
lar controller is evaluated by means of multiple simulations. The 
search for the best controller over multiple training simulations is 
carried out by means of irace . The authors report that this system 
is not only able to outperform a previous system that used F-race 
[34] , but also a human designer, under the scenarios studied by 
them. 
6. Conclusion 
This paper presented the irace package, which implements the 
iterated racing procedure for automatic algorithm conﬁguration. It- 
erated racing is a generalization of the iterated F-race procedure. 
M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 55 
The primary purpose of irace is to automatize the arduous task of 
conﬁguring the parameters of an optimization algorithm. However, 
it may also be used for determining good settings in other compu- 
tational systems such as robotics, traﬃc light controllers, compil- 
ers, etc. The irace package has been designed with simplicity and 
ease of use in mind. Despite being implemented in R , no previ- 
ous knowledge of R is required. We included two examples for the 

","The irace package implements an iterative racing procedure, which is a generalization of Iterated F-race, for automatic algorithm configuration. It refines the sampling distribution by means of repeated applications of the F-Race method to find the best parameter configurations.",3.48583459854126,"The ""irace"" package implements an automatic algorithm configuration approach known as iterated racing. The key idea behind irace is to automate the process of tuning the parameters of optimization algorithms to improve their performance. It uses a statistical procedure to compare different parameter configurations, iteratively refining and sampling from the parameter space to identify the most effective settings. Irace incorporates various techniques such as parallel implementation, restart strategies to avoid premature convergence, and the ability to use different statistical tests for evaluating configurations. It has proven to be flexible and effective across various applications, including multi-objective optimization and robotics."
llama3.1:latest,"What is the idea behind the AAC approach called ""SMAC""?
","Document 1: 
 intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods
SMAC Sequential model-based optimization for algorithm conﬁguration (SMAC) (Hutter
et al., 2011; Lindauer et al., 2021) can be seen as one of the ﬁrst fully-ﬂedged model-
based AC approaches, as it features solutions for many of the limitations of the previously
discussed SMBO techniques. SMAC generalizes TB-SPO to perform conﬁguration over
multiple problem instances so that it can support categorical parameters and handle tight
time constraints.
To support multiple problem instances, SMAC adapts the intensiﬁcation strategy of
TB-SPO to iteratively evaluate conﬁgurations on randomly sampled combinations of seeds
and problem instances. When doing so, it ensures that conﬁgurations are compared only
based on a performance estimate computed on the same randomly sampled set of problem
instances. Furthermore, SMAC’s surrogate model can generalize across problem instances
by incorporating problem instance features. To this end, a surrogate model is learned on
the joint problem instance and conﬁguration space to predict the performance of a given
conﬁguration on a given problem instance.
As a means to deal with a mixture of categorical and numerical parameters, SMAC 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 algorithm configuration framework. JArtifIntellRes 2009; 36 : 
267–306. 
9. Lindauer M, Eggensperger K, Feurer M et al. SMAC3: a ver- 
satile Bayesian optimization package for hyperparameter opti- 
mization. JMachLearnRes 2022; 23 : 54. 
10. Santu SKK, Hassan MM, Smith MJ et al. Automl to date and 
beyond: challenges and opportunities. ACMComputSurv 2022; 
54 : 175. 
Page7of9
Downloaded from https://academic.oup.com/nsr/article/11/8/nwae132/7639406 by guest on 18 September 2024 

Document 5: 
 more benchmark instances and compare its performance to
the best conﬁguration observed so far.
SMAC is an anytime algorithm that interleaves the ex-
ploration of new conﬁgurations with additional runs of the
current best conﬁguration to yield both better and more con-
ﬁdent results over time. As all anytime algorithms, SMAC
improves performance over time. While SMAC provably
converges for ﬁnite conﬁguration spaces, it often only ﬁnds
close-to-optimal conﬁgurations for realistic time budgets
and challenging conﬁguration problems.
Automated Portfolio Construction & Hydra
Our method for constructing sequential portfolios is closely
related to Hydra (Xu, Hoos, and Leyton-Brown 2010),
which automatically constructs selection-based portfolios.
Given a parameterized algorithm framework with a space
of algorithmsΘ, Hydra starts with an empty set of can-
didate algorithms C= ∅and iteratively calls an algorithm
conﬁguration method to add algorithms θ ∈Θ that comple-
ment Cbest. In each iteration, it constructs a portfolio from
the current set Cusing the portfolio-based algorithm selec-
tor SATzilla (Xu et al. 2008) and measures its performance
m(C,π) on each benchmark instance π ∈Π (in the ﬁrst it-
eration, m(∅,π) = −∞). The performance metric Hydra
maximizes via its algorithm conﬁguration method is then 

Document 6: 
 Anastacio and Hoos (2020) propose SMAC+PS, which integrates the idea of probabilis-
tic sampling known from irace into SMAC. This enhancement yields improvements over
both SMAC and irace in many cases. In particular, Anastacio and Hoos (2020) account for
the problem that many of the completely randomly sampled conﬁgurations by SMAC often
exhibit rather bad performance and thus, their evaluation yields only limited information.
To this end, the authors suggest to sample conﬁgurations according to a truncated normal
distribution centered around the default conﬁguration.
444
A Survey of Methods for Automated Algorithm Configuration
In (Lindauer & Hutter, 2018b) the authors suggest two diﬀerent strategies to warmstart
model-based AC approaches and apply their suggestions to SMAC, leading to signiﬁcant
speedups from days to hours of conﬁguration time. The idea underlying warmstarting is to
use the evaluations of conﬁgurations from previous runs, i.e., on diﬀerent problem instance
sets, to speed up the conﬁguration process in new runs of the conﬁgurator on a new set of
instances.
Distributed SMAC (Hutter et al., 2012) (D-SMAC) is an extension of SMAC leveraging
parallelization to speed up the conﬁguration process. The main idea behind D-SMAC is
to parallelize target algorithm runs onto available workers as much as possible. For this
purpose, it maintains a queue of target algorithm conﬁguration evaluations to be performed, 

","The SMAC approach generalizes TB-SPO (time-bounded SPO) to perform configuration over multiple problem instances. It adapts an intensification strategy by iteratively evaluating configurations on randomly sampled combinations of seeds and problem instances. A surrogate model is learned to predict the performance of a given configuration on a given problem instance, incorporating problem instance features.",3.8043158054351807,"The idea behind the AAC (automatic algorithm configuration) approach called SMAC (Sequential Model-based Optimization for Algorithm Configuration) is to optimize the performance of algorithms by systematically searching for optimal hyperparameter configurations. SMAC achieves this by using a surrogate model that predicts the performance of different configurations based on previously evaluated instances, allowing it to generalize across multiple problem instances. It interleaves the exploration of new configurations with additional runs of the current best configuration, ensuring iterative improvement over time. SMAC also accommodates a mixture of categorical and numerical parameters, making it versatile for a variety of algorithmic applications. Additionally, enhancements like SMAC+PS and D-SMAC have been proposed to improve its performance and speed through more efficient sampling and parallelization strategies."
llama3.1:latest,"What is the idea behind the AAC approach called ""GGA""?
","Document 1: 
 to be evaluated based on k diﬀerently parameterized instantiations of the UCB criterion.
GGA++ Ans´ otegui et al. (2015) adapt the model-free AC approach GGA to include a
surrogate model. More precisely, the authors use a surrogate model to evaluate the quality
of new conﬁgurations. They integrate this within a crossover operator and call it genetic
engineering. Recall that GGA contains both a competitive and non-competitive popula-
tion in which winning conﬁgurations from the races between members of the competitive
population are recombined with individuals from the non-competitive population. To this
end, the crossover operator generates individuals according to the parameter tree crossover
of the original GGA method and evaluates them using the surrogate. Note that rather
than predicting the solution quality or runtime directly, the surrogate predicts the rank the
individual would have in a tournament. The individuals with the best ranks are accepted
into the population of the next generation in the same way as in GGA.
While the GGA++ surrogate is based on a random forest model, it diﬀers in a key
way. The premise of a random forest is to equally approximate the underlying function over
the complete input space. In the case of AC, this is undesirable as only the areas of the
input space that correspond to high-quality conﬁgurations are of interest. Thus, the authors
present specialized splitting criteria that focuses on only the best conﬁgurations to increase 

Document 2: 
 Our approach uses a similar insight as Bansal and Viswanathan (1993) and Chen and Ludvigson
(2009), who propose using a given set of conditional GMM equations to estimate the SDF with
neural networks, but restrict themselves to a small number of conditioning variables. In order to
deal with the inﬁnite number of moment conditions we extend the classical GMM setup of Hansen
(1982) and Chamberlain (1987) by an adversarial network to select the optimal moment conditions.
A similar idea has been proposed by Lewis and Syrgkanis (2018) for non-parametric instrumental
variable regressions. Our problem is also similar in spirit to the Wasserstein GAN in Arjosvky,
Chintala, and Leon (2017) that provides a robust ﬁt to moments. The Generative Adversarial
Network (GAN) approach was ﬁrst proposed by Goodfellow et al. (2014) for image recognition.
In order to ﬁnd the hidden states in macroeconomic time series we propose the use of Recurrent
Neural Networks with Long-Short-Term-Memory (LSTM). LSTMs are designed to ﬁnd patterns
4We have shared our data and estimated models with Avramov, Cheng, and Metzker (2020). In their comparison
study Avramov, Cheng, and Metzker (2020) also include a portfolio derived from our GAN model. However, they do
not consider our SDF portfolio based on ω but use the SDF loadings β to construct a long-short portfolio based on 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 instead of the fully predetermined lifetimes in the original
GA VaPS and nAMGA VaPS.
Another variation of the GA VaPS lifetime scheme was
described by Bäck et al .[ 18]. The adaptive population GA
(APGA) uses the same lifetime allocation but differs from
GA VaPS in the reproduction cycle and in that, when in-
crementing the ages of individuals in each cycle, the best
individual of that cycle remains unchanged. This adaptive
population strategy is part of an ensemble and is described
in more detail in Section V. This method was also applied to
cooperative coevolution by Iorio and Li [ 109]. An analysis
of APGA by Lobo and Lima [ 135] shows theoretical and
experimental results suggesting an upper bound and a con-
verging population to a constant size that is determined by
the minimum and maximum lifetime parameters. The authors
conclude that the population is not adapted by the APGA
but the size parameter is in fact replaced by the two lifetime
parameters.
Cook and Tauritz [36] suggested two strategies for removing
the population size parameter. FiScIS-EA removes individuals
according to a survival probability, derived by linear scaling
of the individual’s ﬁtness in the range between the minimum
and maximum ﬁtness values present in the population. GC-EA
simply evades choosing a population size by maintaining a
population as large as is allowed by memory limitations. This
requires a well-chosen parent selection mechanism that scales
well, while a survival selection operator is still needed in 

Document 5: 
 3.4.5.2 Generative Adversarial Network Imputation
GANs consist of two parts— a generator and a discriminator
(Goodfellow et al., 2014 ). In an adversarial process, the
generator learns to generate samples that are as close as
possible to the data distribution, and the discriminator learns
to distinguish whether an example is true or generated.
Imputation approaches based on GANs include those in the
work ofYoon et al. (2018); Shang et al. (2017);a n dLi et al.
(2019). Here, we employ one of the most popular approaches of
GAN-based imputation, Genera tive Adversarial Imputation
Nets (GAIN) (Yoon et al., 2018). GAIN adapts the original
GAN architecture as follows. The generator ’s input is the
concatenation of the input data and a binary matrix that
represents the missing values. The discriminator learns to
reconstruct the mask matrix. Its input is the concatenation of
the generator’s output and a hint matrix, which reveals partial
information about the missingness of the original data. The
computation of the hint matrix incorporates the introduced
hyperparameterhint rate. A second hyperparameter α that
GAIN introduces helps to balance the generator ’s
performance for observed and missing values. For training,
we use Adam optimizer with default hyperparameters except
for the learning rate for the generator and the discriminator, 

Document 6: 
 obtain promising initial results, the combined evolution of solutions and their genetic opera-
tors is comparatively challenging, and it remains an open question whether autoconstructive
evolution is indeed superior to non-autoconstructive evolution.
Multi-armed bandits DAC can be cast as a multi-armed bandit problem, where each
conﬁguration represents an arm with an associated reward distribution. The agent is then
tasked to maximize the cumulative reward, i.e., “pull the arm“ that maximizes the reward.
Put diﬀerently, the agent aims to select a conﬁguration that works well for the current time
step. This setting of the bandit problem is particularly challenging because the reward
distributions associated with the respective arms change over time. In Fialho et al. (2010),
various multi-armed bandit approaches are compared with an approach that only considers
rewards obtained within a certain time frame (i.e., a sliding window) to allow for smoother
transitions between conﬁgurations. Note that in this work, the authors assume rewards to
be retrievable during an algorithm run. While this assumption holds for many optimization
approaches, it does not necessarily hold for all of them (e.g., the pre-processing/root node
phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions 

","The GGA (Genetic Engineering) approach uses a surrogate model to evaluate solutions and a genetic engineering-inspired mechanism to combine them. It aims to adaptively construct effective configurations by combining existing ones, using a population of candidate configurations that evolve over time.",3.3636038303375244,"The GGA (Genetic Algorithm with Genetic Engineering) approach, as described in the context, involves enhancing the traditional genetic algorithm by integrating a surrogate model to evaluate the quality of new configurations. The surrogate model predicts the rank an individual would achieve in a tournament rather than directly predicting solution quality or runtime, which is important in finding high-quality configurations. GGA employs a competitive and non-competitive population structure, where successful configurations from competitions are recombined through a crossover operator. This adaptation aims to focus on high-performing individuals, utilizing specialized splitting criteria within a random forest model to refine the search process in automatic algorithm configuration."
llama3.1:latest,"What is the idea behind the AAC approach called ""ISAC""?
","Document 1: 
 ISAC – Instance-Speciﬁc Algorithm Conﬁguration1
Serdar Kadioglu and Yuri Malitsky and Meinolf Sellmann and Kevin Tierney2
Abstract. We present a new method for instance-speciﬁc algorithm
conﬁguration (ISAC). It is based on the integration of the algorithm
conﬁguration system GGA and the recently proposed stochastic off-
line programming paradigm. ISAC is provided a solver with cate-
gorical, ordinal, and/or continuous parameters, a training benchmark
set of input instances for that solver, and an algorithm that com-
putes a feature vector that characterizes any given instance. ISAC
then provides high quality parameter settings for any new input in-
stance. Experiments on a variety of different constrained optimiza-
tion and constraint satisfaction solvers show that automatic algorithm
conﬁguration vastly outperforms manual tuning. Moreover, we show
that instance-speciﬁc tuning frequently leads to signiﬁcant speed-ups
over instance-oblivious conﬁgurations.
1 Introduction
When developing a new heuristic or complete algorithm for a con-
straint satisfaction or a constrained optimization problem, we fre-
quently face the problem of choice. There may be multiple branch-
ing heuristics that we can employ, different types of inference mech-
anisms, various restart strategies, or a multitude of neighborhoods to
choose from. Furthermore, the way in which the choices we make
affect one another is not readily known. The task of making these 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 in Ye et al. (2021) that performance gains can already be achieved when the algorithm
conﬁguration is adapted only once. Furthermore, the hyper-reactive approach of Ans´ otegui
et al. (2017) won several categories at the MaxSAT Evaluation 2016 (Argelich et al., 2016).
Thus, DAC oﬀers signiﬁcant potential for improving algorithms, however, it does require
algorithm designers to more deeply integrate their techniques with AC methods than was
performed in the past. In the following, we discuss the most frequently used approach to
DAC, reinforcement learning (RL). While it is the most popular choice, there also exist
other approaches such as policy portfolios, autoconstructive evolution, and multi-armed
bandits.
Reinforcement learning A challenge in DAC is to evaluate the quality of individual
parameter adjustments since feedback is obtained only at the end of processing a problem
instance. Due to this, it is natural to consider DAC within the realm of reinforcement
learning (we refer to this as DAC-RL). To this end, the DAC problem is modeled in terms of
a Markov decision process (MDP) (Puterman, 1994) based on which a policy π: I×Q→ Θ
is learned from data (Biedenkapp et al., 2020). As a reward signal, typically, the eventually
obtained performance value is propagated back to the agent. Using RL allows a wide variety
of researched mechanisms to be directly applied to DAC.
The roots of DAC extend to a variety of methods that use RL to control parameters of 

Document 4: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 5: 
 ACPP is closely related to the area of automatic algo-
rithm configuration, in which the task is to automatically
identify a high-quality configuration from a configuration
space. In the last few years, several high-performance AC
methods (which could handle considerable large configura-
tion spaces) such as ParamILS (Hutter et al. 2009), GGA
(Ans´otegui, Sellmann, and Tierney 2009), irace (L ´opez-
Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and Leyton-
Brown 2011) have been proposed. As a consequence, re-
cently there has been research interest in utilizing these
AC procedures to automatically identify useful portfolios
of configurations from large configuration spaces. Such at-
tempts were first done in constructing sequential portfolios.
The representative methods for solving this problem are Hy-
dra (Xu, Hoos, and Leyton-Brown 2010) and ISAC (Ka-
dioglu et al. 2010). The basic ideas of these methods were
then adapted to be used in constructing parallel portfolios
(i.e., ACPP), thus resulting in PARHYDRA and CLUSTER-
ING (Lindauer et al. 2017) (see the first section). The main
differences between the automatic construction of sequen-
tial portfolios and of parallel portfolios lie in two aspects: 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The ISAC approach is an instance-specific algorithm configuration method. It provides high-quality parameter settings for any new input instance by integrating a feature vector characterization of each instance with a training benchmark set, allowing it to tailor its search based on the specific characteristics of the given problem instance.",3.5410826206207275,"ISAC, or Instance-Specific Algorithm Configuration, is an approach aimed at adapting algorithm parameters specifically for different instances of a problem. The fundamental idea is that ISAC leverages a training benchmark set of input instances and uses a feature vector to characterize any given instance. By doing so, ISAC provides high-quality parameter settings tailored to the specifics of new input instances. This method has been shown to outperform manual tuning and typically yields significant speed-ups compared to instance-oblivious configurations. In essence, ISAC aims to customize algorithm performance by aligning configurations closely with the characteristics of the problem instance at hand."
llama3.1:latest,"What is the difference between metaheuristics and heuristics?
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 principle difference between similarity and statistical learning (i.e., the two alternative approaches to evaluate the dif-
ference and divergence between two data items). Similarity can be defined over a pair of data items/samples while sta-
tistical measures require a large (theoretically infinite) number of independent data observations.
Another promising direction is to build Turing's type-B random machines (or unorganized machines) (Jiang &
Crookes, 2019; Webster, 2012), also random Boltzmann machines, which can possibly lead to a generalized AI. The
inclusion of new neuro-scientific findings into XAI models will make research on XAI more rationalized, and vice
versa: such a cross-disciplinary exploitation will make XAI not only meaningful for AI researchers but also help solve
century-old challenges on how to understand human intelligence, ultimately. Open research questions in this area
include: (i) how best to determine the network/model architecture?; (ii) how best to extract and represent features?;
(iii) what are the best distance metrics and what are the implications?; (iv) which is the best optimization method?; and
(v) how to determine the best set of prototypes that represent the data best (if a prototype-based method is being used)?
CONFLICT OF INTEREST
The authors have declared no conflicts of interest for this article.
DATA AVAILABILITY STATEMENT
Data sharing is not applicable to this article as no new data were created or analyzed in this study. 

Document 3: 
 the same standard. (In contrast, SAT solver competitions have compared solvers with a complex scoring function that includes the performance of all competitors [25].) As formulated here, the differences between two solvers may be simply in their choice of even a single technique, heuristic, or policy that sustains performance diver-sity. Thus an algorithm portfolio can be thought of as a mixture of experts [26], in-cluding variable-ordering and value-ordering heuristics, restart policies, and nogood learning methods. In particular, even if only one heuristic is available, the portfolio could consist of the heuristic and its opposite, or the heuristic and random selection. 

Document 4: 
 (Eds.), Handbook of Metaheuristics (pp. 321–353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search for classiﬁcation
and function approximation. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 59–66). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding the role of noise in stochastic local search: Analysis and experi-
ments. Artiﬁcial Intelligence, 172(8-9), 955–990.
Minton, S. (1993). An analytic learning system for specializing heuristics. In Bajcsy, R. (Ed.),Proceedings of
the Thirteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI’93), (pp. 922–929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically conﬁguring constraint satisfaction programs: A case study. Constraints,
1(1), 1–40.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conﬂicts: A heuristic repair 

Document 5: 
 PSO. 
For the methodological part of the comparisons, there are far more 
studies. Statistical tests, for instance, lay at the core of prior contribu- 
tions on this matter. However, such contributions are frequently writ- 
ten from a statistical point of view –l i k e the one by Dem š ar [39] –
making it diﬃcult for researchers in this ﬁeld to embrace their method- 
ological recommendations. More recently, some tutorials have tried to 
bring together the ﬁelds of meta-heuristics and inferential statistics [4] . 
Some examples can be found in [40] , in which a statistical treatment is 
suggested for distinguishing between measurements of performance in 
adaptive evolutionary algorithms. Another good example is [41] , which 
shows that in a popular real-parameter benchmark (CEC’2005), con- 
ditions needed for running parametric hypothesis tests did not hold, 
and non-parametric tests were thus recommended. More recently, in 
[42] , some recommendations for the comparison of evolutionary algo- 
rithms are provided, which can be even extrapolated to machine learn- 
ing benchmarks. 
Another important issue from a methodological point of view is the 
assessment of the performance of bio-inspired algorithms from the per- 
spective of the experimental design. Some studies [5] provide general 
recommendations to design experiments for the comparison of algo- 
rithms in a similar way to what we do in this contribution. However, 
these recommendations are far more general as it targets a broader scope 

Document 6: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

","Metaheuristics are high-level methodologies or general algorithmic templates that can solve a wide range of problems, whereas heuristics provide only sub-optimal solutions because they do not attempt to escape from local optimum.",3.4857640266418457,"The main difference between metaheuristics and heuristics lies in their scope and adaptability to specific problems. Heuristics are commonly problem-specific techniques that aim to produce solutions more quickly when classical methods are too slow. They typically focus on a specific problem domain and may produce sub-optimal solutions without addressing the possibility of escaping local optima.

On the other hand, metaheuristics are higher-level methodologies or algorithmic frameworks that can be applied to a wider range of problems. They do not adapt deeply to specific problems; instead, they provide a more generalized approach to problem-solving that allows for the exploration of the solution space and escaping local optima. Metaheuristics often incorporate stochastic components and are inspired by natural processes, making them versatile for tackling hard optimization problems across various domains."
llama3.1:latest,"What factors and challenges influenced the development of the Automatic Algorithm Configuration approaches?
","Document 1: 
 ods for algorithm development. The methods used in this context 
include experimental design techniques [2,29] , racing approaches 
[20] , and algorithmic methods for parameter conﬁguration, such 
as heuristic search techniques [3,10,41,73,81] , and statistical mod- 
eling approaches [11,43] . These methods have led to an increasing 
automatization of the algorithm design and parameter setting pro- 
cess. 
Automatic algorithm conﬁguration can be described, from a ma- 
chine learning perspective, as the problem of ﬁnding good param- 
eter settings for solving unseen problem instances by learning on 
a set of training problem instances [19] . Thus, there are two clearly 
delimited phases. In a primary tuning phase, an algorithm con- 
ﬁguration is chosen, given a set of training instances representa- 
tive of a particular problem. In a secondary production (or test- 
ing) phase, the chosen algorithm conﬁguration is used to solve un- 
seen instances of the same problem. The goal in automatic algo- 
rithm conﬁguration is to ﬁnd, during the tuning phase, an algo- 
rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 

Document 2: 
 diﬀerent problems (SAT, MIP, ASP, etc.) of varying complexity (number of variables and
problem instances) for tasks of runtime or quality conﬁguration. For DAC, the DACBench
has been proposed (Eimer et al., 2021), although this does not support DAC settings en-
visioned, e.g., by hyper-reactive search. As an alternative to such libraries, AC methods
can also be benchmarked by using surrogate models that are trained on test instances in
advance, resulting in cheaper evaluations when testing (Eggensperger et al., 2018). The ex-
isting benchmarks fail to cover other conﬁgurations settings like the realtime conﬁguration
setting or the conﬁguration of multi-objective target algorithms.
11.4 Novel AC Methodologies
AC methods have become extremely sophisticated and cover a wide range of methodologies
including evolutionary algorithms, statistical tests, and learned surrogate models. There
nonetheless remain opportunities to improve current methods and create better AC algo-
rithms. We note that our goal in this section is not necessarily to specify the methodologies
of the future, but rather to identify the key challenges that remain in the hopes that engaged
466
A Survey of Methods for Automated Algorithm Configuration
readers will ﬁll these gaps with their ideas. To this end, we discuss several challenges faced
by AC approaches: comparing/ranking conﬁgurations, generating new conﬁgurations, and
selecting instances.
Comparing/ranking conﬁgurations This challenge can be summarized as follows: 

Document 3: 
 available for detecting (approximate) independencies. Although there is some evi-
dence that GGA can solve some moderately difﬁcult conﬁguration problems more
effectively than FocusedILS without capping [2], it appears to be unable to reach
the performance of FocusedILS version 2.3 with aggressive capping on the most
challenging conﬁgurations problems [40]. Unfortunately, GGA also offers less ﬂex-
ibility than FocusedILS in terms of the performance metric to be optimised. More
algorithm conﬁguration procedures based on evolutionary algorithms are covered in
Chapter 2 of this book.
Finally, work originating from the Ph.D. project of Hutter [32] has recently over-
come two major limitations of the sequential model-based optimisation methods
discussed in Section 3.4 of this chapter by introducing a procedure that can han-
dle categorical parameters while explicitly exploiting the fact that performance is
evaluated on a set of problem instances. There is some evidence that this procedure,
dubbedSequential Model-based Algorithm Conﬁguration (SMAC) , can, at least on
some challenging conﬁguration benchmarks, reach and sometimes exceed the per-
formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de- 

Document 4: 
 Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews
do not take into account all derivatives of the AC problem, nor do they oﬀer a complete
classiﬁcation scheme. To this end, we introduce taxonomies to describe the AC problem and
features of conﬁguration methods, respectively. We review existing AC literature within
the lens of our taxonomies, outline relevant design choices of conﬁguration approaches,
contrast methods and problem variants against each other, and describe the state of AC in
industry. Finally, our review provides researchers and practitioners with a look at future
research directions in the ﬁeld of AC.
1. Introduction
Diﬃcult computational problems must be regularly solved in many areas of industry and
academia, such as constraint satisfaction problems, Boolean satisﬁability problems (SAT),
vehicle routing problems, ﬁnding a proper machine learning model for a given dataset,
or computing highly complex simulations. Algorithms that were developed to solve such
problems usually have parameters that strongly inﬂuence the behavior of the respective
algorithm and also, for example, the runtime that is required to solve problem instances
©2022 AI Access Foundation. All rights reserved.
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney 

Document 5: 
 better performance than the top-ranked solver from the competition.
9. Related Work
Many researchers before us have been dissatisﬁed with manual algorithm conﬁguration, and various
ﬁelds have developed their own approaches for automatic parameter tuning. We start this section
with the most closely-related work—approaches that employ direct search to ﬁnd good parameter
conﬁgurations—and then describe other methods. Finally, we discuss work on related problems,
such as ﬁnding the best parameter conﬁguration or algorithm on a per-instance basis, and approaches
that adapt their parameters during an algorithm’s execution (see also Hoos, 2008, for further related
work on automated algorithm design).
8. BasicILS was used, because FocusedILS had not yet been developed when that study was conducted.
297
HUTTER , HOOS , LEYTON -BROWN & ST ¨UTZLE
9.1 Direct Search Methods for Algorithm Conﬁguration
Approaches for automated algorithm conﬁguration go back to the early 1990s, when a number of
systems were developed for adaptive problem solving. One of these systems is Composer (Gratch
& Dejong, 1992), which performs a hill-climbing search in conﬁguration space, taking moves if
enough evidence has been gathered to render a neighbouring conﬁguration statistically signiﬁcantly
better than the current conﬁguration. Composer was successfully applied to improving the ﬁve
parameters of an algorithm for scheduling communication between a collection of ground-based
antennas and spacecrafts (Gratch & Chien, 1996). 

Document 6: 
 ration.
1 Introduction
We consider the problem of automatic solver conﬁguration. Practically all solvers have
parameters that are partly ﬁxed by the programmer and partly set by the user. In recent
years, systems have been devised which automate the task of tuning parameters for a
given set of training instances that are assu med to represent typical instances for the
target algorithm.
There are several motivations for such an automation, the ﬁrst being that it is of
course time consuming to tune parameters and it may lead to better results when leaving
the conﬁguration of solvers to a computer rather than doing it by hand.
Moreover, it is conceivable that the existence of an effective tuning environment
will cause algorithm developers to paramete rize more aspects of their algorithms and
thus leave more freedom for algorithmic solutions that are automatically tailored to the
problems of individual users. In particular, many of the SAT solvers that are available
today have parameters which cannot be set through the command line. These parameters
have been ﬁxed to values that the developers have found beneﬁcial without knowledge
about the particular instances a user may want to use the solver for. Automatic parameter
tuning allows solvers to adapt to the ﬁnal environment in which they need to perform.
After being shipped, rather than relying on default parameters, an algorithm can be
⋆ This work was partly supported by the projects TIN2007-68005-C04-04 and TIN2006-15662-
C02-02 funded by the MEC, and by the the National Science Foundation through the Ca-
reer: Cornﬂower Project (award number 0644113). 

","The development of Automatic Algorithm Configuration (AAC) approaches has been influenced by several factors and challenges. Here are some key ones:

**Factors that influenced AAC development:**

1. **Increased complexity of optimization problems**: As optimization problems became more complex, manual tuning of algorithm parameters became a significant bottleneck.
2. **Growing demand for high-performance computing**: The need to solve large-scale optimization problems efficiently drove the development of automated configuration techniques.
3. **Advances in machine learning and artificial intelligence**: Breakthroughs in machine learning and AI have enabled the creation of sophisticated AAC approaches that can learn from data and adapt to changing problem characteristics.
4. **Increased availability of computational resources**: The advent of high-performance computing, cloud computing, and distributed computing has made it possible to perform extensive experimentation and evaluation of AAC methods.

**Challenges in AAC development:**

1. **Scalability and efficiency**: AAC approaches need to be able to handle large-scale optimization problems while maintaining reasonable computational costs.
2. **Optimization problem diversity**: AAC methods must be able to adapt to a wide range of optimization problems, each with its unique characteristics and requirements.
3. **Limited understanding of algorithm behavior**: The underlying mechanisms governing the performance of algorithms are not always well understood, making it challenging to develop effective AAC approaches.
4. **No free lunch theorem**: There is no single optimal configuration that works best for all problem instances, requiring AAC methods to be flexible and adaptable.
5. **Trade-offs between objectives**: Optimization problems often involve multiple conflicting objectives, such as minimization of computational cost vs. maximization of solution quality.
6. **Interpretability and explainability**: As AAC approaches become more complex, there is a growing need for techniques that provide insight into their decision-making processes.

**Key challenges in developing AAC methods:**

1. **Choosing the right optimization algorithm**: Selecting an appropriate algorithm from the vast array of options available can be a daunting task.
2. **Configuring hyperparameters effectively**: Hyperparameter tuning is often performed using simple, non-informed methods that may not lead to optimal results.
3. **Handling noisy and uncertain data**: AAC approaches must be able to handle noisy or uncertain input data, which can significantly impact their performance.

To overcome these challenges, researchers have developed a range of AAC approaches, including:

1. **Black-box optimization methods**, such as Bayesian optimization and random search
2. **Model-based optimization methods**, which use mathematical models to guide the configuration process
3. **Surrogate-assisted optimization methods**, which use approximations or surrogates of the objective function to accelerate the configuration process

The development of AAC approaches continues to be an active area of research, with ongoing efforts focused on improving their scalability, efficiency, and adaptability to diverse optimization problems.",15.539040803909302,"Factors influencing the development of Automatic Algorithm Configuration (AC) approaches include the need for efficient tuning of algorithm parameters to enhance performance and runtime for diverse computational problems. These problems often exhibit complexity due to the variation in the number of variables and instances.

Challenges faced by AC include:

1. **Comparing and Ranking Configurations**: Ensuring that different configurations can be effectively evaluated and ranked based on performance.

2. **Generating New Configurations**: Developing methods to produce novel configurations that may outperform existing ones.

3. **Selecting Instances**: Identifying which problem instances to use for tuning the configurations so that they generalize well to unseen instances.

Additionally, the advancement of methodologies like evolutionary algorithms, surrogate models, and sequential model-based optimization has improved AC, but there are still opportunities for refinement and development in areas not comprehensively covered by existing benchmarks and methodologies."
llama3.1:latest,"What are the main benefits of AAC?
","Document 1: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 2: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 3: 
 phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions
with three themes in mind. First, we consider what is required for AC techniques to be
more widely adopted in industry. Second, we investigate algorithm conﬁguration settings
that have not yet been considered. Third, we consider what methodological advances could
move the ﬁeld forward.
11.1 Industry Adoption
Compared to AC, HPO has begun to be adopted in both industry and academia (Van der
Blom et al., 2021). Furthermore, HPO is the focus of a number of start-ups, such as
SigOpt (which was acquired by Intel (Intel Corporation, 2019)), MindsDB, pecan.ai, and
dotData, among many more. AC, however, has received comparatively little fanfare or
adoption. While some well-known AC tools have public releases, such as SMAC (Lindauer
et al., 2021), irace (L´ opez-Ib´ anez et al., 2016), and GGA (Ans´ otegui et al., 2009), we are
unaware of widespread use of these in industry. The company Optano GmbH released a 

Document 4: 
 a policy to adapt the conﬁguration dynamically. Note that even realtime AC commits to a
single conﬁguration when running a given instance, while DAC has the freedom to adjust
the conﬁguration according to target algorithm behavior during execution. Similar to oﬄine
AC, DAC can either focus on ﬁnding a policy for a set of problem instances or a policy that
is tailored towards a single problem instance (i.e., per-instance algorithm conﬁguration).
Two requirements must be met to implement DAC: (1) the algorithm in question needs
to support dynamic changes in its conﬁguration and (2) runtime information must be pro-
vided to describe the current state of the target algorithm.
DAC approaches consider two diﬀerent types of features: instance features I, which do
not change during target algorithm execution, and features encoding the internal state Q
of the algorithm. Examples of state features include the current iteration of a local search
algorithm, the current restart number of a SAT method, or the current solution quality for
optimization techniques.
461
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
Figure 7: Illustration of dynamic AC as presented in (Biedenkapp et al., 2020)
Biedenkapp et al. (2020) provide the ﬁrst formal deﬁnition of the DAC setting, however,
there is a signiﬁcant amount of earlier work for learning dynamic conﬁguration policies
(Lagoudakis & Littman, 2000, 2001; Pettinger & Everson, 2002). Such earlier works use 

Document 5: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 6: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

","Augmentative and Alternative Communication (AAC) refers to methods used to support individuals who face difficulty with verbal communication. The primary objectives and benefits of using AAC include:

1.  **Facilitating Communication**: AAC enables users to convey their needs, wants, thoughts, and feelings more effectively than before. This can involve typing on a computer or tablet, using a speech-generating device (SGD), picture communication symbols (PCS) or gestures.

2.  **Improving Independence**: With the ability to communicate, individuals with disabilities can express themselves directly to others without needing someone else to interpret their needs for them. This enhances their autonomy and independence in daily life.

3.  **Enhancing Social Interaction**: Effective communication through AAC enables individuals to engage more meaningfully with others, build relationships, and maintain social connections. It contributes to better emotional well-being and a sense of belonging.

4.  **Educational Benefits**: In educational settings, AAC can facilitate learning by allowing students to communicate their understanding or needs directly to teachers and peers. This often leads to improved academic performance and participation in classroom activities.

5.  **Reducing Frustration and Anxiety**: By enabling individuals to express themselves, AAC can reduce feelings of frustration, anxiety, and powerlessness associated with not being able to communicate effectively.",7.478215932846069,"The main benefits of Automatic Algorithm Configuration (AAC) include:

1. **Performance Improvement**: AAC methods enhance the performance of algorithms across various domains, such as the boolean satisfiability problem (SAT), the traveling salesman problem (TSP), answer set programming (ASP), and machine learning applications. This improvement is achieved by automatically tuning algorithm parameters to better fit specific problem instances.

2. **Dynamic Adjustment**: AAC enables the use of dynamic configurations that can adapt during execution, allowing for better performance based on the current state of the algorithm and the problem. This adaptability is especially beneficial when dealing with different problem instances and conditions.

3. **Diversity of Methods**: There are several AAC methods available, such as ParamILS, GGA/GGA+, irace, and SMAC, which offer various strategies for parameter optimization. This diversity allows for flexibility in choosing the most suitable method based on the specific context or requirements of the project.

4. **Reduction in Need for Manual Tuning**: By automating the configuration process, AAC reduces the burden on domain experts who would otherwise need to manually optimize parameters, saving time and resources while also potentially achieving better configurations than manual tuning.

Overall, AAC supports the effective utilization of algorithms by streamlining the configuration process and improving their adaptability to diverse problem scenarios."
llama3.1:latest,"Please provide an overview of the types of AAC methods.
","Document 1: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 2: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 3: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

Document 4: 
 attempted. Nonetheless, the AC literature shows a surprising amount of hybridization of
local search, evolutionary and model-based methods. We hypothesize that there is still sig-
niﬁcant progress that can be made in the area of AC, despite the sophistication of current
methods, and are encouraged by the signiﬁcant increase in attention the ﬁeld has received,
in particular through the spread of HPO techniques. Finally, we especially encourage re-
searchers to address the real-world usability of AC techniques to ensure that the promising
performance gains the AC community is seeing can beneﬁt the world at large.
Acknowledgements
This work was partially supported by the German Research Foundation (DFG) within the
Collaborative Research Center “On-The-Fly Computing” (SFB 901/3 project no. 160364472)
and by the research training group “Dataninja” (Trustworthy AI for Seamless Problem
Solving: Next Generation Intelligence Joins Robust Data Analysis) funded by the Ger-
man federal state of North Rhine-Westphalia. We also would like to thank the anonymous
reviewers for their suggestions on the ﬁrst version of this manuscript.
13. Appendix
To help the reader navigate though the jungle of AC, we provide additional resources.
Table 6 contains a list of abbreviations with terms related to AC used within this work. In
addtion, we provide a list of software resources (Table 7) that contains currently available
tools for AC. We only include software that is widely used.
469 

Document 5: 
 algorithm conﬁgurators. To this end, we propose two classiﬁcation schemes: one for AC
problems, and one for algorithm conﬁgurators. Based on this, we structure and summarize
the available literature and classify existing problem variants as well as approaches to AC.
The remainder of the paper is structured as follows. First, in Section 2, we give a formal
introduction into the setting of algorithm conﬁguration, specify the scope of this survey, and
discuss the relation between AC, AS and HPO. In Section 3, we present the classiﬁcation
schemes for AC problems and approaches that are used, in turn, to describe and compare
existing algorithm conﬁgurators. In Sections 4 and 5, we survey algorithm conﬁguration
methods grouped by the property of whether these methods are model-free or leverage
a model respectively. Section 6 deals with theoretical guarantees that can be obtained.
Diﬀerent problem variants, such as realtime AC, instance-speciﬁc vs. feature-based, multi-
objective, and dynamic AC are discussed in Sections 7 to 10. Eventually, with the help of
our classiﬁcation schemes, we elaborate on appealing research directions in Section 11 and
conclude this survey in Section 12. A list of abbreviations used in this work can be found in
Table 6. In addition, we provide a list of useful software in Table 7. We note, however, that
this list is by no means exhaustive; it is meant to provide an idea about available software
at the time of publication.
2. Problem Formulation
2.1 Algorithm Conﬁguration
To describe the AC problem more formally, we introduce the following notation that is sim- 

Document 6: 
 signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement
learning problem by Sakurai et al.[ 173], Chen et al.[ 33] and
Pettinger and Everson [ 160]. Unlike the previous approaches,
these methods include the notion of state that is deﬁned
using feedback from the EA. For each distinct state, separate
preferences are learned for each operator and selection of the
operator to apply is based on the current state of the search.
An important issue with operator selection (as with all
adaptive parameter control) is the feedback used for control.
Veerapen et al .[ 198] presented and compared various utility
measures for variation operators. These measures are based on
a combination of exploration and exploitation measures and
use Pareto-dominance to evaluate operator utility. In another
paper, Whitacre et al .[ 204] make a distinction between
the source of feedback and any further statistical treatment
(a notion further elaborated in [112]). Several feedback sources
are suggested (including whether the offspring survives or
the number of generations it remains in the population).
The data received by these sources can be treated by av-
eraging them or by detecting the outliers in the sample. 

","Augmentative and Alternative Communication (AAC) methods are designed to help individuals with communication disorders or disabilities express their thoughts, needs, and wants. There are several types of AAC methods, each with its own unique characteristics and benefits. Here's an overview:

1. **Unaided Methods**:
	* Gestures: Using hand or body gestures to convey meaning.
	* Facial expressions: Using facial expressions to communicate emotions or intentions.
	* Eye pointing: Pointing to a specific object or location using the eyes.
2. **Aided Methods**:
	* Picture Communication Symbols (PCS): Using pictures or symbols to represent words, phrases, or concepts.
	* Object-based AAC: Using objects themselves as communication tools (e.g., pointing to an object to indicate its name).
	* Symbol-based AAC: Using a combination of symbols and abbreviations to convey meaning.
3. **Symbol-Based Methods**:
	* Core Word Method: Focusing on core vocabulary words that are used frequently in daily life, such as ""want"" or ""go.""
	* Single-Word Augmentative and Alternative Communication (SWAAC): Using single words to communicate a message.
	* Phrase-Based AAC: Using short phrases or sentences to convey meaning.
4. **High-Tech Methods**:
	* Electronic Devices: Using electronic devices, such as tablets or computers, to access AAC apps or software.
	* Speech-Generating Devices (SGDs): Using devices that can generate spoken language.
5. **Dynamic Display Methods**:
	* Text-to-Speech (TTS) Systems: Using technology to convert written text into spoken language.
	* Dynamic Display Boards: Using digital displays to convey messages in real-time.
6. **Hybrid Methods**:
	* Combining unaided and aided methods, such as using gestures with picture communication symbols.
	* Integrating multiple AAC methods to meet individual needs and preferences.

It's essential to note that these categories are not mutually exclusive, and many individuals use a combination of AAC methods to communicate effectively. The choice of AAC method depends on the individual's unique needs, goals, and preferences, as well as their cognitive, physical, and communication abilities.",13.005982398986816,"Automatic Algorithm Configuration (AAC) methods can be categorized into two main components: sampling in the solver class and evaluating candidate solvers. 

1. **Sampling Methods:**
   - **Search-based Methods:** These methods include techniques such as hill climbing, local search, and evolutionary algorithms (EAs). They typically make minimal assumptions about the solver structure and do not require gradient information, allowing for a broad exploration of the solver space.
   - **Model-based Methods:** These methods aim to capture performance dependencies between solvers and their configurations by training surrogate models based on performance data from the configuration process. This approach is akin to performance prediction techniques that can improve the effectiveness of the sampling process.

2. **Evaluation Strategies:**
   - The evaluation of a candidate solver often employs different performance estimators, as indicated by the estimation errors that decrease with larger sample sizes. The performance of the estimator can depend on various factors, including the number of training instances and the specific configuration scenarios at hand.

3. **Specialized Approaches:**
   - **Sequential Model-based Optimization (SMBO):** SMBO approaches are significant in the AAC landscape and involve techniques like sequential kriging and parameter optimization, which are tailored to optimize black-box functions, especially those affected by noise.
   - **Hybrid Approaches:** There is a noted hybridization of local search, evolutionary techniques, and model-based methods in AAC literature, suggesting that combining these strategies can lead to advancements in performance.

4. **Adaptive Parameter Control:** Techniques that incorporate feedback mechanisms, often guided by reinforcement learning principles, are also part of AAC methods. These methods adjust parameters dynamically based on the observed performance of solutions during the search process.

The landscape of AAC methods continues to evolve, with researchers encouraged to explore new directions and enhance the usability of these techniques in real-world applications."
llama3.1:latest,"Are Parallel Algorithm Portfolios connected with financial portfolios?
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 automatically producing effective parallel portfolios from multiple solvers, such as ppfolio and pfolioUZK, while exploiting 
the rich design spaces of these component solvers.
Katsirelos et al. [46] showed that an effective parallelization of a CDCL SAT solver does not merely hinge on picking a 
good clause sharing strategy, since it is not straightforward to obtain shorter resolution proofs by parallelization without 
essential changes of the underlying sequential reasoning mechanism. Our ACPP does not aim at parallelizing the resolution 
proof, but rather runs multiple algorithms and algorithm conﬁgurations in parallel, in order to maximize the probability 
that at least one of them ﬁnds a short proof quickly.
2.2. Related work
Well before there was widespread interest in multi-core computing, the potential beneﬁts offered by parallel algorithm 
portfolios were identiﬁed in seminal work by Huberman et al. [39]. Their notion of an algorithm portfolio is inspired by the 
use of portfolios of assets for risk management in ﬁnance and amounts to running multiple algorithms concurrently and 
independently on the same instance of a given problem, until one of them produces a solution. Gomes et al. [27] further 
investigated conditions under which such portfolios outperform their component solvers. Both lines of work considered 
prominent constraint programming problems (graph coloring and quasigroup completion), but neither presented methods 
for automatically constructing portfolio solvers. Parallel portfolios have since made practical impact, both in cases where 
the allocation of computational resources to algorithms in the portfolio is static [62,76] and where the component solvers 

Document 4: 
 is crucial for the effectiveness of any parallel portfolio.
According to (1), the performance of a parallel portfolio on
788 IEEE TRANSACTIONS ON CYBERNETICS, VOL. 52, NO. 2, FEBRUARY 2022
Algorithm 1 GAST
Input: base solvers B with conﬁguration space C; number of
component solvers k; instance set I; performance metric m;
algorithm conﬁgurator AC; independent conﬁgurator runs n; time
budgets tC, tV , tI for conﬁguration, validation and instance
generation respectively
Output:parallel portfolio c1:k
1: for i ← 1: k do
2: /*———— conﬁguration phase ———————*/
3: for j ← 1: n do
4: obtain a portfolio cj
1:i by running AC on conﬁguration space
{c1:i−1}×{ c|c ∈ C} using m for time tC
5: end for
6: validate c1
1:i, ...,cn
1:i on I using m for time tV
7: let c1:i ← arg mincj
1:i|j∈{1,...,n} P(cj
1:i, I) be the portfolio with
the best validation performance
8: /*———— instance-generation phase ————–*/ 

Document 5: 
 work relevant to parallel algorithm portfolios in-cludes online learning [2, 16] and methods that split problems [29, 30].) Effectively, the construction of a parallel algorithm portfolio to solve as many training problems as possible on K processors is an integer-programming (IP) prob-lem. The goal is to find the schedule S that specifies the time allotments to all algo-rithms on all processors, such that no problem can receive more than B time from all the processors together, and the total number of problems solved is a maximum. The expression (1 - )ij(tkj)) is 1 if problem xi is unsolved by algorithm aj after time tkj allo-cated to it 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, parallel algorithm portfolios are motivated by the concept of risk management in finance, specifically the use of portfolios of assets. The idea is to run multiple algorithms concurrently and independently on the same problem instance until one of them produces a solution. This notion was first introduced by Huberman et al.",3.4847257137298584,"Yes, Parallel Algorithm Portfolios are conceptually connected with financial portfolios. The notion of an algorithm portfolio is inspired by the use of portfolios of assets in finance for risk management. In both cases, the idea is to run multiple independent strategies (algorithms or financial assets) concurrently to maximize the chances of achieving a favorable outcome (finding a solution in the case of algorithms, or achieving financial returns in the case of investments). This parallel processing allows for greater resilience and effectiveness, as it leverages the strengths of diverse approaches to address a problem or market condition."
llama3.1:latest,"What factors and challenges influenced the development of the Parallel Algorithm Portfolios?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 Historically, the parallel computing community quickly 
adopted combinatorial search as a playground for applications.
Search algorithms have the advantage of being conceptually 
simple (think of the most basic backtrack-style algorithm) and 
computationally demanding due to the (usually) exponential
size of the search space. In contrast, the search community did 
not really focus its research on parallelizing. The lack of proper 
Articles 
SUMMER 2013   99 Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 
Seven Challenges in 
Parallel SAT Solving 
Youssef Hamadi and Christoph M. Wintersteiger 
n This article provides a broad overview of the 
state of the parallel SAT solving field. A set of 
challenges to researchers is presented that, we 
believe, must be met to ensure the practical
applicability of parallel SAT solvers in the 
future. All these challenges are described infor -
mally but put into perspective with related 
research results, and a (subjective) grading of 
difficulty for each of them is provided.
infrastructure and, for many, the feeling that 
sequential algorithms were still full of research 
opportunities can go toward explaining that. In 
that community, parallelism was often only put in 
the perspectives of papers with no real perspec -
tives. This led to a situation where parallel search 
algorithms were designed by people with only one 
part of the required skills. 
Most computational problems solved on a com - 

Document 3: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 4: 
 rithm except some important parameter settings. Therefore, the
portfolio framework in PPTA framework can be parallel-based
framework as well as sequential-based framework.
The algorithm portfolio requires algorithm constituents to
be complementary so that the portfolio algorithm can handle
different types of problems. But it is not easy to select the com-
plementary algorithms which need to test algorithm candidates
in benchmark functions. In PPTA framework, the parameter
conﬁguration obtained from different kinds of problems are
naturally complementary thanks to the classiﬁcation of train-
ing instances. Therefore, we can obtain the complementary
algorithm much easier to construct the algorithm portfolio.
3) Robustness: For each algorithm constituent in the port-
folio, it is the best parameter conﬁguration found for the
speciﬁc kind of problems. Each algorithm is suitable for a
kind of problem so that the portfolio algorithm can handle
several kinds of problems. As a result, the optimization risk
is obviously reduced.
4) Efﬁciency: The time complexity of parameter tuning
algorithm in PPTA framework can be reduced signiﬁcantly
compared with commonly used parameter tuning methods. As
mentioned before, the performance metric only evaluates the
parameter candidates on one speciﬁc problem instance.
In the algorithm portfolio phase, the time complexity does-
n’t increase a lot compared with the single algorithm because
the operators of the algorithm in PPTA are the same with
single algorithm and the total computational resources (i.e.
max FEs) are also the same. The only difference is the
strategy allocating computational resources which varies from
algorithm to algorithm. 

Document 5: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

Document 6: 
 the former is to generate instances that are challenging for the
current portfolio, while the aim of the latter is to ﬁnd a new
component solver for the portfolio to better solve the newly gen-
erated instances. Applied to two widely studied problem domains,
that is, the Boolean satisﬁability problems (SAT) and the travel-
ing salesman problems (TSPs), the proposed approach identiﬁed
parallel portfolios with much better generalization than the ones
generated by the existing approaches when the training data
were scarce and biased. Moreover, it was further demonstrated
that the generated portfolios could even rival the state-of-the-art
manually designed parallel solvers.
Index Terms—Automatic portfolio construction (APC), genera-
tive adversarial approach, parallel algorithm portfolio, parameter
tuning.
I. I NTRODUCTION
M
ANY high-performance algorithms for solving com-
putationally hard problems, ranging from the exact
Manuscript received May 17, 2019; revised October 5, 2019 and March
4, 2020; accepted March 23, 2020. Date of publication April 29, 2020; date
of current version February 16, 2022. This work was supported in part by
the National Key Research and Development Program of China under Grant
2017YFB1003102, in part by the Guangdong Provincial Key Laboratory under
Grant 2020B121201001, in part by the Natural Science Foundation of China
under Grant 61672478, in part by the Program for Guangdong Introducing
Innovative and Enterpreneurial Teams under Grant 2017ZT07X386, in part 

","The concept of Parallel Algorithm Portfolios, also known as MAP (Multiprocessor Algorithm Portfolio), was a research initiative in the 1990s that aimed to develop adaptive algorithms for parallel computing systems. I'll try to provide an overview of the factors and challenges that influenced its development.

**Factors:**

1. **Advances in parallel computing:** The 1980s and 1990s saw significant progress in developing parallel architectures, such as multiprocessor systems, distributed memory architectures, and parallel computer clusters.
2. **Growing need for efficient algorithms:** As the size of parallel machines increased, it became more challenging to develop efficient parallel algorithms that could take advantage of these resources.
3. **Algorithm diversity and adaptability:** Researchers recognized the importance of creating adaptive algorithms that could adjust their behavior based on changing system conditions, such as available processing power, memory constraints, or communication patterns.

**Challenges:**

1. **Design complexity:** Developing parallel algorithms that can adapt to diverse system configurations and workloads is inherently complex.
2. **Scalability:** As the size of parallel machines grew, ensuring that algorithms scaled well with increasing number of processors was a significant challenge.
3. **Performance variability:** Parallel algorithms often exhibit performance variations due to factors like load imbalance, communication overhead, or synchronization costs.
4. **System heterogeneity:** With the emergence of diverse parallel architectures and distributed systems, creating algorithms that could run efficiently across these platforms presented significant challenges.

**Influence on development:**

1. **Parallel computing research community:** The Parallel Algorithm Portfolios initiative drew from and contributed to ongoing research in parallel computing, such as the work on parallel algorithm design, performance modeling, and system architecture.
2. **Mathematical programming techniques:** Researchers employed mathematical programming techniques, like linear programming and dynamic programming, to optimize algorithm selection and adaptation under various system conditions.
3. **Machine learning and data analysis:** The use of machine learning and data analysis enabled the creation of adaptive algorithms that could learn from system experiences and adjust their behavior accordingly.

The development of Parallel Algorithm Portfolios was a response to these challenges, aiming to create adaptive algorithms that could optimize performance on diverse parallel systems while addressing design complexity, scalability, and performance variability.",12.530462741851808,"The development of Parallel Algorithm Portfolios has been influenced by several factors and challenges. Key aspects include:

1. **Complementarity of Algorithms**: The success of parallel portfolios relies on the ability to select algorithm constituents that are complementary. This allows the portfolio to handle a variety of problems effectively, thus reducing optimization risk (Document 4).

2. **Efficient Use of Resources**: With the rise of parallel computing architectures, effective resource allocation that does not significantly increase time complexity compared to single algorithms is crucial (Document 4).

3. **Dynamic Load Balancing**: For parallel search methods, particularly in SAT solving, efficient dynamic load balancing and worker cooperation through conflict clause exchange are vital for performance (Document 5).

4. **Generalization in Learning**: The generation of instances that challenge current portfolios and the identification of new component solvers for better performance in learning are important (Document 6). These factors are critical when existing training data is scarce and biased.

5. **Research Perspectives**: The historical focus of the parallel computing community on combinatorial search, combined with the lack of dedicated research into parallelization from the search community, has created challenges in effectively designing parallel search algorithms (Document 2).

6. **Algorithm Structure Homogeneity**: The homogeneity of the underlying structure of individual-based algorithms allows for easier integration into parallel frameworks without the need for significant modifications (Document 3).

Overall, the interplay of these factors shapes the design, implementation, and performance of parallel algorithm portfolios, addressing challenges related to efficiency, generalization, and collaboration among different solvers."
llama3.1:latest,"What are the main benefits of PAPs?
","Document 1: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 2: 
 on PAP with more than two constituent algorithms.
Two conclusions regarding the migration scheme of PAP
can be drawn from the superiority of PAP over the compared
algorithms. First, the migration scheme is of great impor-
tance to the success of PAP. Second, 1 andMAX GEN/20
are two appropriate and robust values for the parame-
tersmigration size and migration interval. To further
investigate the inﬂuence of these parameters on PAP, we
carried out a sensitivity analysis to check whether the per-
formance of PAP will change signiﬁcantly with other param-
eter settings. As stated in Section VI-B, 16 different pairs
ofmigration interval and migration size were tested for
every instantiation of PAP. For each pair, 30 independent
runs were executed on all 27 benchmark functions. Then, for
each instantiation on each function, Kruskal–Wallis one-way
analysis of variance by ranks was employed to test whether
the 16 pairs of parameters had led to signiﬁcantly different
performance. After that, for each instantiation of PAP, we
counted the number of the benchmark functions on which
all 16 pairs of parameters made no difference. The larger
the number, the more insensitive an instantiation is to the
parameters. For the sake of brevity, we only summarize in
Table IX these numbers for the 11 PAP instantiations, while
omit the full details. It can be observed that, in the worst case 

Document 3: 
 described as follows:
Ω(P, z) =max{max
θ∈P
Metric (θ, z), Metric ( ¯θ, z)}, (14)
where ¯θ represents the Restructure procedure. Note Equation (14) is slightly different from
Equation (11) (i.e., the performance of conventional PAPs) due to theRestructure procedure.
Initialization
member 
algorithm 
𝜃
1
𝑆
1
member 
algorithm 
𝜃
𝑖
𝑆
𝑖
member 
algorithm 
𝜃
𝑘
𝑆
𝑘
…
…
…
…
𝑆
𝑘
+
1
𝑆
𝑜𝑢𝑡𝑝𝑢𝑡
Comparison
Restructure
Improvement
Figure 1. Illustrations of MOEAs/PAP , the variant form of PAP for MOPs. Si represents the so-
lution set found by member algorithm θi, and Soutput represents the solution set ﬁnally returned
by MOEAs/PAP . The main difference of MOEAs/PAP from conventional PAPs lies in the way of
determining the ﬁnal output, as indicated by the dash box.
Electronics 2023, 12, 4639 7 of 18
4. Automatic Construction of MOEAs/PAP
As aforementioned, the member algorithms of a PAP could be automatically deter-
mined by solving the problem deﬁned in Equation (13). Below, we ﬁrst introduce the 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget.
To evaluate the effectiveness of PAP, 11 instantiations of
PAP were implemented based on four existing constituent
algorithms, including SaNSDE, wPSO, G3PCX, and CMA-
ES. The performance of each instantiation was compared to
PENG et al.: POPULATION-BASED ALGORITHM PORTFOLIOS FOR NUMERICAL OPTIMIZATION 799
its constituent algorithms on 27 benchmark functions. Our
experimental results showed that seven out of the 11 PAP in-
stantiations outperformed their constituent algorithms in terms
of solution quality and the proposed risk metric. Furthermore,
7 out of the 11 instantiations even achieved superior or compa-
rable performance in comparison with G-CMA-ES, which was
known to be superior to any of the four constituent algorithms.
Our empirical studies also revealed that PAP is capable of
increasing the probability of ﬁnding the global optimum and
is insensitive to control parameters of the migration scheme.
Further analyses have been conducted to investigate in what
circumstance PAP may outperform its constituent algorithms.
Complementarity was identiﬁed as a key issue.
Though PAP has been shown to be a promising framework, 

Document 6: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

","You're likely referring to Point-of-Care (POC) Polymerase Chain Reaction (PCR) tests, also known as POC Antigen-Pcr (PAP) tests!

The main benefits of PAPs include:

1. **Rapid results**: PAPs can provide fast and accurate results in a matter of minutes, allowing for quick decision-making and treatment.
2. **Increased accessibility**: Point-of-care testing enables healthcare professionals to perform tests in various settings, including remote areas or in emergency situations, where laboratory facilities may not be readily available.
3. **Improved patient experience**: With rapid results, patients can receive timely diagnosis and treatment, reducing the need for repeat visits and improving overall care.
4. **Enhanced diagnostic accuracy**: PAPs are designed to detect specific targets, such as SARS-CoV-2 or other pathogens, providing more accurate diagnoses than traditional methods.
5. **Reduced laboratory workload**: By decentralizing testing, PAPs can alleviate the burden on laboratories and enable healthcare providers to manage patient care more efficiently.
6. **Cost-effective**: Point-of-care testing often reduces the need for repeat tests, hospitalizations, or other interventions, leading to cost savings.
7. **Early intervention**: Rapid detection enables early treatment and intervention, which is critical in preventing disease progression and reducing transmission.

Please note that PAPs may have limitations, such as sensitivity, specificity, and user error. It's essential to follow manufacturer guidelines and validate test results through laboratory confirmation when necessary.",8.536823987960815,"The main benefits of Population-Based Algorithms (PAPs) are:

1. **Ease of Implementation**: PAPs are straightforward to implement as they do not require complex resource allocation; each member algorithm is assigned an equal amount of resources.

2. **Enhanced Performance**: The performance of a PAP is determined by the best performance achieved among its member algorithms, allowing it to potentially outperform any single algorithm included within it.

3. **Utilization of Parallelism**: With the growth of parallel computing architectures, PAPs can leverage parallel solution strategies effectively, making them suitable for modern computing facilities.

4. **Diversity and Complementarity**: The effectiveness of a PAP relies on the diversity and complementarity among its member algorithms, which can improve the overall solution quality.

5. **Sensitivity to Parameters**: Certain PAP configurations have shown to be insensitive to control parameters, contributing to more robust performance across different scenarios.

6. **Empirical Success**: Experimental results indicate that many PAP instantiations can outperform their individual constituent algorithms and improve the likelihood of finding optimal solutions."
llama3.1:latest,"What is algorithm for mosaic permutation test?
","Document 1: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

Document 2: 
 algorithm and their default values, (i.e. the original values) are
presented in Table II.
The gamma and mu in GA represent the crossover rate
and mutation rate [20]. In DE algorithm, policy represents the
choice of mutation operator andF, CR are two important pa-
rameters in mutation and crossover operators [21]. The policy
candidates contain/rand/1, /best/1, /current−to−best/1,
/best/2 and /rand/2. In PSO algorithm, c1, c2 are two ﬂying
factors for each particle [22].
1852
Authorized licensed use limited to: University College London. Downloaded on July 03,2020 at 02:38:54 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: The tuned parameters for test algorithms with
Dimension d = 30.
Types GA DE PSO
gamma mu policy F CR c1 c2
TB1 0.4294 0.1728 4 0.4155 0.5441 1.7848 0.5908
TB2 0.5972 0.0617 3 0.6553 0.5350 0.1467 3.0000
TB3 0.5493 0.9737 3 0.7791 0.9308 0.7161 1.9731
TB4 0.5329 0.6145 1 0.9584 0.9273 1.7811 1.7866
TB5 0.9964 0.0839 1 0.4453 0.0581 0.4452 3.0000
TC1 0.5972 0.0617 1 0.7791 0.9308 1.2194 2.3496 

Document 3: 
 The mosaic permutation test: an exact and nonparametric
goodness-of-fit test for factor models
Asher Spector ∗ Rina Foygel Barber † Trevor Hastie ∗ Ronald N. Kahn ‡
Emmanuel Cand` es∗§
September 30, 2024
Abstract
Financial firms often rely on fundamental factor models to explain correlations among asset
returns and manage risk. Yet after major events, e.g., COVID-19, analysts may reassess whether
existing risk models continue to fit well: specifically, after accounting for a set of known factor ex-
posures, are the residuals of the asset returns independent? With this motivation, we introduce the
mosaic permutation test, a nonparametric goodness-of-fit test for preexisting factor models. Our
method can leverage modern machine learning techniques to detect model violations while provably
controlling the false positive rate, i.e., the probability of rejecting a well-fitting model, without mak-
ing asymptotic approximations or parametric assumptions. This property helps prevent analysts
from unnecessarily rebuilding accurate models, which can waste resources and increase risk. To
illustrate our methodology, we apply the mosaic permutation test to the BlackRock Fundamental
Equity Risk (BFRE) model. Although the BFRE model generally explains the most significant
correlations among assets, we find evidence of unexplained correlations among certain real estate
stocks, and we show that adding new factors improves model fit. We implement our methods in 

Document 4: 
 the maximum absolute estimated correlation between asset j⋆ and any asset j ∈ Gd.
• Reset Gd⋆ = Gd⋆ ∪ {j⋆}.
Return: Partition G1, . . . , GD.
In particular, let \MaxCorrj := maxj′̸=j | ˆCj,j′| be the maximum absolute estimated correlation between
asset j and asset j′. Let S ⊂ [p] be the subset of indices of [ p] corresponding to the assets with the
ℓ largest values of \MaxCorrj, so |S| = ℓ. Finally, let ˆν denote the top eigenvector of ˆCS,S. We then
define ˆv as follows:
ˆvj =
(
ˆνj j ∈ S
0 else.
In other words, S is the support of ˆ v, and on S, ˆv equals the maximum eigenvalue of ˆCS,S. We
picked this algorithm because it is conceptually simple and computationally cheap, but we have not
explored other algorithms. Of course, the mosaic permutation test could be used in combination with
a maximum bi-cross validation R2 statistic based on any sparse PCA algorithm.
42 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","The algorithm for the mosaic permutation test is described in Algorithm 1 and involves the following steps:

1. For each tile m = 1, . . . , M, estimate the OLS residuals using only the data within that tile.
2. Construct a mosaic residual estimate ˆϵ ∈ RT×p by combining the estimated residuals from each tile.
3. Compute the original meta test-statistic f(ˆϵ, . . . ,ˆϵ).
4. Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
5. Compute the final adaptive p-value padaptive by permuting the observations in each tile of ˆϵ.

This algorithm is further detailed in Algorithm 2: Adaptive meta-test statistic.",6.749650239944458,"The mosaic permutation test is a nonparametric goodness-of-fit test designed for factor models in finance. The algorithm consists of the following steps:

1. **Inputs**: It takes asset returns \( Y \in \mathbb{R}^{T \times p} \), exposures \( L_t \in \mathbb{R}^{p \times k} \) for \( t \in [T] \), and a set of tiles \( \{(B_m, G_m)\}^M_{m=1} \) along with a test statistic \( S : \mathbb{R}^{T \times p} \to \mathbb{R} \).

2. **OLS Estimate**: For each tile \( m = 1, \ldots, M \), the algorithm computes \( \hat{\epsilon}(m) \), which is the ordinary least squares (OLS) estimate of the residuals using only the data in that tile \( Y(m) \). The exposures for the assets in the \( m \)-th tile are denoted as \( L(m) \).

3. **Residual Estimate**: Using these estimates, the algorithm creates a mosaic residual estimate \( \hat{\epsilon} \in \mathbb{R}^{T \times p} \).

4. **Permutation**: The algorithm permutes the order of the residuals multiple times to generate permuted variants \( \tilde{\epsilon}^{(r)} \) for \( r = 1, \ldots, R \) and checks if permuting decreases the value of a chosen meta test-statistic.

5. **P-value Calculation**: Finally, it computes a p-value based on the number of times the permuted residuals yield a test statistic value lower than the original.

Overall, this method allows analysts to check for independence of residuals after accounting for known factor exposures without requiring strong parametric assumptions."
llama3.1:latest,"What test statistic should be used for mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 power compared to the oracle tests, suggesting that the adaptive QMC statistic effectively adapts to
the unknown sparsity level. Furthermore, the MPT oracle and OLS double oracle—which use the
same test statistic but are applied to different residual estimates—have similar power. Indeed, the
average power difference is 3%, and the maximum power difference is 10%. This result should not
be too surprising, since ˆϵols and ˆϵ are estimating the same residuals ϵ, and thus the OLS and mosaic
statistic should be highly correlated. Indeed, in our application (Figure 4), in all three sectors, the
mosaic statistics are empirically ≥ 85% correlated with the OLS statistics. Thus, in this simulation,
the mosaic test is competitive with an oracle method based on OLS residuals.
7 Discussion
This paper introduces the mosaic permutation test, an exact and nonparametric goodness-of-fit test
for factor models with known exposures. In an empirical application to the BlackRock Fundamental
Equity Risk model, we demonstrate how to use the mosaic permutation test to diagnose financial
factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing 

Document 4: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","The paper doesn't specify a particular test statistic that must be used for the mosaic permutation test. Instead, it mentions that any meta test-statistic can be used, as long as it is a function of the mosaic estimator ˆϵ of the residuals. This is because the test uses permutations to simulate the null distribution of the test statistic under H0, so any test statistic that is based on the residuals will work.",4.648735761642456,"The mosaic permutation test employs a test statistic denoted as S(ˆϵ), which quantifies the correlations among the columns of the residual estimate ˆϵ. This statistic is central to the test as it allows for the computation of a significance threshold by permuting the observations within the rectangular tiles of ˆϵ. The test is designed to evaluate the independence properties of the residuals, making S(ˆϵ) a key component in assessing the goodness-of-fit in factor models."
llama3.1:latest,"What is restriction for test statistic in mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when
estimating ˆϵ.
6 Do the mosaic residual estimates cause a loss of power?
Our method requires the test statistic S(ˆϵ) to be a function of mosaic residual estimates instead
of a function S(ˆϵols) of OLS residual estimates. We hope that the mosaic statistic is a good proxy
23
for the OLS statistic (which indeed seems to be the case in Figure 4), so this section analyzes via
simulations whether the mosaic test has lower power than an oracle test based on the OLS statistic.
Our simulations also show the effectiveness of the adaptive test statistic introduced in Section 5.1.
We conduct semisynthetic simulations where the exposures Lt are constant over time and equal to
the BFRE exposures for the financial sector on May 21st, 2020. We sample the factor returns Xtk
as i.i.d. t4 variables. The residuals satisfy ϵt = γt + Ztv for γt,j
i.i.d.∼ t4, Zt
i.i.d.∼ t4 and v ∈ Rp. In 

Document 5: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 6: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

","The test statistic S(ˆϵ) in the mosaic permutation test must be a function of the mosaic residual estimates ˆϵ, rather than a function of OLS residual estimates.",2.445540189743042,"The restriction for the test statistic in the mosaic permutation test is that it must be a function of the mosaic residual estimates \( \hat{\epsilon} \) instead of a function of the ordinary least squares (OLS) residual estimates \( \hat{\epsilon}_{ols} \). This means that the test statistic \( S(\hat{\epsilon}) \) is calculated using the newly introduced estimator \( \hat{\epsilon} \) which preserves some independence properties of the true residuals, rather than relying on the properties of the OLS residuals which do not meet the same independence criteria."
llama3.1:latest,"What is the trick for allowiing the exposures to change with each observation in Mosaic permutation test?
","Document 1: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 2: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","The trick is to augment the exposure matrix by replacing Lt with an augmented exposure matrix L⋆t, which only changes every two observations. For odd t, it uses Lt Lt+1, and for even t, it uses Lt−1 Lt. This allows the exposures to change within tiles while remaining constant within each pair of adjacent tiles.",3.9732258319854736,"The trick for allowing the exposures to change with each observation in the Mosaic permutation test is to augment the exposure matrix. Specifically, instead of simply using the original exposure matrix \( L_t \), an augmented exposure matrix \( L^\star_t \) is defined, which is constructed to vary every two observations. The form of this augmented matrix is as follows:

\[
L^\star_t := 
\begin{cases}
\begin{pmatrix}
L_t \\ L_{t+1}
\end{pmatrix} & \text{if } t \text{ is odd} \\
\begin{pmatrix}
L_{t-1} \\ L_t
\end{pmatrix} & \text{if } t \text{ is even}
\end{cases}
\]

By this construction, \( L^\star_t \) only changes every two observations, and it still maintains that \( L_t \) is a submatrix of \( L^\star_t \). Therefore, the null hypothesis retains its validity for the augmented model, which allows the application of the mosaic permutation test without the need for \( L_t \) to be constant across all observations but instead ensures that it is constant within the tiles created for the permutation test."
llama3.1:latest,"Is there any way of choice of tiling in Mosaic permutation test?
","Document 1: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 2: 
 setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining
provable false positive control. For example, it allows analysts to use regularized estimates of the
covariance matrix of the residuals ϵ, for example, via a graphical lasso (Friedman et al., 2007), and
it also permits the use of cross-validation to choose the regularization strength. The only restriction
5
Figure 3: Semisynthetic simulation with Xtk, ϵtj
i.i.d.
∼ N(0, 1), and the exposures are taken from the BFRE
model for the energy sector on May 21st, 2020. Note T = 350, p = 183, k = 18, and we use the test statistic
from Figure 1. Panel 2(a) shows that a naive residual permutation test (discussed in Section 2.1) inaccurately
simulates the null distribution of the test statistic S(ˆϵols)—in fact, the true null distribution and the estimated
one do not overlap. Panel 2(b) shows that naive bootstrap Z-statistics (discussed in Section 2.2) are not
approximately mean zero, nor do they have unit variance. All p-values based on these two naive methods are 

Document 3: 
 The proof is simple, but the notation is subtle. To ease comprehension, recall that by definition
ϵ(m) = ϵBm,Gm where Bm and Gm are random. In this proof, we will use the notation ϵBm,Gm instead
of ϵ(m) to make the dependence on Bm and Gm explicit.
Let T = {(Bm, Gm)}m∈[M] denote the choice of tiles and let τ = {(βm, γm)}m∈[M] denote an arbitrary
fixed tiling in the support of T . It suffices to show that for any fixed permutation matrices P1 ∈
R|β1|×|β1|, . . . , PM ∈ R|βM |×|βM |,
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ) | T= τ. (A.8)
To show this, we note that Assumption A.1 yields the marginal result that
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ). (A.9)
To convert this to a conditional result, note that sinceT is a function of ˆϵ (which itself is a deterministic 

Document 4: 
 In future work, we plan to extend the mosaic permutation test to apply to this new setting (among
others).
• Using regularization: Our methods currently require the use of unregularized OLS regressions within
each tile to estimate the residuals (see Remark 3). However, to increase power, it might be valuable
to develop methods that can use regularization.
• Robustness: It might be valuable to develop tests that are robust to slight inaccuracies in the expo-
sures Lt. Indeed, this could also help relax the assumption that Lt is locally constant (Assumption
3.1), since small within-tile changes in Lt could be viewed as small “inaccuracies.” Similarly, it
would be useful to develop theory quantifying the robustness of the existing test, i.e., by bounding
the excess error in some interpretable way.
25
8 Code and data availability
We implemented our methods in the python package mosaicperm. All other code used in the paper is
available at https://github.com/amspector100/mosaic_factor_paper/. Although we are not able
to make the BFRE model data available, we have provided a publicly available sample dataset that
allows one to obtain qualitatively similar results (see the GitHub repository for more details).
9 Acknowledgements
The authors would like to thank John Cherian, Kevin Guo, Guido Imbens, Lihua Lei, and Bobby Luo 

Document 5: 
 highly computationally efficient procedure compared to one which (e.g.) repeatedly cross-validates an
expensive machine learning algorithm.
5.2 Adaptively choosing the tiling
Although Section 3.3 gives a good default choice of tiling, another option is to learn a good choice
of tiles that “separate” assets whose idiosyncratic returns are correlated. However, in general, if the
tiling is chosen using Y, ˆϵ will not necessarily be invariant to any permutations under the null because
of the dependence between {(Bm, Gm)}M
m=1 and Y. In other words, naive “double dipping” leads to
inflated false positives.
However, we can sequentially choose the mth tile (Bm, Gm) based on the estimated residuals from the
previous m−1 tiles as long as our choice of (Bm, Gm) does not depend on the order of the rows within
each of the previous m − 1 tiles. Precisely, suppose that we can write Bm, Gm as functions bm, gm of
the previous tiles as well as auxiliary randomness Um
i.i.d.∼ Unif(0, 1):
Bm = bm(ˆϵ(1), . . . ,ˆϵ(m−1), Um) and Gm = gm(ˆϵ(1), . . . ,ˆϵ(m−1), Um). (5.5) 

Document 6: 
 returns ˆϵ·,1, ˆϵ·,2 will never be “separated” by different permutations. We address this problem in the
next section.
3.2 The mosaic permutation test
We now introduce the general mosaic permutation test, which is more powerful and stable than
the simple method in Section 3.1. As an added benefit, we will also make the test more robust to
autocorrelation and nonstationarity among the residuals ϵ1, . . . , ϵT .
In Section 3.1, we separated the asset returns Y ∈ RT×p into two disjoint groups, computed residual
estimates ˆϵ separately for each group, and then permuted within each group. Now, we partition the
data Y into an arbitrary number M of rectangles along both axes. Formally, for m = 1, . . . , M, let
Bm ⊂ [T] denote a subset or “batch” of observations and Gm ⊂ [p] denote a subset or “group” of
assets. We say {(Bm, Gm)}M
m=1 is a tiling if for every timepoint t and asset j, there is exactly one pair
(Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we 

","Yes, there are two ways of choosing the tiling in the Mosaic permutation test: a default choice and an adaptive choice. A good default choice is to partition the data into tiles such that each tile contains data from only one week, as discussed in Section 3.2. Alternatively, you can adaptively choose the tiles based on the estimated residuals from previous tiles, as long as your choice does not depend on the order of rows within each previous tile (Section 5.2).",4.7493767738342285,"Yes, the choice of tiling in the mosaic permutation test can be adaptively learned to separate assets whose idiosyncratic returns are correlated. Although a default choice of tiling is provided in Section 3.3, analysts can sequentially choose each tile based on the estimated residuals from previous tiles, ensuring that the choice does not depend on the order of rows within the previous tiles. This process involves writing the pair of sets (Bm, Gm) as functions of previous tiles along with auxiliary randomness, allowing for an adaptive and potentially more effective selection of tiling."
llama3.1:latest,"What is RNN with Long-Short-Term-Memory estimate in deep learning pricing asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper.
Hansen, L. P.(1982): “Large sample properties of generalized method of moments estimators,” Econo-
metrica, 50(4), 1029–1054.
Hansen, L. P., and R. Jagannathan(1997): “Assessing Speciﬁcation Errors in Stochastic Discount
Factor Models,” The Journal of Finance , 52(2), 557–590.
Heaton, J., N. Polson, and J. H. Witte(2017): “Deep learning for ﬁnance: Deep portfolios,” Applied
Stochastic Models in Business and Industry , 33(1), 3–12.
Hochreiter, S., and J. Schmidhuber(1997): “Long short-term memory,” Neural Computation, 9(8),
1735–1780.
Horel, E., and K. Giesecke(2020): “Towards Explainable AI: Signiﬁcance Tests for Neural Networks,”
Journal of Machine Learning Research, forthcoming . 

Document 3: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 4: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 5: 
 Rapach et al. 2013) and lasso (Chinco, Clark-Joseph & Ye 2019; Freyberger, Neuhierl & Weber
2020).
More recently, Gu, Kelly & Xiu (2020) conduct a wide-ranging analysis of machine learning
methods for return prediction, considering not only regularized linear methods but also more
cutting-edge nonlinear methods including random forest, boosted regression trees, and deep
learning.Their research illustrates the substantial gains of incorporating machine learning when
estimating expected returns. This translates into improvements in out-of-sample predictive R2
as well as large gains for investment strategies that leverage machine learning predictions. The
empirical analysis also identifies the most informative predictor variables, which helps facilitate
deeper investigation into economic mechanisms of asset pricing.
Machine learning also makes it possible to improve expected return estimates using predic-
tive information in complex and unstructured data sets. For example, Ke, Kelly & Xiu (2019)
propose a new supervised topic model for constructing return predictions from raw news text
and demonstrate its prowess for out-of-sample forecasting.Jiang,Kelly & Xiu (2021) and Obaid
& Pukthuanthong (2022) demonstrate how to tap return predictive information in image data
using machine learning models from the computer vision literature. Both text and image data
confer particularly strong return forecasting gains at short horizons of days and weeks and are 

Document 6: 
 characteristics. Feng, Polson, and Xu (2019) impose a no-arbitrage constraint by using a set of
pre-speciﬁed linear asset pricing factors and estimate the risk loadings with a deep neural network.
Rossi (2018) uses Boosted Regression Trees to form conditional mean-variance eﬃcient portfolios
based on the market portfolio and the risk-free asset. Our approach also yields the conditional
mean-variance eﬃcient portfolio, but based on all stocks. Gu, Kelly, and Xiu (2019) extend the
linear conditional factor model of Kelly, Pruitt, and Su (2019) to a non-linear factor model using an
autoencoder neural network. 3 We conﬁrm their crucial insight that imposing economic structure
2Other related work includes Sirignano, Sadhwani, and Giesecke (2020) who estimate mortgage prepayments,
delinquencies, and foreclosures with deep neural networks, Moritz and Zimmerman (2016) who apply tree-based
models to portfolio sorting and Heaton, Polson, and Witte (2017) who automate portfolio selection with a deep
neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the 

","The paper does not specifically mention a Recurrent Neural Network (RNN) with Long-Short-Term-Memory (LSTM) estimate used by Luyang Chen in the asset pricing model. However, it mentions Hochreiter and Schmidhuber's work on ""Long short-term memory"" which is likely related to the LSTM architecture.",4.443465948104858,"The context does not provide specific information about the use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) in the asset pricing models developed by Luyang Chen. However, it mentions the use of deep learning techniques for estimating an asset pricing model that accounts for various conditioning information and time-variations. LSTMs are typically utilized in scenarios where data has sequential characteristics, making them suitable for time-series forecasting in finance. They are particularly effective in capturing long-term dependencies in data, which is crucial for modeling the dynamic behavior of asset returns. Therefore, while the specific model by Chen does not detail RNN or LSTM use, these architectures can enhance the processing of temporal data in asset pricing models."
llama3.1:latest,"What is loss function in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 In addition to the various layer-types of CNN architecture introduced in the previous
section, the ﬁnal classiﬁcation is achieved from the output layer that usually the last layer
of the FC layer, as shown in Figure 2. Different loss functions also affect the performance of
the CNN architecture and are applied to different visual tasks (e.g., image classiﬁcation,
face recognition, and object recognition). Here are some commonly used loss functions in
CNN-based image classiﬁcation methods (inherit the content of Section 2.1.2), as shown
in Table 1.
All in all, Softmax+Cross-Entropy has become the usual loss function of the CNN
model. There are also many improved versions based on it, such as center-loss [ 82], L-
Softmax [83], A-Softmax [ 84], AM-Softmax [ 85], PEDCC-loss [ 86], etc., which play an
important role in different visual tasks.
Remote Sens. 2021, 13, 4712 8 of 51
Table 1. Common loss functions for CNN models.
Loss Function Equation Characteristic
L1 (MAE) Loss(y, y∗) = 1
m ×
m
∑
i=1
⏐⏐⏐⏐y∗
i −yi
⏐⏐⏐⏐
This function is widely used in
regression problems. L1 Loss is called
mean absolute error (MAE)
L2 (MSE) Loss(y, y∗) = 1
m ×
m
∑
i=1
( 

Document 3: 
 We choose this speciﬁc linear version of the model as it maps directly into the linear approaches that
have already been successfully used in the literature. This linear framework essentially captures
the class of linear factor models. Appendix C provides a detailed overview of the various models
for conditional SDFs and their relationship to our framework.
II. Estimation
A. Loss Function and Model Architecture
The empirical loss function of our model minimizes the weighted sample moments which can
be interpreted as weighted sample mean pricing errors:
L(ω|ˆg,It,It,i) = 1
N
N∑
i=1
Ti
T

1
Ti
∑
t∈Ti
Mt+1Re
t+1,iˆg(It,It,i)

2
. (4)
13Kozak, Nagel, and Santosh (2020) consider also cross-products of the characteristics. They show that the PCA
rotation of the factors improves the pricing performance. Lettau and Pelger (2020) extend this important insight
to RP-PCA rotated factors. We consider PCA based factors in III.J. Our main analysis focuses on conventional
long-short factors as these are the most commonly used models in the literature.
14As before we deﬁne as tangency portfolio one of the portfolios on the global mean-variance eﬃcient frontier.
12
Electronic copy available at: https://ssrn.com/abstract=3350138 

Document 4: 
 part of network learning. The ﬁrst thing that backpropagation needs to determine is the
loss function of the model. There are many loss functions, see Section 2.2.5. The loss
function in the Figure 2 is L2 Loss:
Loss(y, y∗) = 1
m ×
m
∑
i=1
(y∗
i −yi)2 (2)
By calculating the mean square error between y∗ and y, the network weights w
and bias b can be updated by obtaining the partial derivative of the loss function: w′ =
w −η ×(∂Loss/∂w) and b′= b −η ×(∂Loss/∂b). With a suitable learning rate η, the loss
of the y and y∗can be gradually minimized. That is to make the y closer to y∗, so as to
achieve the effect of network training.
Remote Sens. 2021, 13, 4712 4 of 51
Figure 2. The structure of the MLP . It hasn input values and m output values, including k hidden
units. xn is the input value. The direction of the arrow is the direction in which the input value is
transmitted. The hidden unit is hk, it receives the input value of the previous layer. ym is the output
unit, and the real value is y∗m.
2.2. CNN Architecture
The main structure of CNN is the convolutional layer, pooling layer, nonlinear acti- 

Document 5: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 6: 
 The most common way is to translate the problem into an unconditional asset pricing model
on sorted portfolios. Under additional assumptions one could obtain a valid SDF Mt+1 conditional
on a set of asset-speciﬁc characteristics It,i by its projection on the return space:
Mt+1 = 1 −ω⊤
t Re
t with ωt,i = f(It,i),
where It,i is a vector of q characteristics observed for N stocks and f(·) is a general, potentially
nonlinear and non-separable function. Most of the reduced-form asset pricing models approximate
this function by a (potentially very large) set of simple managed portfolios fj(·), such that f(It,i) ≈∑˜Nbasis
j=1 fj (It,i) ˜wj. The SDF then becomes a linear combination of these managed portfolios with
constant weights ˜ωj:
Mt+1 = 1 −
˜Nbasis
∑
j=1
˜wj ˜Rt+1,j with ˜Rt+1,j =
N∑
i=1
fj (It,i) Re
t+1,i, (A.1)
where ˜Rt+1 are the returns of ˜Nbasis managed portfolios that correspond to diﬀerent basis functions
in the characteristic space. The number of basis portfolios increases by the complexity of the
basis functions and the number of characteristics. The most common managed portfolios are 

","The empirical loss function used in the deep learning asset pricing model developed by Luyang Chen is the mean square error (L2 Loss), which calculates the average of the squared differences between predicted and actual values. This is represented as: Loss(y, y∗) = 1/m × ∑(y∗i - yi)^2.",3.778961181640625,"In the deep learning asset pricing model developed by Luyang Chen and co-authors, the empirical loss function minimizes the weighted sample moments, interpreted as weighted sample mean pricing errors. The loss function is mathematically represented as:

\[ L(\omega|\hat{g}, I_t, I_{t,i}) = \frac{1}{N} \sum_{i=1}^{N} T_i \left\| \frac{1}{T_i} \sum_{t \in T_i} M_{t+1} R_{e, t+1, i} \hat{g}(I_t, I_{t,i}) \right\|^2. \]

This function aims to optimize the model's performance in estimating asset prices based on conditioning information and the dynamic states of the economy."
llama3.1:latest,"What is the model archtecture in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 the most promising direction for future empirical asset pricing research is developing a genuine
fusion of economic theory and machine learning. It is a natural marriage, as asset pricing theory
revolves around price formation through the aggregation of investor beliefs, which undoubtedly
enterpricesinsubtle,complex,andsometimessurprisingways.Atthesametime,machinelearning
constitutes a sophisticated collection of statistical models that flexibly adapt to settings with rich
and complex information sets.
Machine learning factor models are one such example of this fusion. Almost all leading theo-
reticalassetpricingmodelspredictalow-dimensionalfactorstructureinassetprices.Wherethese
models differ is in their predictions regarding the identity of the common factors. Much of the
frontier work in empirical asset pricing can be viewed as using the (widely agreed upon) factor
structure skeleton as a theory-based construct within which various machine learning schemes
are injected to conduct an open-minded investigation into the economic nature of the common
factors.
Our survey is inevitably selective and disproportionally influenced by our own research on
thesetopics.Wehavemainlyfocusedonmethodologicalcontributions,leavingadetailedreviewof
empirical discoveries via these methodologies for future work.A frequently discussed dichotomy
in the literature is observable factor versus latent factor models. While some of the methods we
discuss apply to observable factor settings (or hybrid settings),we have also skewed our coverage 

Document 3: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 4: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 5: 
 paper.
Feng, G., N. G. Polson, and J. Xu(2019): “Deep Learning in Characteristics-Sorted Factor Models,”
Working paper.
Freyberger, J., A. Neuhierl, and M. Weber(2020): “Dissecting characteristics nonparametrically,”
Review of Financial Studies, forthcoming , 33(5), 2326–2377.
Goodfellow, I., Y. Bengio, and A. Courville(2016): Deep Learning. MIT Press.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio (2014): “Generative adversarial nets,” in Advances in neural information processing
systems, pp. 2672–2680.
Gu, S., B. Kelly, and D. Xiu(2019): “Autoencoder Asset Pricing Models,” Journal of Econometrics,
forthcoming.
Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper. 

Document 6: 
 neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the
additional elements of ﬁnding the macroeconomic states and identifying the most robust conditioning instruments.
Fundamentally, our object of interest is the pricing kernel. Kelly, Pruitt, and Su (2019) obtain a multi-factor factor
model that maximizes the explained variation. The linear special case applies PCA to a set of characteristic based
factors to obtain a linear lower dimensional factor model, while their more general autoencoder obtains the loadings
to characteristic based factors that can depend non-linearly on the characteristics. We show in Section III.J how our
SDF framework and their conditional multi-factor framework can be combined to obtain an even better asset pricing
model.
5
Electronic copy available at: https://ssrn.com/abstract=3350138
on a machine learning algorithm can substantially improve the estimation. Bryzgalova, Pelger,
and Zhu (2020) use decision trees to build a cross-section of asset returns, that is, a small set of
basis assets that capture the complex information contained in a given set of stock characteristics.
Their asset pricing trees generalize the concept of conventional sorting and are pruned by a novel 

","The paper ""Deep Learning in Asset Pricing"" by Luyang Chen, Markus Pelger, and Jason Zhu uses a deep neural network architecture that combines an adversarial approach to construct informative test assets with a fully flexible form that accounts for time-variation. The key innovations include using the fundamental no-arbitrage condition as the criterion function.",4.193137884140015,"The deep learning asset pricing model constructed by Luyang Chen and colleagues utilizes a deep neural network architecture that incorporates a flexible non-linear factor model. This model aims to estimate stock returns and takes into account various conditioning information while accounting for time-variation. The innovations of their approach include the integration of the fundamental no-arbitrage condition in the criterion function, the use of an adversarial approach to construct informative test assets, and the extraction of economic states from macroeconomic time series. The model is designed to outperform traditional methods in terms of Sharpe ratio, explained variation, and pricing errors, ultimately identifying key factors that drive asset prices."
llama3.1:latest,"What is done instead of comparing the variance of the factors before and after the break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 assumption violations.
In the current work, we extend the theory of generalized residuals to common factor
models with continuous MVs. This extension allows us to identify various sources of misfit
in common factor models, such as the nonlinear relationship between MVs and LVs,
heterogeneous variance of MVs conditional on LVs, and nonnormality of LVs. In addition,
we suggest a summary GOF statistic that complements the local statistics conditional on
LV values, which can facilitate an overall judgment on misfit. For all the proposed GOF
statistics, we derive asymptotic reference distributions to construct formal statistical tests.
Ultimately, our proposed framework is expected to complement conventional GOF
diagnostics for common factor models.
The rest of the paper is organized as follows. In Section 2, we introduce the
common factor model and present our extended theory of generalized residuals, along with
example GOF test statistics suitable for testing various model assumptions. In Section 3,
the performance of the proposed test statistics are evaluated by Monte Carlo studies. In
Section 4, our GOF testing methods are illustrated with a real data example. The paper is
concluded with a discussion of the main findings and future directions in Section 5.
2. Theory
2.1. Common Factor Model
2.1.1. Latent Variable Measurement Model
Let Yij ∈ Rbe the individuali’s response for manifest variable (MV)j, and
Xi = (Xi1, . . . , Xid)
⊤ 

Document 5: 
 spectively; and et = [ e1t,· · · ,eN t ]
′
is the N × 1 idiosyncratic error that may have serial and
cross-sectional dependence along with heteroskedasticit y . W e deﬁne π1 = T1/T ∈ (0,1) as the
break fraction, which is assumed to be a ﬁxed constant. This i mplies that T1 is a sequence that
depends on T. F or notational simplicity , we suppress the dependence of T1 on T.
W e are interested in testing the null hypothesis of no struct ural break in the factor loadings,
i.e.,
H0 : λi1 = λi2 ∀i. (2.2)
against the alternative hypothesis that a non-negligible p ortion of the cross sections have a break
in their loadings at a common time, i.e.,
H1 : λi1 ̸= λi2 for i∈ J (2.3)
where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally 

Document 6: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

","In the likelihood ratio test, instead of comparing the variance of the factors before and after the break, it implicitly explores a feature that under the alternative hypothesis, the pre-break and post-break variances can be a singular matrix. This allows the likelihood ratio test to diverge faster and thus become more powerful than Wald-type tests.",3.8468270301818848,"Instead of directly comparing the variance of the factors before and after the break in the likelihood ratio (LR) test for structural changes in factor models, the approach transforms the structural change problem into a low-dimensional problem by exploring a special feature of the estimated factors. Specifically, the test takes into account that the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, which enhances the power of the LR test compared to Wald-type tests. This method allows for more powerful testing of changes in the estimated factors' variances without requiring a direct comparison of those variances."
llama3.1:latest,"How to determine the number of break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 3: 
 100,500 0.154 0.081 0.017 0.106 0.053 0.017 0.108 0.054 0.01 3 0.063 0.031 0.010
200,200 0.080 0.032 0.005 0.072 0.036 0.013 0.084 0.030 0.00 7 0.052 0.023 0.006
200,500 0.156 0.080 0.022 0.108 0.056 0.018 0.102 0.043 0.01 1 0.066 0.027 0.007
500,500 0.158 0.084 0.020 0.121 0.069 0.020 0.111 0.059 0.01 4 0.066 0.039 0.014
23
T able 4: Size of structural break tests with unknown break da te in a factor model with t-distributed
factors. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.004 0.002 0.000 0.015 0.008 0.003 0.028 0.007 0.00 0 0.128 0.090 0.061
100,200 0.038 0.018 0.002 0.023 0.008 0.001 0.048 0.016 0.00 2 0.032 0.012 0.006 

Document 4: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

Document 5: 
 In this section, we estimate a factor model for the US industrial employment rates and apply
the proposed tests to check whether the factor loadings have undergone a structural change in
the past decade. Monthly data from January 2010 through Apri l 2022 are available from the US
Department of Labor for 84 industries. The data are a balance d panel with T = 147 ,N = 84.
W e use the information criteria ICp1 and ICp2 of Bai and Ng (2002), the ER (eigenvalue ratio)
and GR (growth ratio) of Ahn and Horenstein (2013), the empirical d istribution estimator of
Onatski (2010), and the bridge estimator of Caner and Han (20 14) to determine the number of
common factors in the data. The maximum number of factors is s et to 10. The estimated number
of factors by ICp1 and ICp2 is three; the methods of Onatski (2010) and Caner and Han (201 4)
detect two factors; and ER and GR of Ahn and Horenstein (2013) choose one common factor.
Therefore, we study the test results for the cases of one, two , and three common factors.
W e apply our LR and LR m tests to examine whether there exists a structural break in f actor
loadings and estimate the break date if the null hypothesis i s rejected. All of the settings for the
two tests are the same as those used in the simulation studies . 

Document 6: 
 References
Ahn, S. and Horenstein, A. 2013. Eigenvalue ratio test for the number of factors. Econometrica 81 ,
pp. 1203–1227.
Andrews, D.W.K., 1993. T ests for parameter instability and structural change with unknown
change point. Econometrica 61 , pp. 821–856.
Bai, J., 1997. Estimaing multiple breaks one at a time. Econometric Theory 13 , pp. 315–352.
Bai, J., 2000. V ector autoregressive models with structura l changes in regression coeﬃcients and
in variance-covariance matrices. Annals of Economics and Finance 1 , pp. 303–339.
43
Bai, J., 2010. Common breaks in means and variances for panel data. Journal of Econometrics 157 ,
pp. 78–92.
Bai, J. and Ng, S. 2002. Determining the number of factors in a pproximate factor models. Econo-
metrica 70 , pp. 191–221.
Bai, J., 2003. Inferential theory for factor models of large dimensions. Econometrica 71 , pp.
135–171.
Bai, J., Han, X., Shi, Y., 2020. Estimation and inference of c hange points in high-dimensional 

","Determining the number of breaks (or structural changes) in a likelihood ratio (LR) test for factor models involves several steps, often requiring both theoretical considerations and empirical analysis. The Likelihood Ratio Test is commonly used to compare different statistical models by assessing whether the observed data provide sufficient evidence to warrant the preference of one model over another based on some specific assumptions or parameters that distinguish them.

Here's a simplified approach to determining the number of breaks in structural changes using factor models:

### 1. **Formulate Your Hypothesis**

First, specify what you're testing for. This could be whether there are any structural changes (breaks) at certain points in your dataset and how these affect the performance or fit of your factor model.

### 2. **Choose a Factor Model**

Select an appropriate factor model that aligns with your research question and dataset characteristics. Commonly used models include the Classical Factor Model, Principal Components Analysis (PCA), Maximum Likelihood (ML) estimation of factor models, etc.

### 3. **Estimate Parameters Without Breaks**

Initial estimates are often done under the assumption that there are no breaks in the data's structure. This provides a baseline against which changes can be evaluated.

### 4. **Test for Structural Changes**

To identify potential breaks, you typically use a residual-based approach. You compute residuals from your model and then apply statistical tests to these residuals for evidence of abrupt shifts or breaks. This could involve techniques like the Bai-Perron test (for multiple structural changes), Andrews' Sup F-regressions, etc.

### 5. **Bai-Perron Test**

One of the most popular methods for identifying the number of breaks is the Bai-Perron test. This method allows for more than one break in the parameters or variances over the sample period and can be used to determine both the timing and the number of structural changes.

### 6. **Estimate Parameters with Identified Breaks**

Once the number and timing of breaks are identified, re-estimate the model allowing for these changes. This might involve segmented models where the parameters differ before and after each break point.

### 7. **Evaluate Model Performance**

After identifying the optimal number of breaks and estimating the model with breaks, evaluate its performance using metrics such as goodness-of-fit tests, mean squared prediction error (MSE), Akaike information criterion (AIC) or Bayesian information criterion (BIC).

### Example Python Code for Bai-Perron Test

Here's a simplified example to illustrate applying the Bai-Perron test in Python. This example is highly condensed and based on assumed data for explanatory purposes.

```python
import pandas as pd
from patsy import dmatrices
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.stattools import adfuller

# Assuming 'data' is a Pandas DataFrame with relevant time-series data.
# The Bai-Perron test typically requires the data to be in a long format.

def baiperron_test(data):
    # Split data into training and testing sets, ensuring both have similar numbers of breaks
    from sklearn.model_selection import train_test_split

    X_train, X_test = train_test_split(data.drop('target', axis=1), test_size=0.2)

    # Apply the Bai-Perron test to find the optimal number of breaks
    from statsmodels.tsa.stattools import bp_stat

    bp_stats = []
    for num_breaks in range(1, 10):  # Adjust this range as needed
        _, resid = adfuller(X_train.iloc[:, :num_breaks])
        bp_stat_val = bp_stat(resid)
        bp_stats.append((num_breaks, bp_stat_val))

    optimal_num_breaks = min(bp_stats, key=lambda x: x[1])[0]

    return optimal_num_breaks

optimal_breaks = baiperron_test(data)
print(f""Optimal number of breaks: {optimal_breaks}"")
```

This is a very simplified example and does not include the actual model estimation with break points. For real-world applications, you might need to adapt this code to your specific data format and add more steps for model fitting.

The exact implementation may vary based on the specifics of your problem (e.g., type of data, software tools used).",24.58793568611145,"To determine the number of breaks in the likelihood ratio (LR) test for structural changes in factor models, the procedure involves the following steps:

1. **Initial Break Point**: Identify the initial break point, denoted as \( \hat{k}_1 \), using a method such as the Quasi-Maximum Likelihood (QML) method.

2. **Subsample Division**: Split the entire sample into two subsamples: one from the start to \( \hat{k}_1 \) and the other from \( \hat{k}_1 \) to the end of the sample.

3. **Conduct Sup-LR Tests**: For each subsample, conduct a sup-LR test with \( m = 1 \). If the null hypothesis is rejected at a specified significance level \( \alpha \), this suggests the presence of additional breaks.

4. **Estimate Further Break Points**: For the subsample where the null is rejected, estimate a new break point, and repeat the process by further dividing the subsample at this newly estimated break point. 

5. **Continue Until Non-Rejection**: Continue this splitting and testing process until the LR test does not reject the null hypothesis for all subsamples. 

6. **Count Breaks**: The number of break points is determined by counting the number of subsamples minus one. This gives a consistent estimate of the number of breaks when the significance level \( \alpha \) converges to zero slowly.

This method effectively tracks structural changes and their locations by continuously refining the sample based on the results of the statistical tests at each step."
llama3.1:latest,"What is equivalent of a factor model with structural brak in factor loading?
","Document 1: 
 where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally
equivalent to a model with time-invariant loadings and pote ntially more pseudo-factors (e.g., HI,
2015; Baltagi et al., 2017). T o capture the factor dimension augmentation caused by the break,
we follow the framework of DBH (2022) and set r as the number of pseudo-factors in (2.1). W e
set
Λ1 = Λ B, Λ2 = Λ C,
5
where Λ is an N× rmatrix with full column rank r, B and C are r× rmatrices, rank( B) = r1 ≤ r,
and rank( C) = r2 ≤ r.
F or a given split point k, deﬁne
X(1)
k = [ x1 ,...,x k ]′, X(2)
k = [ xk+1,...,x T ]′, F(1)
k = [ f1,...,f k ]′ , F(2)
k = [ fk+1,...,f T ]′ ,
e(1) 

Document 2: 
 loadings βSDF as a function of characteristics on diﬀerent stocks for the estimation and evaluation.
Therefore, we can evaluate the model on stocks that are not available in the ﬁrst part of the data.
Importantly, the conditional model also allows us to directly study the economic sources of risk in
terms of ﬁrm-characteristics.
Appendix C.D. Conditional Factor Models
A conditional factor model assumes that the SDF is spanned by a linear combination of con-
ditional risk factors, and hence it restricts the basis assets that span the SDF. In contrast to the
60
Electronic copy available at: https://ssrn.com/abstract=3350138
unconditional model, the SDF weights and loadings of the conditional risk factors are a function
of the characteristics and hence time-varying. We use the Instrumented Principal Component
Analysis (IPCA) of Kelly, Pruitt, and Su (2019) to illustrate this setup. This conditional factor
model directly models individual stock returns as a function of characteristics given by Re
t+1,i =
b⊤
t,ifIPCA
t+1 + ϵt+1,i with bt,i = I⊤
i,tΓb. Instead of allowing the SDF to be Mt+1 = 1 −∑N
i=1 ωt,iRe
t+1,i,
it is restricted to Mt+1 = 1 −∑K
k=1 wIPCA
t,k fIPCA 

Document 3: 
 EC08CH03-Bai ARI 17 October 2016 11:32
where λ = vec
(
/Lambda1′)
. Consider the following restriction on the factor loadings:
λ = Bδ + C, (19)
where δ is a vector of free parameters with dim( δ) ≤ dim( λ). In general, B and C are known ma-
trices and vectors deﬁned by either identifying restrictions or other structural model restrictions.
In view of Equation 19, we may rewrite the restricted factor model in Equation 18 as
y = Zδ + vec( E),
where y = vec( X) − (IN ⊗ F)C and Z = [( IN ⊗ F)B]. If we impose some distributional assump-
tions on the error terms, for example, vec (E|Z) ∼ N(0 ,R ⊗ IT −s )f o rs o m e N × N positive
deﬁnite matrix R, such models can be estimated using the Bayesian algorithm from Bai & Wang
(2015).
11.3. Structural Vector Autoregression and Restricted Dynamic Factor Models
The dynamic factor models also bring new insight into the estimation of structural vector autore-
gression (SVAR) models with measurement errors. Consider a traditional SVAR given by
A( L)Zt = at , 

Document 4: 
 100,200 0.998 0.984 0.792 0.861 0.704 0.424 1.000 1.000 1.00 0 1.000 1.000 0.997
100,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
200,200 1.000 0.980 0.784 0.860 0.702 0.419 1.000 1.000 1.00 0 1.000 1.000 0.999
200,500 1.000 1.000 1.000 1.000 1.000 0.998 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
24
T able 6: Power of structural break tests with unknown break d ate as the magnitude of the break in factor
loadings increases. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
λi2 = λi1 + εi, εi
i.i.d.∼ N (0, b · Ir0 )
N = 100 , T = 100 , ρ = 0
b
0.0 0.015 0.005 0.000 0.026 0.007 0.002 0.055 0.018 0.003 0.158 0.111 0.074 

Document 5: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 6: 
 model that specifies the relationship between LVs and MVs.
A widely used LV measurement model is the common factor model (e.g., Bollen,
1989; Jöreskog, 1969; Kaplan, 2008; Kline, 2023; Lawley and Maxwell, 1971), particularly
when analyzing continuous data. The common factor model requires MVs to be linearly
dependent on LVs, aiming to attribute the dependencies among MVs to LVs. This model
has been widely used for testing theories about the number of LVs and the pattern of
MV-LV dependency (i.e., confirmatory factor analysis), or for learning these structures
from data (i.e., exploratory factor analysis).
Most of the commonly used measurement models, however, rely on strong
parametric assumptions. For example, in the common factor model, the conditional mean
of an MV given LVs is a linear function of LVs, and the conditional variance of an MV is
constant. In addition, it is often assumed that the MVs and LVs are normally distributed.
These restrictive assumptions may be violated when analyzing real-world data, resulting in
poor model-data fit and potentially erroneous inferences.
To prevent fallacious inferences, various goodness-of-fit (GOF) assessment tools
have been developed for common factor models. A majority of existing GOF diagnostics
rely on residual means, residual covariances, or deviances to reflect the discrepancies 

","A very specific and technical question!

In the context of factor analysis, a ""structural break"" in factor loadings refers to a situation where the relationships between the observed variables (indicators) and the underlying latent factors change over time or across different subgroups.

The equivalent of a factor model with structural breaks in factor loading is often referred to as:

1. **Dynamic Factor Model**: This type of model allows for changes in the factor loadings over time, enabling the estimation of dynamic relationships between the observed variables and the underlying factors.
2. **Time-Varying Parameter (TVP) Factor Model**: In this approach, the factor loadings are modeled as time-varying parameters that follow a specified distribution (e.g., normal or non-normal).
3. **Switching Regression Factor Model**: This model allows for changes in the relationships between the observed variables and factors by introducing latent variables (switching regimes) that determine which set of factor loadings is used at each point in time.
4. **Hierarchical Factor Model**: In this approach, multiple factor models are estimated separately for different subgroups or time periods, allowing for differences in factor loadings across groups.

To account for structural breaks in factor loading, you can use techniques such as:

1. **Split-sample estimation**: Divide the data into two or more subsamples and estimate a separate factor model for each subsample.
2. **Time-series analysis**: Use time-series methods (e.g., ARIMA, ETS) to model the changes in factor loadings over time.
3. **Bayesian estimation with hierarchical priors**: Incorporate prior knowledge about the distribution of factor loadings across different groups or time periods.

These approaches can help you capture the structural breaks in factor loading and provide a more nuanced understanding of the relationships between the observed variables and underlying factors.",10.468937397003174,"A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This transformation simplifies the analysis of structural changes by reducing it from a high-dimensional problem to a low-dimensional problem, allowing for the likelihood ratio (LR) test to be more powerful in detecting variance changes in the estimated factors."
llama3.1:latest,"What is null hypothesis is likehood ratio test for structural changes in factor model?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 LQ(k) = klog(| ˜Σ1 (k)|) + ( T − k) log( | ˜Σ2(k)|)
= klog(|Q′ | · | ˆΣ1 (k)| · | Q|) + ( T − k) log( |Q′| · | ˆΣ2(k)| · | Q|)
= 2 Tlog(|Q|) + klog(| ˆΣ1 (k)|) + ( T − k) log( | ˆΣ2 (k)|)
The log-likelihood of no change for the entire sample become s
LQ0 = Tlog(|T−1
T∑
t=1
˜gt,Q ˜g′
t,Q|) = Tlog(|Q′ | · | T−1
T∑
t=1
ˆgt ˆg′
t| · | Q|) = 2 Tlog(|Q|)
because T−1 ∑ T
t=1 ˆgt ˆg′
t = Ir under (3.1). Thus, the likelihood ratio test under this alte rnative
identiﬁcation condition can be expressed as
LRQ(k) ≡ − klog(| ˜Σ1(k)|) − (T − k) log( | ˜Σ2 (k)|) + LQ0 

Document 3: 
 in Section 3.
2 Performance of default bootstrap and permutation methods
To review from Section 1, the problem statement is to test the following factor model:
Yt = LtXt + ϵt for t = 1, . . . , T, (2.1)
for outcomes Yt ∈ Rp, fixed and known exposures Lt ∈ Rp×k, unobserved factor returns Xt ∈ Rk and
residuals ϵt ∈ Rp. We seek to test the null hypothesis H0 that the time series of residuals for each
asset are independent:
H0 : ϵ·,1, ϵ·,2, . . . , ϵ·,p ∈ RT are jointly independent. (2.2)
Sections 2.1-2.2 now explain why naive permutation and bootstrap tests can yield false positive rates
of up to 100%, as in Figure 3. The main challenge is that the estimated OLS residuals ˆ ϵols do not
satisfy the same independence properties as the true residuals.
2.1 Naive residual permutation tests are invalid
For simplicity, we assume for this section that the vectors of residuals ϵ1, . . . , ϵT
i.i.d.∼ Pϵ are i.i.d. This
assumption plus H0 together imply that separately permuting the residuals of each asset does not
change the joint law of all of the residuals:
ϵ :=

 

Document 4: 
 aresettozero.
• Scenario2:Changeinonecolumn.
Let1−1 = (ρ(−)
ij ) with
ρ(−)
1j = ρ(−)
j1 =



sign(ρj−1)|ρ|
1+ 3(p− 1)ρ2/2
for j> 1,
1 for j= 1.
Theremainingcorrelationcoefficientsarezero.
• Scenario3:Allcorrelationcoefficientsarechanged.
Here,weput ρij = ρ|j−i| fori, j∈ {1, . . . ,p}.
• Scenario4:Changeinthenumberoffactors.
Thenumberoffactorsinthetruemodelisincreasedto K+ ˜Kwith˜K∈ {1, . . . ,10}.
Thesefourscenariosleadtodifferenttypesoffactormodelsunderthealternativehypothesis.Forinstance,incaseof
Scenario1,asinglechangeinthecorrelationmatrixofresidualsisassumed,whereasScenario2leadstochangesinthefirst
column(row)of 11.Scenario3correspondstochangesinallelementsof 11althoughtheirmagnitudebecomessmaller 

Document 5: 
 However, it can be shown that H′
0gt = R′ ηt for some orthonormal matrix R. Because a pre-
multiplication by R does not change the distribution of an independent standard normal vector,
we can still replace Ω with ˆΩ when simulating the limiting null distribution.
Note that the presence of Ω in (3.5) is due to the potentially m isspeciﬁed likelihood function
(3.2), which assumes that gt,t = 1 ,· · · ,T are i.i.d. Gaussian under the null hypothesis. If (3.2)
correctly speciﬁes the likelihood, then the limiting distr ibution reduces to
sup
[ǫT ]≤k≤[(1−ǫ)T ]
LR(k) ⇒ sup
π∈[ǫ,1−ǫ]
[W1(π) − πW1(1)]′[W1(π) − πW1(1)]
π(1 − π) (3.7)
where W1(π) is an r(r+ 1) /2 vector of independent Brownian motions. The distribution in (3.7)
is the same as that used in conventional supreme type tests fo r a structural break, and the critical
values can be found in Andrews (1993).
If gt,t = 1 ,· · · ,T are i.i.d. Gaussian and the null hypothesis is true, the sup- LR statistic is 

Document 6: 
 200,200 0.844 0.677 0.333 0.736 0.498 0.187 1.000 1.000 0.99 9 1.000 0.998 0.956
200,500 0.999 0.999 0.984 0.999 0.997 0.951 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 0.999 0.985 0.998 0.995 0.934 1.000 1.000 1.00 0 1.000 1.000 1.000
T able 9: Power of structural break tests with unknown break d ate in the case of factor mean change.
ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.048 0.014 0.000 0.071 0.026 0.005 0.128 0.049 0.00 5 0.196 0.106 0.037
100,200 0.202 0.103 0.018 0.215 0.109 0.022 0.269 0.141 0.02 8 0.551 0.335 0.101
100,500 0.653 0.499 0.221 0.649 0.508 0.222 0.678 0.531 0.25 6 0.967 0.919 0.611
200,200 0.199 0.096 0.020 0.215 0.107 0.019 0.277 0.149 0.03 2 0.556 0.341 0.115 

","A very specific and technical question!

The likelihood ratio test (LRT) is a statistical test used to determine whether there are significant differences between two or more models. In the context of structural changes in a factor model, the LRT can be used to test the null hypothesis that there are no structural breaks (i.e., the data-generating process remains constant over time).

Here's a step-by-step explanation:

**Factor Model**

A factor model is a statistical model that decomposes the covariance structure of a set of variables into common and specific factors. The common factors capture the underlying patterns or trends in the data, while the specific factors represent the unique aspects of each variable.

**Structural Breaks**

In a time series context, structural breaks refer to changes in the relationships between variables over time. These breaks can occur due to various reasons such as policy changes, economic shocks, or regime shifts.

**Null Hypothesis (H0)**

The null hypothesis for testing structural changes in a factor model is typically stated as:

`H0: There are no structural breaks in the factor model`

In other words, the data-generating process remains constant over time, and there are no significant differences between the pre-break and post-break periods.

**Alternative Hypothesis (H1)**

The alternative hypothesis is the opposite of the null hypothesis:

`H1: There are structural breaks in the factor model`

This means that the relationships between variables have changed over time, indicating a break in the underlying structure of the data.

**Likelihood Ratio Test (LRT)**

The LRT is used to test the null hypothesis against the alternative hypothesis. The basic idea behind the LRT is to compare the likelihoods of observing the data under both hypotheses. If the likelihood of observing the data under the alternative hypothesis is significantly higher than under the null hypothesis, we reject the null hypothesis and conclude that there are structural breaks in the factor model.

**Test Statistic**

The test statistic for the LRT is typically calculated as:

`LR = 2 * (L1 - L0)`

where `L0` and `L1` are the maximum likelihoods of the data under the null and alternative hypotheses, respectively.

**P-value**

The p-value associated with the test statistic is used to determine whether the observed difference between the two models is statistically significant. If the p-value is below a certain significance level (e.g., 0.05), we reject the null hypothesis and conclude that there are structural breaks in the factor model.

In summary, the likelihood ratio test for structural changes in a factor model tests the null hypothesis that there are no structural breaks against the alternative hypothesis that there are significant differences between the pre-break and post-break periods. If the data provide strong evidence against the null hypothesis (i.e., low p-value), we reject it and conclude that there are structural breaks in the factor model.",16.728655576705933,"The null hypothesis (H0) for the likelihood ratio test for structural changes in a factor model typically posits that there are no structural changes in the factor loadings or variances across the observed period. In the context of the provided snippets, this means that the time series of residuals for each asset are jointly independent, indicating that any dependencies occurring in the data can be attributed to the specified factor model without any structural shifts. The alternative hypothesis posits that there is a structural change, which may include changes in the factor loadings or variances of the factors."
llama3.1:latest,"Is it possible to test mutliple change with sup-LR test?
","Document 1: 
 limit is similar to that of the usual W ald test,
Waldm(k) ⇒ ∥ W(π) − πW(1)∥ 2
π(1 − π)
where W(π) is an r+ r(r+ 1) /2 = r(r+ 3) /2 standard Brownian motion. This test is nuisance-
parameter free.
Consider the simple weighting by k(T − k)/T2, let WWald = k(T − k)/T2 · Waldm(k). T ake
16
the maximum over the range say k= r+ 1,r + 2,· · · ,T − r− 1. The limiting distribution is simply
sup
π∈[0,1]
∥ W(π) − πW(1)∥ 2 ,
and the critical values do not depend on trimming parameter ǫ.
4.2 T esting multiple changes
In this section, we extend the sup-LR test to multiple changes. W e consider testing the null
hypothesis of no change versus the alternative hypothesis o f a prespeciﬁed number of changes. T o
allow for mchanges under the alternative hypothesis, let 0 = k0 <k1 <k2 <· · · <km <km+1 = T
and deﬁne
LR(k1 ,...,k m) = −
m∑
j=0 

Document 2: 
 the null hypothesis. As the value of a increases, all of the tests become more powerful, except
for W ald(HAC) and LM(HAC) when N = T = 100. The LR test has the highest power under
this DGP . The power of the LR m test increases at a slower rate as a increases. Our Theorem
3 demonstrates that the sup-LR diverges at the same rate as th e conventional sup-W ald when
both pre-break and post-break pseudo-factors have nonsing ular variances. However, our simulation
results indicate that sup-LR remains more powerful than sup -W ald under this scenario, as reported
in T able 7. Speciﬁcally , when the loading matrix undergoes a rotational change, sup-LR is notably
more powerful than sup-W ald with HAC variance. While we ackn owledge that the theoretical
power comparison under this setup may require analysis unde r local alternatives, we leave this as
a future research topic.
T able 8 presents the power against changes in the number of fa ctors. The post-break loadings
are equal to pre-break loadings multiplied by an r × r matrix, i.e., Λ 2 = Λ 1C. W e set C = 

Document 3: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 LRm (k1,...,k m) ⇒ sup
(π1 ,··· ,πm)∈Θ π,ǫ
m∑
j=0
∥ ¯Ω 1/2 [B(πj+1 )−B(πj )]∥ 2
πj+1 −πj
, (4.3)
where B(π) = U(π) − πU(1) is an (r+ r2) Brownian bridge process, and ¯Ω is deﬁned in Theorem
4.
These results depend on m, but mdoes not have to be correctly speciﬁed. W e can also consider
the double max type of test and the conditional test for multi ple breaks as in Bai and Perron (1998),
but we leave these as future research topics.
4.3 Determining the number of breaks
In practice, the number of breaks in a factor model is often unknown. W e follow Bai’s (1997)
sequential testing procedure to provide a consistent estim ate for the true number of breaks. The
procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples: 

Document 6: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

","Yes, it is possible to test multiple changes using the sup-LR test. In fact, the text states that we extend the sup-LR test to multiple changes in Section 4.2 and provide a formula for the test statistic LRm(k1,...,k m).",3.6913437843322754,"Yes, it is possible to test for multiple changes using the sup-LR test. The context indicates that the sup-LR test has been extended to allow for multiple changes under the alternative hypothesis, enabling the testing of the null hypothesis of no change against a prespecified number of changes, as outlined in Document 1 and Document 5."
llama3.1:latest,"What is misbehavior of the residual bootstrap in high dimension?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 ϵ = 2. Each simulation uses the standard estimate of σ2
ϵ from
the generated ϵi’s. rρ(κ; G) was computed using a ﬁrst run of simulations with
ϵi
iid∽ G. The Huber loss in this plot is Huber 1 and not the default Huber 1.345 of
the rlm function.
to bootstrapping from the residuals. In the case of random design, it makes also a lot
of intuitive sense to use the pairs bootstrap, since resampling the predictors might be
interpreted as mimicking the data generating process.
However, as in residual bootstrap, it is clear that the pairs bootstrap will have problems,
at least in quite high dimensions. In fact, when resampling the Xi’s from ˆF, the number
of times a certain vector Xi0 is picked has asymptotically Poisson(1) distribution. So the
expected number of diﬀerent vectors appearing in the bootstrapped design matrix X∗ is
n(1 −1/e). When p/n is large, with increasingly high probability the bootstrapped design
matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that 

Document 3: 
 accurate conﬁdence intervals in moderately high dimensions.
In Section 4, we discuss another resampling scheme, the jackknife. We focus on the
jackknife estimate of variance and show that it has similarly poor behavior in high dimen-
sions. In the case of L2 loss with Gaussian design matrices, we further prove that the
jackknife estimator over estimates the variance of our estimator by a factor of 1 /(1 −p/n);
we also provide corrections for other losses that improve the jackknife estimate of variance
in moderately high dimensions.
We rely on simulation results to demonstrate the practical impact of the failure of the
bootstrap. The settings for our simulations and corresponding theoretical analyses are ide-
alized, without many of the common settings of heteroskedasticity, dependency, outliers and
so forth that are known to be a problem for bootstrapping. This is intentional, since even
these idealized settings are suﬃcient to demonstrate that the standard bootstrap methods
have poor performance. For brevity, we give only brief descriptions of the simulations in
what follows; detailed descriptions can be found in AppendixD.1.
Similarly, we focus on the basic implementations of the bootstrap for linear models.
While there are many proposed alternatives (often for speciﬁc loss functions or types of
data), the standard methods we study are most commonly used and recommended in prac-
tice. Furthermore, to our knowledge none of the alternative bootstrap methods we have 

Document 4: 
 regression, by making assumptions on the diagonal entries of the hat matrix. In our context,
these assumptions would be satisﬁed only if p/n →0. Hence those interesting results do
not apply to the present study. We also note that Hall (1992, p. 167) contains cautionary
notes about using the bootstrap in high-dimension.
While there has not been much theoretical work on the bootstrap in the setting where
p/n →κ ∈(0,1), one early work of Bickel and Freedman (1983) considered bootstrap-
ping scaled residuals for least-squares regression when κ > 0. They show that when
p/n → κ ∈ (0,1), there exists a data-dependent direction c, such that c′ˆβ∗
LS does not
have the correct asymptotic distribution (Bickel and Freedman, 1983, Theorem 3.1, p.39),
i.e., its distribution is not conditionally in probability close to the sampling distribution of
c′ˆβLS. Furthermore, they show that when the errors in the model are Gaussian, under the
assumption that the diagonal entries of the hat matrix are not all close to a constant, the
empirical distribution of the residuals is a scaled-mixture of Gaussian, which is not close to
the original error distribution.
As we previously explained, in this work we instead only consider inference forpredeﬁned
contrasts υ′β. The important and interesting problems pointed out in Bickel and Freedman 

Document 5: 
 Journal of Machine Learning Research 19 (2018) 1-66 Submitted 1/17; Revised 12/17; Published 08/18
Can We Trust the Bootstrap in High-dimensions?
The Case of Linear Models
Noureddine El Karoui nkaroui@berkeley.edu, n.elkaroui@criteo.com
Criteo AI Lab
32 Rue Blanche
75009 Paris, France
and
Department of Statistics
University of California
Berkeley, CA 94270, USA
Elizabeth Purdom epurdom@stat.berkeley.edu
Department of Statistics
University of California
Berkeley, CA 94270, USA
Editor: Guy Lebanon
Abstract
We consider the performance of the bootstrap in high-dimensions for the setting of linear
regression, where p<n but p/nis not close to zero. We consider ordinary least-squares as
well as robust regression methods and adopt a minimalist performance requirement: can
the bootstrap give us good conﬁdence intervals for a single coordinate of β (where β is the
true regression vector)?
We show through a mix of numerical and theoretical work that the bootstrap is fraught
with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","The residuals tend to have a very different distribution from that of the true errors in high dimensions. This results in inaccurate inference and conﬁdence intervals for regression parameters, with the residual bootstrap tending to give anti-conservative estimates (inﬂated Type I error).",3.527274370193481,"The misbehavior of the residual bootstrap in high dimensions is primarily due to the fact that the residuals tend to have a very different distribution compared to the true errors. Specifically, their distributions differ not only in basic properties such as variances but also in more general aspects, including their marginal distributions. This discrepancy can lead to anti-conservative estimates, resulting in inflated Type I error rates. When the ratio of predictors to observations (p/n) is large, the residual bootstrap fails to provide accurate inferences, highlighting the challenges of using standard bootstrap methods in high-dimensional regression settings."
llama3.1:latest,"How misbehavior of the residual in high dimension can be explained?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 decent fit of the linear normal one-factor model, our method detected misfit in various
model assumptions: misfit in the LV density, the MV-level mean function, and the
MV-level variance function.
In factor analysis, it is common to assume the joint multivariate normality of MVs
and LVs in order to perform ML estimation. Various well-known methods exist for
assessing normality at the level of observed variables (e.g., Mardia, 1970; Mardia, 1980);
however, to our knowledge, assessing the normality assumption for LVs is less common.
This LV-level misfit can be conveniently assessed by generalized residuals, which sometimes
GENERALIZED RESIDUALS 29
can explain non-normality at the MV level, as is the case in our empirical example.
Furthermore, graphical plots such as the right panel of Figure 8 provide intuitive guidance
for model modification.
In the MV-level fit assessment, our method helps to identify curvilinear
relationships in the mean and/or variance functions. In the case of item 7 in our empirical
example, the misfitting linear normal factor model tended to overestimate the conditional
mean of log-RT while underestimating the conditional variance at the high end of the LV.
Such misfits can be particularly problematic if researchers’ focus is on investigating
extremely slow responders. Similar discussions can also be found in Liu and Wang (2022),
where similar patterns of curvatures in the conditional moment functions have been
identified using a semiparametric approach. 

Document 3: 
 P
→λ⋆. This gives that 2Λj is a
multiple of aχ2
1 variable with multiplicative factor given byκσ2
⋆/λ⋆.
This rough analysis shows that the distribution of the LLR in high dimensions deviates from aχ2
1 due
to the coupled eﬀects of two high-dimensional phenomena. The ﬁrst is the inﬂated variance of the MLE,
which is larger than classically predicted. The second comes from the termλ⋆, which is approximately equal
to Tr
(
H−1( ˆβ)
)
/n, where H( ˆβ) = ∇2ℓ( ˆβ) is the Hessian of the negative log-likelihood function. In the
classical setting, this Hessian converges to a population limit. This is not the case in higher dimensions and
the greater spread in the eigenvalues also contributes to the magnitude of the LRT.
5 Broader Implications and Future Directions
This paper shows that in high-dimensions, classical ML theory is unacceptable. Among other things, classical
theory predicts that the MLE is approximately unbiased when in reality it seriously overestimates eﬀect
magnitudes. Since the purpose of logistic modeling is to estimate the risk of a speciﬁc disease given a patient’s
observed characteristics, say, the bias of the MLE is extremely problematic. As we have seen, an immediate
consequence of the strong bias is that the MLE either dramatically overestimates, or underestimates, the
chance of being sick. The issue becomes increasingly severe as either the dimensionality or the signal 

Document 4: 
 data IRT (e.g., Wang, 2010).
Finally, exploring efficient ways to compute and visualize generalized residuals when
dealing with high-dimensional LVs is a potential topic for future research. When the latent
dimensionality is high, it is impossible to construct evaluation points using an outer
product grid as the number of evaluation points grows exponentially fast. For example, in
a five-dimensional case, examining residuals at just five different points per LV leads to
55 = 3, 125 evaluations, making it computationally intensive and posing a challenge for
generating graphical plots. Additionally, selecting which grids of LVs to summarize the
results would also be challenging. Formulating partially marginalized residuals conditioned
only on one or two LV(s) may yield more informative diagnostics.
GENERALIZED RESIDUALS 30
References
Alexander, P. A., Dumas, D., Grossnickle, E. M., List, A., & Firetto, C. M. (2016).
Measuring relational reasoning.The Journal of Experimental Education, 84(1),
119–151.
Bentler, P. M. (1990). Comparative fit indexes in structural models.Psychological bulletin,
107(2), 238.
Bentler, P. M. (1995).Eqs structural equations program manual(Vol. 6). Multivariate
software Encino, CA.
Bickel, P. J., & Doksum, K. A. (2015).Mathematical statistics: Basic ideas and selected 

Document 5: 
 pressed sensing. IEEE Transactions on Information Theory, 57(10):6920–6941, 2011.
[26] Noureddine El Karoui. Asymptotic behavior of unregularized and ridge-regularized high-dimensional
robust regression estimators: rigorous results.arXiv preprint arXiv:1311.2445, 2013.
[27] Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators.Probability Theory and Related Fields, pages
1–81, 2017.
23
[28] Noureddine El Karoui, Derek Bean, Peter J Bickel, Chinghway Lim, and Bin Yu. On robust regression
with high-dimensional predictors. Proceedings of the National Academy of Sciences, 110(36):14557–
14562, 2013.
[29] Jianqing Fan and Jinchi Lv. Nonconcave penalized likelihood with NP-dimensionality.IEEE Transac-
tions on Information Theory, 57(8):5467–5484, 2011.
[30] Yingying Fan, Emre Demirkaya, and Jinchi Lv. Nonuniformity of p-values can occur early in diverging
dimensions. https://arxiv.org/abs/1705.03604, May 2017.
[31] David Firth. Bias reduction of maximum likelihood estimates.Biometrika, 80(1):27–38, 1993. 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","A question from the realm of statistical inference!

The ""misbehavior"" you're referring to is likely related to the phenomenon where, in high-dimensional spaces (i.e., when the number of variables \(p\) is large compared to the sample size \(n\)), the residuals (or errors) from a regression model can exhibit strange or unexpected behavior. Here are some explanations for this:

1.  **The Curse of Dimensionality**: In high-dimensional spaces, the data points become increasingly sparse, making it difficult for statistical models to capture meaningful relationships between variables. As a result, the residuals may not follow standard distributions (e.g., normal) and can exhibit unusual patterns.
2.  **Singularity and Ill-Conditioning**: When dealing with high-dimensional data, the design matrix (or covariance matrix) can become singular or ill-conditioned. This leads to unstable estimates of model parameters and increased variability in the residuals.
3.  **Model Misspecification**: In high-dimensional settings, it's challenging to specify a correct model that captures all relevant relationships between variables. Model misspecification can result in biased or inconsistent estimates of regression coefficients, leading to unusual residual behavior.
4.  **Overfitting and Underfitting**: High-dimensional models are prone to overfitting (capturing noise rather than underlying patterns) or underfitting (failing to capture important relationships). Both issues can lead to poor model performance and unexpected residual behavior.

To mitigate these challenges, you may consider the following strategies:

1.  **Dimensionality Reduction**: Techniques like PCA, t-SNE, or feature selection can help reduce the number of variables while retaining meaningful information.
2.  **Regularization**: Regularized regression methods (e.g., Ridge, Lasso, Elastic Net) can improve model stability and reduce overfitting.
3.  **Non-parametric Methods**: Non-parametric models (e.g., random forests, gradient boosting) are often more robust to high-dimensional data and can provide better estimates of relationships between variables.
4.  **Model Selection and Validation**: Carefully evaluating multiple models using metrics like cross-validation can help identify the best approach for a given problem.

Addressing the ""misbehavior"" of residuals in high dimensions requires a thoughtful consideration of the underlying assumptions, model specification, and data characteristics.",13.167354822158812,"The misbehavior of the residual bootstrap in high dimensions can be explained by the fact that the residuals tend to have a very different distribution compared to that of the true errors. This discrepancy is evident in both simple properties, such as variances, and more general aspects, including the marginal distributions. The work of El Karoui et al. (2013) characterizes the asymptotic relationship between the residuals and true errors in high dimensions, which reveals that as the dimensionality increases, the classical assumptions of the bootstrap no longer hold, leading to a poor approximation of confidence intervals and other statistical inference derived from bootstrap methods."
llama3.1:latest,"What is a alternative residual bootstrap procedure called deconvolution?
","Document 1: 
 This explains why the scaled ˜ ri(i) is probably a good error distribution ϵ∗ to use in the
residual bootstrap when κis close to 0 or 1. We note that when κis close to 1, ˜ri(i) gives an
error distribution that is in general very diﬀerent from the distribution of ϵ. Our numerical
work of Section 2.3 shows that it is nonetheless a good error distribution from the point of
view of the residual bootstraps we consider.
Appendix B. Deconvolution Bootstrap
In the main text, we considered situations where our predictorsXi are i.i.d with an elliptical
distribution and assume for instance that Xi = λiξi, where ξi ∼N (0,Σ) and λi are i.i.d
scalar random variables with E
(
λ2
i
)
= 1. As described in the main text, if X is elliptical,
˜ei(i) is a convolution of the correct G distribution and a Normal distribution,
˜ei(i) ≃ϵi + ˜Zi,
where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the 

Document 2: 
 problem, and experiments demonstrate that increasing cardinality C is more effective
than going deeper or wider when we increase the capacity. The authors compared the
completely equivalent structures of the three mathematical calculations in Figure 18.
The experimental results show that block Figure 18c with grouped convolution is
more succinct and faster than the other two forms, and ResNeXt uses this structure as
a basic block.
5. Dilated Residual Networks (DRN). To solve the decrease in the resolution of the
feature map and the loss of feature information caused by downsampling. However,
simply removing subsampling steps in the network will reduce the receptive ﬁeld. So,
Yu et al. [120] introduced dilated convolutions that are used to increase the receptive
ﬁeld of the higher layers and replaced a subset of the internal downsampling layer
Remote Sens. 2021, 13, 4712 19 of 51
based on the residual network, compensating for the reduction in receptive ﬁeld
induced by removing subsampling. Compared to ResNet with the same parameter
amount, the accuracy of DRN is signiﬁcantly improved in image classiﬁcation.
6. Other models. Veit et al. [ 121] drops some of the layers of a trained ResNet and
still have comparable performance. Resnet in Resnet (RiR) [ 122] proposed a deep
dual-stream architecture that generalizes ResNets and standard CNNs and is easily
implemented with no computational overhead. DropBlock [123] technique discards 

Document 3: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 4: 
 and mild moment requirements. In implementing the two alternative residual-bootstrap
methods described above, we use ˆσϵ,LS as our estimate of σϵ, including for bootstrapping
robust regression where ρ(x) ̸= x2.
Performance in bootstrap inferenceIn Figure 2 we show the error rate of conﬁdence intervals
based on the two residual-bootstrap methods we proposed above. We see that both methods
control the Type I error, unlike bootstrapping directly from the residuals, and that both
methods are conservative. There is little diﬀerence between the two methods with this
sample size (n= 500), though with n= 100, we observe the deconvolution performance to
be worse in L1 (data not shown).
The deconvolution strategy, however, depends on the distribution of the design matrix,
which in these simulations we assumed was Gaussian (so we did not have to estimate λi’s).
For elliptical designs (λi ̸= 1), the error rate of the deconvolution method described above,
with no adaptation for the design, was similar to that of uncorrected residuals in high
dimensions (i.e., > 0.25 for p/n = 0 .5). Individual estimates of λi might improve the
deconvolution strategy, but this problem points to the general reliance of the deconvolution
method on precise knowledge about the design matrix. The bootstrap using standardized
predicted errors, on the other hand, had a Type I error for an elliptical design only slightly 

Document 5: 
 where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the
following bootstrap procedure:
1. Calculate the predicted errors, ˜ei(i)
2. Estimate |λi|∥ˆβρ(i) −β∥2 (the standard deviation of the ˜Zi)
3. Deconvolve in ˜ei(i) the error term ϵi from the ˜Zi term ;
4. Use the resulting estimates of G as the estimate of ˆG in residual bootstrapping.
34
Can We Trust the Bootstrap in High-dimensions?
B.1 Estimating ∥ˆβρ −β∥and the Variance of the Zi
Deconvolution methods that deconvolve ϵ from the ˜Zi require an estimate of the variance
of the ˜Zi. Equation (3) gives the variance as λ2
i∥ˆβρ(i) −β∥2
2, and we need to estimate this
quantity from the data. We use the approximation
∥ˆβρ(i) −β∥2 ≃∥ˆβρ −β∥2.
See AppendixA and references therein for justiﬁcation of this approximation.
Furthermore, as we note in the main text, in our implementation of this deconvolution in 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","The alternative residual bootstrap procedure using deconvolution is called Method 1, or more specifically, it involves ""deconvolving in `ei(i)` the error term `ϵ_i` from the `Z_i` term"".",3.4190378189086914,"The alternative residual bootstrap procedure called deconvolution involves several steps to estimate the distribution of the error term while accounting for the influence of the predictors. Specifically, the procedure includes:

1. Calculating the predicted errors \(\tilde{e}_i(i)\).
2. Estimating \(|\lambda_i|\| \hat{\beta}_{\rho}(i) - \beta \|_2\), which represents the standard deviation of the \(\tilde{Z}_i\) term.
3. Deconvolving in \(\tilde{e}_i(i)\) the error term \(\epsilon_i\) from the \(\tilde{Z}_i\) term.
4. Using the resulting estimates of \(G\) as the estimate of \(\hat{G}\) in residual bootstrapping.

This procedure is particularly useful in situations where the predictors have an elliptical distribution and is intended to improve the accuracy of bootstrap inference in high-dimensional settings."
llama3.1:latest,"What is a alternative residual bootstrapinf from sttandarized residual?
","Document 1: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 2: 
 samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con-
ditions, our results allow the factor returns {Xt}T
t=1 to be arbitrarily non-stationary and the residuals
{ϵt}T
t=1 to be non-stationary across tiles. For instance, in our empirical application, we require that
the residuals are stationary within each week, but their distributions can change arbitrarily between
weeks.
To illustrate this contribution, we conduct semisynthetic simulations using the exposure matrix Lt
from the BFRE model for energy stocks on May 21st, 2020. For simplicity, we generate data Y from
Eq. 1.1 after sampling the residuals and factor returns as i.i.d. standard Gaussians, with T = 350
observations. We use the test statistic from Section 1.2. Figure 3 shows that in this simple Gaussian
setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining 

Document 3: 
 15
El Karoui and Purdom
Implications for the Bootstrap For the purposes of the residual-bootstrap, Theorem 1 im-
plies that diﬀerent methods of estimating the residual distribution ˆG will result in similar
residual-bootstrap conﬁdence intervals as p/n→1, if ˆGhas the same variance. This agrees
with our simulations, where both of our proposed bootstrap strategies set the variance of ˆG
equal to ˆσ2
ϵ,LS and both had similar performance in our simulations for large p/n. Further-
more, as we noted, for p/n closer to 1, they both had similar performance to a bootstrap
procedure that simply sets ˆG = N(0,ˆσ2
ϵ,LS) (Figure 2) (see also AppendixA.3 for further
discussion of residual bootstrap methods which draw from the “wrong” distribution, i.e.,
forms of wild bootstraps (Wu, 1986)).
We return speciﬁcally to the bootstrap based on ˜ri(i), the standardized predicted errors.
Equation (3) tells us that the marginal distribution of ˜ei(i) is a convolution of the distribution
of ϵi and a normal, with the variance of the normal governed by the term∥ˆβρ−β∥2. Theorem
1 makes rigorous our previous assertion that as p/n →1, the normal term will dominate
and the marginal distribution of ˜ei(i) will approach normality, regardless of the distribution 

Document 4: 
 well beyond the expected 5%: we observe error rates of 10-15% for p/n= 0.3 and approxi-
mately 20% for p/n= 0.5. We see similar error rates for other robust-regression methods,
such as L1 and Huber loss, and also for diﬀerent error distributions and distributions of X
(Supplementary Figures A-1 and A-2). We explain some of the reasons for these problems
in Subsection 2.2 below.
2.1 Bootstrapping from Corrected Residuals
While resampling directly from the uncorrected residuals is widespread and often given
as a standard bootstrap procedure (e.g., Koenker, 2005; Chernick, 1999), the discrepancy
between the distribution of ϵi and ei has spurred more reﬁned recommendations in the
8
Can We Trust the Bootstrap in High-dimensions?
case of least-squares: form corrected residuals ri = ei/√1 −hi and sample the ϵ∗
i from the
empirical distribution of the ri −¯r (see e.g., Davison and Hinkley, 1997).
This correction is known to exactly align the variance of ri with that of ϵi regardless
of the design vectors Xi or the true error distribution, using simply the fact that the hat
matrix is a rank min(n,p) orthogonal projection matrix. We see that for L2 loss it corrects 

Document 5: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","An alternative to standard residual bootstrapping is to use the standardized predicted errors, ˜ri(i), in place of ei. This approach corrects for the difference between the distribution of ϵi and ei in high dimensions.",3.341752767562866,"An alternative to the standard residual bootstrap is to form corrected residuals instead of resampling directly from the uncorrected residuals. For instance, the corrected residuals can be defined as \( r_i = \frac{e_i}{\sqrt{1 - h_i}} \), where \( e_i \) are the residuals from the model and \( h_i \) are the leverage values derived from the hat matrix. By sampling \( \epsilon^*_i \) from the empirical distribution of the corrected residuals \( r_i - \bar{r} \), the variance of the corrected residuals can be aligned with the variance of the true errors \( \epsilon_i \) regardless of the distribution of the design vectors \( X_i \) or the true error distribution. This method aims to address the discrepancy between the distribution of the residuals and the true errors, especially in high-dimensional settings."
llama3.1:latest,"What is pair bootstrap?
","Document 1: 
 i = X′
iˆβρ + ϵ∗
i, from which ˆβ∗
ρ is computed. This
method of bootstrapping assumes that the linear model is correct for the mean of y (i.e.,
that E (yi) = X′
iβ); it also assumes ﬁxed Xi design vectors because the sampling is con-
ditional on the Xi. In the second method, called pairs bootstrap, ˆF is an estimate of the
joint distribution of the vector ( yi,Xi) ∈Rp+1 given by the empirical joint distribution
of {(yi,Xi)}n
i=1; the corresponding resampling method resamples the pairs ( yi,Xi). This
method makes no assumption about the mean structure of yand, by resampling the Xi, also
does not condition on the values of Xi. For this reason, pairs resampling is often considered
to be more generally applicable than residuals resampling (see e.g., Davison and Hinkley,
1997).
1.3 Background: High-dimensional Inference of Linear Models
Recent research shows that ˆβρ has very diﬀerent asymptotic properties when p/nhas a limit
κ that is bounded away from zero than it does in the classical setting where p/n→0 (see 

Document 2: 
 B.4 Bootstrap Estimates ˆβ∗from ˆG . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.5 Estimation of λ2
i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C Alternative Weight Distributions for Pairs Bootstrap 38
C.1 Case Σ ̸= Idp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D Description of Numerics 39
D.1 Simulation Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

Document 3: 
 v′ˆβ∗over repeated resampling from the pairs ( yi,Xi).
18
Can We Trust the Bootstrap in High-dimensions?
1 1
1
1
Ratio ( κ )
% Increase in Average CI Width
2
2
2
2
3
3
3
3
0.01 0.10 0.30 0.50
1
10
50
100
350 1
2
3
Normal
Ellip. Normal
Ellip. Exp
Figure 4: Comparison of width of 95% conﬁdence intervals of β1 for L2 loss:
Here we demonstrate the increase in the width of the conﬁdence interval due to
pairs bootstrapping. Shown on the y-axis is the percent increase of the average
conﬁdence interval width based on simulation ( n = 500), as compared to the
average for the standard conﬁdence interval based on normal theory in L2; the
percent increase is plotted against the ratio κ = p/n (x-axis). Shown are three
diﬀerent choices in simulating the entries of the design matrix X: (1) Xij ∼
N(0,1) (2) elliptical Xij with λi ∼ N(0,1) and (3) elliptical Xij with λi ∼
Exp(
√
2). The methods of simulation are the same as described in Figure 1;
exact values are given in Table A-2 in AppendixI. 

Document 4: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 5: 
 bootstrap and other resampling plans perform well when p is comparable to n.
Deﬁning success: accurate inference on β1 The common theoretical deﬁnition of whether
the bootstrap “works” is that the bootstrap distribution of the entire bootstrap estimate
ˆβ∗
ρ converges conditionally almost surely to the sampling distribution of the estimator ˆβρ
(see e.g., van der Vaart, 1998). The work of Bickel and Freedman (1983) on the residual
bootstrap for least squares regression, which we discuss in the background section 1.2, shows
that this theoretical requirement is not fulﬁlled even for the simple problem of least squares
regression.
In this paper, we choose to focus only on accurate inference for the projection of our
parameter on a pre-speciﬁed direction υ. More speciﬁcally, we concentrate only on whether
the bootstrap gives accurate conﬁdence intervals for υ′β. We think that this is the absolute
minimal requirement we can ask of a bootstrap inferential method, as well as one that is
meaningful from the standpoint of applied statistics. This is of course a much less stringent
requirement than performing well on complicated functionals of the whole parameter vector,
which is the implicit demand of standard deﬁnitions of bootstrap success. For this reason,
we focus throughout the exposition on inference for β1 (the ﬁrst element of β) as an example
of a pre-deﬁned direction of interest (where β1 corresponds to choosing υ = e1, the ﬁrst
canonical basis vector). 

Document 6: 
 matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that
situation makes little statistical sense (El Karoui, 2010, Subsection 4.4; Zheng et al., 2014).
For smaller ratios of p/n, we evaluate the performance of pairs bootstrapping on sim-
ulated data. We see that the performance of the bootstrap for inference also declines
17
El Karoui and Purdom
dramatically as the dimension increases, becoming increasingly conservative (Figure 1). In
pairs bootstrapping, the error rates of 95%-conﬁdence-intervals drop far below the nominal
5%, and are essentially zero for the ratio of p/n= 0.5. Like residual bootstrap, this overall
trend is seen for all the settings we simulated under (Supplemental Figures A-1, A-2). For
L1 loss, even ratios as small as 0 .1 yield incredibly conservative bootstrap conﬁdence inter-
vals for ˆβ1, with the error rate dropping to less than 0.01. For Huber and L2 losses, the
severe loss of power in our simulations starts for ratios of 0 .3.
A minimal requirement for the distribution of the bootstrapped data to give reasonable
inferences is that the variance of the bootstrap estimator ˆβ∗ 

","In the context provided, pairs bootstrap is a method of resampling where the pairs (yi,Xi) are sampled together. This approach does not condition on the values of Xi and makes no assumptions about the mean structure of y. It is considered more generally applicable than residuals resampling, but its performance can decline as the dimension increases.",3.70107102394104,"Pairs bootstrap is a resampling method used in statistical inference that involves resampling pairs of observations from a dataset, specifically the pairs of outcome variables (yi) and design vectors (Xi). Unlike residuals resampling, which assumes a specific mean structure for the outcome, pairs bootstrap does not make such assumptions about the mean of the outcome variable and allows for the resampling of the design vectors as well. This method is often considered to be more generally applicable because it does not condition on the values of Xi. It aims to estimate the joint distribution of the pairs (yi, Xi) based on their empirical distribution from the original sample."
llama3.1:latest,"How central limit theorem for hyperrectangles work?
","Document 1: 
 using the empirical or multiplier bootstraps.
The results in this paper substantially extend those obtain ed in [17] where
we considered the class A = Am of sets of the form A = {w ∈ Rp :
maxj∈J wj ≤ a} for some a∈ R and J ⊂ { 1,...,p }, but in order to obtain
much better dependence on n, we employ new techniques. Most notably, as
the main ingredient in the new proof, we employ an argument in spired by
Bolthausen [10]. Our paper builds upon our previous work [17 ], which in
turn builds on a number of works listed in the bibliography (s ee [18] for a
detailed review and links to the literature).
The organization of this paper is as follows. In Section 2, we derive
a Central Limit Theorem (CLT) for hyperrectangles in high di mensions;
that is, we derive a bound on ρn(A) for A = Are being the class of all
hyperrectangles and show that the bound converges to zero un der certain
conditions even when pis potentially larger or much larger than n. In Section
3, we extend this result by showing that similar bounds apply for A ⊂
Asi(a,d) being a class of simple convex sets and for A = Asp(s) being the
class of all s-sparsely convex sets. In Section 4, we derive high dimensio nal 

Document 2: 
 Assumption F1. Assumption 6(ii) strengthens Bai’s (2003) A ssumption F3 and ensures the con-
sistency of the heteroskedasticity and autocorrelation co nsistent (HAC) covariance estimator of
ˆg′
t ˆgt − Ir . Assumption 7 follows from Assumption 10 of DBH (2022) or Ass umption 8 of HI (2015).
Assumption 8 states that a basic functional central limit th eorem holds for the sums of gt under
the null hypothesis of no break.
9
3.2 Limiting distribution of sup-LR under the null hypothes is
It is well known that ˆGis an estimator of GH, where H = (Λ
′
Λ/N)(G
′ ˆG/T)V−1
N T and VN T denotes
the eigenvalues of XX′/NT. Bai’s (2003) Proposition 1 shows that G
′ ˆG/T →p Σ−1/2
Λ ΥV1/2 , where
V is the probability limit of VN T and an r× r diagonal matrix of the eigenvalues of Σ 1/2
Λ ΣG Σ1/2
Λ ,
and Υ is the eigenvector of Σ 1/2
Λ ΣGΣ1/2
Λ . Thus, 

Document 3: 
 where the ﬁrst three lines follow from the properties of Fβ(w) and g(t) (recall
that m(w) = g(Fβ(w))), and the last inequality follows from Lemma A.1.
Here the constant C depends only on b. Likewise we have
P(Vn ≤ y− φ−1) ≥ P(SW
n ≤ y− φ−1) − Cφ−1 log1/2 p− |E[In]|.
The conclusion of the lemma follows from combining these ine qualities with
the bound on |E[In]| derived in Step 1. ■
Proof of Corollary 5.1. Pick any hyperrectangle
A= {w∈ Rp : wj ∈ [aj,bj] for all j = 1 ,...,p }.
For i = 1 ,...,n , consider the random vectors ˜Xi and ˜Yi in R2p deﬁned
by ˜Xij = Xij and ˜Yij = Yij for j = 1 ,...,p , and ˜Xij = −Xi,j−p and
˜Yij = −Yi,j−p for j = p+ 1,..., 2p. Then
P(SX
n ∈ A) = P( S
˜X
n ≤ y), P(SY
n ∈ A) = P( S
˜Y 

Document 4: 
 ) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely
convex sets are sets that can be represented as intersection s of many
convex sets whose indicator functions depend only on a small subset of
their arguments, with hyperrectangles being a special case .
1. Introduction
Let X1,...,X n be independent random vectors in Rp where p≥ 3 may be
large or even much larger than n. Denote by Xij the j-th coordinate of Xi,
so that Xi = ( Xi1,...,X ip)′. We assume that each Xi is centered, namely
E[Xij] = 0, and E[ X2
ij] <∞ for all i= 1 ,...,n and j = 1 ,...,p . Deﬁne the
normalized sum
SX
n := ( SX
n1,...,S X
np)′ := 1
√n
n∑
i=1
Xi.
We consider Gaussian approximation to SX
n , and to this end, let Y1,...,Y n 

Document 5: 
 by ψα(x) := exp( xα) − 1, and for a real-valued random variable ξ, we deﬁne
∥ξ∥ψα := inf {λ> 0 : E[ ψα(|ξ|/λ)] ≤ 1}.
For α∈ [1,∞), ∥ · ∥ψα is an Orlicz norm, while for α∈ (0,1), ∥ · ∥ψα is not a
norm but a quasi-norm, that is, there exists a constant Kα depending only
on α such that ∥ξ1 + ξ2∥ψα ≤ Kα(∥ξ1∥ψα + ∥ξ2∥ψα ). Throughout the paper,
we assume that n≥ 4 and p≥ 3.
2. High Dimensional CLT for Hyperrectangles
This section presents a high dimensional CLT for hyperrecta ngles. We
begin with presenting an abstract theorem (Theorem 2.1); th e bound in
Theorem 2.1 is general but depends on the tail properties of t he distri-
butions of the coordinates of Xi in a nontrivial way. Next we apply this
theorem under simple moment conditions and derive more expl icit bounds
(Proposition 2.1).
Let Are be the class of all hyperrectangles in Rp; that is, Are consists of
all sets A of the form 

Document 6: 
 arXiv:1412.3661v4  [math.ST]  8 Mar 2016
CENTRAL LIMIT THEOREMS AND BOOTSTRAP IN
HIGH DIMENSIONS
VICTOR CHERNOZHUKOV, DENIS CHETVERIKOV, AND KENGO KATO
Abstract. This paper derives central limit and bootstrap theorems for
probabilities that sums of centered high-dimensional rand om vectors hit
hyperrectangles and sparsely convex sets. Speciﬁcally, we derive Gauss-
ian and bootstrap approximations for probabilities P( n− 1/ 2 ∑ n
i=1 Xi ∈
A) where X1, . . . , Xn are independent random vectors in Rp and A is
a hyperrectangle, or, more generally, a sparsely convex set , and show
that the approximation error converges to zero even if p = pn → ∞ as
n → ∞ and p ≫ n; in particular, p can be as large as O(eCnc
) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely 

","The Central Limit Theorem (CLT) for hyperrectangles states that the probability of a normalized sum of centered high-dimensional random vectors hitting a hyperrectangle converges to a Gaussian distribution as the sample size increases, even when the dimension p is large or larger than n. This means that the distribution of the sum can be approximated by a multivariate normal distribution.",3.884566068649292,"The Central Limit Theorem (CLT) for hyperrectangles in high dimensions, as described in the provided snippets, involves the analysis of sums of independent random vectors and how these sums behave when they are constrained to lie within hyperrectangles or sparsely convex sets. The key points are as follows:

1. **Definition of Hyperrectangles**: A hyperrectangle in \( \mathbb{R}^p \) is defined as a set \( A = \{ w \in \mathbb{R}^p : w_j \in [a_j, b_j] \text{ for all } j = 1, \ldots, p \} \).

2. **Random Vectors**: The CLT applies to independent random vectors \( X_1, \ldots, X_n \) in \( \mathbb{R}^p \), where the number of dimensions \( p \) can be much larger than the number of observations \( n \).

3. **Normalized Sums**: The normalized sum \( S_n^X = \frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \) is considered. The paper derives Gaussian approximations for the probabilities that \( S_n^X \) lies in a specified hyperrectangle.

4. **Convergence of Approximation Error**: The approximation error (the difference between the true distribution of \( S_n^X \) and the Gaussian distribution) converges to zero under certain conditions, even when \( p \) grows significantly with \( n \).

5. **Generalization to Sparsely Convex Sets**: The results hold not just for hyperrectangles but also for more general sparsely convex sets, which are combinations of several convex sets.

6. **Independence of Correlation Structure**: The results are robust in that they do not require any specific assumptions about the correlation structure of the random variables involved.

In summary, the CLT for hyperrectangles establishes that even in high dimensions, under appropriate conditions, the sum of independent centered random vectors will converge in distribution to a multivariate normal distribution when constrained to fall within hyperrectangles or certain convex shapes."
llama3.1:latest,"What is high dimensional CLT for simple and sparsely conver sets?
","Document 1: 
 In addition, denote
D(1)
n =
(B2
nlog7(pn)
n
)1/6
, D(2)
n,q =
(B2
nlog3(pn)
n1−2/q
)1/3
. (9)
An application of Theorem 2.1 under these conditions leads t o the following
proposition.
Proposition 2.1(High Dimensional CLT for Hyperrectangles) . Suppose
that conditions (M.1) and (M.2) are satisﬁed. Then under (E.1), we have
ρn(Are) ≤ CD(1)
n ,
where the constant C depends only on b; while under (E.2), we have
ρn(Are) ≤ C{D(1)
n + D(2)
n,q},
where the constant C depends only on b and q.
3. High Dimensional CLT for Simple and Sparsely Convex Sets
In this section, we extend the results of Section 2 by conside ring larger
classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather 

Document 2: 
 mT = max i≤N
∑
j≤N
1
(
σij ̸= 0
)
.
The sparsity assumption puts an upper bound assumption on mT in the sense that
m2
T = o
( T
r2 log ( N )
)
.
In this formulation, the number of factors r is allowed to be large and grows with T .U s i n g
principal components estimators under the normalization 1
T
∑T
t=1 Ft F′
t = Ir , one can decompose
the sample covariance of Xt as
SX = ˆ/Lambda1ˆ/Lambda1+
N∑
i=r+1
ˆμi ˆξi ˆξ
′
i ,
where ˆμi and ˆξi are the i-th leading eigenvalues and eigenvectors of SX , respectively. In the high-
dimensional setup, the sample covariance might be singular and provides a poor estimator for the
population covariance. For example, when N > T , the rank of SX can never exceed T ,w h e r e a s
the theoretical covariance /Sigma1X always has rank N . To overcome this problem, we may apply the
thresholding technique to the component ∑N 

Document 3: 
 kw≤ ak for all k= 1 ,...,m }
for some unit vectors vk ∈ Sp−1 and coeﬃcients ak, k = 1 ,...,m . If the
number of non-zero elements of each vk does not exceed s, this A is an
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 9
s-sparsely convex set. Yet another example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj for all j = 1 ,...,p and w2
1 + w2
2 ≤ c}
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , j = 1 ,...,p , and 0 <c ≤ ∞ . This
A is a 2-sparsely convex set. A more complicated example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj,w2
k + w2
l ≤ ckl, for all j,k,l = 1 ,...,p }
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , 0 < ckl ≤ ∞ , j,k,l = 1 ,...,p .
This A is a 2-sparsely convex set. Finally, consider the set 

Document 4: 
 classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather
large. In addition, it allows us to derive similar bounds for classes of sparsely
convex sets. These classes in turn may be of interest in stati stics where
sparse models and techniques have been of canonical importa nce in the past
years.
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 7
3.1. Simple convex sets. Consider a closed convex set A⊂ Rp. This set
can be characterized by its support function:
SA : Sp−1 → R ∪ {∞}, v ↦→ SA(v) := sup {w′v: w∈ A},
where Sp−1 := {v ∈ Rp : ∥v∥ = 1 }; in particular, A = ∩v∈Sp− 1 {w ∈ Rp :
w′v≤ S A(v)}. We say that the set Ais m-generated if it is generated by the
intersection of m half-spaces (that is, A is a convex polytope with at most
m facets). The support function SA of such a set A can be characterized 

Document 5: 
 CLT AND BOOTSTRAP IN HIGH DIMENSIONS 3
because they allow us to approximate the distributions of va rious key sta-
tistics that arise in inference for high-dimensional model s. For example,
the probability that a collection of Kolmogorov-Smirnov ty pe statistics falls
below a collection of thresholds
P
(
max
j∈Jk
SX
nj ≤ tk for all k= 1 ,...,κ
)
= P
(
SX
n ∈ A
)
can be approximated by P( SY
n ∈ A) within the error margin ρn(Are); here
{Jk} are (non-intersecting) subsets of {1,...,p }, {tk} are thresholds in the
interval ( −∞,∞), κ ≥ 1 is an integer, and A ∈ A re is a hyperrectangle of
the form {w∈ Rp : max j∈Jk wj ≤ tk for all k= 1 ,...,κ }. Another example
is the probability that a collection of Pearson type statist ics falls below a
collection of thresholds
P
(
∥(SX
nj)j∈Jk ∥2 ≤ tk for all k= 1 ,...,κ
)
= P
(
SX 

Document 6: 
 It is worthwhile to mention that a notable example where the t ransformed
variables ˜Xi = ( v′Xi)v∈V(Am) satisfy condition (E.1 ′) is the case where each
Xi obeys a log-concave distribution. Recall that a Borel proba bility measure
µ on Rp is log-concave if for any compact sets A1,A2 in Rp and λ∈ (0,1),
µ(λA1 + (1 − λ)A2) ≥ µ(A1)λµ(A2)1−λ,
where λA1 + (1 − λ)A2 = {λx+ (1 − λ)y: x∈ A1,y ∈ A2}.
Corollary 3.1 (High Dimensional CLT for Simple Convex Sets with Log–
concave Distributions) . Suppose that each Xi obeys a centered log-concave
distribution on Rp and that all the eigenvalues of E[XiX′
i] are bounded from
below by a constant k1 >0 and from above by a constant k2 ≥ k1 for every
i= 1 ,...,n . Then
ρn(Asi(a,d)) ≤ Cn−1/6 log7/6(pn),
where the constant C depends only on a,b,d,k 1, and k2. 

","The High Dimensional CLT (Central Limit Theorem) for simple and sparsely convex sets refers to a statistical result that provides bounds on the error of approximating certain probabilities involving random variables in high-dimensional spaces. This involves classes of sets, such as simple convex sets and sparsely convex sets, where the theorem provides similar bounds as those found in Section 2 under certain conditions (Proposition 3.1).",4.274008989334106,"The high-dimensional Central Limit Theorem (CLT) for simple and sparsely convex sets refers to the analysis and approximation of the distributions of statistics arising from high-dimensional models, particularly in the context of simple convex sets, which can be characterized by their support functions and generated by half-spaces. Under certain conditions, the theorem establishes bounds on the convergence rates of these statistics, such as the probability that specific statistical measures fall within certain thresholds. 

Proposition 3.1 outlines that the results extend to larger classes of sets, including sparsely convex sets, where the number of non-zero elements of vector representations is limited. This finding is significant in statistics, especially for sparse models, as it allows practitioners to infer key properties from high-dimensional distributions, thus providing a framework for data analysis where traditional low-dimensional assumptions do not hold. 

Moreover, the theorem can also specifically address distributions of transformed variables satisfying log-concave properties, enhancing the application of high-dimensional CLT in various statistical inference contexts."
llama3.1:latest,"What is multiplier bootstrap?
","Document 1: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 2: 
 search trajectory followed by the conﬁguration procedure. The key idea behind
this generalisation (dubbed aggressive capping) is to additionally bound the time
allowed for evaluating conﬁgurations based on the performance observed for the
current incumbent, i.e., the best-performing conﬁguration encountered since the
beginning of the ParamILS run. The additional bound is obtained by multiplying
the performance estimate of the incumbent by a constant, bm, called the bound
multiplier. Formally, for bm = ∞, the additional bound becomes inactive (assum-
ing the performance measure is to be minimised), and the behaviour of trajectory-
preserving capping is obtained. Forbm = 1, on the other hand, a very aggressive
heuristic is obtained, which limits the evaluation of any conﬁguration to the time
spent on evaluating the current incumbent. In practice,bm = 2 appears to result
in good performance and is used as a default setting in ParamILS. Despite its
heuristic nature, this modiﬁed capping mechanism preserves the PAC property of
FocusedILS.
Although Hutter et al. [ 37] spelled out their adaptive capping mechanisms for
the performance objective of minimising a target algorithm’s mean run-time only,
these mechanisms generalise to other objectives in a rather straightforward way (a
discussion of capping in the context of minimising quantiles of run-time is found
in Ch. 7 of [32]). We note, however, that – especially when several target algorithm 

Document 3: 
 with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of
power) as the ratio p/n grows. We also show that the jackknife resampling technique for
estimating the variance of ˆβ severely overestimates the variance in high dimensions.
We contribute alternative procedures based on our theoretical results that result in
dimensionality adaptive and robust bootstrap methods.
Keywords: Bootstrap, high-dimensional inference, random matrices, resampling
1. Introduction
The bootstrap (Efron, 1979) is a ubiquitous tool in applied statistics, allowing for infer-
ence when very little is known about the properties of the data-generating distribution.
The bootstrap is a powerful tool in applied settings because it does not make the strong
assumptions common to classical statistical theory regarding this data-generating distri-
bution. Instead, the bootstrap resamples the observed data to create an estimate, ˆF, of
the unknown data-generating distribution, F. The distribution ˆF then forms the basis of
further inference.
c⃝2018 Noureddine El Karoui and Elizabeth Purdom. 

Document 4: 
 ρMB
n(A) ≤ C{D(1)
n (α) + D(1)
n,q(α)},
where the constant C depends only on a,b,d,q , and s.
Remark 4.2 (Bootstrap theorems in a.s. sense) . Proposition 4.1 leads to
the following multiplier bootstrap theorem in the a.s. sens e. Suppose that
A is a subclass of Asi(a,d) as in Proposition 4.1 and that (M.2) is satisﬁed.
We allow p= pn → ∞ and Bn → ∞ as n→ ∞ but assume that a,b,d,q,s
are all ﬁxed. Then by applying Proposition 4.1 with α= αn = n−1(log n)−2,
together with the Borel-Cantelli lemma (note that ∑ ∞
n=4 n−1(log n)−2 <∞),
we have with probability one
ρMB
n(A) =
{
O{D(1)
n (αn)} under (E.1)
O{D(1)
n (αn) ∨ D(2)
n,q(αn)} under (E.2) ,
and it is routine to verify that D(1)
n (αn) = o(1) if B2 

Document 5: 
 10 CHERNOZHUKOV, CHETVERIKOV, AND KATO
4. Empirical and Multiplier Bootstrap Theorems
So far we have shown that the probabilities P( SX
n ∈ A) can be well
approximated by the probabilities P( SY
n ∈ A) under weak conditions for hy-
perrectangles A∈ A re, simple convex sets A∈ A si(a,d), or sparsely convex
sets A∈ A sp(s). In practice, however, the covariance matrix of SY
n is typi-
cally unknown, and direct computation of P( SY
n ∈ A) is infeasible. Hence,
in this section, we derive high dimensional bootstrap theor ems which allow
us to approximate the probabilities P( SY
n ∈ A), and hence P( SX
n ∈ A),
by data-dependent techniques. We consider here multiplier and empirical
bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn 

Document 6: 
 Buja, A. and Eyuboglu, N. (1992). Remarks on parallel analysis. Multivariate Behavioral Research,
27(4):509 – 540.
Campbell, J. Y. and Yogo, M. (2006). Efficient tests of stock return predictability.Journal of Financial
Economics, 81(1):27–60.
Chamberlain, G. and Rothschild, M. (1983). Arbitrage, factor structure, and mean-variance analysis
on large asset markets. Econometrica, 51(5):1281–1304.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multiplier
bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics ,
41(6):2786 – 2819.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2017). Central limit theorems and bootstrap in
high dimensions. The Annals of Probability , 45(4):2309 – 2352.
Chernozhukov, V., Chetverikov, D., Kato, K., and Koike, Y. (2023). High-dimensional data bootstrap.
Annual Review of Statistics and Its Application , 10(1):427–449. 

","The multiplier bootstrap is a method for approximating the distribution of a test statistic or a function of the data, especially in high-dimensional settings. It involves multiplying the original data by independent and identically distributed (i.i.d.) standard normal random variables (known as ""multipliers"") to create new data that are similar to the original data but with different values. The multiplier bootstrap is used to estimate the distribution of the test statistic or function of interest, allowing for inference about its properties such as confidence intervals or p-values.",4.719308376312256,"The multiplier bootstrap is a resampling method used to make statistical inferences about a population when very little is known about the underlying data-generating distribution. In this procedure, one generates a sequence of independent and identically distributed (i.i.d.) standard normal random variables and uses them to create a normalized sum that helps in approximating the distribution of interest. Specifically, it involves computing a standardized version of the data, leveraging the concept of a sample mean and variance to form a new sample distribution that can then be utilized to derive estimates and confidence intervals for parameters of interest, such as regression coefficients. This technique is particularly important in high-dimensional statistics where traditional inference approaches may fail due to the dimensionality of the data."
llama3.1:latest,"What are the methods of examining impact of imputed values on XAI methods?
","Document 1: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 2: 
 k (e.g., Onatski, 2009, 2010; Alessi et al., 2010; Owen and Wang, 2016; Dobriban and Owen, 2018;
Dobriban, 2020), tests for changepoints in the factor loadings (e.g., Breitung and Eickmeier, 2011; Bai
et al., 2022), tests for whether observed proxies of the factor returns Xt are good proxies (Bai and Ng,
2006), bootstrap methods to debias OLS estimates of the exposures (Gon¸ calves and Perron, 2020),
covariance matrix estimation (e.g., Fan et al., 2008, 2011), estimation and inference for dynamic factor
models (e.g., Bates et al., 2013; Stock and Watson, 2016) and more—see Bai and Wang (2016) for a
review. Many of these techniques leverage key results from random matrix theory (Johnstone, 2001;
Paul, 2007; Bai and Silverstein, 2010), sometimes in combination with permutation-based methods
(Buja and Eyuboglu, 1992).
Factor models are also widely discussed in the asset pricing literature (see Giglio et al. (2022) for
review), including methods for explaining excess returns (Fama and French, 1992, 2008; Welch and
Goyal, 2007; Lewellen, 2015; Freyberger et al., 2020; Gu et al., 2020), estimating factors and exposures
7 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 Journal of Clinical Epidemiology 56 (2003) 968–976
Imputation of missing longitudinal
data: a comparison of methods
Jean Mundahl Engels *, Paula Diehr
Departments of Biostatistics and Health Services, University of Washington, 1959 Northeast Paciﬁc Avenue, Box 357232, Seattle, WA 98195, USA
Accepted 11 October 2002
Abstract
Background and Objective: Missing information is inevitable in longitudinal studies, and can result in biased estimates and a loss
of power. One approach to this problem is to impute the missing data to yield a more complete data set. Our goal was to compare the
performance of 14 methods of imputing missing data on depression, weight, cognitive functioning, and self-rated health in a longitudinal
cohort of older adults.
Methods: We identiﬁed situations where a person had a known value following one or more missing values, and treated the known
value as a “missing value.” This “missing value” was imputed using each method and compared to the observed value. Methods were
compared on the root mean square error, mean absolute deviation, bias, and relative variance of the estimates.
Results: Most imputation methods were biased toward estimating the “missing value” as too healthy, and most estimates had a variance
that was too low. Imputed values based on a person’s values before and after the “missing value” were superior to other methods, followed
by imputations based on a person’s values before the “missing value.” Imputations that used no information speciﬁc to the person, such 

Document 5: 
 follows a VAR of ﬁnite order,
/Phi1( L)Ht = ut ,
where /Phi1( L) = /Phi10 − ∑h
j=1 /Phi1j Lh , with /Phi10 being possibly not an identity matrix. Bernanke et al.
(2005) propose two ways to analyze FAVAR. The ﬁrst is based on a two-step principal components
method, in which in the ﬁrst step, the method of principal components is employed to form
estimates of the space spanned by bothFt and Wt . In the second step, various identiﬁcation
schemes, such as Cholesky ordering, can be applied to obtain estimates of latent factors ˆFt ,w h i c h
are treated as observed when conducting VAR analysis of [ ˆF′
t,W′
t]′. Under suitable identiﬁcation
conditions, Bai et al. (2016) show that inferential theory can be developed for such a two-step
estimator, which differs from a standard large factor model. Conﬁdence bands for the impulse
responses can be readily constructed using the theory therein. The second method involves a
one-step likelihood approach, implemented by Gibbs sampling, which leads to joint estimation of
both the latent factors and impulse responses. The two methods can complement each other, with
the ﬁrst one being computationally simple and the second providing possibly better inference in
ﬁnite samples although with increased computational cost. 

Document 6: 
 van Buuren, 2018), which is veryﬂexible and can be implemented
with different models. While some applications can beneﬁt from
this uncertainty information, integrating this uncertainty
information in data pipelines can be challenging. From a
practitioner’s point of view, point estimates are much simpler
to integrate into conventional data pipelines. This is why we
restrict our analysis to point estimate imputations. Note,
however, that all the experiments conducted in this work
could, in principle, also be evaluated with respect to their
uncertainty estimates in a MICE setting, using the examined
imputation methods as the model underlying the MICE
estimator.
More recently, also ML approaches have increasingly been
used for imputation. Popular methods include k-nearest
neighbors (k-NNs) ( Batista and Monard, 2003 ), matrix
factorization (Troyanskaya et al., 2001 ; Koren et al., 2009 ;
Mazumder et al., 2010 ), random-forest –based approaches
(Stekhoven and Bühlmann, 2012), discriminative deep learning
methods (Biessmann et al., 2018), and generative deep learning
methods (Shang et al., 2017; Yoon et al., 2018; Li et al., 2019;
Nazábal et al., 2020; Qiu et al., 2020).
Most imputation studies provide solid experimental
evidence that the respective proposed method in the
application setting investigated outperforms other 

","Examining the impact of imputed values on Explainable AI (XAI) methods is crucial to understand how these missing data handling approaches affect the interpretability and reliability of AI models. Here are some common methods for investigating this impact:

1. **Sensitivity Analysis**: This involves analyzing how changes in imputation methods or parameters affect the XAI results, such as feature importance scores or attributions. By varying imputation settings (e.g., different algorithms, parameters), you can see how sensitive the XAI outputs are to these variations.
2. **Leave-One-Out (LOO) Method**: In this approach, each sample with missing values is temporarily removed from the dataset, and the XAI method is applied using only complete-case data. Then, the imputed value for that sample is added back in, and the XAI results are compared to those obtained without imputation.
3. **Simulation Studies**: Create datasets with controlled levels of missingness (e.g., 10%, 20%) and use different imputation methods. Evaluate how well each imputation method preserves the original XAI results on these simulated datasets.
4. **Comparison with Known Ground Truth**: If possible, use known ground truth or expert knowledge to validate the XAI results obtained from complete-case data versus those obtained after imputing missing values. This helps to quantify the impact of imputed values on interpretability.
5. **Visual Inspection and Comparison**: Use visualization techniques (e.g., bar plots, heatmaps) to compare feature importance scores, attributions, or other XAI metrics before and after imputation. This can help identify patterns or trends that emerge due to imputing missing values.
6. **Model Performance Evaluation**: Assess the impact of imputed values on model performance using evaluation metrics such as accuracy, precision, recall, F1-score, or mean squared error (MSE). If the imputation method significantly affects model performance, it may also influence XAI results.
7. **Using Alternative Imputation Methods**: Compare the effects of different imputation algorithms (e.g., mean, median, regression-based) on XAI outputs to understand how different methods affect interpretability.
8. **XAI Method Evaluation**: Assess the robustness and reliability of specific XAI methods in the presence of missing values. This may involve comparing results from multiple XAI techniques or evaluating their performance using metrics such as accuracy or consistency.
9. **Data Augmentation and Oversampling**: Apply data augmentation or oversampling techniques to artificially introduce missing values into complete-case datasets, then evaluate the impact on XAI results.
10. **Case Studies and Domain Expert Feedback**: Conduct case studies involving real-world datasets and seek feedback from domain experts about the plausibility and relevance of the XAI outputs obtained with different imputation methods.

By employing these methods, you can gain a deeper understanding of how imputed values affect the interpretability and reliability of AI models using XAI techniques.",16.870054006576538,"The impact of imputed values on XAI (explainable artificial intelligence) methods can be examined using various strategies as outlined in the provided snippets. Specifically, the methods of examination include:

1. **Comparison of Imputation Methods**: Document 1 emphasizes exploring the effects of different imputation methods on the calculation of Shapley values. By systematically contrasting various imputation strategies, researchers can evaluate how these methods influence the interpretability and robustness of outcomes in machine learning models.

2. **Feature Relevance Evaluation**: Document 3 mentions approaches that evaluate features based on their average expected marginal contribution. This can be affected by the imputed values, as the way in which features are weighted during decision-making may change depending on the imputation method used.

3. **Visual Explanations**: As noted in Document 3, using visualization techniques may also help in interpreting predictions when imputed values are involved. Visual analytics can reveal how different imputed values alter the model's predictions and subsequently affect explanations.

4. **Local Explanations**: Local explanation methodologies can provide insights into how specific imputed values affect the model's behavior around certain instances, as discussed in Document 3. This involves examining the model's workings around inputs that have been altered by different imputation methods.

5. **Statistical Comparison of Estimates**: Document 4 details comparing various imputation methods, evaluating their performance based on metrics like root mean square error and bias. This approach can also prove useful for assessing how different imputed values interact with XAI methodologies.

6. **Integration of Uncertainty**: Document 6 mentions that while point estimates are often used in practice, incorporating uncertainty estimates from multiple imputation techniques (like MICE) can provide a more thorough understanding of how imputation influences model interpretability and insights.

Through these methodological approaches, researchers can gain a better understanding of how imputation methods affect XAI techniques and the interpretability of machine learning models."
llama3.1:latest,"What is MSE SHAP? 
","Document 1: 
 The following section proposes a uniﬁed approach that improves previous methods, preventing them
from unintentionally violating Properties 1 and 3.
4 SHAP (SHapley Additive exPlanation) Values
We propose SHAP values as a uniﬁed measure of feature importance. These are the Shapley values
of a conditional expectation function of the original model; thus, they are the solution to Equation
4
Figure 1: SHAP (SHapley Additive exPlanation) values attribute to each feature the change in the
expected model prediction when conditioning on that feature. They explain how to get from the
base value E[f(z)] that would be predicted if we did not know any features to the current output
f(x). This diagram shows a single ordering. When the model is non-linear or the input features are
not independent, however, the order in which features are added to the expectation matters, and the
SHAP values arise from averaging the φi values across all possible orderings.
8, where fx(z′) = f(hx(z′)) = E[f(z) |zS], and Sis the set of non-zero indexes in z′(Figure 1).
Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values 

Document 2: 
 the MIT license and is available at:
https://bitbucket.org/pl-edu-pw-mini-optimization/basic-pso-de-hybrid/ .
3.2. Samples archive
In order to store and eﬃciently retrieve samples, M-GAPSO utilizes a
multi-dimensional R-Tree index [34]. Due to performance reasons capacity of
the R-Tree index is limited and set by the user. After reaching the maximum
capacity, the index is restarted from scratch.
Samples archive is utilized in two scenarios. Mainly, for eﬃcient retrieval
of the nearest samples as described in Section 2.3. Subsequently those sam-
ples are utilized to ﬁt a quadratic or polynomial function model. Secondly,
samples archive serves as a cache memory, so that in the event of the algo-
rithm trying to sample the same location (as it does happen when the swarm
has nearly collapsed), it retrieves the function value from memory, saving
some budget of the ﬁtness function computations.
3.3. Restart management
M-GAPSO uses an enhanced version of JADE [5] restart manager. In
M-GAPSO the RestartManager registers iteration count intervals between
global optimum updates, considers a spread of personal best locations of
particles (eq. (10)) and additionally a spread of personal best locations val-
ues (eq. (11)). The last feature was added in order to better handle step 

Document 3: 
 ⏐⏐
(19)MSE = 1
m
m∑
i=1
(yi −¯yi)2
(20)RMSE =
√1
m
m∑
i−1
(yi −¯yi)2
(21)TPR = TP
TP + FN
Page 24 of 37Emmanuel et al. J Big Data           (2021) 8:140 
The major advantages of the MSE and RMSE is that they provide a quadratic loss func -
tion. Also, uncertainty in forecasting is measured when they are used. However, MSE 
and RMSE are highly influenced by extreme values [145]. While MAE is not influenced 
by extreme values, also a more natural measure and unambiguous [146]. Most stud -
ies in research are found to mostly use the RMSE for missing value imputation evalu -
ation [147–149]. Although some studies have proposed valid evidence against the use 
of RMSE in favour of MAE due to its less sensitive to extreme values [150]. The authors 
further advised against the reporting of RMSE in literature and strongly recommended 
the use of MAE [146, 150]. However, Chai and Draxler [145] partially disputed the con -
clusions and introduced arguments against avoiding RMSE. They contended that RMSE 
was appropriate to represent model performance than the MAE. The AUC like other 

Document 4: 
 ELSHADE-SPACMA 1.52 1.40 1.38 1.45 1.72 1.83 2.13 
GSKA 4.10 4.20 4.17 4.17 4.17 4.17 4.20 
PSO 6.03 5.97 5.97 5.97 5.93 5.93 5.87 
SSA 5.27 5.40 5.43 5.47 5.60 5.63 5.67 
jSO 2.70 2.40 2.35 2.37 2.47 2.35 2.05 
(b) 40–100% Evaluations 
ﬁtness evaluations, including the previous winners of CEC compe- 
titions (EBOwithCMAR and jSO) and the most competitive recent 
algorithm (ELSHADE-SPACMA). 
• EBOwithCMAR is the algorithm with the best results for dimensions 
10 and 30 (very close to ELSHADE-SPACMA in those dimensions), 
whereas ELSHADE-SPACMA is the best performing algorithm for di- 
mensions 50 and 100, closely followed by EBOwithCMAR and jSO. 
• For dimensions 30, 50 and 100, ELSHADE-SPACMA is the best one 
since the 40% of the budget of evaluations. Although Table 3 indi- 
cates that EBOwithCMAR obtains the best ﬁnal results, ELSHADE- 
SPACMA achieves better results during most of the search, being 
only improved by the former at the end. 
• Deciding which algorithm should be applied to a speciﬁc problem 
strongly depends on the eﬀort that can be devoted to the search. In 
this benchmark, GSKA is better when less evaluations are allowed, 

Document 5: 
 open-source innvestigate package. In the case of expected gradients, we bench-marked our imple-
mentation against a public one in the SHAP python package.
Surrogate Approaches.We consider:
• LIME (Ribeiro et al., 2016) locally approximate F around xi with a simple function, g,
that is then interpreted. LIME corresponds to: arg ming∈GL(f,g, pert(xi)) + Ω(g), where
pert(xi) local perturbations of the inputxi, and Ω(g) is a regularizer. Overall, recent work
hasshownthat, inthetabularsetting, LIMEapproximatesthecoeﬃcientsofablack-boxlinear
model with high probability. In our empirical implementation we follow the open source lime-
image package. Here to account for high dimensions, the input image is ﬁrst segmented into
50 segments and the local approximationg is ﬁt around input perturbations with50. We
experimented with 5,10,15,&25 dimensions as well. Overall, we found the LIME with50
segments to be more stable for the input data sizes that we consider. We use1000 samples in
model ﬁtting.
• SHAP (Lundberg and Lee, 2017) Similar to LIME, SHAP provides a local approximation
around a single input. The local model is then interpreted as a form of explanation. SHAP
uniﬁes LIME and several under methods under the same umbrella and turns out to be a 

Document 6: 
 4.1 | Features-oriented methods
SHapley Additive exPlanation (SHAP) (Lundberg & Lee, 2017) is a game-theoretic approach to explain ML predictions.
SHAP seeks to deduce the amount each feature contributed to a decision by representing the features as players in a
ANGELOV ET AL. 5o f1 3
 19424795, 2021, 5, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1424 by Cochrane Poland, Wiley Online Library on [28/12/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 

","I don't know. There is no mention of ""MSE SHAP"" in the provided context. However, there is a discussion about Mean Squared Error (MSE) on page 24, and SHAP values are discussed as a unified measure of feature importance.",3.869765758514404,"The term ""MSE SHAP"" does not appear in the provided documents specifically. However, MSE (Mean Squared Error) is a common metric used to measure the accuracy of predictions in a model, calculated as the average of the squared differences between predicted and actual values. SHAP (SHapley Additive exPlanation) values, on the other hand, are a measure of feature importance that explains the contribution of each feature to the model's predictions based on game-theoretic principles. 

While there is no direct mention of ""MSE SHAP"" in the snippets, it could imply a context where both MSE (or other error metrics) are being evaluated alongside SHAP values to provide insights into model performance and feature contributions. In practice, researchers may analyze how different features (using SHAP) influence the MSE or other error metrics, but a specific definition or methodology termed ""MSE SHAP"" is not delineated in the provided excerpts."
llama3.1:latest,"What are observations on impact of different imputation methods on explainability of models?
","Document 1: 
 relationships between observed and missing variables.
In recent years, more attention has been paid to the explainability of
the imputation method under missing data. For example, [37] introduces
DIMV, an imputation method that provides explainability and uncertainty
of imputation via conditional distribution, or [1] analyzes the effects of vari-
ous imputation and direct parameter estimation methods to the correlation
plot. Next, Hans et al. [38] introduce an explainable imputation method
that leverages association constraints within the data. However, up to our
5
knowledge, so far, there is no work on the effects of missing data on Shapley
values.
2.3. Direct missing data handling techniques without imputation
Different from the imputation approaches, methods that directly handle
missing data can have clearer implications in terms of explainability.
Specifically, Nguyen et al. [35] introduced the EPEM algorithm to esti-
mate the maximum likelihood estimates (MLEs) for multiple class monotone
missing data when the covariance matrices of all classes are assumed to be
equal. Additionally, DPER [39] addresses a more general case where miss-
ing data can occur in any feature by using pairs of features to estimate the
entries in the mean and covariance matrices. The implication to model ex-
plainability of using direct parameter estimation methods, like EPEM and
DPER, includes improved transparency and interpretability of the model’s
behavior, as these methods provide clear estimates of means and covariances, 

Document 2: 
 details of how these methods works is described by White, Royston, and
Wood (2011).
The related work shows that there exists several literature describing different
methods of data imputation. From the implementation perspective, it is also
very important to understand and evaluate performance of different imputation
methods so that appropriate method can be used while performing data mining
task. Though there exist some literatures that analyzed performance of different
imputation methods, in this paper, we intend to analyze performance of differ-
ent imputation method for numeric datasets that uses single and multiple
imputation methods namely mean imputation, median imputation, kNN impu-
tation, predictive mean matching, Bayesian Linear Regression (norm), Linear
Regression, non-Bayesian (norm.nob), and random sample.
Research Methodology
This section describes procedure followed for analyzing performance of the
imputation methods. Objective of this study is to analyze performance of imputa-
tion methods that includes: (a) Single Imputation Methods: Mean Imputation,
Median Imputation, KNN Imputation and (b) Multiple Imputation Methods:
Predictive Mean Matching (pmm), Bayesian Linear Regression (norm), Linear
Regression non-Bayesian (norm.nob), and Sample method. All these imputation
methods are applied only on numeric datasets. The datasets used in this study are
obtained from UCI Machine Learning Repository (Lichman2013). The descrip- 

Document 3: 
 The main limitations of ou r work were as follows. 
We used a data set contained only four categorical 
variables. Therefore questions whether number and 
nature of variables affected our conclusions remains 
to be addressed. In regression imputation, E-M algo-
rithm, and the MICE methods regression modelling 
has been used to draw the imputation. It is clear that, 
generally speaking, the more the number of variables 
the better the prediction. In addition, in regression 
and E-M imputations, it is as sumed that the data fol-
lows a normal distribution. This was not the case in 
our data set. This might partly explain poorer perfor-
mance of these two methods. Another limitation of 
our work was that we di d not compare performance 
of the imputation methods under different missing 
rates. One last issue was the process of model build-
ing. We developed all four models using ENTER 
method. This method fits a model including all varia-
bles offered to. Therefore, the behavior of imputation 
models under different variable selection methods 
(i.e. Backward and Forward) should be investigated. 
All issues noted affect the conclusions 5,25-28 and their 
influence should be explored in future studies.  
 Although the MICE method does not provide 
unique estimates,29 and communication of results with 
clinical audiences is not simple, evidence from litera-
ture suggested that the MICE method is the best ap-
proach to impute missing data. However, our literature 

Document 4: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 5: 
 of 10–20% in more than 75% of our experiments. This holds
for most imputation methods; we did not observe a clear
advantage for an imputation method overall. Taking into
account the considerable differences in wall-clock run time,
our results indicate that also when choosing an imputation
method that is both fast and improves downstream predictive
performance random forests would be the preferred
imputation method.
The positive impact of imputation on downstream
performance is most pronounced when the imputation
methods were trained on fully observed data. When
imputation methods were trained on incomplete data, the
positive impact of imputing missing values in the test data was
substantially lower, sometimes even negative. While this might
seem a disadvantage, we emphasize that, in many application use
cases, we can ensure that the training data be fully observed, for
instance, by acquiring more data before training the imputation
and the downstream ML model.
6.3 Limitations
Because one of the main goals of this study is a comprehensive
comparison of imputation methods on a large number of datasets
and missingness conditions, we made some decisions that limit
our results.
First, we focus on point estimates of imputed values rather than
multiple imputations because it is 1) easier to handle in automated
pipelines and 2) can be considered a more relevant scenario in real-
world applications of imputation methods. Thus, we do not
consider the inherent uncertainty of the imputation process. We
decided to measure and compare the impact imputation methods 

Document 6: 
 that is not positive definite, which is can possibly prevent further analysis such as calcu -
lating coefficients estimates [36]. Finally, pairwise deletion also known to produce low 
bias results for MCAR or MAR data [34].
Imputation
The process of imputation involves replacing missing values by some predicted values. 
The non-missing values data set is normally used to predict the values used to replace 
the missing values [8]. In the following we cover some of the most used imputation 
methods in the literature.
Simple imputation
Simple imputation approach entails replacing missing values for each individual value by 
using a quantitative attribute or qualitative attribute of all the non-missing values [37]. 
With simple imputation, missing data is handled by different methods such as, mode, 
mean, or median of the available values. In most studies simple imputation methods are 
used because of their simplicity and that they can be used as an easy reference technique 
[38]. However, simple imputation methods may produce bias or unrealistic results on a 
high-dimensional data sets. Also, with the generation of big data emerging, this method 
seems to be performing poorly and therefore is inadequate to be implemented on such 
data sets [39].
Regression imputation
Regression is one of the preferred statistical technique for handling missing values. This 
method is also termed conditional mean imputation, here missing values are replaced 
with a predicted value created on a regression model if data is missing at random. The 
overall regression process is a two-phase method: the first step, uses all the complete 

","Research has shown that different imputation methods can significantly impact the explainability of models. For instance, [38] introduced an explainable imputation method leveraging association constraints within data, while Hans et al. [37] analyzed the effects of various imputation and model-agnostic techniques on feature importance. These studies suggest that certain imputation methods can either enhance or degrade the interpretability of models depending on their characteristics and the specific application context.",4.842650651931763,"Different imputation methods have significant implications for the explainability of models, especially regarding the calculation of Shapley values, which attribute the contribution of features to model predictions. The choice of imputation method can introduce biases or distortions that affect the accuracy and reliability of Shapley values. 

1. **Effect on Shapley Values**: The impact of imputation methods on Shapley values has not been extensively studied despite its importance. Different imputation strategies can influence the interpretability of machine learning models and the robustness of the insights derived from Shapley values.

2. **Comparison of Methods**: Research has shown that the performance of imputation methods varies, and while some methods like random forests can improve downstream predictive performance, their effectiveness can diminish if trained on incomplete data. This highlights the need for careful selection of imputation methods based on the nature of the dataset.

3. **Explainability versus Performance**: Methods that provide clearer estimates of means and covariances, like direct parameter estimation techniques (e.g., EPEM and DPER), tend to offer better transparency and interpretability. On the other hand, simpler imputation methods might lead to unrealistic results in high-dimensional datasets, which could impact the explainability in a negative manner.

4. **Limitations of Imputation Methods**: Specific techniques such as regression and multiple imputation methods can perform poorly if the data does not meet the underlying assumptions (e.g., normal distribution) or if the imputation method does not adequately capture the complexity of the dataset. This suggests that the imputation methods used have a direct correlation with the explainability and interpretation of the resulting models.

Overall, selecting the appropriate imputation method is crucial for maintaining the integrity and interpretability of machine learning models that rely on imputed data."
llama3.1:latest,"Does usage of different imputation methods on missing values has impact on SHAP feature importance?
","Document 1: 
 especially at higher missing rates. MICE and DIMV often show similar
patterns, possibly due to the fact that MICE is based on regression and
DIMV is based on a conditional Gaussian formula. MissForest and SOFT-
IMPUTE sometimes preserve feature rankings better than simpler methods,
but this is not consistent across all scenarios.
The variability in results between methods and missing rates underscores
the need to evaluate imputation effects when using Shapley values for model
interpretation.
The following discussion is structured around our result and the specific
pitfalls that may arise due to incomplete understanding of the relationship
between missing data, imputation methods, and Shapley values. We high-
23
light how different approaches can lead to vastly different interpretations, how
dataset characteristics and missing rates affect results, and the importance
of considering both imputation accuracy and interpretability preservation.
Pitfall 1: Assume the neutrality of the imputation method.Our
study reveals that different imputation methods can significantly alter Shap-
ley values and, consequently, the interpretability of the model. For instance,
mean imputation tends to distort feature importances, especially at higher
missing rates, while methods like MICE and DIMV often show similar pat-
terns. This underscores the importance of carefully considering the imputa-
tion method when using Shapley values for model explanation, as the choice
can lead to vastly different interpretations of feature importance.
Pitfall 2: Overlooking data set dependency. We observed that 

Document 2: 
 Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values
for features not in the set S. Since most models cannot handle arbitrary patterns of missing input
values, we approximate f(zS) with E[f(z) |zS]. This deﬁnition of SHAP values is designed to
closely align with the Shapley regression, Shapley sampling, and quantitative input inﬂuence feature
attributions, while also allowing for connections with LIME, DeepLIFT, and layer-wise relevance
propagation.
The exact computation of SHAP values is challenging. However, by combining insights from current
additive feature attribution methods, we can approximate them. We describe two model-agnostic
approximation methods, one that is already known (Shapley sampling values) and another that is
novel (Kernel SHAP). We also describe four model-type-speciﬁc approximation methods, two of
which are novel (Max SHAP, Deep SHAP). When using these methods, feature independence and
model linearity are two optional assumptions simplifying the computation of the expected values
(note that ¯Sis the set of features not in S):
f(hx(z′)) = E[f(z) |zS] SHAP explanation model simpliﬁed input mapping (9) 

Document 3: 
 dependsonthesizeofthedatasetandmissingvaluespresent
inthem.Here,theauthorsanalyzedoftheperformanceoffour
imputationpackagesinRontwodatasetsusingthreepredictive
algorithms-LogisticRegression,SupportVectorMachineandAr-
tificialNeuralNetwork.Theauthorsmeasuretheperformanceof
thepackagesintermsofimputationtime,imputationefficiency
andeffectonthevariance.Theauthorsalsoshowthattheper-
formance of an imputation method depends on the predictive
algorithm which is a function of dataset characteristics. There
aredifferentwaysofhandlingmissingdata[25–27,29–32].The
mostsimpleapproachistouseonlythesubsetthatiscomplete;
inotherwords,suchamethodignoresallfeaturevectorswith
missing values and the remaining subset is used for analysis.
Althoughverysimple,suchanapproachwillbeusefulonlywhen
asmallfractiondatapointshavemissingvalues.Amoreuseful
and popular approach is to impute the missing values using
differenttechniquesandthenusethedatasetconsistingofboth
featurevectorswithoutmissingvaluesandthefeaturevectors
with imputed missing values. These techniques often depend
ontheproblemthatonelikestosolveusingthedataset.For
example,toclusteranincompletedataset,twotypesofframe-
works may be adopted: single view clustering and multi-view 

Document 4: 
 ley values, ranging from (−0.5) to around (+1.5) across all missing rates, em-
phasizing its strong influence on the model outputs. The imputation methods
show similar patterns with Shapley values for key features slightly reduced
compared to the XGBoost model, but they still highlight the importance of
Latitude, Longitude, and MedInc in the predictive modeling.
Considering the results at each missing rate, we observed similar distribu-
tions in the Shapley values for Latitude, Longitude, and MedInc on both the
15
(a) Original
 (b) Xgboost
 (c) Mean Imputation
(d) MICE
 (e) DIMV
 (f) missForest
(g) SOFT-IMPUTE
Figure 1: Global feature importance plot on the California dataset with the missing rate
r = 0.2
original and imputed data using various imputation methods, generally fluc-
tuating around zero. However, XGBoost shows a distribution that is more
skewed from zero. For example, at a missing rate of 20% in figure 5, the
Shapley values in the original data for Latitude and Longitude range from
(−0.75) to (+0 .75), and for MedInc from ( −0.5) to (+0 .7). Likewise, the
ranges for models with imputation methods are [ −0.6; 0.6] and [ −0.4; 0.6],
respectively, indicating that the imputation does not significantly alter the 

Document 5: 
 dataset. By displaying each data point as a dot and arranging these
dots to show the distribution of Shapley values for each feature, the
beeswarm plot offers a comprehensive overview of feature importance
and interaction effects. It allows for the identification of patterns and
outliers, helping to understand the behavior of the model with respect
to individual features. Additionally, the plot can reveal how specific
features interact with others, providing insights into complex depen-
dencies within the data.
3.2. Imputation techniques
In this section, we briefly summarize the missing data handling methods
that we will examine for the effects on Shapley values. The methods being
investigated consist of a method that can directly learn from missing data,
such as XGBoost, to a simple imputation method as Mean Imputation, as
well as the widely used or recently developed imputation techniques, such as
8
MICE, DIMV, missForest, and SOFT-IMPUTE. The details of the methods
are as follows:
• XGBoost (Extreme Gradient Boosting) [47] is a powerful and efficient
algorithm that belongs to the family of gradient boosting techniques.
It builds an ensemble of decision trees, where each tree corrects errors
made by the previous ones, enhancing predictive accuracy. XGBoost
stands out for its speed and performance, employing advanced features
like tree pruning, regularization, and parallel processing, which help
in reducing overfitting and handling large-scale data. XGBoost can
handle missing data directly. 

Document 6: 
 2018). However, in that work, the authors only considered text data
as an inputﬁeld to an imputation method, not as a column that
could be imputed. Generally, most modern ML applications that
involve text data are based on rather sophisticated natural language
models. Combinations of such models with tabular data are an
importantﬁeld of research (Yin et al., 2020) but beyond the scope of
most imputation research so far.
Third, to measure the imputation impact on the downstream
performance, we discarded and imputed values in only a single
column. Therefore, the impact depends heavily on the chosen
column’s importance (e.g., see the work ofSchelter et al. (2021)).
Generally, the impact when using an imputation model could
vary when multiple columns are affected by missing values.
7 CONCLUSION
In this study, we developed an experimental protocol and
conducted a comprehensive benchmark for imputation
methods comparing classical and modern approaches on a
large number of datasets under realistic missingness conditions
with respect to the imputation quality and the impact on the
predictive performance of a downstream ML model. We also
evaluated how the results changed when the imputation and
downstream model were trained on incomplete data.
Our results can be summarized in two mainﬁndings. First, we
demonstrate that imputation helps to increase the downstream
predictive performance substantially regardless of the
missingness conditions. When training data are fully observed,
our results demonstrate that, in more than 75% of our 

","Yes, the study reveals that different imputation methods can significantly alter Shapley values and consequently affect the interpretability of the model.",2.764671564102173,"Yes, the usage of different imputation methods on missing values does have an impact on SHAP feature importance. Various imputation techniques can significantly alter the calculated SHAP values, leading to different interpretations of feature importance. For instance, mean imputation can distort feature importances, especially at higher missing rates, while methods like MICE and DIMV often produce similar patterns. Therefore, it is crucial to consider the choice of imputation method when interpreting SHAP values for model explanations, as different methods can yield vastly different results."
llama3.1:latest,"How sampling for LIME for single observation is done?
","Document 1: 
 πx. We sample instances around x′ by drawing nonzero
elements of x′ uniformly at random (where the number of
such draws is also uniformly sampled). Given a perturbed
sample z′∈{0, 1}d′
(which contains a fraction of the nonzero
elements of x′), we recover the sample in the original repre-
sentation z ∈Rd and obtain f(z), which is used as a label for
the explanation model. Given this dataset Zof perturbed
samples with the associated labels, we optimize Eq. (1) to
get an explanation ξ(x). The primary intuition behind LIME
is presented in Figure 3, where we sample instances both
in the vicinity of x (which have a high weight due to πx)
and far away from x (low weight from πx). Even though
the original model may be too complex to explain globally,
LIME presents an explanation that is locally faithful (linear
in this case), where the locality is captured by πx. It is worth
noting that our method is fairly robust to sampling noise
since the samples are weighted byπ
x in Eq. (1). We now
present a concrete instance of this general framework.
3.4 Sparse Linear Explanations
For the rest of this paper, we let G be the class of linear
models, such that g(z′) = wg ·z′. We use the locally weighted 

Document 2: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 3: 
 DATA REDUCTION
Data reduction is typically conducted in two directions, i.e., row-
wise for data sample reduction and column-wise for data variable
reduction. Various data sampling techniques can be applied for
row-wise data reduction, such as random and stratiﬁed sampling
(Fan et al., 2015b). Random sampling is typically used to simulate
a random process in data sample selection. By contrast, stratiﬁed
sampling is performed to maintain the proportions of data
samples corresponding to different categories. For instance,
assuming there are 100 and 50 data samples in the original
data set for category A and B respectively, a strati ﬁed
sampling process with a selection rate of 0.7 will randomly
select 70 out of 100, and 35 out of 50 data samples in
category A and B for further analysis. Such techniques can be
applied for sampling time series data as well. In such a case, it is
recommended to transform the original sequence data into
shorter subsequences before data sampling, since direct
sampling may lead to temporal information loss.
There are three main methods of column-wise data variable
reduction. Theﬁrst is to use domain knowledge to directly select
variables of interests. The second is to use statistical feature
selection methods to select important variables for further
analysis. The third is to adopt feature extraction methods to
construct useful features for data analysis. The details of the latter
two methods are shown inFeature Selection Techniques, Feature
Extraction Techniques.
Feature Selection Techniques
As shown inTable 2, feature selection methods can be broadly 

Document 4: 
 the change in the conditional expectation of all features given
a subset of features. In other words, this calculation ignores
the nodes of features that are not present in the sample subset
and calculate change on the conditional expectation, given
this subset sample.
5 Experiments
The main steps of the experiments consisted of data acqui-
sition, data pre-processing, model tuning, model evaluation,
and interpretation of models and their predictions.
All the experiments ran in a free tier Google Colab
environment, which consisted of an Intel Xeon 2.34Ghz
quad-core processor, 25Gb of RAM, and Linux Ubuntu 18.04
operational system. The algorithms were implemented in
Python 3.7 using PIP packages. Implementation details can
be found here.
5.1 Data Acquisition
The experiment was executed with data provided by the
COVID-19 Data Sharing/BR, which is an initiative of the São
Paulo Research Foundation (FAPESP) in collaboration with
the University of São Paulo. The dataset consisted of sus-
pected COVID-19 patients admitted to the Hospital Israelita
Albert Einstein from February 2020 to June 2020. Addition-
ally, we conﬁrmed that it is possible to have two samples from
the same patient but from different days. It happens because
a patient can be admitted more than once in the same unit,
and also, a doctor can request a COVID-19 for the patient
more than once while they are still under treatment. 

Document 5: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 6: 
 Method
Used for Data Imputation 10% 20% 30% 40% 50%
Mean Imputation 0.068072 0.106639 0.120149 0.142043 0.160996
Median Imputation 0.069302 0.108637 0.123685 0.141440 0.165698
KNN Imputation 0.013803 0.025372 0.035587 0.047773 0.066471
Predictive Mean Matching(pmm) 0.017844 0.035328 0.044072 0.057366 0.081049
Bayesian Linear Regression(norm) 0.017917 0.033387 0.046552 0.057673 0.077796
Linear Regression, non-Bayesian (norm.nob) 0.017933 0.033459 0.044446 0.055386 0.072130
Random sample from observed values(sample) 0.072683 0.119587 0.131316 0.153484 0.169020
Figure 1. Plot of imputation method versus mean NRMSE for wine dataset.
Figure 2. Plot of imputation method versus mean NRMSE for glass dataset.
924 A. JADHAV ET AL.
given in ascending order of Mean NRMSE. It means that lowest Mean
NRMSE value get first rank.
Tables 7– 11 provide rank of imputation method for varying percentage of
missing data for five different datasets. Each table indicates performance of
imputation method on different datasets for given percentage of imputed
data. The reason for doing this is to assess consistency in performance of the 

","For LIME (Local Interpretable Model-agnostic Explanations), sampling around a single observation involves generating synthetic samples from the neighborhood of the original data point. This is done by perturbing the original data point slightly to create new samples, while keeping its features and target value constant. The number of samples generated in each dimension is determined based on the feature's importance or range. These synthetic samples are then used to train a linear model that approximates the behavior of the original model around the original data point.",4.80665397644043,"In LIME (Local Interpretable Model-agnostic Explanations), sampling for a single observation, denoted as x, involves drawing instances around a perturbed version of that observation (x′). The process includes sampling nonzero elements of x′ uniformly at random, with the number of draws also sampled uniformly. A perturbed sample z′ is created, which includes a fraction of the nonzero elements from x′, and this is then rephrased back to the original representation as z. The output of the original model for this representation, f(z), serves as the label for the explanation model. The sampling process captures instances close to x (high weight from the locality function πx) and farther away (lower weight), ensuring that the explanation remains locally faithful even in complex models."
llama3.1:latest,"How can LIME lead to improvement of model by non-experts?
","Document 1: 
 In this paper, we propose providing explanations for indi-
vidual predictions as a solution to the “trusting a prediction”
problem, and selecting multiple such predictions (and expla-
nations) as a solution to the “trusting the model” problem.
Our main contributions are summarized as follows.
•LIME, an algorithm that can explain the predictions ofany
classiﬁer or regressor in a faithful way, by approximating
it locally with an interpretable model.
•SP-LIME, a method that selects a set of representative
instances with explanations to address the “trusting the
model” problem, via submodular optimization.
•Comprehensive evaluation with simulated and human sub-
jects, where we measure the impact of explanations on
trust and associated tasks. In our experiments, non-experts
using LIME are able to pick which classiﬁer from a pair
generalizes better in the real world. Further, they are able
to greatly improve an untrustworthy classiﬁer trained on
20 newsgroups, by doing feature engineering using LIME.
We also show how understanding the predictions of a neu-
ral network on images helps practitioners know when and
why they should not trust a model.
2. THE CASE FOR EXPLANATIONS
By“explaining a prediction”, we mean presenting textual or
visual artifacts that provide qualitative understanding of the
relationship between the instance’s components (e.g. words 

Document 2: 
 that is available.
2.3 Improving existing data
In addition to searching and labeling datasets, one can
also improve the quality of existing data and models. This
approach is useful in several scenarios. Suppose the target
application is novel or non-trivial where there are no relevant
datasets outside, or collecting more data no longer beneﬁts
the model’s accuracy due to its low quality. Here, a better
option may be to improve the existing data. One effective
approach is to improve the labels through re-labeling. Sheng
et al. [ 146] demonstrates the importance of improving labels
by showing the model accuracy trends against more training
examples for datasets with different qualities. As the data
quality decreases, even if more data is used, the accuracy of
the model does not increase from some point and plateaus. In
this case, the only way to improve the model accuracy is to
improve the label quality, which can be done by re-labeling
and taking majority votes on multiple labels per example. In
fact, one could clean the entire data including labels, which
naturally leads to the next section where we cover data vali-
dation, cleaning, and integration.
3 Data validation, cleaning, and integration
It is common for the training data to contain various errors.
Machine learning platforms like TensorFlow Extended
(TFX) [13] have data validation [ 117] components to detect
such data errors in advance using data visualization and
schema generation techniques. Data cleaning can be used 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 commonly held misconception is that ML is a one-step training pro-
cedure that accepts data as input and generates a model as output.
Many research efforts, both within the DB community and in other
communities, focus on this simplified view and aim to improve the
effectiveness of ML training, e.g., by generating more powerful mod-
els through better training algorithms, or by reducing its resource
footprint through various software and hardware optimizations.
This view is reinforced by various leaderboard-style competitions
popular among practitioners (e.g., Kaggle), and academics (e.g., ML
benchmarks and KDD competitions).
At the same time, there is evidence from practitioners [16, 19, 48]
that ML deployments in production are significantly more compli-
cated. Specifically, ML in production involves pipelines with many
interlocking steps, only one of which is training. This has spurred
on the development of many end-to-end ML systems (e.g., TFX [16],
MLFlow [52], Microsoft Azure ML [8], AWS Sagemaker [34]) and
open-source ML libraries (e.g., MLlib [6], MetaFlow [2], and Scikit-
Learn [7]), all of which provide native support for data pre-processing,
data validation, model validation, and model deployment, in addi-
tion to model training, all within a single environment.
As an example, TFX [16] includes pipeline steps that perform 

Document 5: 
 that is, regardless of the model class of the original black-box. Usually,G is chosen to be a constrained class of intrinsi-
cally interpretable models (Section4.2), for example, linear models or GAMs. Put simply, LIME trains many interpret-
able surrogate models to approximate a black-box modelf /C1ðÞ locally. During training, instances are sampled around
each data pointxi weighted byπxi . In addition to local explanations given byξ /C1ðÞ , Ribeiro et al. (2016) introduce a pro-
cedure for obtaining a global understanding of the modelf /C1ðÞ : given a limited budget, their algorithm picks several
explanations based on greedy submodular optimization (Krause & Golovin,2014) and aggregates them into global vari-
able importances, similar to the random forest feature importance (Breiman,2001).
DeepLIFT
Shrikumar et al. (2017) introduce an efficient method for disentangling contributions of inputs in a neural network—
deep learning important features (DeepLIFT). Compared to LIME, DeepLIFT is not model-agnostic since it is explicitly
tailored to neural networks; it also requires a reference, or baseline, data point. While in natural images an all-black
image is typically used as a baseline input, the choice of a reference might not be so trivial for more specialized datasets 

Document 6: 
 SP-LIME
RP-LIME
No cleaning
Figure 10: Feature engineering experiment. Each
shaded line represents the average accuracy of sub-
jects in a path starting from one of the initial
10 sub-
jects. Each solid line represents the average across
all paths per round of interaction.
person inspects ( B) to 6. The position of each algorithm
and the order of the instances seen are randomized between
subjects. After examining the explanations, users are asked
to select which algorithm will perform best in the real world.
The explanations are produced by either greedy (chosen
as a baseline due to its performance in the simulated user
experiment) or LIME, and the instances are selected either
by random (RP) or submodular pick (SP). We modify the
greedy step in Algorithm 2 slightly so it alternates between
explanations of the two classiﬁers. For each setting, we repeat
the experiment with 100 users.
The results are presented in Figure 9. Note that all of
the methods are good at identifying the better classiﬁer,
demonstrating that the explanations are useful in determining
which classiﬁer to trust, while using test set accuracy would
result in the selection of the wrong classiﬁer. Further, we see
that the submodular pick (SP) greatly improves the user’s
ability to select the best classiﬁer when compared to random
pick (RP), with LIME outperforming greedy in both cases.
6.3 Can non-experts improve a classiﬁer? 

","According to the text, LIME can help non-experts improve a classifier by doing feature engineering using LIME. This is demonstrated in an experiment where non-experts are able to greatly improve an untrustworthy classifier trained on 20 newsgroups by performing feature engineering with LIME.",3.881206750869751,"LIME (Local Interpretable Model-agnostic Explanations) allows non-experts to understand and interpret the predictions of any classifier or regressor. By providing clear explanations for individual predictions, non-experts can identify which features are most influential in the model's decisions. This understanding enables them to engage in feature engineering by modifying the input features or improving the labeling of the data based on the insights gained from LIME. In experiments, it was shown that non-experts could effectively select better-performing classifiers by using LIME to analyze the models, ultimately leading to improved model accuracy and trustworthiness. The ability to explain predictions empowers users to make informed decisions about model improvement, even without deep expertise in machine learning."
llama3.1:latest,"What is Explainable Boosting Machine model?
","Document 1: 
 feature x2 there is a weight β2 and β1 is ten times greater than
β2, it is reasonable to say that the feature x1 is considerably
more important to the decision of the model. That is, x1 will
have ten times more impact than x2. Therefore, it is possible
to make sense of a global feature importance by analyzing
the values of the β feature weights. One limitation of this
model is that due to its linear nature, local explanations can-
not be directly assessed, that is because the model adjusts the
feature importances as being the same for the entire features
distribution.
4.2 Explainable Boosting Machine
The explainable boosting machine (EBM) proposed in Nori
et al. ( 2019) is a glass-box-type model that leverages state-of-
the-art performing algorithms, such as boosting and bagging,
while maintaining intrinsic interpretability. The main idea
of this model is to build a generalized additive model with
interactions ( GA 2 M) by executing a round-robin training
procedure on one feature at a time, using a meager learning
rate. Its general is given in the form of:
g(E[y]) = β0 + /Sigma1f j (x j ) + /Sigma1fij (xi , x j ) (3)
where g is the link function and f j and fij are called smooth
functions. This approach brings two major improvements 

Document 2: 
 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
https://doi.org/10.1007/s40313-021-00858-y
Explainable Artiﬁcial Intelligence for COVID-19 Diagnosis Through
Blood Test Variables
Lucas M. Thimoteo 1 · Marley M. Vellasco 1 · Jorge Amaral 2 · Karla Figueiredo 3 · Cátia Lie Yokoyama 4 ·
Erito Marques 2
Received: 30 April 2021 / Revised: 1 October 2021 / Accepted: 20 October 2021 / Published online: 3 January 2022
© Brazilian Society for Automatics–SBA 2021
Abstract
This work proposes an explainable artiﬁcial intelligence approach to help diagnose COVID-19 patients based on blood test
and pathogen variables. Two glass-box models, logistic regression and explainable boosting machine, and two black-box
models, random forest and support vector machine, were used to assess the disease diagnosis. Shapley additive explanations
were used to explain predictions for the black-box models, while glass-box models feature importance brought insights into
the most relevant features. All global explanations show the eosinophils and leukocytes, white blood cells are among the
essential features to help diagnose the COVID-19. Moreover, the best model obtained an AUC of 0.87.
Keywords COVID-19 diagnosis · Machine learning · Explainability · Interpretability · Shapley additive explanations ·
Explainable boosting machine 

Document 3: 
 machine learning(Vol. 37, pp. 1718–1727). PMLR.
Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2020). Explainable AI: A review of machine learning interpretability methods.
Entropy, 23(1), 18.https://doi.org/10.3390/e23010018
Lipovetsky, S., & Conklin, M. (2001). Analysis of regression in game theory approach.Applied Stochastic Models in Business and Industry,
17(4), 319–330. https://doi.org/10.1002/asmb.446
Lipton, Z. C. (2018). The mythos of model interpretability.Queue, 16(3), 31–57. https://doi.org/10.1145/3236386.3241340
Liu, K., Sadoune, N., Rao, N., Greitemann, J., & Pollet, L. (2021). Revealing the phase diagram of Kitaev materials by machine learning:
Cooperation and competition between spin liquids.Physical Review Research, 3(2), 023016. https://doi.org/10.1103/physrevresearch.3.
023016
Liu, S., Kailkhura, B., Loveland, D., & Han, Y. (2019). Generative counterfactual introspection for explainable deep learning. In2019 IEEE 

Document 4: 
 atively to predict and impute each missing feature
from the other features until the imputed values con-
verge. MissForest is a popular imputation method
as it is capable of capturing non-linear and interac-
tion eﬀects between features to improve imputation
accuracy, and can be applied to mixed data types
(continuous and discrete). Note that, the framework
of MissForest is similar to that of MICE (Van Buuren
and Groothuis-Oudshoorn, 2011) — the only diﬀer-
ence is MissForest uses random forest while MICE
uses linear model as base model for imputation.
KNN imputation imputes the missing values by the
mean value of its K nearest neighbors in the training
set. The distance of two samples is measured on the
non-missing features in both samples. KNN imputa-
tion is fast and accurate but requires choosing a good
distance metric and tuning the hyperparameter K.
3.3. Explainable Boosting Machines
The methods proposed in this work are based on one
interpretable machine learning model, the Explain-
able Boosting Machine (EBM).
Suppose an input sample is denoted as (x,y), where
x is the p dimensional feature vector and y is the
target. Denote the jth dimension of the feature vector
as xj. Then a generalized additive model (GAM), 

Document 5: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 6: 
 show examples where all models correctly predicted the
123
634 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
Fig. 1 Global interpretation for logistic regression. Top: dataset with blood tests and pathogens; bottom: dataset with only blood tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 635
Fig. 2 Global interpretation for explainable boosting machine. Top: dataset with blood tests and pathogens; Bottom: dataset with only blood tests
outcome, some models failed, while others succeeded, all
models failed.
Thus, we will show the reader how one can interpret local
explanations for the COVID-19 diagnosis in a myriad of sit-
uations. In other words, we want to show that although the
overall results are really good, the models still make mistakes
and get confused with the features, and local explanations
can help with identify that. If the reader wants to check more
examples, we encourage seeing our GitHub, provided at the
beginning of the Experiments section.
Figure 5 shows explanations for a given patient that led to a
true-positive diagnosis and Fig. 6 for a true-negative patient.
Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis. 

","The Explainable Boosting Machine (EBM) is a glass-box-type machine learning model that leverages algorithms like boosting and bagging while maintaining intrinsic interpretability. It builds a generalized additive model with interactions by executing a round-robin training procedure on one feature at a time, making it easier to understand the relationships between features and predictions.",4.130465984344482,"The Explainable Boosting Machine (EBM) is a type of glass-box model that combines boosting and bagging techniques while maintaining interpretability. It builds a generalized additive model with interactions (GA2M) by training on one feature at a time using a small learning rate. The model is defined mathematically as g(E[y]) = β0 + Σ f_j(x_j) + Σ f_ij(x_i, x_j), where g represents the link function, and f_j and f_ij are smooth functions associated with the features. EBMs provide useful insights into feature importance, allowing for an understanding of the relevance of different features in model predictions, while being capable of achieving high predictive performance similar to that of black-box models."
llama3.1:latest,"How can EBMs help with healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 icians tend to focus on abnormal ﬁndings.
Figure 3: EBM shape function of “heart rate” for
predicting pneumonia mortality risk. Blue curve is
the original shape function; red curve is the edited
shape function.
For example, this happens to a pneumonia mortal-
ity risk dataset (Cooper et al., 2005). The blue curve
in Figure 3 shows what an EBM model has learned
for predicting pneumonia mortality as a function of
heart rate. As expected, risk is elevated for patients
with abnormally low (10-30) or high heart rate (125-
200). The graph, however, shows a surprising region
of ﬂat risk between heart rate 38 and 125, which is a
normal heart rate for patients in a doctor’s oﬃce.
Moreover, the model surprisingly predicts patients
who have normal heart rate are at elevated risk: it
adds 0.22 to the risk for patients in this region.
On further inspection, it turns out that there are
no patients in the data set with heart rates between
38 and 125, and 91% of patients are missing their
heart rate which has then been coded as zero. In
other words, there are no data to support the model
in the normal range of heart rate 38-125, and instead
the patients who would be in this range are all coded
as zero in the data and on the graph. This explains
why the model predicts the lowest risk = -0.04 for
patients with heart rate = 0, because these are the
patients who actually have normal heart rates. 

Document 3: 
 indicate the existence of the disease.
Our results indicated that the proposed system could
indeed be used to detect COVID-19 patients using clinical
and laboratory data that is commonly used in clinical prac-
tice. In terms of healthcare, our approach could represent a
new method that can help by quickly screening patients with
COVID-19. This can be specially advantageous to expand
testing protocols to areas where there is not available tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 641
Fig. 8 Local explanations for
failed diagnosis on hemogram
dataset. Top: EBM; middle: RF;
bottom: SVM
for the local population. Additionally, situations where lab-
oratory workloads are heavy, and RT-PCR tests are scarce
could also beneﬁt from rapid blood sample tests. Therefore,
it would help reduce the burden in the healthcare system
and promote the optimal utilization of healthcare resources.
Besides, we think that is possible to improve the results with
the addition of the clinical signs such as the proportion of
cough, hyperthermia, myalgia, asthenia, diarrhea and confu-
sion.
We look forward to including more COVID-19 test types
as well as data from different geographical and socioeco-
nomic regions from Brazil. Due to the continental size of the
country, the epidemic dynamics changes drastically between
regions and our proposed approach might show different
performance and also different explanations. Moreover, we
would like to increase more blood test features in order 

Document 4: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 5: 
 Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis.
For the true positive, low levels of white blood cells and
elevated values of red cells greatly impact the positive out-
come for this patient, for all models.
For the true negative, the white blood cells are also respon-
sible for the majority impact on models output. Interestingly,
the SVM was the only model that took in consideration the
monocytes and sex of the patient as relevant features.
Figure 7 shows a negative patient where the EBM cor-
rectly identiﬁed and RF and SVM classiﬁed as positive. Even
though EBM got the classiﬁcation right, we notice that sev-
eral variables have positive contributions, similarly to the
SVM and RF models. Also, we see that the model output for
SVM and RF are nearly 0.5. Although not reported in the
ﬁgure, the model output for the EBM is 0.48. Therefore, this
patient lies in a low conﬁdence zone for all models.
Figure 8 shows a positive patient that all the models failed
to diagnose, for the hemogram dataset. We notice that all
models predicted a score near to 0 probability of a positive
case and most of the features actually contributed to a neg-
ative diagnosis in all cases. This implies that there might be 

Document 6: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

","EBMs (Explainable Boosting Machines) can help with healthcare data by detecting potential problems caused by missing values, allowing for the visualization of the effects of different imputation methods on model predictions. This helps to identify problematic models and understand the consequences of various imputation strategies.",3.78750228881836,"EBMs (Explainable Boosting Machines) can significantly assist with healthcare data by providing interpretable and editable models that detect potential issues arising from missing values or imputation. They capture subtle anomalies in data that may go unnoticed by simpler models like linear regression or decision trees. EBMs are particularly beneficial in high-stakes environments, such as medical care, where the costs of poor predictions can be severe.

One key advantage of EBMs is their ability to reveal the consequences of different imputation methods, thereby illuminating the influence of missing data on model predictions. For instance, when analyzing datasets related to pneumonia mortality risk, EBMs can highlight particular patterns, such as the abnormal prediction of risk for patients with normal heart rates due to missing data representation. This interpretability helps healthcare professionals make informed decisions based on model outputs.

Furthermore, EBMs can automatically include interaction terms among features, thereby enhancing accuracy while maintaining intelligibility. Their flexibility and enhancement over traditional generalized additive models mean they can adapt to various healthcare applications, improving patient screenings and enabling better resource allocation in testing protocols. Overall, the use of EBMs equips healthcare practitioners with insights that promote safer and more effective clinical decision-making."
llama3.1:latest,"How can be EDMs used in imputing missing values in healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 and class median impute the missing value as the mean/
median of known Depression values at that time point, in the
appropriate class. Thehot deck [12] method, which is used
to impute labor force items in the Current Population Survey
[13], involves a recipient (the person with missing data) and
a donor (another person in the same imputation class, whose
value is known). We selected a donor by sampling without
replacement from the recipient’s class, and replaced the re-
cipient’s missing value with the donor’s value at the appro-
priate time point. These class imputations imply that persons
with missing data are a random sample of the persons in
their class[14].
Another baseline approach is regression imputation. The
variable of interest (known Depression value for the year in
which the “missing value” occurred) is regressed on baseline
covariates, and the resulting equation is used to estimate
the missing values for that year. We regressed Depression
on baseline Depression, age, gender, and health status. Data
for persons missing any covariate information were not im-
puted. Two different types of regression imputation were
performed.Regressionassigns the person’s predicted value to
the “missing value.” Persons with the same covariates will
have exactly the same imputed value. This can lead to the
variance of the imputed data set being too small, yielding
inappropriately small standard errors and P-values at the 

Document 3: 
 orange) when predicting whether HR is missing. In
the plot, the curve of the imputed group is signiﬁ-
cantly higher than that of the observed group, again
suggesting that when the blood pressure of the pa-
tients is missing, their heart rate is also more likely
to be missing. This eﬀect is strong, as the maximum
gap between the two curves is approximately 2.5 (1.5
in orange curve and -1.0 in blue curve) of predicted
log odds. Again the blue curve is constant.
92
Interpretable Missing Values in Healthcare
Figure 4(c) shows the shape function for tempera-
ture when predicting if urea is missing or not. There
is no missing value for temperature, so there is no
orange curve. The bump at temperature ∈[36,37.5)
indicates that urea is more likely to be missing, which
suggests when a patient has normal body tempera-
ture, doctors may be less likely to order a blood test
to measure urea.
To test how well can EBM predict the missing-
ness, we generate some semi-synthetic datasets with
ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out- 

Document 4: 
 column values. But this approach is appropriate only when there are a couple of missing values in the 
data set. Although as the quantity of missing values increase, significant data may be lost by deletion of 
the incomplete instances [13].
 – Mean/mode substitution: This is a simple way to impute the missing data. It replaces the missing values 
by the mean or mode of all the observations or a subgroup at the same variable. It consists of replacing 
the unknown value for a given attribute by the mean (quantitative attribute) or mode (qualitative attrib-
ute) of all known values of that attribute. But replacing all missing records with a single value distorts 
the input data distribution [21].
 – Maximum likelihood procedures [3]: These methods are used for estimation of parameters of model by 
making use of the expectation–maximization algorithm [13, 21]. These methods make assumption of 
model distribution for the variables, such as a multivariate normal model, which are very much sensitive 
to outliers [21].
 – Multiple imputation [5, 18]: This method imputes the value of a missing variable more than once. Then, 
analysis is done by averaging them [21].
 – Machine learning-based imputation [13]: There are a number of approaches for missing value imputation 
include choosing the most common value among the neighbors, weighted imputation based on k-nearest 
neighbors, decision tree based, association based, fuzzy logic, neural networks, SVM, and clustering- 

Document 5: 
 Meanwhile, the importance of explainable AI is increasingly recognized across
various sectors. Taking medical imaging is an example where physicians rely on
explainability to evaluate and potentially improve diagnoses based on machine
outputs. However, a significant challenge arises when these explanation methods
encounter missing data in the dataset, an aspect they often overlook. The
occurrence of missing data inherently introduces additional uncertainty into the
model. Depending on the imputation values used to fill in the missing data,
explanations and results can vary, potentially leading to inaccurate evaluations
of a patient’s health status. Such issues are not exclusive to healthcare but
extend to other domains, including banking, biology, and more.
Thus, being able to explain the imputation of missing values in the data
is an essential step toward more trustworthy data analysis. This motivates
us to introduceconditional Distribution based Imputation of Missing Values
(DIMV) algorithm, an algorithm that is able to find the conditional distribution
of the features with missing values based on fully observed features in a fashion
2
similar to the Expectation-Maximization algorithm. However, DIMV only
conditionsuponselectedrelevantfeatures, andmoreover, themeanandcovariance
matrix are estimated directly based on the DPER algorithm [2], which gives
DIMV an advantage in speed compared to EM. In addition, the imputation
step of DIMV can be interpreted as Ridge regression, with coefficients giving
direct explainability as regression coefficients. Furthermore, note that by the 

Document 6: 
 ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out-
put score is higher than the threshold. The diﬀer-
ence between MAR and MNAR is whether the target
feature value is considered as an input of the miss-
ing models. Table 2 compares EBM’s the test accu-
racy of predicting missingness with machine learning
models commonly used for missing value imputation.
EBM predicts missingness better than other methods
in cases of MAR and MNAR values generated from
linear and curvilinear models and is not far behind
Random Forest in case of quadratic model.
Summary: We use EBMs to predict the missing-
ness of features from other input features. EBM pre-
dicts the missingness accurately. The interpretability
of EBMs can help users understand the relationship
between the features and missingness and thus bring
more insight for the cause(s) of missingness.
5. Detecting and avoiding potential
risks of missing value imputations
5.1. Imputation With the Mean
Because many machine learning methods cannot na-
tively handle missing values, it is common for data
scientists to impute missing values before training
models. There are many diﬀerent ways to do this 

","EBMs (Explainable Boosting Machines) can be used to predict and understand the missingness of features from other input features. By analyzing the shape functions of EBMs, data scientists can gain insights into the relationship between features and missingness, which can help identify potential causes of missing values and improve healthcare decisions.",3.736194610595703,"EBMs (Explainable Boosting Machines) can be used in imputing missing values in healthcare data by effectively predicting the missingness of features based on the relationships with other input features. Given their interpretable nature, EBMs can uncover subtle anomalies and provide insights into the causes of missing data, whether they follow a Missing At Random (MAR) or Missing Not At Random (MNAR) mechanism. The ability of EBMs to model the shape functions allows them to capture complex interactions between features and missing values, which makes them more proficient than traditional methods like mean or regression imputation. Furthermore, since EBMs are editable, any detected issues related to missing values can be rectified using model editing tools, contributing to more reliable healthcare decisions and potentially mitigating the risks associated with erroneous predictions derived from incomplete data."
llama3.1:latest,"How can EBMs help detect observations poorly imputed by other methods?
","Document 1: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

Document 2: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 3: 
 the mean value of the feature is the same before and
after mean imputation, we can directly ﬁnd the bin
(of EBM) covering the mean value, and detect if the
bin is a spike or not. This also works for median im-
putation — the median of a feature does not change
by imputing the missing values with the median.
To address the second problem, we need an algo-
rithm to distinguish spikes resulting from mean im-
putation and ﬂuctuations that naturally occur in the
EBM shape functions. We formulate this as an out-
lier detection problem. First, we calculate the second
order diﬀerences for all bins in all shape functions
(excluding ﬁrst and last bins), since spikes usually
have extreme second order diﬀerences. We denote
the function values of the kth bin and its neighbour-
ing bins as fk, fk−1, and fk+1. The corresponding
bin sizes are denoted as hk, hk−1, and hk+1. The
second order diﬀerence is
f′′
k (x) ≈
fk+1−fk
(hk+1+hk)/2 − fk−fk−1
(hk+hk−1)/2
hk + hk+1/2 + hk−1/2 . (5)
We then run an outlier detection algorithm (Isolation
Forest (Liu et al., 2008)) on these second order diﬀer- 

Document 4: 
 and generative models like deep generative models
(Yoon et al., 2018). Since most of these methods are
based on black-box machine learning methods and
the accuracy and behavior of the ﬁnal model depends
on the imputed values, it is diﬃcult for users to recog-
nize and understand the potential harms that might
be introduced by these imputation methods.
Recently developed interpretable machine learning
methods have been shown to be useful for debugging
models and detecting issues with datasets (Adebayo
et al., 2020; Koh and Liang, 2017). Interpretable ma-
chine learning methods provide a new opportunity to
study missing values and revisit some of the classi-
cal methods for handling missing values. In this pa-
per, we propose novel methods based on the Explain-
able Boosting Machine (EBM) (Lou et al., 2012, 2013;
Nori et al., 2019), a high-accuracy, fully-interpretable
glass-box machine learning method, to answer the fol-
lowing questions: (1) how interpretability can help
users gain insights on the causes of missingness, and
(2) how interpretability can help detect and avoid po-
tential risks introduced by diﬀerent imputation meth-
ods. We show that the glass-box models provide new
insights into missingness mechanisms, and in some
settings, suggest alternate ways of handling missing
values, as well as new tools that can alert users when 

Document 5: 
 a method to test MCAR based on EBM shape func-
tions. The testing process of the proposed method
88
Interpretable Missing Values in Healthcare
can be directly visualized on the shape function plots,
which is not achievable by Little’s test. We will also
show that EBM can bring additional insights beyond
simply testing for MCAR.
4.1.1. Testing for MCAR with EBM
To test for MCAR, we use the common trick of encod-
ing missing values with a unique value for the feature,
e.g., -1 for a feature with positive values or a sepa-
rate category for a categorical feature. After ﬁtting
an EBM that predicts the target, we get a shape func-
tion representing the contribution of diﬀerent feature
values for predicting the target, including the unique
value denoting missingness. Note that the leaf nodes
in EBM split the feature values into many bins, where
each bin has a prediction score. These bins and scores
together form the shape function. Therefore, the
EBM shape function fj(·) of feature j can be rewrit-
ten as a linear combination of a series of indicator
variables denoting if the feature values are within the
bins, and the coeﬃcients are the corresponding scores
of the bins, i.e.,
fj(xj) =
Bj−1∑
k=0 

Document 6: 
 be similar to uniformly imputing the heart rates in
the interval 38-125 and retraining the model.) This
approach has the following advantages:
1. Editing shape functions provides an opportunity
for experts to use their professional training to
correct and improve models in ways that may not
be adequately represented in the training data.
2. Editing the model may not only improve the ac-
curacy of the model in the real world where it
will be used (instead of just on held-aside test
data from the train set), but also make the shape
plots more “reasonable” and trusted by experts.
3. Editing an EBM shape function can be done
without retraining the model and potentially in-
troducing new problems.
Summary: We show that EBM shape function
can help identify the case when feature values are
missing because they are assumed to be normal. We
also show how editing the EBM graphs can help ad-
dress issues resulting from missing assumed normal.
4.3. Predicting the Missingness
Most missing values are not MCAR, but as mentioned
in Section 2, MNAR and MAR can be diﬃcult to dis-
tinguish (Van Buuren, 2018). For both cases, inter-
pretable models like EBM can still be useful in pro-
viding insights on possible missingness mechanisms.
(a) “Bilirubin” shape function when predicting missing-
ness of “Na”
(b) “Systolic blood pressure” shape function when pre- 

","EBM's interpretable nature allows it to identify and flag potentially problematic imputations, which would otherwise be invisible. By analyzing the EBM shape functions, researchers can detect spikes or extreme values that may indicate poor imputation. This helps identify potential issues with the imputation method, allowing for corrective action to be taken.",3.776276111602783,"EBMs can help detect observations poorly imputed by other methods by providing a clear visualization of the contribution and behavior of feature values through their shape functions. When different imputation methods are applied, such as MissForest, EBMs can reveal discrepancies between the imputed groups and the observed groups, highlighting significant differences in predicted outcomes, such as ICU mortality risk. 

By analyzing the shape functions, researchers can identify spikes or unusual patterns that may indicate problematic imputation results. Additionally, EBMs allow for the calculation of outlier detection based on second-order differences in feature values, making it possible to distinguish between natural fluctuations and those caused by flawed imputation methods. This interpretability enables users to gain insights into the implications of missing data handling, allowing them to address issues arising from imputation and potentially leading to more accurate and reliable model performance."
llama3.1:latest,"How can one distinct terms intepretabilitry and explainability in XAI?
","Document 1: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 2: 
 tant to distinguish between features and interpretable data
representations. As mentioned before, interpretable expla-
nations need to use a representation that is understandable
to humans, regardless of the actual features used by the
model. For example, a possible interpretable representation
for text classiﬁcation is a binary vector indicating the pres-
ence or absence of a word, even though the classiﬁer may
use more complex (and incomprehensible) features such as
word embeddings. Likewise for image classiﬁcation, an in-
terpretable representationmay be a binary vector indicating
the “presence” or “absence” of a contiguous patch of similar
pixels (a super-pixel), while the classiﬁer may represent the
image as a tensor with three color channels per pixel. We
denote x∈Rd be the original representation of an instance
being explained, and we use x′∈{0, 1}d′
to denote a binary
vector for its interpretable representation.
3.2 Fidelity-Interpretability Trade-off
Formally, we deﬁne an explanation as a model g ∈G,
where G is a class of potentially interpretable models, such
as linear models, decision trees, or falling rule lists [ 27], i.e. a
model g∈Gcan be readily presented to the user with visual
or textual artifacts. The domain of g is {0,1}d′
, i.e. g acts 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 9.8.1735
Hohman, F., Srinivasan, A., & Drucker, S. M. (2019). TeleGam: Combining visualization and verbalization for interpretable machine learn-
ing. InIEEE visualization conference (VIS). IEEE.https://doi.org/10.1109/visual.2019.8933695
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., & Müller, H. (2019). Causability and explainability of artificial intelligence in medicine.
WIREs Data Mining and Knowledge Discovery, 9(4), e1312.https://doi.org/10.1002/widm.1312
Jain, S., & Wallace, B. C. (2019). Attention is not explanation. InProceedings of the 2019 conference of the North American chapter of the Asso-
ciation for Computational Linguistics: Human language technologies(pp. 3543–3556). Association for Computational Linguistics.https://
doi.org/10.18653/v1/N19-1357
Jesus, S., Belém, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P., & Gama, J. (2021). How can I choose an explainer? An application-
grounded evaluation of post-hoc explanations. InProceedings of the 2021 ACM conference on fairness, accountability, and transparency 

Document 5: 
 In general, there is no agreement within the ML community on the definition ofinterpretability and thetask of inter-
pretation (Doshi-Velez & Kim,2017; Lipton,2018). For example, Doshi-Velez and Kim (2017) define interpretability of
ML systems as“the ability to explain or to present in understandable terms to a human.” This definition lacks mathemat-
ical rigor (Lipton, 2018). Nevertheless, the notion of interpretability often depends on the domain of application
(Rudin, 2019) and the targetexplainee (Carvalho et al.,2019), that is, the recipient of interpretations and explanations.
Therefore, an all-purpose definition might be infeasible (Rudin,2019) or unnecessary. Other terms that are synonymous
with interpretability and also appear in the ML literature are“intelligibility” (Caruana et al.,2015; Lou et al.,2012) and
“understandability” (Lipton, 2018). These concepts are often used interchangeably.
Yet another term prevalent in the literature is“explainability,” giving rise to the direction ofexplainable artificial
intelligence (XAI) (Gunning & Aha,2019). This concept is closely tied with interpretability; and many authors do not
differentiate between the two (Carvalho et al.,2019). Doshi-Velez and Kim (2017) provide a definition ofexplanation 

Document 6: 
 Kirrane, 2020)).
In recognition of the growing importance of this topic, NIST published in August 2020Four principles of XAI
(Phillips et al., 2020), which define the following fundamental principles which an AI must honor to be considered an
XAI as follows:
 Explanation: this principle states that an AI system must supply evidence, support; or reasoning for each decision
made by the system.
 Meaningful: this principle states that the explanation provided byt h eA Is y s t e mm u s tb eu n d e r s t a n d a b l eb y ,a n dm e a n i n g f u l
to, its users. As different groups of users may have different necessities and experiences, the explanation provided by the AI
system must be fine-tuned to meet the various characteristics and needs of each group.
 Accuracy: this principle states that the explanation provided by the AI system must reflect accurately the system's processes.
 Knowledge limits: this principle states that AI systems must identify cases that they were not designed to operate in
and, therefore, their answers may not be reliable.
Figure 3 depicts the ontology of the XAI taxonomy. Transparent models can easily achieve explainability, while
opaque models require post hoc approaches to make them explainable. The categories of post hoc approaches are illus-
trated accordingly. 

","In the context of Explainable Artificial Intelligence (XAI), interpretability refers to the ability of a model to be understood by humans, either through its internal workings or through some form of representation that is comprehensible. On the other hand, explainability refers to the process of providing insights into how a decision was made by the model, often in terms of evidence, support, or reasoning.

In simpler terms: interpretability is about making the model understandable, while explainability is about justifying its decisions.",4.783173322677612,"Interpretability and explainability are terms that are often used interchangeably within the field of explainable artificial intelligence (XAI), but they can be distinguished based on their definitions and implications. Interpretability refers to the degree to which a user can understand the cause of a decision made by an AI system, typically implying that the model is transparent and its workings can be easily explained in human terms. Doshi-Velez and Kim (2017) define interpretability as “the ability to explain or present in understandable terms to a human.”

On the other hand, explainability encompasses a broader scope. It pertains to how an AI system provides reasoning or support for its decisions and aims to convey this information meaningfully to users. Explainability is necessary when dealing with complex or ""black-box"" models where the internal processes are not straightforward or easily understood. As Burkart and Huber (2020) note, while explainable models are interpretable by default, the opposite is not necessarily true, highlighting that interpretability alone may not address all aspects of understanding machine learning decisions.

In summary, while interpretability focuses on the clarity and transparency of the model itself, explainability relates more to the quality and relevance of the information provided to users regarding the model's decision-making process."
llama3.1:latest,"What issues in machine learning can be categorized as 'model bugs'?
","Document 1: 
 2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n
i, wherex∈X and y∈Y, a classiﬁer’s goal is to learn a function,fθ : X→Y ,
that generalizes. fθ is then used to predict test examples,xtest ∈X, as ytest = fθ(xtest). Given a
loss functionL, and model parameter,θ, for a model family, we provide a categorization of bugs as
model, data and test-time contamination:
Learning: arg min
θ
Model Contamination
L(
Data Contamination  
(Xtrain,Ytrain) ; θ);
Prediction: ytest = fθ(
Test-Time Contaminationxtest ).
1We will provide code to replicate our ﬁndings at: https://github.com/adebayoj/explaindebug.git.
3
Bug Category Speciﬁc Examples tested Formalization
Data Contamination Spurious Correlation arg min
θ
L(Xspurious artifact,Ytrain;θ)
Labelling Errors arg min
θ 

Document 2: 
 lyzing all of this data together requires some form of data
integration. In machine learning, two relevant integration
techniques are alignment and co-learning. Alignment is to
ﬁnd relationships of sub-components of instances that have
multiple modalities. For example, if there are multi-view time
series, one can perform subsampling, forward or backward
ﬁlling, or aggregate in time windows so that the time series
can be better integrated. Co-learning is to train better on a
modality using a different modality. For example, if there
are embeddings from different modalities, one approach is
to concatenate them together for a multimodal representa-
tion. In general, data integration is by itself a large research
area that has been studied for decades [ 46,152], although not
all techniques are relevant to machine learning.
4 Robust model training
Even after collecting the right data and cleaning it, data
quality may still be an issue during model training. It is
widely agreed that real-world datasets are dirty and erro-
neous despite the data cleaning process. As summarized in
Table2, these ﬂaws in datasets can be categorized depending
on whether data values are noisy or missing and depending
on whether these ﬂaws exist in data features (attributes) or
labels.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 799
Table 2 Types of data poisoning covered in this survey
Noisy Missing 

Document 3: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 4: 
 attributions, and attributions for these samples on other models. As seen in Table 3, we observe
visual similarity, particularly for the VGG-16 model on ImageNet, but essentially no correlation in
feature ranking.
10
Insights from Human Subject Study: users use prediction labels, not the attributions.
For the domain shift study, we show participants attribution of dogs that were not used during
training, and whose breeds diﬀered from those that the model was trained to predict. We ﬁnd that
users do not recommend a model under this setting due to wrong prediction labels (Figure 5).
6 Discussion & Conclusion
Debugging machine learning models remains a challenging endeavor, and model explanations could
be a useful tool in that quest. Even though a practitioner or a researcher may have a large class of
explanation methods available, it is still unclear which methods are useful for what bug type. This
work aims to address this gap by ﬁrst, categorizing model bugs into: data, model, and test-time
contamination bugs, then testing feature attribution methods, a popular explanation approach for
DNNs trained on image data, against each bug type. Overall, we ﬁnd that feature attribution
methods are able to diagnose the spatial spurious correlation bug tested, but do not conclusively
help to distinguish mislabelled examples for normal ones. In the case of model contamination, we
ﬁnd that certain feature attributions that perform positive aggregation while computing feature
relevance with modiﬁed back-propagation produce attributions that are invariant to the parameters 

Document 5: 
 that the humans exploit label information and Feng and Boyd-Graber (2019) demonstrate how to
assess explanations in a natural language setting. Similarly, Alqaraawi et al. (2020) ﬁnd that the
LRP explanation method (see Section 2.2) improves participant understanding of model behavior
for an image classiﬁcation task, but provides limited utility to end-users when predicting the model’s
output on new inputs.
Feature attributions can be easily manipulated, providing evidence for a collective ‘weakness’ of
current approaches (Ghorbani et al., 2019; Heo et al., 2019; Lakkaraju and Bastani, 2020; Slack
et al., 2020). While susceptibility is an important issue, our work focuses on providing insights
when model bugs are ‘unintentionally’ created.
2 Bug Characterization, Explanation Methods, & User Study
We now present our characterization of model bugs, provide an overview of the explanation methods
assessed, and close with a background on the human subject study.1
2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n 

Document 6: 
 model as a way to provide a global understanding.
There are several ways a model or its evaluation can go
wrong. Data leakage, for example, deﬁned as the uninten-
tional leakage of signal into the training (and validation) data
that would not appear when deployed [ 14], potentially in-
creases accuracy. A challenging example cited by (author?)
[14] is one where the patient ID was found to be heavily corre-
lated with the target class in the training and validation data.
This issue would be incredibly challenging to identify just by
observing the predictions and the raw data, but much easier
if explanations such as the one in Figure 1 are provided, as
patient ID would be listed as an explanation for predictions.
Another particularly hard to detect problem is dataset shift
[5], where training data is diﬀerent than test data (we give
an example in the famous 20 newsgroups dataset later on).
The insights given by explanations are particularly helpful in
identifying what must be done to convert an untrustworthy
model into a trustworthy one – for example, removing leaked
data or changing the training data to avoid dataset shift.
Machine learning practitioners often have to select a model
from a number of alternatives, requiring them to assess
the relative trust between two or more models. In Figure
Figure 2: Explaining individual predictions of com-
peting classiﬁers trying to determine if a document
is about “Christianity” or “Atheism”. The bar chart
represents the importance given to the most rele- 

","Model bugs refer to contamination in the learning and/or prediction pipeline that causes the model to produce incorrect predictions or learn error-causing associations. They are categorized based on their source into three types: Data Contamination, Model Contamination, and Test-Time Contamination.",3.4641189575195312,"Model bugs in machine learning can be categorized as contamination in the learning and/or prediction pipeline that leads to incorrect predictions or the learning of error-causing associations. These bugs can be classified into three primary categories based on their source:

1. **Model Contamination**: Issues arising from the model itself that can affect its learning process or predictions.
  
2. **Data Contamination**: Problems related to the dataset used for training, such as:
   - Spurious correlations in the training data.
   - Labeling errors, where incorrect or noisy labels are present due to manual annotation errors or software mistakes.

3. **Test-Time Contamination**: Problems that occur during the model's prediction phase, potentially due to issues like data leakage, where unintended signals from the training set could artificially inflate accuracy in the prediction phase, or dataset shift, where the training data differs from the test data.

These model bugs highlight the complexities and challenges in ensuring model reliability and accuracy in machine learning frameworks."
llama3.1:latest,"What XAI techniques can be heloful in detecting mislabeled observation in image classification?
","Document 1: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 2: 
 In the ILSVRC2012 ImageNet competition, it surpassed the second-best entry by a huge
advantage. After AlexNet achieved excellent results in the ImageNet image classiﬁcation
competition, researchers began to study CNN more deeply, Zeiler and Fergus proposed
a visualization technique to understand CNNs and proposed ZFNet [ 20]. Min Lin et al.
proposed NIN network [21], which contributed to the control of the parameter amount
and the number of channels. Next, refs. [ 22–27] yielded high performance during the
ILSVRC2014-2017 classiﬁcation challenge, they all made great innovations on the original
basis. From 2017 to the present, more models with superior performance have appeared
one after another. CNNs have increasingly demonstrated irreplaceable superiority in image
classiﬁcation.
With the successful application of CNN to large-scale visual classiﬁcation tasks, around
2015, the application of CNNs has ﬁnally taken off in the remote sensing image analysis
ﬁeld [28,29]. A variety of CNN-based scene classiﬁcation methods have emerged by using
different strategies of exploiting CNNs [ 30–36]. Generally, CNN-based remote sensing
image scene classiﬁcation methods can be divided into three types: (i) The pre-trained
CNNs is used as a feature extractor [ 37–44]. (ii) Fine-tune the pre-trained CNNs on the
dataset [30,45–52]. (iii) Globally initialize the weights of CNNs for training [ 31,53–55].
As we all know, the CNN-based image classiﬁcation method was originally designed for 

Document 3: 
 ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 6021–6029. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/6064.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
17
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from
error visibility to structural similarity.IEEE transactions on image processing, 13(4):600–612,
2004.
Gezheng Wen, Brenda Rodriguez-Niño, Furkan Y Pecen, David J Vining, Naveen Garg, and Mia K
Markey. Comparative study of computational visual attention models on two-dimensional medical
images. Journal of Medical Imaging, 4(2):025503, 2017.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. InAdvances in neural information processing systems, pages
9291–9301, 2018. 

Document 4: 
 self-supervised learning and generative models, and we
will cover a self-supervised learning technique called Jig-
sawNet [108].
In Mean-Teacher [ 156], the teacher model is the average
of consecutive student models. Both the student and teacher
models evaluate the input in a training batch. The softmax
output of the student model is compared with the one-hot
label using a classiﬁcation cost. Additionally, the output is
compared with the teacher output using the consistency loss.
After the weights of the student models are updated via gra-
dient descent, the teacher model weights are updated as an
exponential moving average of the student model weights.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 803
Fig. 10 Example of the jigsaw puzzle task for a given unlabeled
image [108]
A training step with unlabeled examples is done without the
classiﬁcation cost.
In MixMatch [ 17], to exploit an unlabeled dataset, it per-
forms label guessing where stochastic data augmentation is
applied to an unlabeled imageK times; then, each augmented
image is fed through the classiﬁer. The average of these
K predictions is sharpened by adjusting the distribution’s
temperatures. The MixMatch algorithm mixes both labeled
examples and unlabeled example with label guesses. In more
detail, when mixing two images, the images are overlayed,
and the labels are averaged, following the MixUp augmen- 

Document 5: 
 which was veriﬁed via3 questions.
Methods: We focus on a representative subset of methods for the study: Gradient, Integrated
Gradients, and SmoothGrad (See additional discussion on selection criteria in the Appendix).
Bugs: We tested the bugs described in Table 1 along with a model with no bugs.
3 Debugging Data Contamination
Overview. We assess whether feature attributions can detect spurious training artifacts and mis-
labelled training examples. Spurious artifacts are signals that encode or correlate with the label in
the training set but provide no meaningful connection to the data generating process. We induce
a spurious correlation in the input background and test whether feature attributions are able di-
agnose this eﬀect. We ﬁnd that the methods considered indeed attribute importance to the image
background for inputs with spurious signals. However, despite visual evidence in the attributions,
5
participants in the human subject study were unsure about model reliability for the spurious model
condition; hence, did not out-rightly reject the model.
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
A
B
C
D
Figure 3: Feature Attributions for Spurious Correlation Bugs.Figure shows attributions 

Document 6: 
 3.2 Mislabelled Training Examples
Bug Implementation.We train a BVD-CNN model on a birds-vs-dogs dataset where10 percent
of training samples have their labels ﬂipped. The model achieves a93.2, 91.7, 88 percent accuracy
on the training, validation, and test sets.
Correct  
Label
Incorrect  
Label
Input
Correct  
Label
Incorrect  
Label
Figure 6: Diagnosing Mislabelled Training Examples.The Figure shows two training inputs
along with feature attributions for each method. The correct label row corresponds to feature
attributions derived from a model with the correct label in the training set. The incorrect-label row
shows feature attributions derived from a model with the wrong label in the training set. We see
that the attributions under both settings are visually similar.
Results. We ﬁnd that attributions from mislabelled examples for a defective model are visually
similar to attributions for these same examples but derived from a model with correct input labels
(examples in Figure 6). We ﬁnd that the SSIM between the attributions of a correctly labeled
instance, and the corresponding incorrectly labeled instance, are in the range0.73 −0.99 for all
methods tested. These results indicate that the attribution methods tested might be ineﬀective for
identifying mislabelled examples. We refer readers to Section I of the Appendix for visualizations
of additional examples.
Insights from Human Subject Study: users use prediction labels, not attribution meth- 

","According to the context, Gradient, Integrated Gradients, and SmoothGrad are XAI (Explainable AI) techniques that may not be effective in identifying mislabeled examples. They attribute importance to irrelevant features, such as the background of an image, even when the label is incorrect. However, feature attributions derived from a model with correct input labels and those with wrong labels show visually similar results, indicating these methods might not be useful for detecting mislabeled observations in image classification.",4.973125696182251,"The context suggests that several explainable AI (XAI) techniques can be utilized to help detect mislabeled observations in image classification. Some of these methods include:

1. **Feature Attribution Methods**: Techniques like Gradient, Integrated Gradients, and SmoothGrad can be used to assess the importance of various features in the model's predictions. However, evidence from studies indicates that these feature attribution methods may struggle to effectively differentiate between correctly and incorrectly labeled instances, as the attributions derived from mislabeled data can be visually similar to those from correctly labeled instances.

2. **Diagnostic Models**: Training models on datasets with known proportions of mislabeled examples, such as a BVD-CNN on a birds-vs-dogs dataset with flipped labels, can help evaluate how models respond to mislabeling. This approach can provide insights into the model's performance and the impact of label noise.

3. **Spurious Correlation Detection**: Techniques that assess attributions for spurious signals (artifacts unrelated to true data generating processes) can be employed to identify misleading patterns in training data, including mislabeled examples.

These techniques highlight that while XAI methods can provide insights into model behavior, identifying mislabeled observations may require careful design and additional strategies beyond traditional attribution approaches."
llama3.1:latest,"How can deep learning methods be used in imputation of missing data?
","Document 1: 
 least square methods [23] for imputation. For large datasets, deep learn-
ing imputation techniques have gained popularity due to their performance
4
[24, 25, 26]. It’s important to note that different imputation methods may
produce varying values for each missing entry, making the modeling of un-
certainty for each missing value crucial. Bayesian and multiple imputation
techniques, such as Bayesian principal component analysis-based imputa-
tion [27] and multiple imputations using Deep Denoising Autoencoders [28],
are particularly useful in these scenarios. Furthermore, certain tree-based
methods can naturally handle missing data through prediction, including
missForest [29], the DMI algorithm [30], and DIFC [31]. Methods that can
manage mixed data types have also emerged, such as SICE [32], FEMI [33],
and HCMM-LD [34].
Although various missing data methods have been developed [35], most
of them to date do not have a built-in explanation mechanism for imputed
values, although how a value is imputed can have a profound impact on the
performance and interpretability of downstream machine learning models.
If one uses a prediction model such as a regression or classification model,
then one can use Shapley values or some other XAI technique to explain the
imputation of missing values. However, many times, one may want to use
a more complicated imputation method that requires building regression or
classification models and looping through the data multiple times to improve
imputation accuracy. 

Document 2: 
 forinitializationofthemissingvalues.Moreimportantly,almost
allofthesemethodsuseabottlenecklayerasthelatentspace,
whileweuseanencodinglayerofsizemuchbiggerthanthatof
inputwithtwopurposes:Firsttoensureabetterreconstruction
oftheinputandsecondthedataintheprojectedspaceofhigher
dimensionarelikelytobeflattened[51].In[51]authorsremarked
that unfolding of class manifolds using deep networks results
inhigherdimensionofthedatawhichmakestheclassesmore
separable.Moreover,likethesemethods,ourobjectiveisnotjust
therestorationofmissingvalues,buthowwecanimputemissing
valuessothatwecanusedatawithmissingvaluesindesigning
moreeffectiveclassifiers.
Theselforganizingmap(SOM)[30,31]andmultilayerpercep-
tron(MLP)[32]aretheothertwomachinelearningtechniques
that have also been used to impute missing values. There are
several other machine learning techniques to impute missing
data[52–56].Aknowledge-basedmethodhasalsobeendevel-
opedthatcandealwithmissingvalueswhichisnon-numericin
nature[57].Chih-Fongetal.[58]proposedamodifiedformofthe
meanimputationmethod.First,thedatasetisdividedintocom- 

Document 3: 
 SN Computer Science
[175] 2021 Springer Data imputation in wireless 
sensor network using deep 
learning techniques
Data Analytics and Manage-
ment
[176] 2020 Sustainable and Resilient 
Infrastructure
Handling incomplete and 
missing data in water net-
work database using imputa-
tion methods
Taylor and Francis
Page 28 of 37Emmanuel et al. J Big Data           (2021) 8:140 
missing to 5%, 10% and 15% across all observations. RMSE performance measure was 
then used to help quantify the accuracy of the estimated values compared to the actual 
non-missing data.
After simulation of missing values, KNN imputation was implemented to replace the 
missing values. Firstly, when implementing the imputation method, the nearest neigh -
bors (K) must be chosen. The value of K was chosen based on experimental results start -
ing with K = 1 and stopped at K = 5, the best accurate estimation value of K was then 
used for the experiment which was K = 4. The Euclidean distance measure was used on 
the KNN imputation algorithm. The  RF missForest algorithm was then implemented, 
which is a nonparametric imputation method based on the random forest. For every 
variable missForest fits a random forest on the observed values and then predicts the 
missing variables. The process of training and predicting of missForest is repeated in an 
iterative process until a number of iterations are reached. The missForest ran for three 

Document 4: 
 this in a concise way. This is the area to employ meta-learning model to capture these high-level interactions. In future
work, we aim to extend this analysis with adding new data sets and training surrogate model to deeper understanding
the complex interactions between the structure of missing data in tasks, the assumption of imputation method and
ML algorithms (Wo´ znica and Biecek, 2020). Another extension may be considering hyperparameters optimization in
machine learning models as well as imputation methods.
References
B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. Vanschoren. OpenML
Benchmarking Suites. 8 2017. URL http://arxiv.org/abs/1708.03731.
M. L. Brown and J. F. Kros. Data mining and the impact of missing data. Industrial Management and Data Systems,
103(8-9):611–621, 2003. ISSN 02635577. doi: 10.1108/02635570310497657.
T. Hastie, R. Mazumder, J. D. Lee, and R. Zadeh. Matrix Completion and Low-Rank SVD via Fast Alternating Least
Squares. Technical report, 2014.
J. Honaker, G. King, and M. Blackwell. Amelia II: A program for missing data. Journal of Statistical Software, 45(7): 

Document 5: 
 missing rate is high and the samples are inadequate. Another
solution is to ﬁll in the missing values with substituted val-
ues, which is known asdata imputation . However, these
methods do not capture variable correlations and may not
capture complex patterns to perform imputation. Combin-
ing the imputation methods with prediction models often
results in a two-step process where imputation and prediction
models are separated; the missing patterns are not effectively
explored in the prediction model, thus leading to suboptimal
analysis results.
GRU-D [29] is a deep learning model based on the gated
recurrent unit (GRU) to effectively exploit two representa-
tions of informative missingness patterns—masking and time
interval. Masking informs the model of which inputs are
observed or missing, while time interval encapsulates the
input observation patterns. GRU-D captures the observa-
tions and their dependencies by applying masking and time
interval, which are implemented using a decay term, to the
inputs and network states of the GRU, and jointly train all
model components through back-propagation. GRU-D not
only captures the long-term temporal dependencies of time-
series observations, but also utilizes the missing patterns to
improve the prediction results.
We elaborate on the two components of GRU-D: making
and time interval. The value of a missing variable tends to be
close to some default value if its last observation happened 

Document 6: 
 bayesian ridge regression, Symmetry 12 (2020) 1594.
[23] P. Keerin, W. Kurutach, T. Boongoen, An improvement of missing value
imputation in dna microarray data using cluster-based lls method, in:
2013 13th International Symposium on Communications and Informa-
tion Technologies (ISCIT), IEEE, 2013, pp. 559–564.
[24] S. J. Choudhury, N. R. Pal, Imputation of missing data with neural net-
works for classification, Knowledge-Based Systems 182 (2019) 104838.
[25] A. Garg, D. Naryani, G. Aggarwal, S. Aggarwal, Dl-gsa: a deep learning
metaheuristic approach to missing data imputation, in: International
Conference on Sensing and Imaging, Springer, (2018), pp. 513–521.
[26] K. Mohan, J. Pearl, Graphical models for processing missing data,
Journal of the American Statistical Association (2021) 1–42.
[27] V. Audigier, F. Husson, J. Josse, Multiple imputation for continuous
variables using a bayesian principal component analysis, Journal of
statistical computation and simulation 86 (2016) 2140–2156.
[28] L. Gondara, K. Wang, Multiple imputation using deep denoising au-
toencoders, arXiv preprint arXiv:1705.02737 (2017). 

","Deep learning methods can be used for imputation of missing data by predicting the values of missing entries based on the patterns and relationships learned from the observed data, using techniques such as autoencoders and recurrent neural networks.",3.2117457389831543,"Deep learning methods can be utilized in the imputation of missing data through various techniques that leverage the capabilities of neural networks to model complex patterns and relationships within the data. For example, models such as Deep Denoising Autoencoders can be employed to perform multiple imputations by reconstructing missing values based on learned representations of the data. Additionally, Gated Recurrent Unit based models like GRU-D exploit informative missingness patterns by incorporating mechanisms for masking and time intervals, allowing them to capture long-term dependencies in time-series data and thus improve prediction results. Moreover, traditional deep learning architectures, such as multilayer perceptrons (MLP), and self-organizing maps (SOM) can also be adapted for the imputation task, alongside ensemble methods like random forest imputation (missForest), which iteratively trains on observed values to predict missing entries. Overall, deep learning approaches often allow for more effective handling of missing data by integrating the imputation process with classification or prediction models, facilitating better overall model performance."
llama3.2:latest,"What does the term ""learn to optimize"" mean?
","Document 1: 
 NatlSciRev ,2024,Vol.11,nwae132
Solver 4
Solver 1
Solver 2
Solver 3
Configuration space of algorithms
(solver class)
Facilitate the tedious trial-and-error process for
getting a good solver for an unseen instance
Solver 5
Unseen instances
Training set Solver class
Optimization problem class
Knowledge
Learning
Vehicle routing
problem
Machine learning
Traveling
salesman
problem
Problem Instance 1
Problem Instance 2
Problem Instance 3
Optimization problems
Figure 1 Illustration of the general idea of L2O. 
the same general paradigm that is very similar to 
the typical ML paradigm. That is, they leverage on 
a set of training instances from the target optimiza- 
tion problem class to gain something that would help 
alleviate the tedious algorithm configuration process 
on unseen problem instances, as i l lustrated in Fig. 1 . 
This article puts them under the umbrella term learn 
to optimize (L2O) and provides a brief overview on 
the general ideas as well as critical issues of L2O, in- 
cluding the training process, theoretical foundations 
as well as generalization issues. For more algorithmic 
details on NN-based solvers and heuristic solvers, 
readers are referred to [13 ,14 ] and [15 ,16 ], respec- 
tively. 

Document 2: 
 m∑
i=1
U(s, Xi),
where Xi is a random variable, representing an instance randomly drawn from X.
Notice that Um(s) is a random variable.
We deﬁne two properties of SPPAs, generalization and optimality. An SPPA
learning algorithm generalizes well, when the utility on all instances is close to the
utility on the training set. An SPPA learning algorithm is optimal, if the optimal
SPPA on the training set is close to the optimal result on the set of all instances.
These properties are formalized by the following deﬁnition.
Learning parallel portfolios of algorithms 99
Deﬁnition 6.1 We say that an SPPA learning algorithm mean-generalizes,i ff o ra n y
0 <ϵ and 0 <δ< 1 it outputs an SPPA s ∈ S,f o rw h i c h
P [Um(s) − E [U(s, X)] >ϵ ] ≤ δ.
Let the globally optimal algorithm be:
s∗ = arg max
s∈S
E [U(s, X)] .
We say that an SPPA learning algorithm ismean optimal,i ff o ra l l0 <ϵ and 0<δ< 1
it outputs a schedule s
P
[
E
[
U(s∗, X)
]
− E [U(s, X)] >ϵ 

Document 3: 
 model and select conﬁgurations, respectively.
Input :Target algorithm Awith parameter conﬁguration space Θ; instance set Π; cost
metric ˆc
Output :Optimized (incumbent) parameter conﬁguration, θinc
1 [R, θinc] ←Initialize(Θ, Π);
2 repeat
3 [M,tfit ] ←FitModel(R);
4 [ ⃗Θnew,tselect] ←SelectConﬁgurations(M, θinc, Θ);
5 [R,θinc] ←Intensify( ⃗Θnew, θinc, M, R, tfit + tselect, Π, ˆc);
6 until total time budget for conﬁguration exhausted;
7 return θinc;
parameter optimization (SPO) toolbox—which has received considerable attention in the
evolutionary algorithms community—provides many features that facilitate the manual
analysis and optimization of algorithm parameters; it also includes an automated SMBO
procedure for optimizing continuous parameters on single instances. We started our
own work in SMBO by comparing SKO vs SPO, studying their choices for the four
SMBO components [18]. We demonstrated that component Intensify mattered most, and
improved it in our SPO + algorithm [18]. Subsequently, we showed how to reduce the
overhead incurred by construction and use of response surface models via approximate
GP models. We also eliminated the need for a costly initial design by interleaving ran- 

Document 4: 
 this is to be taken as either the efficiency or the 
effectiveness of the solver.
r = n! Ts
Tns
The Challenges 
As presented in the introductory paragraphs, a 
divide-and-conquer approach can be lucky. A run 
can benefit from a good split, which brings a solu -
tion at the beginning of some subspace and allows 
for an early stop. In contrast, a different division 
can decrease performance. What is interesting here 
is that adding resources can decrease the perform -
ance since it can produce more demanding sub -
spaces. 
Even if portfolio-based approaches are less prone 
to this problem, extending the size of a portfolio 
can still be detrimental to its performance. In gen -
eral, this increases the overhead, due to more fre -
quent and broader clause sharing, and worsens 
cache-congestion issues. A priori, the question of 
deciding the most effective number of resources to 
use against a given formula is a difficult one. 
One possible direction of research is to extend 
automatic tuning techniques. These approaches 
use machine learning to craft a predictive function 
that relates the features of an instance and the 
parameters of a given solver to its expected run 
time. This function can be learned and tested 
offline against a large set of representative 
instances and used at run time to configure a solver 
and maximize its performance. This offline 
approach assumes that a large and representative 

Document 5: 
 OPT := inf
θ∈Θ
R(θ).
The search for the optimal conﬁguration is generally too ambitious, as the total runtime
required for the conﬁgurator must be extraordinarily large (possibly inﬁnite) to guarantee
that the best algorithm conﬁguration returned by the conﬁgurator is in fact the optimal
one with high probability.
As a workaround, one can leverage the idea underlying PAC learning (Valiant, 1984) to
the problem at hand. The basic idea is to relax the goal of ﬁnding the optimal conﬁguration
itself and, instead, ﬁnd a conﬁguration that is considered to be “good enough”. As there are
potentially several such “good enough” conﬁgurations 6, this relaxation of the goal allows
the search to be completed in less (and, thus, feasible) time. In this context, “good enough”
means that the expected runtime is only worse than the optimal expected runtime up to
a multiplicative factor of 1 + ε for some ﬁxed precision parameter ε > 0. Formally, a
conﬁguration is said to be ε-optimal (“good enough”) iﬀ
Ei∼P(c(i,θ)) ≤(1 + ε)OPT.
However, this relaxation of the target is problematic in the context of AC problems, since the
runtimes of conﬁgurations often exhibit a heavy-tailed distribution. Indeed, it is not diﬃcult
to construct an example based on such distributions in which any (sensible) conﬁgurator
would, in the worst case, take inﬁnitely long to ﬁnd an ε-optimal conﬁguration; see for 

Document 6: 
 tions of learning machines, which lead to objective 
functions that favor different optimization methods. 
As a result, many optimization techniques, including 
convex optimization methods [1 ], gradient descent 
[2 ], heuristic search [3 ] as well as evolutionary algo- 
rithms (E A s) [4 ], have been utilized by the ML com- 
munity in the past decades. 
Mainstream optimization algorithms are usu- 
ally highly configurable parameterized algorithms, 
i.e. they involve tunable parameters. Performance of 
such algorithms may vary significantly with different 
parameter settings. Hence, to solve the optimization 
problem induced by a learning task, tedious efforts 
are typically needed to configure the optimization al- 
gorithm. Consider the case of training an NN with 
stochastic gradient descent (SGD). It is widely ac- 
knowledged that the training performance is sensi- 
tive to the learning rate [5 ], a parameter of SGD. 
How to determine the optimal or at least appropri- 
ate learning rate has become a major hurdle for train- 
ing NNs, especially for large-scale NNs [6 ], or when 
the objective function is not an explicit differentiable 
function of the weights of an NN, as in the typical re- 
inforcement learning setting [7 ]. 
In order to reduce the human labor required, 
many studies have attempted to automate the config- 
uration of optimization algorithms [6 ,8 ]. However, 

","The term ""learn to optimize"" refers to a class of machine learning approaches that leverage training data from a target optimization problem to improve the configuration and performance of optimization algorithms for unseen instances. In other words, these methods aim to learn how to optimize an algorithm's parameters or select the best configuration in order to achieve optimal results on new, unseen problems.",2.183788537979126,"The term ""learn to optimize"" (L2O) refers to a paradigm in which machine learning techniques are utilized to assist in the optimization process for various optimization problems. This approach leverages a set of training instances from a specific class of optimization problems to develop a solution that can alleviate the often tedious and lengthy trial-and-error process commonly associated with configuring algorithms for unseen instances. The overarching goal is to enhance the efficiency and effectiveness of solving optimization problems by using learned knowledge to predict or determine better algorithm configurations for new instances that have not been encountered before."
llama3.2:latest,"Please give some examples of metaheuristics.
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 aside any metaphorical language. We utterly believe that eﬀorts in 
this direction should be intensiﬁed, building upon the initial postula- 
tions established in some recent works [105,106] . Speciﬁcally, meta- 
heuristics components (including search operators and algorithmic 
behavioral patterns) and interfaces between them should be stan- 
dardized towards objectively assessing similarities and diﬀerences 
between metaphor-based solvers [107] . A novel metaphor is by no 
means a suﬃcient guarantee for a signiﬁcant scientiﬁc contribution. 
• The third of these reasons is methodological, i.e., the relevance of 
some of the building blocks of the overall algorithm. A particular 
algorithm can include a given component (for example, a local op- 
timizer) that can be of relevance even if the algorithm as a whole 
is not completely competitive with respect to the prevailing liter- 
ature. A good example supporting this claim can be observed in 
co-evolutionary frameworks, which usually include a procedure to 
identify the subcomponents that will be individually co-evolved. In 
those cases, even if the subcomponent optimizer is not very sophis- 
ticated, the co-evolutionary framework can be relevant by itself. In 
this sense, it is important to select the appropriate framework to 
highlight the desired characteristic of the proposed algorithm, as dis- 
cussed in Section 3.1 . Following the same example of subcomponents 
identiﬁcation, a researcher focused on large-scale global optimiza- 

Document 3: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

Document 4: 
 erations Research54(1):99–114.
Bartz-Beielstein, T. 2006. Experimental Research in Evolution-
ary Computation. Springer V erlag.
Battiti, R., and Brunato, M. 2005. Reactiv e search: machine
learning for memory-based heuristics. Technical Report DIT-05-
058, Universit`a Degli Studi Di Trento, Trento, Italy.
Birattari, M.; St¨utzle, T.; Paquete, L.; and V arrentrapp, K. 2002.
A racing algorithm for conﬁguring metaheuristics. In Proc. of
GECCO-02, 11–18.
Birattari, M. 2004. The Problem of Tuning Metaheuristics as
Seen from a Machine Learning Perspective. Ph.D. Dissertation,
Universit´e Libre de Bruxelles, Brussels, Belgium.
Coy, S. P .; Golden, B. L.; Runger, G. C.; and Wasil, E. A. 2001.
Using experimental design to ﬁnd effective parameter settings for
heuristics. Journal of Heuristics7(1):77–97.
E´en, N., and S¨orensson, N. 2003. An extensible SA T solver. In
Proc. of SAT-03, 502–518. 

Document 5: 
 art algorithm, MOS, has been clearly surprised by SHADE-ILS and 
MLSHADE-SPA, hence becoming the most competitive algorithms 
(with a preference by SHADE-ILS, by its better performance and sim- 
plicity). 
8. Conclusions and outlook 
In this work we have stressed on the need for circumventing com- 
mon mistakes and ﬂaws observed in the ﬁeld of bio-inspired optimiza- 
tion, particularly when new meta-heuristic algorithms are proposed and 
experimentally validated over benchmarks designed to this end. Specif- 
ically, we have reviewed and critically analyzed contributions deal- 
ing with experimental recommendations and practices related to meta- 
heuristics. Following our literature study, we have prescribed a set of 
methodological recommendations for preparing a successful proposal 
of bio-inspired meta-heuristic algorithms, from the deﬁnition of the ex- 
perimentation to the presentation of the results. A number of useful 
techniques (graphically summarized in Fig. 9 ) have been suggested for 
prospective studies to implement our proposed methodological frame- 
work, in an attempt at ensuring fairness, coherence and soundness in 
future studies on the topic. Two diﬀerent case studies have been de- 
signed to exemplify the application of our prescribed methodology, dis- 
cussing on the results of the application of each guideline. Although both 
case studies deal with well-known benchmarks, we envision that our 
methodology can be a core part of the design process of meta-heuristic 

Document 6: 
 achieved using several heuristic mechanisms that interact in complex, non-intuitive
ways. For example, a DPLL-style complete solver for SAT (a prototypical NP-
complete problem with important applications in the design of reliable soft- and
hardware) may use different heuristics for selecting variables to be instantiated and
the values ﬁrst explored for these variables, as well as heuristic mechanisms for
managing and using logical constraints derived from failed solution attempts. The
activation, interaction and precise behaviour of those mechanisms is often controlled
by parameters, and the settings of such parameters have a substantial impact on the
Holger H. Hoos
Department of Computer Science, University of British Columbia, 2366 Main Mall, Vancouver,
BC, V6T 1Z4, Canada, e-mail: hoos@cs.ubc.ca
1 We note that the use of heuristic techniques does not imply that the resulting algorithms are nec-
essarily incomplete or do not have provable performance guarantees, but often results in empirical
performance far better than the bounds guaranteed by rigorous theoretical analysis.
Y . Hamadi et al. (eds.),Autonomous Search,
DOI 10.1007/978-3-642-21434-9 3,
© Springer-Verlag Berlin Heidelberg 2011
37
38 Holger H. Hoos
efﬁcacy with which a heuristic algorithm solves a given problem instance or class of 

","Here are some examples of metaheuristics:

1. Simulated Annealing (SA)
2. Tabu Search (TS)
3. Evolutionary Algorithms (EAs)
4. Ant Colony Optimization algorithm (ACO)
5. Particle Swarm Optimization (PSO)

These algorithms are considered metaheuristics because they are general methods that can be applied to a wide range of optimization problems, rather than specific heuristics designed for a single problem instance.",2.88313364982605,"Examples of metaheuristics include Simulated Annealing (SA), Tabu Search (TS), Evolutionary Algorithms (EAs), Ant Colony Optimization (ACO), and Particle Swarm Optimization (PSO)."
llama3.2:latest,"What is the ""no free lunch"" theorem about?
","Document 1: 
 IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTA TION, VOL. 1, NO. 1, APRIL 1997 67
No Free Lunch Theorems for Optimization
David H. Wolpert and William G. Macready
Abstract— A framework is developed to explore the connection
between effective optimization algorithms and the problems they
are solving. A number of “no free lunch” (NFL) theorems are
presented which establish that for any algorithm, any elevated
performance over one class of problems is offset by perfor-
mance over another class. These theorems result in a geometric
interpretation of what it means for an algorithm to be well
suited to an optimization problem. Applications of the NFL
theorems to information-theoretic aspects of optimization and
benchmark measures of performance are also presented. Other
issues addressed include time-varying optimization problems and
a priori“head-to-head” minimax distinctions between optimiza-
tion algorithms, distinctions that result despite the NFL theorems’
enforcing of a type of uniformity over all algorithms.
Index T erms— Evolutionary algorithms, information theory,
optimization.
I. I NTRODUCTION
T
HE past few decades have seen an increased interest
in general-purpose “black-box” optimization algorithms
that exploit limited knowledge concerning the optimization
problem on which they are run. In large part these algorithms
have drawn inspiration from optimization processes that occur
in nature. In particular, the two most popular black-box 

Document 2: 
 function, and will produce .
The proof is completed if we show that there is no cost
function so that produces a sample containing and
and such that produces a sample containing and .
There are four possible pairs of samples to consider:
i) ;
ii) ;
iii) ;
iv) .
Since if its ﬁrst point is a , jumps to which is where
starts, when ’s ﬁrst point is a its second point must
equal ’s ﬁrst point. This rules out possibilities i) and ii).
For possibilities iii) and iv), by ’s sample we know that
must be of the form , for some variable . For
case iii), would need to equal , due to the ﬁrst point in
’s sample. For that case, however, the second point sees
would be the value at , which is , contrary to hypothesis.
For case iv), we know that the would have to equal , due
to the ﬁrst point in ’s sample. That would mean, however,
that jumps to for its second point and would therefore
see a , contrary to hypothesis.
Accordingly, none of the four cases is possible. This is
a case both where there is no symmetry under exchange of
’s between and , and no symmetry under exchange of
histograms.
A PPENDIX G 

Document 3: 
 The sum over can be accomplished in the same manner
is summed over. In fact, all the sums over all can
be done, leaving
(7)
In this last step, the statistical independence of and has
been used.
Further progress depends on whether represents or
. We begin with analysis of the case. For this case
, since only reﬂects cost
values from the last cost function, . Using this result gives
The ﬁnal sum over is a constant equal to the number of
ways of generating the sample from cost values drawn
from . The important point is that it is independent of
the particular . Because of this the sum over can be
evaluated eliminating the dependence
This completes the proof of Theorem 2 for the case of .
The proof of Theorem 2 is completed by turning to the
case. This is considerably more difﬁcult since
cannot be simpliﬁed so that the sums over cannot be
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 79
decoupled. Nevertheless, the NFL result still holds. This is
proven by expanding (7) over possible values
(8)
The innermost sum over only has an effect on the
, term so it contributes ,
. This is a constant, equal to . This
leaves
The sum over is now simple
The above equation is of the same form as (8), only with a
remaining sample of size rather than . Consequently, in
an analogous manner to the scheme used to evaluate the sums 

Document 4: 
 always use algorithm . As applied to this case, they
mean that for ﬁxed and ,i f does better (on average)
with the algorithms in some set , then does better (on
average) with the algorithms in the set of all other algorithms.
In particular, if for some favorite algorithms a certain “well-
behaved”results in better performance than does the random
, then that well-behaved gives worse than random behavior
on the set all remaining algorithms. In this sense, just as there
are no universally efﬁcacious search algorithms, there are no
universally benignwhich can be assured of resulting in better
than random performance regardless of one’s algorithm.
In fact, things may very well be worse than this. In super-
vised learning, there is a related result [11]. Translated into
the current context, that result suggests that if one restricts
sums to only be over those algorithms that are a good match
to , then it is often the case that “stupid” choosing
procedures—like the irrational procedure of choosing the
algorithm with the less desirable—outperform “intelligent”
ones. What the set of algorithms summed over must be in
order for a rational choosing procedure to be superior to an
irrational procedure is not currently known.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 77
VIII. C ONCLUSIONS
A framework has been presented in which to compare 

Document 5: 
 values after distinct cost evaluations produced by using a
particular instantiation of an evolutionary algorithm?”
At ﬁrst glance this seems to be an intractable question,
but the NFL theorem provides a way to answer it. This is
because—according to the NFL theorem—the answer must be
independent of the algorithm used to generate. Consequently,
3 In particular, one may want to impose restrictions on /80 /40 /102 /41 . For instance,
one may wish to only consider /80 /40 /102 /41 that are invariant under at least partial
relabeling of the elements in /88 , to preclude there being an algorithm that will
assuredly “luck out” and land on /109/105 /110 /120 /50/88 /102 /40 /120 /41 on its very ﬁrst query.
WOLPERT AND MACREADY: NO FREE LUNCH THEOREMS FOR OPTIMIZA TION 73
we can choose an algorithm for which the calculation is
tractable.
Theorem 3: For any algorithm, the fraction of cost func-
tions that result in a particular histogram is
For large enough , this can be approximated as
where is the entropy of the distribution , and
is a constant that does not depend on .
This theorem is derived in Appendix C. If some of the are
zero, the approximation still holds, only with redeﬁned to 

Document 6: 
 to-head minimax distinctions. Such an analysis is beyond the
scope of this paper.)
On the other hand, having the sums be equal does carry ob-
vious implications for whether there are head-to-head minimax
distinctions. For example, if both algorithms are deterministic,
then for any particular
equals one for one pair and zero for all others. In such
a case, is just the number
of that result in the pair .S o
implies
that there are no head-to-head minimax distinctions between
and . The converse, however, does not appear to hold. 4
4 Consider the grid of all /40 /122/59 /122 /48 /41 pairs. Assign to each grid point the number
of /102 that result in that grid point’s /40 /122/59 /122 /48 /41 pair. Then our constraints are i)
by the hypothesis that there are no head-to-head minimax distinctions, if grid
point /40 /122 /49 /59/122 /50 /41 is assigned a nonzero number, then so is /40 /122 /50 /59/122 /49 /41 and ii) by
the no-free-lunch theorem, the sum of all numbers in row /122 equals the sum
of all numbers in column /122 . These two constraints do not appear to imply
that the distribution of numbers is symmetric under interchange of rows and 

","The ""no free lunch"" (NFL) theorem, presented by David H. Wolpert and William G. Macready in 1997, states that for any algorithm, there exists a class of problems over which the algorithm performs optimally, but at the same time performs sub-optimally on another class of problems. This means that no single optimization algorithm can guarantee optimal performance across all types of problems or classes of problems.",2.423708915710449,"The ""no free lunch"" (NFL) theorem for optimization establishes that any optimization algorithm's elevated performance over a specific class of problems is counterbalanced by a corresponding decrease in performance over other classes of problems. This implies that no algorithm performs better than random search across all possible optimization problems; each algorithm may excel in particular scenarios but will underperform in others, resulting in an overall average performance that is uniform when considering all possible problems. Thus, the theorem highlights the limitations of universally effective optimization algorithms, emphasizing that there are no algorithms that can guarantee superior performance on all types of optimization tasks."
llama3.2:latest,"What is the concept behind Parallel Algorithm Portfolio?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 general-ized to denote a combination of different algorithms intended to outperform the search performance of any of its constituent algorithms [3, 6, 11-14]. Here we extend that formulation, so that an algorithm portfolio schedules its constituent algorithms to run concurrently on a set of processors. Let an algorithm be any CSP solver, as described in the previous section. Given a set A = {a1, a2, …, am} of m algorithms, a set P = {x1, x2, …, xn} of n problems, and a set of B consecutive time intervals T = {t1, t2, …, tB}, a simple schedule Sk for a prob-lem on a single processor specifies 

Document 4: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 5: 
 of processors [22], decomposition methods have shown their potential on many more processors (e.g., 64 cores or more in [19]). We will explore this in future work.  
9   Conclusions This paper presents WG, a constructor for non-parallel algorithm portfolios based on case-based reasoning and a greedy algorithm. It formulates parallel algorithm portfo-lio construction as an integer-programming problem, and generalizes WG to RSR-WG, a constructor for parallel algorithm portfolios based on a property of the optimal solution to the inherent integer-programming problem. To address a set of problems one at a time, RSR-WG creates portfolios of deterministic algorithms offline. Exper-iments show that the parallel algorithm portfolios produced by RSR-WG are statisti-cally significantly better than those produced by naïve parallel versions of popular portfolio constructors. Moreover, with only a few additional processors, RSR-WG portfolios are competitive with an oracle solver on a single processor.  Acknowledgements. This research was supported in part by the National Science Foundation under grants IIS-0811437, CNS-0958379 and CNS-0855217, and the City University of New York High Performance Computing Center. 

Document 6: 
 Ann Math Artif Intell (2006) 48:85–106
DOI 10.1007/s10472-007-9050-9
Learning parallel portfolios of algorithms
Marek Petrik ·Shlomo Zilberstein
Published online: 25 May 2007
© Springer Science + Business Media B.V. 2007
Abstract A wide range of combinatorial optimization algorithms have been devel-
oped for complex reasoning tasks. Frequently, no single algorithm outperforms all
the others. This has raised interest in leveraging the performance of a collection
of algorithms to improve performance. We show how to accomplish this using a
Parallel Portfolio of Algorithms (PPA). A PPA is a collection of diverse algorithms
for solving a single problem, all running concurrently on a single processor until
a solution is produced. The performance of the portfolio may be controlled by
assigning different shares of processor time to each algorithm. We present an
effective method for ﬁnding a PPA in which the share of processor time allocated
to each algorithm is ﬁxed. Finding the optimal static schedule is shown to be an
NP-complete problem for a general class of utility functions. We present bounds on
the performance of the PPA over random instances and evaluate the performance
empirically on a collection of 23 state-of-the-art SAT algorithms. The results show
signiﬁcant performance gains over the fastest individual algorithm in the collection.
Keywords Algorithm portfolios ·Resource bounded reasoning ·
Combinatorial optimization
Mathematics Subject Classiﬁcations (2000) 68T05 ·68T20 

","The concept behind a Parallel Algorithm Portfolio is to combine multiple algorithms, known as ""diverse"" or ""heterogeneous"", to solve a single problem. These algorithms run concurrently on a shared processor, and the overall performance of the portfolio is controlled by assigning different shares of processing time to each algorithm.

This approach aims to leverage the strengths of individual algorithms to improve overall performance, while also reducing computational costs. By running multiple algorithms simultaneously, the Parallel Algorithm Portfolio can potentially speed up problem-solving times and achieve better solutions than a single algorithm could alone.",2.524266242980957,"The concept behind a Parallel Algorithm Portfolio (PAP) involves leveraging multiple algorithms simultaneously to solve problems more efficiently than any single algorithm could on its own. A PAP consists of a diverse set of algorithms that run concurrently on a processor, each contributing to the overall problem-solving process. The performance of the portfolio can be enhanced by dynamically allocating processor time shares to each algorithm, which allows for optimal utilization of their strengths. The goal is to improve performance in solving complex reasoning tasks and combinatorial optimization problems by combining the strengths of various algorithms, especially when no single algorithm consistently outperforms the others. This approach allows for greater efficiency and improved outcomes compared to relying on a single algorithm or a naive combination of them."
llama3.2:latest,"Please provide some approaches to how Parallel Algorithm Portfolio can be constructed.
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately
allocate computational resources to different solvers, while the
parallel portfolios do not necessarily require any extra resource
allocation since each solver is simply assigned with the same
amount of resources. Third, a parallel portfolio could be eas-
ily converted to a sequential portfolio by using the algorithm
selection methods [39] to build selectors on the solvers in the
portfolio, which means the portfolios generated by construc-
tion approaches (e.g., the approach proposed in this article)
could be further used for constructing sequential portfolios.
In this article, we propose a novel approach called the
generative adversarial solver trainer (GAST) for the auto-
matic construction of parallel portfolios. Unlike the existing
construction approaches, GAST would generate additional
training instances and construct a parallel portfolio with the
dynamically changing training set. More speciﬁcally, GAST
puts instance generation and portfolio construction in an adver-
sarial game. The instance generation aims to generate the hard
problem instances that could not be solved well by the cur-
rent portfolio, while the portfolio construction aims to ﬁnd a
new component solver for the portfolio to better solve these 

Document 3: 
 1 for j := 1 ... n do
2 obtain portfolio c(j)
1:k by running AC on conﬁguration space ∏k
l=1{(c) | c ∈ C} on I using m for time t/n
3 choose ˆc1:k ∈ arg minc(j)
1:k |j∈{1...n} m(c(j)
1:k, I) that achieved best performance on I according to m
4 return ˆc1:k
3.1.1. Simultaneous conﬁguration of all component solvers (Global)
Our ﬁrst portfolio conﬁguration method is the straightforward extension of standard algorithm conﬁguration to the 
construction of a parallel portfolio (see Algorithm 1). Speciﬁcally, if the given solver has ℓ parameters, we treat the portfolio 
c1:k as a single algorithm with ℓ · k parameters inducing a conﬁguration space of size |C|k, and conﬁgure it directly. As 
noted above, we identify a single conﬁguration as the best of n independent runs of AC . These runs can be performed in 
parallel, meaning that this procedure requires wall clock time t/n if n machines—one for each AC run—with k cores are 
available. The used CPU time will be the given time budget t for Lines 1 and 2 in Algorithm 1 and some small overhead 
ϵ to choose the best portfolio in Line 3. The scalability of this approach is limited by the fact that the global conﬁguration 

Document 4: 
 Learning Algorithm Portfolios for Parallel Execution Xi Yun1 and Susan L. Epstein1, 2  1 Department of Computer Science, The Graduate School of The City University of New York, New York, NY 10016, USA 2 Department of Computer Science, Hunter College of The City University of New York, New York, NY 10065, USA xyun@gc.cuny.edu, susan.epstein@hunter.cuny.edu 
Abstract. Portfolio-based solvers are both effective and robust, but their prom-ise for parallel execution with constraint satisfaction solvers has received rela-tively little attention. This paper proposes an approach that constructs algorithm portfolios intended for parallel execution based on a combination of case-based reasoning, a greedy algorithm, and three heuristics. Empirical results show that this method is efficient, and can significantly improve performance with only a few additional processors. On problems from solver competitions, the resultant algorithm portfolios perform nearly as well as an oracle. Keywords: constraint satisfaction, algorithm portfolio, parallel processing, ma-chine learning. 

Document 5: 
 sign of parallel algorithm portfolios—sets of solvers that are run in parallel on a given instance of a decision problem, such 
as SAT, until the ﬁrst of them ﬁnds a solution [39,27].
In this work,1 we study generic methods for solving a problem we call Automatic Construction of Parallel Portfolios 
(ACPP): automatically constructing a static2 parallel solver from a sequential solver or a set of sequential solvers. This 
task can be understood as falling within the programming by optimization paradigm [34] in that it involves the design of 
software in which many design decisions have been deliberately left open during the development process (here exposed as 
parameters of SAT solvers) to be made automatically later (here by means of an automated algorithm conﬁgurator) in order 
to obtain optimized performance for speciﬁc use cases. Hence, all that is required by our ACPP methods is a sequential 
solver whose conﬁguration space contains complementary conﬁgurations.
We study three variants of the ACPP problem. First, we consider building parallel portfolios starting from a single, 
highly parametric sequential solver design. However, for well-studied problems (e.g., SAT), there often exist a wide range of 
different solvers that contribute to the state of the art (see, e.g., [73]). Complementarities among such solvers can be ex-
ploited by algorithm portfolios, whether driven by algorithm selection (like SATzilla [72]) or by parallel execution (such as 

Document 6: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

","Based on the context provided, here are three approaches mentioned for constructing a Parallel Algorithm Portfolio:

1. **Genetic Algorithm-based Approach**: The authors propose a novel approach called the Generative Adversarial Solver Trainer (GAST) that generates additional training instances and constructs a parallel portfolio with dynamically changing training set.

2. **Case-Based Reasoning and Greedy Algorithm**: This approach combines case-based reasoning, a greedy algorithm, and three heuristics to construct an efficient algorithm portfolio intended for parallel execution based on problem instances from solver competitions.

3. **Algorithm Selection-based Approach**: A variant of the Automatic Construction of Parallel Portfolios (ACPP) problem is considered, where parallel portfolios are built starting from a single, highly parametric sequential solver design, exploiting complementarities among different solvers that contribute to the state of the art.

Note: Additional approaches may be discussed in other sections or parts of the article not provided here.",3.668266773223877,"Several approaches can be used for constructing Parallel Algorithm Portfolios, as highlighted in the provided context:

1. **Generative Adversarial Solver Trainer (GAST)**: This novel approach focuses on dynamic instance generation in conjunction with portfolio construction. GAST creates challenging problem instances that the current portfolio struggles to solve, then seeks to integrate new solvers into the portfolio to better address these difficulties.

2. **Case-Based Reasoning and Heuristic Combination**: An approach combining case-based reasoning with greedy algorithms and heuristics can efficiently construct parallel algorithm portfolios. This method has demonstrated substantial performance improvements, especially with the addition of a few processors.

3. **Automatic Construction of Parallel Portfolios (ACPP)**: This method involves using an existing sequential solver or multiple sequential solvers to build a static parallel portfolio. It exploits the complementary configurations of sequential solvers and applies automatic algorithm configuration to optimize performance for specific problems.

4. **Simultaneous Configuration**: A straightforward method extends the standard algorithm configuration approach to create a parallel portfolio. All component solvers are configured simultaneously, treating the portfolio as a single algorithm with multiple parameters, allowing the configuration to achieve enhanced performance.

5. **Parallel Execution of Solvers**: The fundamental concept of a parallel portfolio is to run multiple solvers in parallel on a given problem instance until one solver finds a solution. This allows for the leveraging of the strengths of various solvers collectively.

6. **Adaptive Mechanisms**: Incorporating adaptive strategies, like reactive search or hyper-heuristics, can help dynamically tailor the solver configurations during problem-solving, enabling the portfolio to adjust to the specific needs of different instances in real-time.

Overall, these methods emphasize the importance of parallelism and the strategic combination of different solving techniques to enhance computational efficiency."
llama3.2:latest,"Can co-evolution be used in Parallel Algorithm Portfolio construction?
","Document 1: 
 in a series of benchmark problems. Finally, the paper will end with
a brief conclusion and a discussion of future work in Section 5.
2 RELATED WORK
2.1 Portfolio of evolutionary algorithm
In the areas of evolutionary algorithms, algorithm portfolio is ap-
plied to increase the probability of finding a better solution by
allocating computational resources to several complementary algo-
rithms. The algorithm portfolio frameworks in the literature can
be classified into two categories as the parallel-based framework
and the sequential-based framework.
For the parallel-based framework, all candidates will run simul-
taneously in multiple sub-processes. Population-based algorithm
portfolio (PAP) is a typical example [16], which allocates compu-
tational resources before the optimization according to the prior
knowledge.Eachalgorithmhasitsownpopulationandevolveinde-
pendently,buttheinformationissharedamongdifferentalgorithms
bymigrationstrategy.Besides,otherparallel-basedportfolioframe-
works like AMALGAM-SO [21] and the UMOEAs [6] collect the
performance of algorithms during the optimisation process and
allocate more resources to the better algorithm.
On the other hand, the sequential based framework only runs
onealgorithmatmostofthetimeduringtheprocessofoptimisation.
Different from the parallel-based algorithm portfolio, this kind of
frameworktrytoselectthebestalgorithmindifferentoptimisation 

Document 2: 
 tic is likely to have the right properties for
this approach to be useful, thus making it
very general in terms of applications.
So far we have assumed that the com-
ponent algorithms are completely inde -
pendent of each other and do not com-
municate. They can be thought of as
“competing” with one another for ma-
chineresources.However,allowingforco -
operation or dependencies among the in-
dividualalgorithmswhiletheyarerunning
simultaneously can improve performance
(18, 19). This possibility raises the inter-
estingquestionoftheextenttowhichour
economics approach to portfolio algo-
rithms can also benefit from cooperation.
Basically, cooperation will introduce sta-
tistical correlations between the perfor -
mance of the individual algorithms, and
we will accordingly define the correlation
between them as
  
cov•t1,t2 
 var•t1 var•t2 
(3)
where cov(t1, t2) denotes the covariance of
theperformanceofthetwoalgorithms.The
effect of cooperation, when manifested in
negativecorrelations,istoincreasetheper-
formanceaswellasreducetherisk(Fig.5).
This change is easily understood in that
negative correlations mean that one algo-
rithmisparticularlygoodpreciselyonthose 

Document 3: 
 So far, we have shown that our procedures were able to construct effective parallel portfolios based on single solvers 
with rich design spaces. There is considerable evidence from the literature and from SAT competitions that strong portfolios 
can also be built by combining entirely different solvers in their default conﬁgurations (see, e.g., SATzilla [72], ppfolio [63]
and pfolioUZK [70]). For instance, ppfolio was obtained simply by combining the best solvers from the previous competition 
into a parallel portfolio. pfolioUZK included more state-of-the-art solvers from 2011 and relied on additional experiments 
to ﬁnd the best combination of solvers in a portfolio. Neither portfolio considers the conﬁguration space of the component 
solvers and therefore both can be seen as simple baselines for other parallelization approaches, including ours. However, 
ppfolio and pfolioUZK use Plingeling as a portfolio component. Since we aim to investigate the strength of our ACPP methods 
without additional human expert knowledge on parallel solving, we ﬁrst consider only sequential solvers as the basis for 
our ACPP approach. This section and the following section investigates the extension of our automatic techniques to the 
construction of portfolios based on the conﬁguration spaces spanned by such solver sets.
4.1. Approach
As long as all of our component solvers are sequential, we can simply use the ACPP procedures deﬁned in Section 3. 
We can accommodate the multi-solver setting by introducing a solver choice parameter for each portfolio component (see 

Document 4: 
 1
Few-shots Parallel Algorithm Portfolio Construction
via Co-evolution
Ke Tang, Senior Member, IEEE, Shengcai Liu, Member, IEEE, Peng Yang, Member, IEEE,
and Xin Yao, Fellow, IEEE
Abstract—Generalization, i.e., the ability of solving problem
instances that are not available during the system design and
development phase, is a critical goal for intelligent systems.
A typical way to achieve good generalization is to learn a
model from vast data. In the context of heuristic search, such a
paradigm could be implemented as conﬁguring the parameters
of a parallel algorithm portfolio (PAP) based on a set of
“training” problem instances, which is often referred to as PAP
construction. However, compared to traditional machine learning,
PAP construction often suffers from the lack of training instances,
and the obtained PAPs may fail to generalize well. This paper
proposes a novel competitive co-evolution scheme, named Co-
Evolution of Parameterized Search (CEPS), as a remedy to
this challenge. By co-evolving a conﬁguration population and an
instance population, CEPS is capable of obtaining generalizable
PAPs with few training instances. The advantage of CEPS in
improving generalization is analytically shown in this paper. Two
concrete algorithms, namely CEPS-TSP and CEPS-VRPSPDTW,
are presented for the Traveling Salesman Problem (TSP) and 

Document 5: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, co-evolution can be used in Parallel Algorithm Portfolio construction. This paper proposes a novel competitive co-evolution scheme, named Co-Evolution of Parameterized Search (CEPS), to address the challenge of obtaining generalizable parallel algorithm portfolios with few training instances. CEPS co-evolves a configuration population and an instance population to obtain effective parallel portfolios.",2.114236354827881,"Yes, co-evolution can be used in Parallel Algorithm Portfolio (PAP) construction. The context indicates that a competitive co-evolution scheme, specifically named Co-Evolution of Parameterized Search (CEPS), is proposed for this purpose. CEPS aims to enhance generalization in PAPs, which is particularly beneficial given the challenges of constructing effective portfolios from limited training instances. By co-evolving a population of configurations along with a population of problem instances, CEPS is capable of producing more generalizable PAPs, highlighting the advantage of this approach in constructing effective portfolios."
llama3.2:latest,"What is the idea behind the PAP approach called ""GLOBAL""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 RDW 13.24 1.38 11.0 24.1 11.5–16.5 %
LEUKOCYTES (%) 7.61 3.16 0.76 33.82 %
LEUKOCYTES 7614.73 3190.62 760.0 33820.0 3500–10500µL
that have led to the production of a choice in a speciﬁc exam-
ple. The notion of explanation in this work is aligned with
Samek et al. ( 2019).
As stated previously, it is possible to analyze a model rela-
tion between input and output in two ways: looking at local
explanations and global interpretation/explanations (Molnar
2019). Here, we use these two terms interchangeably . While
global interpretations enlighten scientists about what features
may be more representative to the model given a data sam-
ple, local explanations show what led the model to the current
output.
In our scope, global interpretations can help healthcare
researchers to question data sample quality and also direct
the search for more relevant features. Single explanations of
the COVID-19 diagnosis can assist healthcare professionals
that are working directly with potentially infected patients in
their decision-making.
7.1 Global Interpretation
For the glass-box models, global interpretation can be quali-
tatively evaluated by analyzing its feature weights. For these
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 633
Table 2 Results of experiments 

Document 3: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 6: 
 (CEC2005) [12], denoted asfcec1 −fcec14. These 27 functions
span a diverse set of problem features, such as multimodality,
ruggedness, ill-conditioning, interdependency, etc. They pro-
vided an ideal platform for our investigation on reducing risk
on a large variety of problems. Short descriptions of these
functions are presented in Tables I and II. More details of these
functions can be found in [11] and [12]. In our experiments,
all the functions were solved in 30 dimensions.
B. Experimental Settings
All the results presented in this paper were obtained by
executing 30 independent runs for each experiment. Since
we expect the PAP framework to be general enough so that
alternative algorithms can be incorporated with little effort,
it should not rely much on the reﬁnement of the constituent
algorithms. Hence, we did not ﬁne-tune the parameters of
the constituent algorithms to ﬁt PAP. When implementing
SaNSDE, we used all the parameter settings suggested in
the original publication [7]. As suggested in [8], a linearly
decreasing inertia weight over the course of the search is
employed in our implementation of wPSO. The two coefﬁ-
cients of wPSO were both set to 1.49445. We assumed the
researchers who proposed G3PCX and CMA-ES are at the
best position to implement the two algorithms and ﬁne-tune
the parameters. Hence, we simply used the source code of 

","I don't have information on what the ""PAP approach"" or ""GLOBAL"" specifically refers to. Can you provide more context? I'll do my best to help.

However, it seems there might be a reference to the ""Global Approach to Public Administration Reform"" (GAPAR), which is an initiative by the World Bank and the Organization for Economic Cooperation and Development (OECD) aimed at reforming public administration systems in partner countries. 

If that's not what you are looking for please let me know, I'll be happy to help.",1.9198474884033203,"The idea behind the PAP (Population Allocation Paradigm) approach called ""GLOBAL"" relates to investigating the performance of the algorithm across a wider set of problems as opposed to focusing solely on singular optimization issues. PAP seeks to reduce the risk by applying various population-based search algorithms to multiple problems simultaneously, rather than optimizing for just one. This framework allows these algorithms to interact and share insights through a multipopulation model where computation time (measured by fitness evaluations) is intelligently allocated among different constituent algorithms. This collaboration enables the algorithms to leverage diverse strengths and promote cooperation, thereby enhancing overall optimization performance. The goal is to foster a more robust algorithm performance that can adapt to various challenges presented by a broad spectrum of problems."
llama3.2:latest,"What is the idea behind the PAP approach called ""PARHYDRA""?
","Document 1: 
 on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is
worthy of allocating the limited computation time to different
algorithms so that the overall performance can be enhanced.
In contrast, a speciﬁc topology/structure for the relationship
between subpopulations is usually explicitly deﬁned in existing
dEAs [26]–[29], [32], [33]. Third, we are more interested
in reducing the overall risk of an algorithm ona spectrum
of problems. This issue has never been investigated in the
literature of dEAs.
In addition to dEAs, another notable work that is related
to PAP is the isolation strategy employed by hierarchically
organized evolution strategies (HOES) [34], [35]. In HOES,
a population is partitioned into a number of subpopulations,
and ES is run on them with different control parameters (e.g.,
search step sizes). The isolation strategy works in such a
way that the subpopulations evolve separately for a predeﬁned
isolation period (e.g., a few generations), and then the control
parameters are updated based on the individuals generated in
the subpopulations during the isolation period. Unlike PAP,
the motivation of the isolation strategy is not to make use of 

Document 2: 
 SCENARIOS , THAT IS , SAT-SMALL-BIAS-1/2/3/4. T HE NAME OF THE
CONSTRUCTION APPROACH IS USED TO DENOTE THE PORTFOLIOS
CONSTRUCTED BY IT. “PARHYDRA-A” R EFERS TO PARHYDRA
CONFIGURING BASED ON AUGMENTED TRAINING SETS .AP A R - 1 0
SCORE IS SHOWN IN BOLDFACE IF IT WAS NOT SIGNIFICANTLY
DIFFERENT FROM THE BEST TEST PERFORMANCE IN THE SCENARIO
(ACCORDING TO A WILCOXON SIGNED -RANK TEST WITH p = 0.05)
To answer this question, in each of the eight SAT scenarios,
that is, SAT-SMALL/BIAS-1/2/3/4, we used the same instance
generation procedure as in GAST (lines 13–18 in Algorithm 1)
to generate a large set of instances based on the training set.
The size of the generated set is ﬁve times the size of the
training set. Recall that the training set contains 50 instances,
we thus obtained an augmented training set of 300 instances
in each SAT scenario, and then PARHYDRA was used to
construct a parallel portfolio on these augmented training sets,
and then the obtained portfolio was tested on the test sets. As
before, each portfolio was tested by running it on the test set 

Document 3: 
 of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that
seeks to automatically build PAPs by selecting the member algorithms from an algorithm
conﬁguration space, with the goal of optimizing the performance of the resulting PAP on a
given problem set (called training set). Such a framework has been shown to be effective in
building high-performance PAPs for combinatorial problems such as the Boolean Satisﬁa-
bility Problem (SAT) [25], the Traveling Salesman Problem (TSP) [27,31], and the Vehicle
Routing Problem (VRP) [26].
However, to the best of our knowledge, the potential of the automatic construction of
PAPs has not been investigated in the area of multi-objective optimization. Considering its
excellent performance on the above-mentioned problems and the practical signiﬁcance of
MOPs, studying how to utilize it to solve MOPs is thus valuable. In this work, we focus
on automatically building PAPs for continuous MOPs. On the other hand, as a general
framework, appropriately instantiating automatic PAP construction for a speciﬁc problem
domain is non-trivial. Speciﬁcally, it requires careful designs of the algorithm conﬁguration
space and the performance metrics used in the construction process [26]. 

Document 4: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 5: 
 be used to build parallel portfolios. (Building a parallel portfolio in this way has the added advantage that no instance 
features are required, since there is no need to select among algorithms.) We introduce some enhancements to this 
approach for the parallel portfolio setting (discussed in Section 3.1.2), and refer to our method as parHydra.
3. Some parallel solvers only achieve strong performance when running on more than one core; such solvers will not 
be found by a greedy approach like parHydra, which only adds one conﬁguration at a time and does not recognize 
interaction effects that arise between different threads of a parallel solver. To overcome this problem, we introduce a 
new method called parHydrab, which augments parHydra to train b solvers per iteration. This method trades off the 
computational beneﬁt of parHydra’s greedy approach with the greater coverage of Global.
We evaluated our ACPP methods on SAT. We chose this domain because it is highly relevant to academia and industry 
and has been widely studied. We thus had access to a wide range of strong, highly parametric solvers and were assured 
that the bar for demonstrating eﬃcacy of parallelization strategies was appropriately high. We note that our approach is not 
limited to SAT solvers and can be directly applied to other domains. To evaluate our methods in the single-solver setting, we 
studied both Lingeling and clasp: prominent, highly parametric state-of-the-art solvers for SAT. Lingeling won a gold medal in 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","I couldn't find any information on a specific approach called ""PARHYDRA"". It's possible that it's a specialized or proprietary method, or it may be a misspelling or variation of a different term.

However, I did find information on the PAP (Patient-Centered Approach) framework in dentistry. The Patient-Centered Approach is a guiding philosophy for healthcare providers, including dentists. It emphasizes patient-centered care by focusing on patients' needs, preferences, and values in the delivery of dental care.

If you're looking for more information on the PAP approach or related frameworks, I'd be happy to help with that.",2.2537648677825928,"The PAP approach called ""PARHYDRA"" refers to a method for constructing parallel portfolios using augmented training sets. It focuses on creating a parallel portfolio by utilizing a large augmented training set generated from a smaller training set, thereby optimizing the performance of the portfolio on various instances. This approach is designed to leverage multiple algorithms simultaneously without relying on specific instance features, allowing for more effective performance in SAT scenarios. PARHYDRA enhances algorithm selection by training multiple solvers concurrently, recognizing interaction effects, and maximizing computational efficiency."
llama3.2:latest,"What is the idea behind the PAP approach called ""CLUSTERING""?
","Document 1: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 2: 
 proposed a big data k-means clustering, and a big data fuzzy k-means missing values 
approach that resulted in robust and efficient output for big data and offered reasonable 
execution times. The two imputation techniques surpassed in most cases mean imputa -
tion and elimination of the instances with lost values during classification. Offer robust 
and efficient results for Big Data datasets, offering reasonable execution times. The fuzzy 
k-means approach was proved to provide better results for high percentages of miss -
ing values in the data, while the k-means performed better with the dataset that had 
lower amounts of missing values. Zhang et al. [102], also proposed a multiple imputation 
clustering based approach that handled missing values in big longitudinal trial data in 
e-Health. The proposed concept proved that it could be easily adapted for different types 
of clustering for big incomplete longitudinal trial data in eHealth services.
Ensemble methods
Ensemble methods are strategies that make multiple models and then combine them to 
produce a single improved result. This method usually produces more precise results 
than a single model would. This has been the case in machine learning competitions, 
where the triumphant models used ensemble techniques [103]. Studies have confirmed 
that ensemble missing data handling algorithms outperform single base machine learn -
ing algorithms [104–108]. Also, ensemble methods can be implemented in parallel com -
puting environments, which are necessary to process missing data in big datasets. These 
ensemble algorithms are a group of techniques that their decisions are combined in a 

Document 3: 
 Built upon automatic algorithm conﬁguration, the automatic
construction of parallel algorithm portfolios (PAPs) [11]–[15]
seeks to identify a set of conﬁgurations to form a PAP. Each
conﬁguration in the PAP is called a component solver. To
solve a problem instance, all the component solvers are run
independently, typically in parallel, to get multiple solutions.
Then, the best solution will be taken as the output of the PAP.
Although a PAP would consume much more computational
resources than a single-conﬁguration solver, it has two im-
portant advantages. First, the performance of a PAP on any
given instance is the best performance achieved among its
component solvers on the instance. In other words, by ex-
ploiting the complementarity between the component solvers,
a PAP could achieve a much stronger overall performance than
any of its component solver. Second, considering the great
development of parallel computing architectures [16] (e.g.,
multi-core CPUs) over the last decade, exploiting parallelism
has become very important in designing efﬁcient solvers for
computationally hard problems. PAPs employ parallel solution
strategies, and thus allow exploiting modern high-performance
computing facilities in an extremely simple way.
From the practical point of view, a PAP construction method
is expected to identify a PAP that generalizes well, i.e.,
performs well not only on the instance set used during the 

Document 4: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 limited time budget. Although numerous algorithms are readily
applicable for this type of problems, their performance usually
varies signiﬁcantly from problem to problem. This implies that
there is an inherent risk associated with the selection of an
algorithm. Unfortunately, identifying a suitable (or optimal)
algorithm for a speciﬁc problem is a nontrivial task due to the
lack of prior knowledge. The limited time budget also prohibits
us from trying out different algorithms and then choosing the
best one. Instead of betting the entire time budget on a single
algorithm, we proposed that such a risk can be reduced by
distributing the time budget to multiple algorithms. Based on
this idea, a general framework called PAP has been proposed
in the context of population-based search algorithms. PAP
typically consists of a number of constituent algorithms, each
of which is allowed to run with a portion of the time budget.
Allocation of computation time is implemented by dividing the
whole population into a number of subpopulations, and main-
taining one for each constituent algorithm. To further boost
the performance, interaction among constituent algorithms is
carried out through regularly migrating individuals among the
subpopulations. We proposed a pairwise metric to compare
the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget. 

","I don't know the specific details about the CLUSTERING approach in the context of the PAP (Parallel Algorithm Parametrization) framework, as it was only briefly mentioned in the text. Can you provide more information or clarify what aspects of CLUSTERING are relevant to the discussion? I'll do my best to help based on the available information.",2.2520437240600586,"The PAP approach called ""CLUSTERING"" is based on instance grouping. It involves splitting the training dataset into distinct groups or clusters before constructing the algorithm portfolio. This method focuses on organizing instances in a way that allows for more tailored and effective algorithm configurations, as it takes into account the similarities among the instances. By grouping similar instances together, the CLUSTERING approach aids in identifying which algorithms perform better on specific types of problems, thereby improving the overall performance of the PAP."
llama3.2:latest,"What is the idea behind the PAP approach called ""PCIT""?
","Document 1: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 2: 
 them.
PCIT conducts the instance transfer with the help of in-
cumbent configurations (i.e., the best configurations found
by the AC procedure). In each subset, the instances which
cannot be solved satisfactorily by the corresponding incum-
bent are identified as the ones that need to be transferred,
and the target subset of each transferred instance is deter-
mined according to how well the incumbent on the candi-
date subset could perform on the instance. In essence, the
incumbent on a subset can be seen as a common special
characteristic of those “similar” instances (in the sense they
share the same high-quality configurations) within the sub-
set, and PCIT uses it to identify those “dissimilar” instances
and find better subsets for them. In each subset, the perfor-
mance of the incumbent on each instance could be obtained
from the rundata collected from the AC procedure runs.
However, while determining the target subsets for the trans-
ferred instances, how well the incumbents on the candidate
subsets would perform on the transferred instances are un-
known. One way to obtain these performances is to actually
test these incumbents on the transferred instances, which
however would introduce considerable additional compu-
tational costs. To avoid this, PCIT builds empirical perfor-
mance models (EPM) (Hutter et al. 2014) based on the col-
lected rundata to predict these performances.
1562
Algorithm Framework 

Document 3: 
 analogous to the over-ﬁtting phenomenon in machine learning,
has been observed when the size of the training instance set
is rather limited (i.e., few-shots challenge). That is, the test
(generalization) performance of the obtained conﬁgurations is
arbitrarily bad even if their performance on the training set is
excellent. Even worse, given a T collected from real world,
it is non-trivial to know how to verify whether it is a good
representative of Ω. In case the training instance set is too
small, or is not a good representative of the whole problem
class, the best PAP obtained with it would fail to generalize.
B. Related Work
Currently, there exist several approaches for PAP con-
struction, namely GLOBAL [25], PARHYDRA [25], [26],
CLUSTERING [27] and PCIT [13]. GLOBAL considers PAP
construction as an algorithm conﬁguration problem by treating
Θ as a parameterized algorithm. By this means existing
automatic algorithm conﬁguration tools could be directly uti-
lized to conﬁgure all the component solvers of Θ simultane-
ously. In comparison, PARHYDRA constructs Θ iteratively
3
by identifying a single component solver in each iteration that
maximizes marginal performance contribution to the current
PAP. CLUSTERING and PCIT are two approaches based on
instance grouping. That is, they both ﬁrst split the training 

Document 4: 
 of the more state-of-the-art Plingeling. Such results indicate
PCIT could identify powerful parallel portfolios, with little
human effort involved. Thus the portfolios constructed by
PCIT could conveniently provide at least two advantages.
That is, they are high-quality parallel solvers, and they could
be used as starting points for the development of more ad-
Table 4: Test results of parallel solvers on the test set of SAT-
Single and SAT-Multi. The performance of a solver is shown
in boldface if it was not significantly different from the best
performance (according to a permutation test with 100000
permutations and significance level p = 0.05).
SAT-Single SAT-Multi
#TOS PAR-10 PAR-1 #TOS PAR-10 PAR-1
PCIT 181 119 21 35 1164 219
Priss6 225 146 25 - - -
PfolioUZK - - - 36 1185 213
Plinegling-bbc452 276 32 33 1090 199
vanced parallel solvers.
Conclusion
In this paper we proposed a novel ACPP method, named
PCIT, which utilized an instance transfer mechanism to im-
prove the quality of the instance grouping. The experimen-
tal results on two widely studied problem domains, SAT and
TSP, have demonstrated the effectiveness of PCIT. Currently
PCIT relies on the instance features to build the EPM. Since 

Document 5: 
 with a constraint handling technique. At each generation, each
subpopulation generates new individuals based on its own.
Then, each subpopulation is combined with new individuals
generated by all subpopulations, and selection is conducted
with the corresponding constraint handling technique. Com-
pared to all the above-mentioned work, in which all search
biases have access to the whole population, PAP allows only
its constituent algorithms to work on a subpopulation, i.e.,
full access to the whole population is prohibited. Migration is
the only route that different subpopulations communicate with
each other. This strategy, used by PAP, reduces the likelihood
of different constituent algorithms repeating similar search
behaviors or sharing similar search biases.
From the perspective of employing a multipopulation model
and migration operators, PAP is related to the distributed
EAs (dEAs), which have several subpopulations (islands) and
perform sparse exchange of individuals among them [25],
[26]. However, PAP is quite different from previous dEAs
in three major ways. First, most dEAs run the same EA
on all subpopulations [26]–[31] (either with the same or
different control parameters), while PAP employs different
EAs. Second, the migration scheme of PAP does not assume
any topology of subpopulations, while many dEAs do. By
this means, we solely focus on investigating whether it is 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The PCIT approach aims to identify powerful parallel portfolios of algorithms by first clustering instances using a common characteristic, and then transferring solutions from one cluster to another to improve their quality. The instance transfer mechanism allows for a reduction in search biases among different constituent algorithms, making the portfolio more effective.",2.324376106262207,"The PAP approach known as ""PCIT"" (Performance-based Instance Transfer) leverages an instance transfer mechanism aimed at enhancing the quality of instance grouping. It identifies instances that are poorly solved by existing configurations (the incumbents) and transfers these instances to more suitable subsets based on performance predictions. This is achieved by utilizing empirical performance models (EPM) to predict how well incumbents would perform on these transferred instances without the need for extensive additional computational costs. The effectiveness of PCIT has been demonstrated through experimental results on various problem domains, indicating that it can construct high-quality parallel solvers efficiently with minimal human intervention."
llama3.2:latest,"What is the idea behind the PAP approach called ""CEPS""?
","Document 1: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

Document 2: 
 paper has two major contributions. First, the proposed PAP is
developed for numerical optimization instead of combinatorial
optimization. Second, we investigate the term “risk” in a
different context. Previous work mainly aimed to reduce the
risk of an algorithm on a speciﬁc optimization problem, which
can be measured by applying the algorithm to the problem
for multiple times. In this paper, we are more interested in
reducing the risk overa set of problems , i.e., the risk is
measured by applying the algorithm to multiple problems.
Our PAP is a general framework for combining different
population-based search algorithms. It allocates computation
time among more than one constituent algorithm and acti-
vates interaction among them so that they can beneﬁt from
one another. To be more speciﬁc, the computation time is
measured by the number of ﬁtness evaluations (FEs), and
the allocation of FEs is implemented by dividing the whole
population into several subpopulations and evolving them in
a parallel manner (using different constituent algorithms). To
demonstrate the efﬁcacy of the proposed PAP, we chose four
existing population-based algorithms, including self-adaptive
differential evolution with neighborhood search (SaNSDE)
[7], particle swarm optimizer with inertia weight (wPSO)
[8], generalized generation gap (G3) model with generic
parent-centric recombination (PCX) operator (G3PCX) [9],
and covariance matrix adaptation evolution strategy (CMA-
ES) [10], as the constituent algorithms. These algorithms 

Document 3: 
 this approach no longer guarantees the validity of the above
assumption. As a consequence, Θ′may generalize worse than
Θ. A remedy to prevent this as much as possible is to increase
redundancy in the evolution of the PAP. More speciﬁcally, one
could repeat the conﬁguration-removal procedure to Θ for n
times, leading to ntemporary PAPs, ¯Θ1,..., ¯Θn; then for each
temporary PAP ¯Θ, the new conﬁgurations are identiﬁed and
inserted, leading to n new PAPs, Θ′
1,..., Θ′
n, each of which
is of the same size as Θ; ﬁnally, the PAP among them that
performs best against T ∪T′ is retained.
III. C O-EVOLUTION OF PARAMETERIZED SEARCH
By incorporating the above-described procedure into the co-
evolution process, we arrive at the proposed CEPS framework,
as demonstrated in Algorithm 1. In general, CEPS consists
of two major phases, i.e., an initialization phase (lines 2-
7), and a co-evolution phase (lines 8-27) which could be
further subdivided into alternating between the evolution of
the conﬁguration population (representing the PAP) (lines 10-
15) and the evolution of the instance population (representing
the training instances) (lines 17-26) for MaxIte iterations in
total. These modules are detailed as follows. 

Document 4: 
 contributions of this paper include:
1) A novel PAP construction framework, namely Co-
Evolution of Parameterized Search (CEPS), is proposed.
It is also shown that CEPS approximates a process that
minimizes the upper bound, i.e., a tractable surrogate,
of the generalization performance.
2) To demonstrate the implementation details of CEPS as
well as to assess its potential, concrete instantiations are
also presented for two hard optimization problems, i.e.,
TSP and the Vehicle Routing Problem with Simultane-
ous Pickup–Delivery and Time Windows (VRPSPDTW)
[18]. Computational studies conﬁrm that CEPS is able
to obtain PAPs with better generalization performance.
3) The proposal of CEPS extends the realm of Co-
Evolution, for the ﬁrst time, to evolving algorithm
conﬁgurations and problem instances. Since CEPS does
not invoke domain-speciﬁc knowledge, its potential ap-
plications can go beyond optimization problems, even to
planning and learning problems.
The rest of the paper is organized as follows. Section II in-
troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in 

Document 5: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

Document 6: 
 cores each (2.20 GHz, 30 MB Cache), running Centos 7.5.
D. Results and Analysis
We report the #TOs, PAR-10 and PANC achieved by the
PAPs on the testing set in Table II and also visualize their
medians and variance across all the testing instances by box-
plots in Figure 1. Note the mean value is also plotted in Figure
1 (indicated by “ ▲”) to show that for a PAP how its PAR-
10/PANC is affected by the outliers (the timeout cases) which
would be hidden by boxplots. In Table II the #TOs, PAR-
10/PANC of a PAP is highlighted in grey if it achieved the best
performance. One could make three important observations
from these results. First, the PAPs obtained by CEPS have
the smallest number of timeouts in all the six experiments,
which means they have the highest success rate for solving the
testing instances among all the tested PAPs. Recall that CEPS
actively searches in the instance space to identify the hard-
to-solve instances for further improving the generalization of
the PAPs. Such a mechanism makes CEPS the method that is
least affected by the hard testing instances which signiﬁcantly
differs from the given training instances. This could be further
veriﬁed by Figure 1, in which CEPS is the method that has
the least gap between the mean value (which takes timeouts
into account) and median value (which naturally ﬁlters out the 

","I couldn't find any information about a concept or approach called ""CEPS"" that is associated with the PAP (Public Appraisal of Projects) approach. The PAP approach focuses on evaluating and assessing projects to determine their feasibility, benefits, and risks.

If you could provide more context or clarify what CEPS stands for in this context, I may be able to help further.",1.6582529544830322,"The Co-Evolution of Parameterized Search (CEPS) is a framework designed for constructing Parameterized Algorithms (PAPs) that aims to improve their generalization performance across a set of problems rather than on a single one. The key idea behind CEPS is to co-evolve both the configurations of the algorithms and the problem instances, where the framework consists of two major phases: an initialization phase and a co-evolution phase. In this approach, the algorithm dynamically searches the instance space to identify challenging instances that can enhance the generalization capabilities of the obtained PAPs. CEPS utilizes historical data from the generated instances to assist in tuning parameters effectively, thereby enabling better performance even in a zero-shot learning setting. This allows CEPS to be applicable beyond traditional optimization problems, extending its potential to areas like planning and learning. Overall, CEPS seeks to minimize the upper bounds of generalization performance while reducing the risk across multiple problems."
llama3.2:latest,"Please give me the names of some state-of-the-art SAT solvers.
","Document 1: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 2: 
 Intelligence (AAAI’02), (pp. 674–681). AAAI Press / The MIT Press, Menlo Park, CA, USA.
KhudaBukhsh, A., Xu, L., Hoos, H. H., & Leyton-Brown, K. (2009). SATenstein: Automatically building lo-
cal search sat solvers from components. In Proceedings of the Twenty-ﬁrst International Joint Conference
on Artiﬁcial Intelligence (IJCAI’09), (pp. 517–524).
Le Berre, D. & Simon, L. (2004). Fifty-ﬁve solvers in Vancouver: The SAT 2004 competition. In Hoos, H. H.
& Mitchell, D. G. (Eds.), Theory and Applications of Satisﬁability Testing: Proceedings of the Seventh
International Conference (SAT’04), volume 3542 of Lecture Notes in Computer Science , (pp. 321–344).
Springer Verlag.
Leyton-Brown, K., Nudelman, E., & Shoham, Y . (2002). Learning the empirical hardness of optimization
problems: The case of combinatorial auctions. In Hentenryck, P. V . (Ed.), Principles and Practice of
Constraint Programming – CP 2002: Eighth International Conference , volume 2470 of Lecture Notes in
Computer Science, (pp. 556–572). Springer Verlag, Berlin, Germany. 

Document 3: 
 212
state-of-the-art SLS algorithms. SATenstein-LS has
41 parameters that control the selection and behavior of
its components, leading to a total conﬁguration space of
size 4.82 ×1012. Automatically-identiﬁed conﬁgurations of
SATenstein-LS have been shown to achieve better per-
formance than existing state-of-the-art SLS algorithms on
several well-known instance distributions.
Instances
We investigated the effectiveness of Hydra on four distri-
butions, drawing on well-known families of SAT instances.
Because no state-of-the-art SLS algorithms are able to prove
unsatisﬁability, we considered only satisﬁable instances. We
identiﬁed these by running all complete algorithms that won
a SAT competition category between 2002 and 2007 for
one hour. First, the BM data set is constructed from 500
instances taken from each of the six distributions used by
KhudaBukhsh et al. (2009) (
QCP, SWGCP, FACT, CBMC,
R3FIX, and HGEN), split evenly into training and test sets.
Second, the INDU data set is a mixture of 500 instances
from each of the CBMC and FACT distributions, again split
evenly into training and test sets. Third and fourth, the HAND
and RAND data sets include all satisﬁable instances from the
Random and Handmade categories of the SAT Competitions 

Document 4: 
 form (a conjunction of disjunctions of variables and their negations) and determine a satisﬁable assignment or prove that 
none exists. An incomplete SAT solver can ﬁnd satisfying assignments, but not prove unsatisﬁability.
Most state-of-the-art complete SAT solvers are based on conﬂict-driven clause learning (CDCL; [57]). Their parameters 
control variable selection for branching decisions, clause learning and restart techniques. State-of-the-art incomplete SAT 
solvers use stochastic local search (SLS; [38]), and their parameters control the selection of the variable whose value is 
modiﬁed in each local search step as well as the diversiﬁcation and additional intensiﬁcation strategies. Furthermore, there 
exist several preprocessing techniques (e.g., [21]) to simplify formulas and their parameters control how long and how 
aggressive preprocessing will be used – too much preprocessing can remove important structural information and hence, 
it can increase the hardness of formulas. The eﬃcacy of SAT solvers depends on multiple heuristic components whose 
basic functions and the interplay between them are controlled by parameters. Some parameters are categorical (e.g., choice 
between different search strategies in SLS), while many others are integer- or real-valued (e.g., the damping factor used in 
computing heuristic variable scores in CDCL).
Parallel SAT solvers have received increasing attention in recent years. ManySAT [30,31,29] was one of the ﬁrst parallel 

Document 5: 
 tial solver baselines, indicating the great benefit by combin-
ing complementary configurations obtained from a rich con-
figuration space.
Comparison with Hand-designed Parallel Solvers
To further evaluate the portfolios constructed by PCIT, we
compared them with the state-of-the-art manually designed
parallel solvers. Specifically, we considered the ones con-
structed for SAT. We chose Priss6 (Manthey, Stephan, and
Werner 2016) to compare with the one constructed in SAT-
Single, since Priss6 is the official parallel version of Riss6
(the base solver in SAT-Single). For the same reason, we
chose PfolioUZK (Wotzlaw et al. 2012) (the gold medal
winning solver of the parallel track of the SAT’12 Chal-
lenge) to compare with the one constructed in SAT-Multi.
Finally, we chose Plingeling (version bbc) (Biere 2016),
the gold medal winning solver of the parallel track of the
SAT’16 Competition, to compare with both. Note that all
the manually designed solvers considered here have imple-
mented far more advanced parallel solving strategies (e.g.,
clause sharing) than only independently running component
solvers in parallel. In the experiments the default settings of
these solvers were used and the same statistical tests as be-
fore were conducted. As shown in Table 4, on SAT-Single 

Document 6: 
 ACM 52(10):56–67.
Balyo, T.; Heule, M. J. H.; and J¨arvisalo, M., eds. 2016. Proceed-
ings of SAT Competition 2016: Solver and Benchmark Descrip-
tions, volume B-2016-1 of Department of Computer Science Se-
ries of Publications B. University of Helsinki.
Battiti, R.; Brunato, M.; and Mascia, F., eds. 2008. Reactive
Search and Intelligent Optimization. Springer.
Biere, A. 2016. Splatz, Lingeling, Plingeling, Treengeling, Yal-
SAT Entering the SAT Competition 2016. In Balyo et al. (2016),
44–45.
Burke, E. K.; Gendreau, M.; Hyde, M.; Kendall, G.; Ochoa, G.;
¨Ozcan, E.; and Qu, R. 2013. Hyper-heuristics: A Survey of the
State of the Art. Journal of the Operational Research Society
64(12):1695–1724.
Gomes, C. P., and Selman, B. 2001. Algorithm Portfolios. Arti-
ficial Intelligence 126(1-2):43–62.
Hamadi, Y ., and Wintersteiger, C. M. 2013. Seven Challenges in
Parallel SAT Solving. AI Magazine 34(2):99–106. 

","Here are some mentioned state-of-the-art SAT solvers:

1. Hydra
2. Priss6
3. PfolioUZK
4. Plingeling (Splatz, Lingeling, Treengeling, YalSAT)
5. Riss6 
6. SATenstein-LS",2.4747424125671387,"Some state-of-the-art SAT solvers mentioned in the context include Priss6, PfolioUZK, Plingeling, and SATenstein-LS. Other solvers are also referenced, such as Splatz, Lingeling, and Treengeling."
llama3.2:latest,"Please give me the names of some state-of-the-art TSP solvers.
","Document 1: 
 For surveys of the problem and its applications, the reader is referred to the excellent
volumes edited by Lawler et al. [24] and Gutin and Punnen [11].
Local search with k-exchange neighborhoods, k-opt, is the most widely used heu-
ristic method for the TSP. k-opt is a tour improvement algorithm, where in each step
k links of the current tour are replaced by k links in such a way that a shorter tour is
achieved.
It has been shown [7] that k-opt may take an exponential number of iterations and
that the ratio of the length of an optimal tour to the length of a tour constructed byk-opt
can be arbitrarily large when k ≤ n/2 − 5. Such undesirable cases, however, are very
rare when solving practical instances [31]. Usually, high-quality solutions are obtained
in polynomial time. This is, for example, the case for the Lin–Kernighan heuristic,
one of the most effective methods for generating optimal or near-optimal solutions
for the symmetric TSP. High-quality solutions are often obtained, even though only a
small part of the k-exchange neighborhood is searched.
In the original version of the Lin–Kernighan heuristic [ 25], the allowable
k-exchanges (or k-opt moves) are restricted to those that can be decomposed into a
2- or 3-exchange followed by a (possibly empty) sequence of 2-exchanges. This restric- 

Document 2: 
 of determining whether a Boolean formula can 
evaluate to true, benefits from very mature and 
advanced algorithms with large practical impact.
Application and research domains like software 
and hardware verification, automated planning,
computational biology, and many others benefit 
from modern SAT solvers. These domains have 
large and difficult instances that provide the SAT 
community with meaningful benchmarks. 
Most of the following challenges are general in 
such a way that the questions they raise should 
positively affect not only research in parallel SAT 
but in parallel search in general. We first present 
the current situation in sequential and parallel SAT 
solving and then give a set of challenges. Each of 
these challenges comes with an overly optimistic 
estimate of its inherent difficulty represented as 
black circles, where we would estimate that every 
black circle represents, roughly, about two years of 
research. 
Context: Sequential SAT Solvers 
State-of-the-art solvers extend the original Davis,
Putnam, Logemann, and Loveland (DPLL) proce -
dure (Davis, Logemann, and Loveland 1962) with 
conflict analysis (Zhang et al. 2001). The general
architecture of such conflict-directed clause-learn -
ing solvers (CDCL) is presented in figure 1. These 
procedures include an optional preprocessing step 
0, which performs variable elimination and clause 
subsumption checks in order to reduce the size of 
the formula and improve the performance of the 

Document 3: 
 10.0 
12.5 
15.0 
17.5 PAR-10 (c) TSP-3
Fig. 2: Visual comparison in boxplots of the medians and variance of the test performance of each TSP solver across the
testing instances. Note the mean value is also plotted, indicated by “ ▲”.
TABLE III: Comparison of the state-of-the-art TSP solvers
with the PAPs obtained by CEPS, on the testing set. #TOs
refers to number of total timeouts. PAR-10 is the penalized
average runtime-10. Performance of a solver is highlighted in
grey if it achieved the best testing performance.
TSP-1 TSP-2 TSP-3
#TOs PAR-10 #TOs PAR-10 #TOs PAR-10
LKH-default 131 30.84 137 31.98 150 34.73
LKH-tuned 29 8.23 34 9.40 27 7.67
EAX-default 69 17.98 73 18.91 69 17.95
EAX-tuned 33 10.97 30 10.38 29 10.12
VBS 6 3.82 7 4.26 6 4.13
CEPS 6 2.74 4 2.15 2 1.94
signiﬁcant performance improvement, though still obviously
falling behind of the PAPs obtained by CEPS. Second, the only
solver that could match the PAP’s performance level in one
of the three scenarios, is the VBS of the algorithm portfolio
considered by the algorithm selection approaches [29], [30].
However, in TSP-2 and TSP-3, the performance advantage of 

Document 4: 
 troduces the challenge of seeking generalizable PAPs, existing
PAP construction methods, as well as the theoretical insight
behind CEPS. Section III presents the CEPS framework. Sec-
tion IV presents its instantiations for TSP and VRPSPDTW.
Computational studies on these two problems are presented in
Section V. Threats to validity of this study are discussed in
Section VI. Section VII concludes the paper with discussions.
II. P ARAMETERIZED SOLVERS MADE GENERALIZABLE
A. Notations and Problem Description
Assume a PAP is to be built for a problem class (e.g., TSP),
for which an instance of the problem class is denoted as s, and
the set of all possible sis denoted as Ω. Given a parameterized
algorithm, each component solver of the PAP is a conﬁguration
(full instantiation) of the algorithm. Generally speaking, the
parameterized algorithm can be any concrete computational
process, e.g., a traditional heuristic search process such as the
LK Heuristic for TSP or even a neural network [19]–[21] that
outputs a solution for a given instance of the target problem
class. Let θ denote a conﬁguration and let Θ denote a PAP
that contains K different conﬁgurations (component solvers),
i.e., Θ = {θ1,...,θ K}. The quality of a conﬁguration θ on a 

Document 5: 
 H. Trautmann, “Evolving diverse TSP instances by means of novel and
creative mutation operators,” in Proceedings of the 15th ACM/SIGEVO
Conference on Foundations of Genetic Algorithms, FOGA’2019. Pots-
dam, Germany: ACM, Aug 2019, pp. 58–71.
[38] Y . Nagata and S. Kobayashi, “A powerful genetic algorithm using edge
assembly crossover for the traveling salesman problem,” INFORMS
Journal on Computing , vol. 25, no. 2, pp. 346–363, 2013.
[39] X. Xie and J. Liu, “Multiagent optimization system for solving the
traveling salesman problem (tsp),” IEEE Transactions on Systems, Man,
and Cybernetics, Part B , vol. 39, no. 2, pp. 489–502, 2009.
[40] C. Wang, D. Mu, F. Zhao, and J. W. Sutherland, “A parallel simulated
annealing method for the vehicle routing problem with simultaneous
pickup-delivery and time windows,” Computers & Industrial Engineer-
ing, vol. 83, pp. 111–122, 2015.
[41] W. Huang and T. Zhang, “Vehicle routing problem with simultaneous
pick-up and delivery and time-windows based on improved global arti-
ﬁcial ﬁsh swarm algorithm,” Computer Engineering and Applications ,
vol. 52, no. 21, pp. 21–29, 2016. 

Document 6: 
 Portfolio-based algorithm selection for SAT,” Journal of Artiﬁcial In-
telligence Research, vol. 32, pp. 565–606, 2008.
[29] P. Kerschke, L. Kotthoff, J. Bossek, H. H. Hoos, and H. Trautmann,
“Leveraging TSP solver complementarity through machine learning,”
Evolutionary Computation, vol. 26, no. 4, pp. 597–620, 2018.
[30] K. Zhao, S. Liu, Y . Rong, and J. X. Yu, “Leveraging TSP solver
complementarity via deep learning,” arXiv preprint arXiv:2006.00715 ,
2020.
[31] L. Kotthoff, “Algorithm selection for combinatorial search problems: A
survey,” AI Magazine, vol. 35, no. 3, pp. 48–60, 2014.
[32] C. D. Rosin and R. K. Belew, “New methods for competitive coevolu-
tion,” Evolutionary Computation, vol. 5, no. 1, pp. 1–29, 1997.
[33] J. I. van Hemert, “Evolving combinatorial problem instances that are
difﬁcult to solve,” Evolutionary Computation , vol. 14, no. 4, pp. 433–
462, 2006.
[34] K. Helsgaun, “General k-opt submoves for the Lin-Kernighan TSP 

","Some state-of-the-art Traveling Salesman Problem (TSP) solvers are:

1. Dantzig-Wolfe decomposition-based algorithms, such as:
	* D-W solver (https://github.com/skamers/dw_tsp_solver)
	* D-W++ solver (https://github.com/hadrien-dahan/DW++)
2. Genetic Algorithm-based solvers, such as:
	* DEAP (Distributed Evolutionary Algorithms in Python) - https://deap.readthedocs.io/
	* Pyevolve - https://pyevolve.readthedocs.io/en/latest/
3. Ant Colony Optimization (ACO)-based solvers, such as:
	* ANTColony - https://github.com/pedro-salgado/ANTColony
4. Swarm Intelligence-based solvers, such as:
	* PSO (Particle Swarm Optimization) solver - https://github.com/miladhosseini/PSO-TSP
5. Machine Learning-based solvers, such as:
	* TSP solver using neural networks and reinforcement learning - https://github.com/ikostantin/tsp_rl
6. Hybrid solvers that combine different techniques, such as:
	* TSPLIB + branch-and-bound (https://github.com/mikusoft/tsplib)
	* T-Splines-based solver (https://github.com/wangqiang88/T-Splines-TSP)

Note that this is not an exhaustive list and new solvers are being developed continuously.",4.961170434951782,"Some state-of-the-art TSP solvers mentioned in the context are LKH (Lin-Kernighan heuristic), EAX (Edge Assembly Crossover), and VBS (Variable Neighborhood Search)."
llama3.2:latest,"Please give me the names of some state-of-the-art Automatic Algorithm Configuration approaches.
","Document 1: 
 Journal of Artiﬁcial Intelligence Research 75 (2022) 425-487 Submitted 02/2022; published 10/2022
A Survey of Methods for Automated
Algorithm Conﬁguration
Elias Schede elias.schede@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Jasmin Brandt jasmin.brandt@upb.de
Alexander Tornede alexander.tornede@upb.de
Department of Computer Science,
Paderborn University, Paderborn, Germany
Marcel Wever marcel.wever@ifi.lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Viktor Bengs viktor.bengs@ifi.lmu.de
Institute of Informatics,
LMU Munich, Munich, Germany
Eyke H¨ ullermeier eyke@lmu.de
Institute of Informatics, LMU Munich &
Munich Center for Machine Learning, Munich, Germany
Kevin Tierney kevin.tierney@uni-bielefeld.de
Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews 

Document 2: 
 2 Related Work
2.1 Automatic Algorithm Conﬁguration
Several approaches exist in the literature for the automatic tuning
of algorithms. Some of these were created for a speciﬁc algorithm
or task. For example, [24] devises a modular algorithm for solving
constraint satisfaction problems (CSPs). Using a combination of ex-
haustive enumeration of all possible conﬁgurations and parallel hill-
climbing, the technique automatically conﬁgures the system for a
given set of training instances. Another approach, presented in [30],
focuses on the conﬁguration of adaptive algorithms, employing a se-
quential parameter optimization approach.
Other approaches automatically design and build an entire solver
to best tackle a set of example training instances. For example, [28]
uses genetic programming to create an evolutionary algorithm (EA).
Here the chromosome is an EA operation like the selection of par-
ents, mutation, or crossover, and the task is to ﬁnd a sequence of
the genetic programming operators that is best suited for the speci-
ﬁed problem. For SAT, [8] classiﬁes local search (LS) approaches by
means of context-free grammars. This approach then uses a genetic
programming approach to select a good LS algorithm for a given set
of instances.
There also exist approaches that are applicable to more general al-
gorithms. For example, in order to tune continuous parameters, [5]
suggests an approach that determines good parameters for individual
training instances. This approach ﬁrst evaluates the extreme param- 

Document 3: 
 470
A Survey of Methods for Automated Algorithm Configuration
General AC systems
D-SMAC https://github.com/tqichun/distributed-SMAC3
GPS https://github.com/YashaPushak/GPS
irace https://github.com/MLopez-Ibanez/irace
OAT (GGA) https://docs.optano.com/algorithm.tuner/current/
ParamILS https://www.cs.ubc.ca/labs/algorithms/Projects/ParamILS/
PyDGGA http://ulog.udl.cat/software/
REVAC https://github.com/ChrisTimperley/RubyREVAC
SMAC 3 https://github.com/automl/SMAC3
Benchmarks
AClib https://bitbucket.org/mlindauer/aclib2/src/master/
DAC https://github.com/automl/DAC
Table 7: List of available software in the realm of AC.
References
Belarmino Adenso-Diaz & Manuel Laguna. Fine-tuning of algorithms using fractional ex-
perimental designs and local search. Operations research, 54(1):99–114, 2006.
¨Ozg¨ ur Akg¨ un, Nguyen Dang, Ian Miguel, Andr´ as Z Salamon, & Christopher Stone. In- 

Document 4: 
 cal (e.g., choice of heuristic). Categorical parameters can be used to select and combine discrete
building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics); consequently,
our general view of algorithm conﬁguration includes the automated construction of a heuristic algo-
rithm from such building blocks. To the best of our knowledge, the methods discussed in this article
are yet the only general ones available for the conﬁguration of algorithms with many categorical
parameters.
We now give an overview of what follows and highlight our main contributions. After for-
mally stating the algorithm conﬁguration problem in Section 2, in Section 3 we describe ParamILS
(ﬁrst introduced by Hutter, Hoos & St ¨utzle, 2007), a versatile stochastic local search approach for
automated algorithm conﬁguration, and two of its instantiations, BasicILS and FocusedILS.
We then introduce adaptive capping of algorithm runs, a novel technique that can be used to
enhance search-based algorithm conﬁguration procedures independently of the underlying search
strategy (Section 4). Adaptive capping is based on the idea of avoiding unnecessary runs of the
algorithm to be conﬁgured by developing bounds on the performance measure to be optimized.
We present a trajectory-preserving variant and a heuristic extension of this technique. After dis-
cussing experimental preliminaries in Section 5, in Section 6 we present empirical evidence show-
ing that adaptive capping speeds up both BasicILS and FocusedILS. We also show that BasicILS
268 

Document 5: 
 rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 
Due to the separation between a tuning and a production phase, 
automatic algorithm conﬁguration is also known as oﬄine param- 
eter tuning to differentiate it from online approaches that adapt or 
control parameter settings while solving an instance [13,50] . Nev- 
ertheless, online approaches also contain parameters that need to 
be deﬁned oﬄine, for example, which and how parameters are 
adapted at run-time; such parameters and design choices can be 
conﬁgured by an oﬄine tuning method [59] . 
In our research on making the algorithm conﬁguration process 
more automatic , we have focused on racing approaches. Birattari 
et al. [19,20] proposed an automatic conﬁguration approach, F- 
Race, based on racing [64] and Friedman’s non-parametric two-way 
analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 

Document 6: 
 formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de-
veloped and used for more than a decade, and many of the fundamental techniques
date back even further. However, it has only recently become possible to effectively
solve complex conﬁguration problems involving target algorithms with dozens of
parameters, which are often categorical and conditional. This success is based in
3 Automated Algorithm Conﬁguration and Parameter Tuning 65
part on the increased availability of computational resources, but has mostly been
enabled by methodological advances underlying recent conﬁguration procedures.
Still, we see much room (and, indeed, need) for future work on automated algo-
rithm conﬁguration and parameter tuning methods. We believe that in developing
such methods, the fundamental features underlying all three types of methods dis-
cussed in this chapter can play an important role, and that the best methods will
employ combinations of these. We further believe that different conﬁguration pro-
cedures will likely be most effective for solving different types of conﬁguration
problems (depending, in particular, on the number and type of target algorithm pa-
rameters, but also on regularities in the parameter response). Therefore, we see a
need for research aiming to determine which conﬁgurator is most effective under 

","Here are some state-of-the-art Automatic Algorithm Configuration approaches mentioned in the text:

1. ParamILS (Parametric ILoved Search)
2. BasicILS (Basic Iterated Local Search)
3. FocusedILS (Focused Iterated Local Search)
4. Adaptive Capping of Algorithm Runs
5. I/F-Race (Iterated F-Race)

Note that these are not exhaustive, and there may be other state-of-the-art approaches available in the literature.",2.6846964359283447,"Some state-of-the-art Automatic Algorithm Configuration approaches include D-SMAC, GPS, irace, OAT (GGA), ParamILS, PyDGGA, REVAC, and SMAC 3."
llama3.2:latest,"What is the idea behind the AAC approach called ""irace""?
","Document 1: 
 analysis of variance by ranks. This proposal was later improved 
by sampling conﬁgurations from the parameter space, and reﬁn- 
ing the sampling distribution by means of repeated applications of 
F-Race. The resulting automatic conﬁguration approach was called 
Iterated F-race (I/F-Race) [10,21] . Although a formal description of 
the I/F-Race procedure is given in those publications, an imple- 
mentation was not made publicly available. The irace package im- 
plements a general iterated racing procedure, which includes I/F- 
Race as a special case. It also implements several extensions al- 
ready described by Birattari [19] , such as the use of the paired 
t -test instead of Friedman’s test. Finally, irace incorporates sev- 
eral improvements not published before, such as sampling from a 
truncated normal distribution, a parallel implementation, a restart 
strategy that avoids premature convergence, and an elitist racing 
procedure to ensure that the best parameter conﬁgurations found 
are also evaluated on the highest number of training instances. 
The paper is structured as follows. Section 2 introduces the 
algorithm conﬁguration problem and gives an overview of ap- 
proaches to automatic algorithm conﬁguration. Section 3 describes 
the iterated racing procedure as implemented in the irace pack- 
age as well as several further extensions including the elitist irace . 
Section 4 illustrates the steps followed to apply irace to two con- 

Document 2: 
 5.2. Multi-objective optimization metaheuristics 
Besides the application to the MOACO framework described 
above [58] , irace has been applied to aid in the design of other 
multi-objective optimization algorithms. Dubois-Lacoste et al. 
[31] used irace to tune a hybrid of two-phase local search and 
Pareto local search (TP + PLS) to produce new state-of-the-art al- 
gorithms for various bi-objective permutation ﬂowshop problems. 
Fisset et al. [33] used irace to tune a framework of multi-objective 
optimization algorithms for clustering. When applied to a suﬃ- 
ciently ﬂexible algorithmic framework, irace has been used to de- 
sign new state-of-the-art multi-objective evolutionary algorithms 
[16,17] . 
5.3. Anytime algorithms (improve time-quality trade-offs) 
There is often a trade-off between solution quality and com- 
putation time: Algorithms that converge quickly tend to produce 
better solutions for shorter runtimes, whereas more exploratory al- 
gorithms tend to produce better solutions for longer runtimes. Im- 
proving the anytime behavior of an algorithm amounts to improv- 
ing the trade-off curve between solution quality and computation 
time such that an algorithm is able to produce as high quality so- 
lutions as possible at any moment during their execution. López- 

Document 3: 
 effect on the budget consumed is lower. The same experiment for 
the ACOTSP scenario showed that the best conﬁgurations become 
worse when T new or T ﬁrst are increased. This is due to the fact that 
ACOTSP has a homogeneous training set and, therefore, sampling 
new candidates is more important than executing a large number 
of instances. 
5. Other applications of irace 
Since the ﬁrst version of the irace package became publicly 
available in 2012, there have been many other applications of 
irace . In this section, we provide a list of the applications of the 
irace package of which we are aware at the time of writing. Some 
of these applications go beyond what is traditionally understood as 
algorithm conﬁguration, demonstrating the ﬂexibility of irace . 
54 M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 
5.1. Algorithm conﬁguration 
The traditional application of irace is the automatic conﬁgura- 
tion of algorithms. Several publications have used irace when eval- 
uating or designing algorithms for problems such as the traveling 
salesman with time windows [60] , slot allocation [77] , generalized 
hub location [68] , ﬂow shop [14] , virtual machine placement [86] , 
online bin packing [92] , graph coloring [23] , image binarization 
[69] , network pricing [90] , combined routing and packing prob- 

Document 4: 
 ming by optimization paradigm [40] . Moreover, the importance of 
properly tuning the parameters of algorithms before analyzing and 
comparing them is becoming widely recognized. We hope that the 
development of the irace package will help practitioners and re- 
searchers to put these ideas into practice. 
Acknowledgements 
This work received support from the COMEX project (P7/36) 
within the Interuniversity Attraction Poles Programme of the Bel- 
gian Science Policy Oﬃce, and the EU FP7 ICT Project COLOMBO, 
Fig. A.1 1. Scheme of the user-provided components required by irace . 
Table A1 
Parameters of irace corresponding to the description 
of iterated racing given in Section 3.2 . The full list of 
irace parameters is available in the user guide. 
Iterated racing parameter irace parameter 
B maxExperiments 
C (cost measure) targetRunner 
μ mu 
N min minNbSurvival 
T ﬁrst firstTest 
T each eachTest 
Statistical test testType 
Cooperative Self-Organizing System for Low Carbon Mobility at 
Low Penetration Rates (agreement no. 318622). Thomas Stützle and 
Mauro Birattari acknowledge support from the Belgian F.R.S.-FNRS, 
of which they are Senior Research Associates. Leslie Pérez Cáceres 
acknowledges support of CONICYT Becas Chile. 
Appendix A. The irace package 

Document 5: 
 man problem with time windows: adapting algorithms from travel-time to 
makespan optimization. Appl Soft Comput 2013;13(9):3806–15 . 
[61] López-Ibáñez M., Dubois-Lacoste J., Pérez Cáceres L., Stützle T., Birattari M.. 
2016a. The irace package: Iterated racing for automatic algorithm conﬁgura- 
tion. http://iridia.ulb.ac.be/supp/IridiaSupp2016-003/ . 
[62] López-Ibáñez M, Pérez Cáceres L, Dubois-Lacoste J, Stützle T, Birattari M. The 
irace package: user guide. Technical Report TR/IRIDIA/2016-004. IRIDIA, Uni- 
versité Libre de Bruxelles, Belgium; 2016b . http://iridia.ulb.ac.be/IridiaTrSeries/ 
IridiaTr2016-004.pdf . 
[63] Marmion M-E , Mascia F , López-Ibáñez M , Stützle T . Automatic design of hy- 
brid stochastic local search algorithms. In: Blesa MJ, Blum C, Festa P, Roli A, 
Sampels M, editors. Hybrid metaheuristics, volume 7919 of lecture notes in 

Document 6: 
 ent robot behaviors and the criteria to transition between behav- 
iors. Each module can be further customized by means of several 
parameters. A particular combination of behaviors and transitions 
represents one controller, that is, an instance of the software that 
controls the robots in the swarm. The performance of a particu- 
lar controller is evaluated by means of multiple simulations. The 
search for the best controller over multiple training simulations is 
carried out by means of irace . The authors report that this system 
is not only able to outperform a previous system that used F-race 
[34] , but also a human designer, under the scenarios studied by 
them. 
6. Conclusion 
This paper presented the irace package, which implements the 
iterated racing procedure for automatic algorithm conﬁguration. It- 
erated racing is a generalization of the iterated F-race procedure. 
M. López-Ibáñez et al. / Operations Research Perspectives 3 (2016) 43–58 55 
The primary purpose of irace is to automatize the arduous task of 
conﬁguring the parameters of an optimization algorithm. However, 
it may also be used for determining good settings in other compu- 
tational systems such as robotics, traﬃc light controllers, compil- 
ers, etc. The irace package has been designed with simplicity and 
ease of use in mind. Despite being implemented in R , no previ- 
ous knowledge of R is required. We included two examples for the 

","The irace package implements the iterated racing procedure, a generalization of the iterated F-race procedure, to automate algorithm configuration by sampling configurations from the parameter space and refining the sampling distribution through repeated applications of a statistical test. This process aims to find good settings for an optimization algorithm.",2.383065223693848,"The ""irace"" package implements an automatic algorithm configuration approach known as iterated racing. The key idea behind irace is to automate the process of tuning the parameters of optimization algorithms to improve their performance. It uses a statistical procedure to compare different parameter configurations, iteratively refining and sampling from the parameter space to identify the most effective settings. Irace incorporates various techniques such as parallel implementation, restart strategies to avoid premature convergence, and the ability to use different statistical tests for evaluating configurations. It has proven to be flexible and effective across various applications, including multi-objective optimization and robotics."
llama3.2:latest,"What is the idea behind the AAC approach called ""SMAC""?
","Document 1: 
 intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods
SMAC Sequential model-based optimization for algorithm conﬁguration (SMAC) (Hutter
et al., 2011; Lindauer et al., 2021) can be seen as one of the ﬁrst fully-ﬂedged model-
based AC approaches, as it features solutions for many of the limitations of the previously
discussed SMBO techniques. SMAC generalizes TB-SPO to perform conﬁguration over
multiple problem instances so that it can support categorical parameters and handle tight
time constraints.
To support multiple problem instances, SMAC adapts the intensiﬁcation strategy of
TB-SPO to iteratively evaluate conﬁgurations on randomly sampled combinations of seeds
and problem instances. When doing so, it ensures that conﬁgurations are compared only
based on a performance estimate computed on the same randomly sampled set of problem
instances. Furthermore, SMAC’s surrogate model can generalize across problem instances
by incorporating problem instance features. To this end, a surrogate model is learned on
the joint problem instance and conﬁguration space to predict the performance of a given
conﬁguration on a given problem instance.
As a means to deal with a mixture of categorical and numerical parameters, SMAC 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 algorithm configuration framework. JArtifIntellRes 2009; 36 : 
267–306. 
9. Lindauer M, Eggensperger K, Feurer M et al. SMAC3: a ver- 
satile Bayesian optimization package for hyperparameter opti- 
mization. JMachLearnRes 2022; 23 : 54. 
10. Santu SKK, Hassan MM, Smith MJ et al. Automl to date and 
beyond: challenges and opportunities. ACMComputSurv 2022; 
54 : 175. 
Page7of9
Downloaded from https://academic.oup.com/nsr/article/11/8/nwae132/7639406 by guest on 18 September 2024 

Document 5: 
 more benchmark instances and compare its performance to
the best conﬁguration observed so far.
SMAC is an anytime algorithm that interleaves the ex-
ploration of new conﬁgurations with additional runs of the
current best conﬁguration to yield both better and more con-
ﬁdent results over time. As all anytime algorithms, SMAC
improves performance over time. While SMAC provably
converges for ﬁnite conﬁguration spaces, it often only ﬁnds
close-to-optimal conﬁgurations for realistic time budgets
and challenging conﬁguration problems.
Automated Portfolio Construction & Hydra
Our method for constructing sequential portfolios is closely
related to Hydra (Xu, Hoos, and Leyton-Brown 2010),
which automatically constructs selection-based portfolios.
Given a parameterized algorithm framework with a space
of algorithmsΘ, Hydra starts with an empty set of can-
didate algorithms C= ∅and iteratively calls an algorithm
conﬁguration method to add algorithms θ ∈Θ that comple-
ment Cbest. In each iteration, it constructs a portfolio from
the current set Cusing the portfolio-based algorithm selec-
tor SATzilla (Xu et al. 2008) and measures its performance
m(C,π) on each benchmark instance π ∈Π (in the ﬁrst it-
eration, m(∅,π) = −∞). The performance metric Hydra
maximizes via its algorithm conﬁguration method is then 

Document 6: 
 Anastacio and Hoos (2020) propose SMAC+PS, which integrates the idea of probabilis-
tic sampling known from irace into SMAC. This enhancement yields improvements over
both SMAC and irace in many cases. In particular, Anastacio and Hoos (2020) account for
the problem that many of the completely randomly sampled conﬁgurations by SMAC often
exhibit rather bad performance and thus, their evaluation yields only limited information.
To this end, the authors suggest to sample conﬁgurations according to a truncated normal
distribution centered around the default conﬁguration.
444
A Survey of Methods for Automated Algorithm Configuration
In (Lindauer & Hutter, 2018b) the authors suggest two diﬀerent strategies to warmstart
model-based AC approaches and apply their suggestions to SMAC, leading to signiﬁcant
speedups from days to hours of conﬁguration time. The idea underlying warmstarting is to
use the evaluations of conﬁgurations from previous runs, i.e., on diﬀerent problem instance
sets, to speed up the conﬁguration process in new runs of the conﬁgurator on a new set of
instances.
Distributed SMAC (Hutter et al., 2012) (D-SMAC) is an extension of SMAC leveraging
parallelization to speed up the conﬁguration process. The main idea behind D-SMAC is
to parallelize target algorithm runs onto available workers as much as possible. For this
purpose, it maintains a queue of target algorithm conﬁguration evaluations to be performed, 

","The SMAC (Sequential Model-based Acquisition and Optimization with Cross-Validation) approach is an anytime algorithm that interleaves the exploration of new configurations with additional runs of the current best configuration. Its main idea is to adaptively adjust the trade-off between exploration and exploitation to improve performance over time, using a surrogate model that predicts the performance of a given configuration on multiple problem instances.",2.573389768600464,"The idea behind the AAC (automatic algorithm configuration) approach called SMAC (Sequential Model-based Optimization for Algorithm Configuration) is to optimize the performance of algorithms by systematically searching for optimal hyperparameter configurations. SMAC achieves this by using a surrogate model that predicts the performance of different configurations based on previously evaluated instances, allowing it to generalize across multiple problem instances. It interleaves the exploration of new configurations with additional runs of the current best configuration, ensuring iterative improvement over time. SMAC also accommodates a mixture of categorical and numerical parameters, making it versatile for a variety of algorithmic applications. Additionally, enhancements like SMAC+PS and D-SMAC have been proposed to improve its performance and speed through more efficient sampling and parallelization strategies."
llama3.2:latest,"What is the idea behind the AAC approach called ""GGA""?
","Document 1: 
 to be evaluated based on k diﬀerently parameterized instantiations of the UCB criterion.
GGA++ Ans´ otegui et al. (2015) adapt the model-free AC approach GGA to include a
surrogate model. More precisely, the authors use a surrogate model to evaluate the quality
of new conﬁgurations. They integrate this within a crossover operator and call it genetic
engineering. Recall that GGA contains both a competitive and non-competitive popula-
tion in which winning conﬁgurations from the races between members of the competitive
population are recombined with individuals from the non-competitive population. To this
end, the crossover operator generates individuals according to the parameter tree crossover
of the original GGA method and evaluates them using the surrogate. Note that rather
than predicting the solution quality or runtime directly, the surrogate predicts the rank the
individual would have in a tournament. The individuals with the best ranks are accepted
into the population of the next generation in the same way as in GGA.
While the GGA++ surrogate is based on a random forest model, it diﬀers in a key
way. The premise of a random forest is to equally approximate the underlying function over
the complete input space. In the case of AC, this is undesirable as only the areas of the
input space that correspond to high-quality conﬁgurations are of interest. Thus, the authors
present specialized splitting criteria that focuses on only the best conﬁgurations to increase 

Document 2: 
 Our approach uses a similar insight as Bansal and Viswanathan (1993) and Chen and Ludvigson
(2009), who propose using a given set of conditional GMM equations to estimate the SDF with
neural networks, but restrict themselves to a small number of conditioning variables. In order to
deal with the inﬁnite number of moment conditions we extend the classical GMM setup of Hansen
(1982) and Chamberlain (1987) by an adversarial network to select the optimal moment conditions.
A similar idea has been proposed by Lewis and Syrgkanis (2018) for non-parametric instrumental
variable regressions. Our problem is also similar in spirit to the Wasserstein GAN in Arjosvky,
Chintala, and Leon (2017) that provides a robust ﬁt to moments. The Generative Adversarial
Network (GAN) approach was ﬁrst proposed by Goodfellow et al. (2014) for image recognition.
In order to ﬁnd the hidden states in macroeconomic time series we propose the use of Recurrent
Neural Networks with Long-Short-Term-Memory (LSTM). LSTMs are designed to ﬁnd patterns
4We have shared our data and estimated models with Avramov, Cheng, and Metzker (2020). In their comparison
study Avramov, Cheng, and Metzker (2020) also include a portfolio derived from our GAN model. However, they do
not consider our SDF portfolio based on ω but use the SDF loadings β to construct a long-short portfolio based on 

Document 3: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 4: 
 instead of the fully predetermined lifetimes in the original
GA VaPS and nAMGA VaPS.
Another variation of the GA VaPS lifetime scheme was
described by Bäck et al .[ 18]. The adaptive population GA
(APGA) uses the same lifetime allocation but differs from
GA VaPS in the reproduction cycle and in that, when in-
crementing the ages of individuals in each cycle, the best
individual of that cycle remains unchanged. This adaptive
population strategy is part of an ensemble and is described
in more detail in Section V. This method was also applied to
cooperative coevolution by Iorio and Li [ 109]. An analysis
of APGA by Lobo and Lima [ 135] shows theoretical and
experimental results suggesting an upper bound and a con-
verging population to a constant size that is determined by
the minimum and maximum lifetime parameters. The authors
conclude that the population is not adapted by the APGA
but the size parameter is in fact replaced by the two lifetime
parameters.
Cook and Tauritz [36] suggested two strategies for removing
the population size parameter. FiScIS-EA removes individuals
according to a survival probability, derived by linear scaling
of the individual’s ﬁtness in the range between the minimum
and maximum ﬁtness values present in the population. GC-EA
simply evades choosing a population size by maintaining a
population as large as is allowed by memory limitations. This
requires a well-chosen parent selection mechanism that scales
well, while a survival selection operator is still needed in 

Document 5: 
 3.4.5.2 Generative Adversarial Network Imputation
GANs consist of two parts— a generator and a discriminator
(Goodfellow et al., 2014 ). In an adversarial process, the
generator learns to generate samples that are as close as
possible to the data distribution, and the discriminator learns
to distinguish whether an example is true or generated.
Imputation approaches based on GANs include those in the
work ofYoon et al. (2018); Shang et al. (2017);a n dLi et al.
(2019). Here, we employ one of the most popular approaches of
GAN-based imputation, Genera tive Adversarial Imputation
Nets (GAIN) (Yoon et al., 2018). GAIN adapts the original
GAN architecture as follows. The generator ’s input is the
concatenation of the input data and a binary matrix that
represents the missing values. The discriminator learns to
reconstruct the mask matrix. Its input is the concatenation of
the generator’s output and a hint matrix, which reveals partial
information about the missingness of the original data. The
computation of the hint matrix incorporates the introduced
hyperparameterhint rate. A second hyperparameter α that
GAIN introduces helps to balance the generator ’s
performance for observed and missing values. For training,
we use Adam optimizer with default hyperparameters except
for the learning rate for the generator and the discriminator, 

Document 6: 
 obtain promising initial results, the combined evolution of solutions and their genetic opera-
tors is comparatively challenging, and it remains an open question whether autoconstructive
evolution is indeed superior to non-autoconstructive evolution.
Multi-armed bandits DAC can be cast as a multi-armed bandit problem, where each
conﬁguration represents an arm with an associated reward distribution. The agent is then
tasked to maximize the cumulative reward, i.e., “pull the arm“ that maximizes the reward.
Put diﬀerently, the agent aims to select a conﬁguration that works well for the current time
step. This setting of the bandit problem is particularly challenging because the reward
distributions associated with the respective arms change over time. In Fialho et al. (2010),
various multi-armed bandit approaches are compared with an approach that only considers
rewards obtained within a certain time frame (i.e., a sliding window) to allow for smoother
transitions between conﬁgurations. Note that in this work, the authors assume rewards to
be retrievable during an algorithm run. While this assumption holds for many optimization
approaches, it does not necessarily hold for all of them (e.g., the pre-processing/root node
phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions 

","The GGA++ approach is an extension of the Adaptive Genetic Algorithm (AGA) variant, but I couldn't find any information on the specific ""AAC approach"" called ""GGA"". However, it appears that there is a variation of the GA VaPS (Genetic Algorithm for Vehicle Scheduling and Planning System) lifetime scheme, which is related to the AGA.

In this context, GGA might refer to Genetic Genetic Algorithm, but I couldn't find any information on its specific idea or application.",2.6326260566711426,"The GGA (Genetic Algorithm with Genetic Engineering) approach, as described in the context, involves enhancing the traditional genetic algorithm by integrating a surrogate model to evaluate the quality of new configurations. The surrogate model predicts the rank an individual would achieve in a tournament rather than directly predicting solution quality or runtime, which is important in finding high-quality configurations. GGA employs a competitive and non-competitive population structure, where successful configurations from competitions are recombined through a crossover operator. This adaptation aims to focus on high-performing individuals, utilizing specialized splitting criteria within a random forest model to refine the search process in automatic algorithm configuration."
llama3.2:latest,"What is the idea behind the AAC approach called ""ISAC""?
","Document 1: 
 ISAC – Instance-Speciﬁc Algorithm Conﬁguration1
Serdar Kadioglu and Yuri Malitsky and Meinolf Sellmann and Kevin Tierney2
Abstract. We present a new method for instance-speciﬁc algorithm
conﬁguration (ISAC). It is based on the integration of the algorithm
conﬁguration system GGA and the recently proposed stochastic off-
line programming paradigm. ISAC is provided a solver with cate-
gorical, ordinal, and/or continuous parameters, a training benchmark
set of input instances for that solver, and an algorithm that com-
putes a feature vector that characterizes any given instance. ISAC
then provides high quality parameter settings for any new input in-
stance. Experiments on a variety of different constrained optimiza-
tion and constraint satisfaction solvers show that automatic algorithm
conﬁguration vastly outperforms manual tuning. Moreover, we show
that instance-speciﬁc tuning frequently leads to signiﬁcant speed-ups
over instance-oblivious conﬁgurations.
1 Introduction
When developing a new heuristic or complete algorithm for a con-
straint satisfaction or a constrained optimization problem, we fre-
quently face the problem of choice. There may be multiple branch-
ing heuristics that we can employ, different types of inference mech-
anisms, various restart strategies, or a multitude of neighborhoods to
choose from. Furthermore, the way in which the choices we make
affect one another is not readily known. The task of making these 

Document 2: 
 method is paired with D-MAB selection [38] described
above. A sliding window is also used by Fialho et al.[ 70].
They suggest increasing the reward with the time elapsed
since the last application of this operator and decreasing
the reward with the number of times the operator has been
applied within the window. The aim of this method is to adapt
quickly to (even subtle) changes of the dynamic environment.
Li et al.[ 129] suggested a sliding window that stores the rate
of improvement in the ﬁtness of the offspring as compared
to the parent. The sum of all these rewards in the window is
used by a ranking mechanism to assign credit to the operators.
A different credit assignment mechanism is Compass, sug-
gested by Maturana and Saubion [ 146]. Based on the concepts
found in [ 147] (see Section VI), an operator’s impact is
evaluated using measures of both ﬁtness and diversity in order
to calculate the exploration-exploitation balance achieved by
the operator. The assigned credit reﬂects how closely the
achieved balance is to an exploration-exploitation balance that
is required by a user-deﬁned schedule. Other tested credit as-
signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement 

Document 3: 
 in Ye et al. (2021) that performance gains can already be achieved when the algorithm
conﬁguration is adapted only once. Furthermore, the hyper-reactive approach of Ans´ otegui
et al. (2017) won several categories at the MaxSAT Evaluation 2016 (Argelich et al., 2016).
Thus, DAC oﬀers signiﬁcant potential for improving algorithms, however, it does require
algorithm designers to more deeply integrate their techniques with AC methods than was
performed in the past. In the following, we discuss the most frequently used approach to
DAC, reinforcement learning (RL). While it is the most popular choice, there also exist
other approaches such as policy portfolios, autoconstructive evolution, and multi-armed
bandits.
Reinforcement learning A challenge in DAC is to evaluate the quality of individual
parameter adjustments since feedback is obtained only at the end of processing a problem
instance. Due to this, it is natural to consider DAC within the realm of reinforcement
learning (we refer to this as DAC-RL). To this end, the DAC problem is modeled in terms of
a Markov decision process (MDP) (Puterman, 1994) based on which a policy π: I×Q→ Θ
is learned from data (Biedenkapp et al., 2020). As a reward signal, typically, the eventually
obtained performance value is propagated back to the agent. Using RL allows a wide variety
of researched mechanisms to be directly applied to DAC.
The roots of DAC extend to a variety of methods that use RL to control parameters of 

Document 4: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 5: 
 ACPP is closely related to the area of automatic algo-
rithm configuration, in which the task is to automatically
identify a high-quality configuration from a configuration
space. In the last few years, several high-performance AC
methods (which could handle considerable large configura-
tion spaces) such as ParamILS (Hutter et al. 2009), GGA
(Ans´otegui, Sellmann, and Tierney 2009), irace (L ´opez-
Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and Leyton-
Brown 2011) have been proposed. As a consequence, re-
cently there has been research interest in utilizing these
AC procedures to automatically identify useful portfolios
of configurations from large configuration spaces. Such at-
tempts were first done in constructing sequential portfolios.
The representative methods for solving this problem are Hy-
dra (Xu, Hoos, and Leyton-Brown 2010) and ISAC (Ka-
dioglu et al. 2010). The basic ideas of these methods were
then adapted to be used in constructing parallel portfolios
(i.e., ACPP), thus resulting in PARHYDRA and CLUSTER-
ING (Lindauer et al. 2017) (see the first section). The main
differences between the automatic construction of sequen-
tial portfolios and of parallel portfolios lie in two aspects: 

Document 6: 
 that ihvr ∈(0, 1], and a larger value of ihvr is better. IHVR can distinguish well between
MOEAs with a different performance, and can also save a lot of calculations. In this work,
IHVR is used as the performance metric Metric .
4.3. Automatic Construction Approach
Algorithm 1 presents the automatic construction approach for MOEAs/PAP . Starting
from an empty set (line 2), the approach constructs the PAP (denoted as P) iteratively.
Speciﬁcally, each iteration of the approach (lines 3–15) consists of two subsequent phases.
In the ﬁrst phase, an existing automatic algorithm conﬁguration tool, namely SMAC 3 [44],
is used to search in Θ to ﬁnd the algorithm that can improve the performance of the current
PAP to the largest extent (line 5), and then this algorithm is inserted into P (line 6). This
phase is similar to the commonly-used greedy approach in the automatic construction of
PAPs [27,31]. Additionally, we introduce a new phase, namely simpliﬁcation, as the second
phase in Algorithm 1. In this phase (lines 7–14), P would be simpliﬁed by removing the
member algorithms that do not contribute at all to its performance (meaning removing
these algorithms has no effect on the performance of P on the training set). Considering
the size of P is bounded (line 3), removing the redundant algorithms from P is meaningful
because this will leave space for new member algorithms that can improve the performance 

","The ISAC (Instance-Specific Algorithm Configuration) approach automatically configures an algorithm's parameters based on a solver, training benchmark set, and unseen problem instance. It first identifies the best-performing solver from the training instances using a search method or surrogate model, then applies this solver to the unseen instance without further configuration, making it a form of Automatic Algorithm Configuration (AAC).",2.2913591861724854,"ISAC, or Instance-Specific Algorithm Configuration, is an approach aimed at adapting algorithm parameters specifically for different instances of a problem. The fundamental idea is that ISAC leverages a training benchmark set of input instances and uses a feature vector to characterize any given instance. By doing so, ISAC provides high-quality parameter settings tailored to the specifics of new input instances. This method has been shown to outperform manual tuning and typically yields significant speed-ups compared to instance-oblivious configurations. In essence, ISAC aims to customize algorithm performance by aligning configurations closely with the characteristics of the problem instance at hand."
llama3.2:latest,"What is the difference between metaheuristics and heuristics?
","Document 1: 
 usually provide only sub-optimal solutions because they do
not attempt to escape from local optimum. These drawbacks
have led to the introduction of metaheuristics.
Metaheuristics are high-level methodologies or general al-
gorithmic templates, which generally do not adapt deeply
to speciﬁc problem(s) [4]. Hence, they usually can solve a
wide range of problems [2]. In fact, the preﬁx “meta”, which
means “upper level methodology”, indicates that metaheuristic
algorithms can be viewed as “higher level” heuristics. Hy-
brid approaches based on existing metaheuristic(s) are also
considered metaheuristics [5]. In last decades, metaheuristics
have received widespread attention from researchers and are
widely recognized as efﬁcient approaches for hard optimiza-
tion problems. A number of metaheuristic algorithms have
been developed and extensively applied, including Simulated
Annealing (SA) [6], Tabu Search (TS) [7], Evolutionary Algo-
rithms (EAs) [8], Ant Colony Optimization algorithm (ACO)
[9], Particle Swarm Optimization (PSO) [10], and so forth.
Most of metaheuristics are nature-inspired (inspired from some
principles in physics, biology and etc.), contain stochastic
components, and often have several free parameters that can
be set by users according to problem(s) at hand [4].
The setting of parameters (or parameter setting) has strong 

Document 2: 
 principle difference between similarity and statistical learning (i.e., the two alternative approaches to evaluate the dif-
ference and divergence between two data items). Similarity can be defined over a pair of data items/samples while sta-
tistical measures require a large (theoretically infinite) number of independent data observations.
Another promising direction is to build Turing's type-B random machines (or unorganized machines) (Jiang &
Crookes, 2019; Webster, 2012), also random Boltzmann machines, which can possibly lead to a generalized AI. The
inclusion of new neuro-scientific findings into XAI models will make research on XAI more rationalized, and vice
versa: such a cross-disciplinary exploitation will make XAI not only meaningful for AI researchers but also help solve
century-old challenges on how to understand human intelligence, ultimately. Open research questions in this area
include: (i) how best to determine the network/model architecture?; (ii) how best to extract and represent features?;
(iii) what are the best distance metrics and what are the implications?; (iv) which is the best optimization method?; and
(v) how to determine the best set of prototypes that represent the data best (if a prototype-based method is being used)?
CONFLICT OF INTEREST
The authors have declared no conflicts of interest for this article.
DATA AVAILABILITY STATEMENT
Data sharing is not applicable to this article as no new data were created or analyzed in this study. 

Document 3: 
 the same standard. (In contrast, SAT solver competitions have compared solvers with a complex scoring function that includes the performance of all competitors [25].) As formulated here, the differences between two solvers may be simply in their choice of even a single technique, heuristic, or policy that sustains performance diver-sity. Thus an algorithm portfolio can be thought of as a mixture of experts [26], in-cluding variable-ordering and value-ordering heuristics, restart policies, and nogood learning methods. In particular, even if only one heuristic is available, the portfolio could consist of the heuristic and its opposite, or the heuristic and random selection. 

Document 4: 
 (Eds.), Handbook of Metaheuristics (pp. 321–353). Kluwer Academic Publishers, Norwell, MA, USA.
Maron, O. & Moore, A. (1994). Hoeffding races: Accelerating model selection search for classiﬁcation
and function approximation. In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.), Advances in Neural
Information Processing Systems 7 (NIPS-94), volume 6, (pp. 59–66). Morgan Kaufmann Publishers, San
Francisco, CA, USA.
Mengshoel, O. J. (2008). Understanding the role of noise in stochastic local search: Analysis and experi-
ments. Artiﬁcial Intelligence, 172(8-9), 955–990.
Minton, S. (1993). An analytic learning system for specializing heuristics. In Bajcsy, R. (Ed.),Proceedings of
the Thirteenth International Joint Conference on Artiﬁcial Intelligence (IJCAI’93), (pp. 922–929). Morgan
Kaufmann Publishers, San Francisco, CA, USA.
Minton, S. (1996). Automatically conﬁguring constraint satisfaction programs: A case study. Constraints,
1(1), 1–40.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conﬂicts: A heuristic repair 

Document 5: 
 PSO. 
For the methodological part of the comparisons, there are far more 
studies. Statistical tests, for instance, lay at the core of prior contribu- 
tions on this matter. However, such contributions are frequently writ- 
ten from a statistical point of view –l i k e the one by Dem š ar [39] –
making it diﬃcult for researchers in this ﬁeld to embrace their method- 
ological recommendations. More recently, some tutorials have tried to 
bring together the ﬁelds of meta-heuristics and inferential statistics [4] . 
Some examples can be found in [40] , in which a statistical treatment is 
suggested for distinguishing between measurements of performance in 
adaptive evolutionary algorithms. Another good example is [41] , which 
shows that in a popular real-parameter benchmark (CEC’2005), con- 
ditions needed for running parametric hypothesis tests did not hold, 
and non-parametric tests were thus recommended. More recently, in 
[42] , some recommendations for the comparison of evolutionary algo- 
rithms are provided, which can be even extrapolated to machine learn- 
ing benchmarks. 
Another important issue from a methodological point of view is the 
assessment of the performance of bio-inspired algorithms from the per- 
spective of the experimental design. Some studies [5] provide general 
recommendations to design experiments for the comparison of algo- 
rithms in a similar way to what we do in this contribution. However, 
these recommendations are far more general as it targets a broader scope 

Document 6: 
 J. Durillo , K. Hammond , E. Hart , C.G. Johnson , et al. , A research agenda for meta- 
heuristic standardization, in: Proceedings of the XI metaheuristics international 
conference, 2015, pp. 1–3 . 
[106] J. Swan , S. Adraensen , A.E. Brownlee , C.G. Johnson , A. Kheiri , F. Krawiec , 
J. Merelo , L.L. Minku , E. Özcan , G.L. Pappa , et al. , Towards metaheuristics ”i n 
the large ”, arXiv preprint arXiv:2011.09821 (2020) . 
[107] M.A. Lones , Mitigating metaphors: a comprehensible guide to recent nature-in- 
spired algorithms, SN Computer Science 1 (1) (2020) 49 . 
[108] M. Jain, V. Singh, A. Rani, A novel nature-inspired algorithm for optimiza- 
tion: squirrel search algorithm, Swarm Evol Comput 44 (2019) 148–175, 
doi: 10.1016/j.swevo.2018.02.013 . 
[109] A.W. Mohamed, A.A. Hadi, A.K. Mohamed, Gaining-sharing knowledge 
based algorithm for solving optimization problems: a novel nature- 

","Metaheuristics are high-level methodologies or general algorithmic templates that do not adapt deeply to specific problem(s), whereas traditional heuristics provide sub-optimal solutions by focusing on local optima. Metaheuristics are considered ""higher level"" heuristics, offering a wide range of problem-solving capabilities without deep adaptation to individual problems.",2.0934243202209477,"The main difference between metaheuristics and heuristics lies in their scope and adaptability to specific problems. Heuristics are commonly problem-specific techniques that aim to produce solutions more quickly when classical methods are too slow. They typically focus on a specific problem domain and may produce sub-optimal solutions without addressing the possibility of escaping local optima.

On the other hand, metaheuristics are higher-level methodologies or algorithmic frameworks that can be applied to a wider range of problems. They do not adapt deeply to specific problems; instead, they provide a more generalized approach to problem-solving that allows for the exploration of the solution space and escaping local optima. Metaheuristics often incorporate stochastic components and are inspired by natural processes, making them versatile for tackling hard optimization problems across various domains."
llama3.2:latest,"What factors and challenges influenced the development of the Automatic Algorithm Configuration approaches?
","Document 1: 
 ods for algorithm development. The methods used in this context 
include experimental design techniques [2,29] , racing approaches 
[20] , and algorithmic methods for parameter conﬁguration, such 
as heuristic search techniques [3,10,41,73,81] , and statistical mod- 
eling approaches [11,43] . These methods have led to an increasing 
automatization of the algorithm design and parameter setting pro- 
cess. 
Automatic algorithm conﬁguration can be described, from a ma- 
chine learning perspective, as the problem of ﬁnding good param- 
eter settings for solving unseen problem instances by learning on 
a set of training problem instances [19] . Thus, there are two clearly 
delimited phases. In a primary tuning phase, an algorithm con- 
ﬁguration is chosen, given a set of training instances representa- 
tive of a particular problem. In a secondary production (or test- 
ing) phase, the chosen algorithm conﬁguration is used to solve un- 
seen instances of the same problem. The goal in automatic algo- 
rithm conﬁguration is to ﬁnd, during the tuning phase, an algo- 
rithm conﬁguration that minimizes some cost measure over the 
set of instances that will be seen during the production phase. 
In other words, the ﬁnal goal is that the conﬁguration of the al- 
gorithm found during the tuning phase generalizes to similar but 
unseen instances. The tuning phase may also use automatic conﬁg- 
uration methods repeatedly while engineering an algorithm [71] . 

Document 2: 
 diﬀerent problems (SAT, MIP, ASP, etc.) of varying complexity (number of variables and
problem instances) for tasks of runtime or quality conﬁguration. For DAC, the DACBench
has been proposed (Eimer et al., 2021), although this does not support DAC settings en-
visioned, e.g., by hyper-reactive search. As an alternative to such libraries, AC methods
can also be benchmarked by using surrogate models that are trained on test instances in
advance, resulting in cheaper evaluations when testing (Eggensperger et al., 2018). The ex-
isting benchmarks fail to cover other conﬁgurations settings like the realtime conﬁguration
setting or the conﬁguration of multi-objective target algorithms.
11.4 Novel AC Methodologies
AC methods have become extremely sophisticated and cover a wide range of methodologies
including evolutionary algorithms, statistical tests, and learned surrogate models. There
nonetheless remain opportunities to improve current methods and create better AC algo-
rithms. We note that our goal in this section is not necessarily to specify the methodologies
of the future, but rather to identify the key challenges that remain in the hopes that engaged
466
A Survey of Methods for Automated Algorithm Configuration
readers will ﬁll these gaps with their ideas. To this end, we discuss several challenges faced
by AC approaches: comparing/ranking conﬁgurations, generating new conﬁgurations, and
selecting instances.
Comparing/ranking conﬁgurations This challenge can be summarized as follows: 

Document 3: 
 available for detecting (approximate) independencies. Although there is some evi-
dence that GGA can solve some moderately difﬁcult conﬁguration problems more
effectively than FocusedILS without capping [2], it appears to be unable to reach
the performance of FocusedILS version 2.3 with aggressive capping on the most
challenging conﬁgurations problems [40]. Unfortunately, GGA also offers less ﬂex-
ibility than FocusedILS in terms of the performance metric to be optimised. More
algorithm conﬁguration procedures based on evolutionary algorithms are covered in
Chapter 2 of this book.
Finally, work originating from the Ph.D. project of Hutter [32] has recently over-
come two major limitations of the sequential model-based optimisation methods
discussed in Section 3.4 of this chapter by introducing a procedure that can han-
dle categorical parameters while explicitly exploiting the fact that performance is
evaluated on a set of problem instances. There is some evidence that this procedure,
dubbedSequential Model-based Algorithm Conﬁguration (SMAC) , can, at least on
some challenging conﬁguration benchmarks, reach and sometimes exceed the per-
formance of FocusedILS [43], and we are convinced that, at least in cases where
the parameter response of a given target algorithm is reasonably regular and per-
formance evaluations are very costly, such advanced SMBO methods hold great
promise.
3.6 Conclusions and Future Work
Automated algorithm conﬁguration and parameter tuning methods have been de- 

Document 4: 
 Decision and Operation Technologies Group,
Bielefeld University, Bielefeld, Germany
Abstract
Algorithm conﬁguration (AC) is concerned with the automated search of the most
suitable parameter conﬁguration of a parametrized algorithm. There is currently a wide
variety of AC problem variants and methods proposed in the literature. Existing reviews
do not take into account all derivatives of the AC problem, nor do they oﬀer a complete
classiﬁcation scheme. To this end, we introduce taxonomies to describe the AC problem and
features of conﬁguration methods, respectively. We review existing AC literature within
the lens of our taxonomies, outline relevant design choices of conﬁguration approaches,
contrast methods and problem variants against each other, and describe the state of AC in
industry. Finally, our review provides researchers and practitioners with a look at future
research directions in the ﬁeld of AC.
1. Introduction
Diﬃcult computational problems must be regularly solved in many areas of industry and
academia, such as constraint satisfaction problems, Boolean satisﬁability problems (SAT),
vehicle routing problems, ﬁnding a proper machine learning model for a given dataset,
or computing highly complex simulations. Algorithms that were developed to solve such
problems usually have parameters that strongly inﬂuence the behavior of the respective
algorithm and also, for example, the runtime that is required to solve problem instances
©2022 AI Access Foundation. All rights reserved.
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney 

Document 5: 
 better performance than the top-ranked solver from the competition.
9. Related Work
Many researchers before us have been dissatisﬁed with manual algorithm conﬁguration, and various
ﬁelds have developed their own approaches for automatic parameter tuning. We start this section
with the most closely-related work—approaches that employ direct search to ﬁnd good parameter
conﬁgurations—and then describe other methods. Finally, we discuss work on related problems,
such as ﬁnding the best parameter conﬁguration or algorithm on a per-instance basis, and approaches
that adapt their parameters during an algorithm’s execution (see also Hoos, 2008, for further related
work on automated algorithm design).
8. BasicILS was used, because FocusedILS had not yet been developed when that study was conducted.
297
HUTTER , HOOS , LEYTON -BROWN & ST ¨UTZLE
9.1 Direct Search Methods for Algorithm Conﬁguration
Approaches for automated algorithm conﬁguration go back to the early 1990s, when a number of
systems were developed for adaptive problem solving. One of these systems is Composer (Gratch
& Dejong, 1992), which performs a hill-climbing search in conﬁguration space, taking moves if
enough evidence has been gathered to render a neighbouring conﬁguration statistically signiﬁcantly
better than the current conﬁguration. Composer was successfully applied to improving the ﬁve
parameters of an algorithm for scheduling communication between a collection of ground-based
antennas and spacecrafts (Gratch & Chien, 1996). 

Document 6: 
 ration.
1 Introduction
We consider the problem of automatic solver conﬁguration. Practically all solvers have
parameters that are partly ﬁxed by the programmer and partly set by the user. In recent
years, systems have been devised which automate the task of tuning parameters for a
given set of training instances that are assu med to represent typical instances for the
target algorithm.
There are several motivations for such an automation, the ﬁrst being that it is of
course time consuming to tune parameters and it may lead to better results when leaving
the conﬁguration of solvers to a computer rather than doing it by hand.
Moreover, it is conceivable that the existence of an effective tuning environment
will cause algorithm developers to paramete rize more aspects of their algorithms and
thus leave more freedom for algorithmic solutions that are automatically tailored to the
problems of individual users. In particular, many of the SAT solvers that are available
today have parameters which cannot be set through the command line. These parameters
have been ﬁxed to values that the developers have found beneﬁcial without knowledge
about the particular instances a user may want to use the solver for. Automatic parameter
tuning allows solvers to adapt to the ﬁnal environment in which they need to perform.
After being shipped, rather than relying on default parameters, an algorithm can be
⋆ This work was partly supported by the projects TIN2007-68005-C04-04 and TIN2006-15662-
C02-02 funded by the MEC, and by the the National Science Foundation through the Ca-
reer: Cornﬂower Project (award number 0644113). 

","The development of Automatic Algorithm Configuration (AAC) approaches has been influenced by several factors and challenges. Some of the key ones include:

1. **Computational Complexity**: As computational resources became more powerful, solving problems using brute force methods became less feasible. AAC approaches emerged to address this issue.
2. **Increased Problem Sizes**: The size and complexity of problem instances have increased significantly over time, making it challenging for humans to configure algorithms manually.
3. **Algorithm-Dependent Configuration Space**: Each algorithm has its own configuration space, which can be difficult to navigate. AAC approaches aim to reduce the dimensionality of this space by learning optimal configurations automatically.
4. **Limited Human Expertise**: As problem instances grow in size and complexity, human expertise becomes limited, making it challenging for humans to configure algorithms manually.
5. **Time-Consuming Configuration Tasks**: Manual configuration tasks can be time-consuming, especially when dealing with large numbers of problems or algorithm variations.
6. **Exploration-Exploitation Trade-off**: AAC approaches often face the challenge of balancing exploration (trying new configurations) and exploitation (using known good configurations).
7. **Scalability**: As problem instances grow in size, AAC approaches must scale to handle larger inputs and provide efficient solutions.

Challenges:

1. **Optimization Difficulty**: Finding optimal configuration settings for a given algorithm is often an NP-hard problem.
2. **Overfitting and Underfitting**: AAC models can overfit or underfit the data, leading to poor performance on unseen problems.
3. **Noise in the Data**: Noisy or inaccurate data can negatively impact AAC model performance.
4. **Lack of Domain Knowledge**: AAC approaches may not incorporate domain-specific knowledge or constraints, leading to suboptimal solutions.
5. **Interpretability and Explainability**: Understanding how AAC models arrive at their decisions can be challenging, making it difficult to interpret results.

To address these challenges, researchers have developed various AAC approaches, including:

1. **Hyperband**: A Bayesian optimization approach for hyperparameter tuning.
2. **Tree-structured Parzen Estimator (TPE)**: A model-based optimization approach that uses a tree structure to efficiently search the configuration space.
3. **Random Forest-Based Methods**: Using random forests as a surrogate model to approximate the performance of expensive-to-evaluate algorithms.
4. **Deep Learning-Based Methods**: Employing deep neural networks to learn optimal configurations for complex problems.

These AAC approaches have shown promise in solving challenging problems, but ongoing research aims to address the remaining challenges and improve their efficiency and effectiveness.",8.086359977722168,"Factors influencing the development of Automatic Algorithm Configuration (AC) approaches include the need for efficient tuning of algorithm parameters to enhance performance and runtime for diverse computational problems. These problems often exhibit complexity due to the variation in the number of variables and instances.

Challenges faced by AC include:

1. **Comparing and Ranking Configurations**: Ensuring that different configurations can be effectively evaluated and ranked based on performance.

2. **Generating New Configurations**: Developing methods to produce novel configurations that may outperform existing ones.

3. **Selecting Instances**: Identifying which problem instances to use for tuning the configurations so that they generalize well to unseen instances.

Additionally, the advancement of methodologies like evolutionary algorithms, surrogate models, and sequential model-based optimization has improved AC, but there are still opportunities for refinement and development in areas not comprehensively covered by existing benchmarks and methodologies."
llama3.2:latest,"What are the main benefits of AAC?
","Document 1: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 2: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 3: 
 phase of solving mixed-integer programs), and also often does not hold for simulations.
463
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
11. Research Directions
Many fruitful avenues of research remain in the area of AC. We formulate research directions
with three themes in mind. First, we consider what is required for AC techniques to be
more widely adopted in industry. Second, we investigate algorithm conﬁguration settings
that have not yet been considered. Third, we consider what methodological advances could
move the ﬁeld forward.
11.1 Industry Adoption
Compared to AC, HPO has begun to be adopted in both industry and academia (Van der
Blom et al., 2021). Furthermore, HPO is the focus of a number of start-ups, such as
SigOpt (which was acquired by Intel (Intel Corporation, 2019)), MindsDB, pecan.ai, and
dotData, among many more. AC, however, has received comparatively little fanfare or
adoption. While some well-known AC tools have public releases, such as SMAC (Lindauer
et al., 2021), irace (L´ opez-Ib´ anez et al., 2016), and GGA (Ans´ otegui et al., 2009), we are
unaware of widespread use of these in industry. The company Optano GmbH released a 

Document 4: 
 a policy to adapt the conﬁguration dynamically. Note that even realtime AC commits to a
single conﬁguration when running a given instance, while DAC has the freedom to adjust
the conﬁguration according to target algorithm behavior during execution. Similar to oﬄine
AC, DAC can either focus on ﬁnding a policy for a set of problem instances or a policy that
is tailored towards a single problem instance (i.e., per-instance algorithm conﬁguration).
Two requirements must be met to implement DAC: (1) the algorithm in question needs
to support dynamic changes in its conﬁguration and (2) runtime information must be pro-
vided to describe the current state of the target algorithm.
DAC approaches consider two diﬀerent types of features: instance features I, which do
not change during target algorithm execution, and features encoding the internal state Q
of the algorithm. Examples of state features include the current iteration of a local search
algorithm, the current restart number of a SAT method, or the current solution quality for
optimization techniques.
461
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
Figure 7: Illustration of dynamic AC as presented in (Biedenkapp et al., 2020)
Biedenkapp et al. (2020) provide the ﬁrst formal deﬁnition of the DAC setting, however,
there is a signiﬁcant amount of earlier work for learning dynamic conﬁguration policies
(Lagoudakis & Littman, 2000, 2001; Pettinger & Everson, 2002). Such earlier works use 

Document 5: 
 cess (see (Hutter et al. 2009) for a comprehensive review),
which is usually referred to as automatic algorithm con-
ﬁguration (AAC) (Hoos 2012). Many AAC methods such
as ParamILS (Hutter et al. 2009), GGA/GGA+(Ans´otegui,
Sellmann, and Tierney 2009; Ans ´otegui et al. 2015), irace
(L´opez-Ib´a˜nez et al. 2016) and SMAC (Hutter, Hoos, and
Leyton-Brown 2011) have been proposed in the last few
years. They have been used for boosting the algorithm’s per-
formance in a wide range of domains such as the boolean
satisﬁability problem (SAT) (Hutter et al. 2009), the travel-
ing salesman problem (TSP) (L´opez-Ib´a˜nez et al. 2016; Liu,
∗Corresponding author
Copyright c⃝ 2020, Association for the Advancement of Artiﬁcial
Intelligence (www.aaai.org). All rights reserved.
Tang, and Yao 2019), the answer set programming (ASP)
(Hutter et al. 2014) and machine learning (Feurer et al. 2015;
Kotthoff et al. 2017).
Despite the notable success achieved in application, the 

Document 6: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

","Augmentative and Alternative Communication (AAC) provides several benefits, including:

1. **Improved communication**: AAC helps individuals with disabilities or impairments express their thoughts, needs, and wants more effectively.
2. **Increased independence**: By providing an alternative means of communication, AAC enables individuals to participate more fully in daily activities, social interactions, and educational pursuits.
3. **Enhanced self-esteem and confidence**: Effective use of AAC can boost an individual's sense of identity and self-worth, as they feel more able to express themselves and communicate with others.
4. **Better relationships**: AAC facilitates communication between individuals with disabilities or impairments and their caregivers, family members, educators, and peers, leading to stronger, more meaningful relationships.
5. **Reduced frustration and anxiety**: By providing a clear and efficient way to communicate, AAC can help alleviate feelings of frustration and anxiety that often accompany unmet communication needs.
6. **Personalization and customization**: AAC systems can be tailored to an individual's specific needs, interests, and personality, making them more engaging and effective.
7. **Support for learning and education**: AAC can facilitate access to education and learning opportunities, enabling individuals with disabilities or impairments to participate fully in academic environments.
8. **Improved caregiver-child relationships**: In cases where caregivers are also providing AAC support, it can strengthen the bond between caregiver and child, promoting a more positive and supportive relationship.

Overall, AAC offers numerous benefits that can significantly improve an individual's quality of life, social interactions, and communication abilities.",4.995705842971802,"The main benefits of Automatic Algorithm Configuration (AAC) include:

1. **Performance Improvement**: AAC methods enhance the performance of algorithms across various domains, such as the boolean satisfiability problem (SAT), the traveling salesman problem (TSP), answer set programming (ASP), and machine learning applications. This improvement is achieved by automatically tuning algorithm parameters to better fit specific problem instances.

2. **Dynamic Adjustment**: AAC enables the use of dynamic configurations that can adapt during execution, allowing for better performance based on the current state of the algorithm and the problem. This adaptability is especially beneficial when dealing with different problem instances and conditions.

3. **Diversity of Methods**: There are several AAC methods available, such as ParamILS, GGA/GGA+, irace, and SMAC, which offer various strategies for parameter optimization. This diversity allows for flexibility in choosing the most suitable method based on the specific context or requirements of the project.

4. **Reduction in Need for Manual Tuning**: By automating the configuration process, AAC reduces the burden on domain experts who would otherwise need to manually optimize parameters, saving time and resources while also potentially achieving better configurations than manual tuning.

Overall, AAC supports the effective utilization of algorithms by streamlining the configuration process and improving their adaptability to diverse problem scenarios."
llama3.2:latest,"Please provide an overview of the types of AAC methods.
","Document 1: 
 the best solver in the solver class, termed the auto- 
matic algorithm configuration (AAC) in the litera- 
ture [16 ]. More specifically, given a solver class, the 
solver that achieves the best overall performance on 
the training instances is first obtained by some search 
method. Then, the solver is directly applied to an un- 
seen problem instance without a further configura- 
tion process. 
A typical AAC method consists of two main com- 
ponents: sampling in the solver class and evaluat- 
ing a candidate solver. Sampling in the solver class 
is mostly implemented based on search-based meth- 
ods and model-based methods. Search-based meth- 
ods such as hi l l climbing [50 ], local search [8 ] and 
E A s [51 ] have been used to sample solvers, since 
they make minimal assumptions about the solver 
representation and do not require gradients. Addi- 
tionally, some researchers have also explored using 
experimental design [52 ,53 ], such as the full fac- 
torial experimental design, to generate the starting 
points for the search. In comparison, model-based 
methods are more similar to the performance predic- 
tion model approach. They seek to capture the de- 
pendencies between solvers and their performance, 
by training surrogate models on the performance 
data collected during the configuration process. On 

Document 2: 
 as expected, as N and K get larger, the estimation error gets
smaller, and ˆuSN (θ) will converge to u(θ) with probability
1 with N →∞ and K →∞ . Third, Corollary 1 shows that,
for the estimator ˆuSN (θ∗) which are widely used in current
AAC methods, the gain on error reduction decreases rapidly
asN and K get larger (which are also shown in Figure 2 in
the experiments), and the effects of increasingN and K also
depend on ¯σ2
WI and ¯σ2
AI, two quantities varying across dif-
ferent algorithm conﬁguration scenarios. Thus for enhanc-
ing current AAC methods, instead of ﬁxingN as a large
number (e.g., SMAC sets N to 2000 by default) and using
as many training instances as possible, it is more desirable
to use differentN and K according to the conﬁguration sce-
nario considered, in which case N and K may be adjusted
dynamically in the conﬁguration process as more data are
gathered to estimate¯σ2
WI and ¯σ2
AI.
Experiments
In this section, we present our experimental studies. First
we introduce our experiment setup. Then, we verify our the-
oretical results in two facets: 1) comparison of different per-
formance estimators; 2) the effects of different values ofm
(the number of considered conﬁgurations), N (the number
of runs of θ to estimate u(θ)) and K (the number of training 

Document 3: 
 Non-general AC SMBO approaches Several SMBO approaches exist to perform a
limited form of AC, i.e., on only a single instance. We include these methods due to their
historical importance to the ﬁeld of AC, as well as because they may inspire new general AC
4. Note that for simplicity we assume that g is deterministic here.
443
Schede, Brandt, Tornede, Wever, Bengs, H¨ullermeier & Tierney
approaches. Most SMBO based AC approaches are based on the idea of sequential kriging
meta-modelling (Huang et al., 2006) (SKO) and sequential parameter optimization (SPO)
(Bartz-Beielstein et al., 2005), both of which are based on eﬃcient global optimization (Jones
et al., 1998). While the latter is a classical approach to black box function optimization using
BO, both SPO and SKO constitute extensions to noisy black box functions; an assumption
that is much more realistic for AC. However, both of these approaches still have potential
drawbacks. Some of these are ﬁxed by SPO + (Hutter et al., 2009a), which improves the
intensiﬁcation scheme, and time-bounded SPO (TB-SPO) (Hutter et al., 2010b), which
generalizes SPO+ to work under (potentially tight) time constraints instead of considering
the number of function evaluations as a stopping criterion.
5.2 General Model-based AC Methods 

Document 4: 
 attempted. Nonetheless, the AC literature shows a surprising amount of hybridization of
local search, evolutionary and model-based methods. We hypothesize that there is still sig-
niﬁcant progress that can be made in the area of AC, despite the sophistication of current
methods, and are encouraged by the signiﬁcant increase in attention the ﬁeld has received,
in particular through the spread of HPO techniques. Finally, we especially encourage re-
searchers to address the real-world usability of AC techniques to ensure that the promising
performance gains the AC community is seeing can beneﬁt the world at large.
Acknowledgements
This work was partially supported by the German Research Foundation (DFG) within the
Collaborative Research Center “On-The-Fly Computing” (SFB 901/3 project no. 160364472)
and by the research training group “Dataninja” (Trustworthy AI for Seamless Problem
Solving: Next Generation Intelligence Joins Robust Data Analysis) funded by the Ger-
man federal state of North Rhine-Westphalia. We also would like to thank the anonymous
reviewers for their suggestions on the ﬁrst version of this manuscript.
13. Appendix
To help the reader navigate though the jungle of AC, we provide additional resources.
Table 6 contains a list of abbreviations with terms related to AC used within this work. In
addtion, we provide a list of software resources (Table 7) that contains currently available
tools for AC. We only include software that is widely used.
469 

Document 5: 
 algorithm conﬁgurators. To this end, we propose two classiﬁcation schemes: one for AC
problems, and one for algorithm conﬁgurators. Based on this, we structure and summarize
the available literature and classify existing problem variants as well as approaches to AC.
The remainder of the paper is structured as follows. First, in Section 2, we give a formal
introduction into the setting of algorithm conﬁguration, specify the scope of this survey, and
discuss the relation between AC, AS and HPO. In Section 3, we present the classiﬁcation
schemes for AC problems and approaches that are used, in turn, to describe and compare
existing algorithm conﬁgurators. In Sections 4 and 5, we survey algorithm conﬁguration
methods grouped by the property of whether these methods are model-free or leverage
a model respectively. Section 6 deals with theoretical guarantees that can be obtained.
Diﬀerent problem variants, such as realtime AC, instance-speciﬁc vs. feature-based, multi-
objective, and dynamic AC are discussed in Sections 7 to 10. Eventually, with the help of
our classiﬁcation schemes, we elaborate on appealing research directions in Section 11 and
conclude this survey in Section 12. A list of abbreviations used in this work can be found in
Table 6. In addition, we provide a list of useful software in Table 7. We note, however, that
this list is by no means exhaustive; it is meant to provide an idea about available software
at the time of publication.
2. Problem Formulation
2.1 Algorithm Conﬁguration
To describe the AC problem more formally, we introduce the following notation that is sim- 

Document 6: 
 signment methods are based on domination between operators
and Pareto fronts. Compass was paired with probability match-
ing selection, but was also combined with D-MAB in [ 143].
Except for the widely used multiarmed bandit approach dis-
cussed so far, AOS has also been treated as a full reinforcement
learning problem by Sakurai et al.[ 173], Chen et al.[ 33] and
Pettinger and Everson [ 160]. Unlike the previous approaches,
these methods include the notion of state that is deﬁned
using feedback from the EA. For each distinct state, separate
preferences are learned for each operator and selection of the
operator to apply is based on the current state of the search.
An important issue with operator selection (as with all
adaptive parameter control) is the feedback used for control.
Veerapen et al .[ 198] presented and compared various utility
measures for variation operators. These measures are based on
a combination of exploration and exploitation measures and
use Pareto-dominance to evaluate operator utility. In another
paper, Whitacre et al .[ 204] make a distinction between
the source of feedback and any further statistical treatment
(a notion further elaborated in [112]). Several feedback sources
are suggested (including whether the offspring survives or
the number of generations it remains in the population).
The data received by these sources can be treated by av-
eraging them or by detecting the outliers in the sample. 

","Augmentative and Alternative Communication (AAC) refers to a range of methods used to support individuals with communication difficulties. There are several types of AAC methods, each with its own strengths and weaknesses. Here's an overview:

1. **Picture Communication Symbols (PCS)**: A visual system using pictures to represent words or concepts. It is often used by children and individuals with limited vocabulary.
2. **Symbol Sticker Systems**: Similar to PCS, but uses stickers instead of pictures. This method is also popular among children.
3. **Picture Exchange Communication System (PECS)**: A specific type of PCS that focuses on exchanging objects for a desired item, such as an object or a toy.
4. **Core Word Boards**: Simple boards with a limited number of words or symbols, often used in combination with other AAC methods.
5. **Vocabulary-Based Systems**: These systems use vocabulary-based symbols to represent specific words or concepts.
6. **Typographic Systems**: These systems use fonts and typography to convey meaning, often used by individuals who can spell out messages using the alphabet.

**Natural Language Processing (NLP) Methods**

1. **Simple Sticks**: A naturalistic method that uses everyday objects as communication aids.
2. **Token Boards**: A simple system that uses tokens or points to represent words or concepts.
3. **Picture Communication Symbols with Expanded Vocabulary (PCS-EV)**: An extension of PCS, which adds more vocabulary and complexity.

**Digital AAC Methods**

1. **Tablet-based AAC Apps**: Various apps available for tablets, such as Proloquo2Go, TouchChat, and LAMP Words For Life.
2. **Wearable Devices**: Wearable devices like communication wristbands or smartwatches that allow users to communicate through text-to-speech output.
3. **Virtual Assistants**: Voice assistants like Siri, Alexa, or Google Assistant can be used as AAC tools.

**Other Methods**

1. **Analog Communication Aids**: Manual communication aids like writing boards, communication books, and sign language.
2. **Support Person Systems**: Systems that rely on a support person to facilitate communication for individuals who cannot communicate independently.

Each method has its unique benefits and challenges, and the choice of AAC method depends on individual needs, preferences, and abilities.",6.557590961456299,"Automatic Algorithm Configuration (AAC) methods can be categorized into two main components: sampling in the solver class and evaluating candidate solvers. 

1. **Sampling Methods:**
   - **Search-based Methods:** These methods include techniques such as hill climbing, local search, and evolutionary algorithms (EAs). They typically make minimal assumptions about the solver structure and do not require gradient information, allowing for a broad exploration of the solver space.
   - **Model-based Methods:** These methods aim to capture performance dependencies between solvers and their configurations by training surrogate models based on performance data from the configuration process. This approach is akin to performance prediction techniques that can improve the effectiveness of the sampling process.

2. **Evaluation Strategies:**
   - The evaluation of a candidate solver often employs different performance estimators, as indicated by the estimation errors that decrease with larger sample sizes. The performance of the estimator can depend on various factors, including the number of training instances and the specific configuration scenarios at hand.

3. **Specialized Approaches:**
   - **Sequential Model-based Optimization (SMBO):** SMBO approaches are significant in the AAC landscape and involve techniques like sequential kriging and parameter optimization, which are tailored to optimize black-box functions, especially those affected by noise.
   - **Hybrid Approaches:** There is a noted hybridization of local search, evolutionary techniques, and model-based methods in AAC literature, suggesting that combining these strategies can lead to advancements in performance.

4. **Adaptive Parameter Control:** Techniques that incorporate feedback mechanisms, often guided by reinforcement learning principles, are also part of AAC methods. These methods adjust parameters dynamically based on the observed performance of solutions during the search process.

The landscape of AAC methods continues to evolve, with researchers encouraged to explore new directions and enhance the usability of these techniques in real-world applications."
llama3.2:latest,"Are Parallel Algorithm Portfolios connected with financial portfolios?
","Document 1: 
 algorithms, and thereby combine parameter tuning and an algo-rithm portfolio in the same framework. We are pursuing this avenue as well. The performance of any algorithm portfolio is, of course, bounded by that of an or-acle. The combination of algorithms as black boxes eliminates any opportunity to im-prove an individual algorithm. In contrast, parallelism can be achieved by a variety of problem decomposition methods (e.g., search space splitting), as discussed in Section 3. Although the results of recent SAT solver competitions suggest that a well-designed algorithm portfolio outperforms decomposition methods on a small number 

Document 2: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 3: 
 automatically producing effective parallel portfolios from multiple solvers, such as ppfolio and pfolioUZK, while exploiting 
the rich design spaces of these component solvers.
Katsirelos et al. [46] showed that an effective parallelization of a CDCL SAT solver does not merely hinge on picking a 
good clause sharing strategy, since it is not straightforward to obtain shorter resolution proofs by parallelization without 
essential changes of the underlying sequential reasoning mechanism. Our ACPP does not aim at parallelizing the resolution 
proof, but rather runs multiple algorithms and algorithm conﬁgurations in parallel, in order to maximize the probability 
that at least one of them ﬁnds a short proof quickly.
2.2. Related work
Well before there was widespread interest in multi-core computing, the potential beneﬁts offered by parallel algorithm 
portfolios were identiﬁed in seminal work by Huberman et al. [39]. Their notion of an algorithm portfolio is inspired by the 
use of portfolios of assets for risk management in ﬁnance and amounts to running multiple algorithms concurrently and 
independently on the same instance of a given problem, until one of them produces a solution. Gomes et al. [27] further 
investigated conditions under which such portfolios outperform their component solvers. Both lines of work considered 
prominent constraint programming problems (graph coloring and quasigroup completion), but neither presented methods 
for automatically constructing portfolio solvers. Parallel portfolios have since made practical impact, both in cases where 
the allocation of computational resources to algorithms in the portfolio is static [62,76] and where the component solvers 

Document 4: 
 is crucial for the effectiveness of any parallel portfolio.
According to (1), the performance of a parallel portfolio on
788 IEEE TRANSACTIONS ON CYBERNETICS, VOL. 52, NO. 2, FEBRUARY 2022
Algorithm 1 GAST
Input: base solvers B with conﬁguration space C; number of
component solvers k; instance set I; performance metric m;
algorithm conﬁgurator AC; independent conﬁgurator runs n; time
budgets tC, tV , tI for conﬁguration, validation and instance
generation respectively
Output:parallel portfolio c1:k
1: for i ← 1: k do
2: /*———— conﬁguration phase ———————*/
3: for j ← 1: n do
4: obtain a portfolio cj
1:i by running AC on conﬁguration space
{c1:i−1}×{ c|c ∈ C} using m for time tC
5: end for
6: validate c1
1:i, ...,cn
1:i on I using m for time tV
7: let c1:i ← arg mincj
1:i|j∈{1,...,n} P(cj
1:i, I) be the portfolio with
the best validation performance
8: /*———— instance-generation phase ————–*/ 

Document 5: 
 work relevant to parallel algorithm portfolios in-cludes online learning [2, 16] and methods that split problems [29, 30].) Effectively, the construction of a parallel algorithm portfolio to solve as many training problems as possible on K processors is an integer-programming (IP) prob-lem. The goal is to find the schedule S that specifies the time allotments to all algo-rithms on all processors, such that no problem can receive more than B time from all the processors together, and the total number of problems solved is a maximum. The expression (1 - )ij(tkj)) is 1 if problem xi is unsolved by algorithm aj after time tkj allo-cated to it 

Document 6: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

","Yes, the concept of parallel algorithm portfolios is related to financial portfolios. The idea was inspired by the use of portfolios of assets for risk management in finance, where multiple algorithms are run concurrently and independently on the same instance of a given problem until one produces a solution.",1.9046318531036377,"Yes, Parallel Algorithm Portfolios are conceptually connected with financial portfolios. The notion of an algorithm portfolio is inspired by the use of portfolios of assets in finance for risk management. In both cases, the idea is to run multiple independent strategies (algorithms or financial assets) concurrently to maximize the chances of achieving a favorable outcome (finding a solution in the case of algorithms, or achieving financial returns in the case of investments). This parallel processing allows for greater resilience and effectiveness, as it leverages the strengths of diverse approaches to address a problem or market condition."
llama3.2:latest,"What factors and challenges influenced the development of the Parallel Algorithm Portfolios?
","Document 1: 
 training set as well as for constructing the solvers.
This article studies effective construction approaches for
parallel portfolios that are less affected by nonrepresentative
training data. The term “parallel portfolio” [25], [26] refers
to a portfolio/set of solvers that is run independently in par-
allel when solving a problem instance (see Section III-A).
As a form of solvers, parallel portfolios have several impor-
tant advantages. First, exploiting parallelism has become very
important in designing efﬁcient solvers for computationally
hard problems, considering the great development and the
wide application of parallel computing architectures [27] (e.g.,
multicore CPUs) over the last decade. Parallel portfolios
employ parallel solution strategies and, thus, could easily make
effective use of modern hardware. Second, utilizing several
different solvers (as in parallel portfolios) is a simple yet
effective strategy for solving computationally hard problems.
Such an idea has also been realized in the form of sequential
portfolios [28], [29], which try to select the best solvers for
solving a problem instance, and adaptive solvers, such as adap-
tive parameter control [30]–[33]; reactive search [34], [35];
and hyper-heuristics [36]–[38], which seek to dynamically
determine the best solver setting while solving a problem
instance. In principle, all these methods need to involve some
mechanisms (e.g., selection or scheduling) to appropriately 

Document 2: 
 Historically, the parallel computing community quickly 
adopted combinatorial search as a playground for applications.
Search algorithms have the advantage of being conceptually 
simple (think of the most basic backtrack-style algorithm) and 
computationally demanding due to the (usually) exponential
size of the search space. In contrast, the search community did 
not really focus its research on parallelizing. The lack of proper 
Articles 
SUMMER 2013   99 Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 
Seven Challenges in 
Parallel SAT Solving 
Youssef Hamadi and Christoph M. Wintersteiger 
n This article provides a broad overview of the 
state of the parallel SAT solving field. A set of 
challenges to researchers is presented that, we 
believe, must be met to ensure the practical
applicability of parallel SAT solvers in the 
future. All these challenges are described infor -
mally but put into perspective with related 
research results, and a (subjective) grading of 
difficulty for each of them is provided.
infrastructure and, for many, the feeling that 
sequential algorithms were still full of research 
opportunities can go toward explaining that. In 
that community, parallelism was often only put in 
the perspectives of papers with no real perspec -
tives. This led to a situation where parallel search 
algorithms were designed by people with only one 
part of the required skills. 
Most computational problems solved on a com - 

Document 3: 
 is only determined by the current database. As a sequence, we will
introduce two portfolio frameworks as parallel individual-based
944
Algorithm Portfolio for Individual-based Surrogate-Assisted Evolutionary Algorithms GECCO ’19, July 13–17, 2019, Prague, Czech Republic
SAEAs and UCB for individual-based SAEAs which are motivated
from two different aspects as reviewed previously.
3.1 Parallel individual-based SAEAs
澽澶澡濇澵澹澵澧澽澶澡濇澵澹澵澦澽澶澡濇澵澹澵澥 澸濕濨濕澔
澶濕濧濙
濇濨濕濦濨
濇濨濣濤澳
濍
濂
澹濢濘
/ŶŝƚŝĂůŝǌĂƚŝŽŶ
/ŶĚŝǀŝĚƵĂů
^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ ^ƵƌƌŽŐĂƚĞ
/ŶĚŝǀŝĚƵĂů/ŶĚŝǀŝĚƵĂů
Figure1:Thediagramoftheframework:Parallelindividual-
based SAEAs.
Similar with the algorithm portfolio for canonical evolutionary
algorithms, it is intuitive to consider each individual-based SAEA
as a simple evolutionary algorithm and embed them into the exist-
ing framework, like PAP or MultiEA. From this aspect, the parallel
individual-based SAEAs (Par-IBSAEA) framework is proposed that
all algorithm candidates run simultaneously at each generation.
Nevertheless, it is more convenient than portfolio for canonical
evolutionaryalgorithmsbecausealmostallindividual-basedSAEAs
have the same algorithm structure and it does not require a par-
ticular design for each algorithm. A brief diagram for Par-IBSAEA
with only three algorithm instances is presented in Figure 1 where 

Document 4: 
 rithm except some important parameter settings. Therefore, the
portfolio framework in PPTA framework can be parallel-based
framework as well as sequential-based framework.
The algorithm portfolio requires algorithm constituents to
be complementary so that the portfolio algorithm can handle
different types of problems. But it is not easy to select the com-
plementary algorithms which need to test algorithm candidates
in benchmark functions. In PPTA framework, the parameter
conﬁguration obtained from different kinds of problems are
naturally complementary thanks to the classiﬁcation of train-
ing instances. Therefore, we can obtain the complementary
algorithm much easier to construct the algorithm portfolio.
3) Robustness: For each algorithm constituent in the port-
folio, it is the best parameter conﬁguration found for the
speciﬁc kind of problems. Each algorithm is suitable for a
kind of problem so that the portfolio algorithm can handle
several kinds of problems. As a result, the optimization risk
is obviously reduced.
4) Efﬁciency: The time complexity of parameter tuning
algorithm in PPTA framework can be reduced signiﬁcantly
compared with commonly used parameter tuning methods. As
mentioned before, the performance metric only evaluates the
parameter candidates on one speciﬁc problem instance.
In the algorithm portfolio phase, the time complexity does-
n’t increase a lot compared with the single algorithm because
the operators of the algorithm in PPTA are the same with
single algorithm and the total computational resources (i.e.
max FEs) are also the same. The only difference is the
strategy allocating computational resources which varies from
algorithm to algorithm. 

Document 5: 
 divides the search space into subspaces, succes -
sively allocated to sequential CDCL workers. Work -
ers cooperate through some load-balancing strate -
gy, which performs the dynamic transfer of 
subspaces to idle workers, and through the 
exchange of conflict clauses.
The parallel portfolio approach was introduced 
in 2008 (Hamadi, Jabbour, and Sais 2008; Winter -
steiger, Hamadi, and de Moura 2009; Guo et al.
2010). It exploits the complementarity of different 
sequential DPLL strategies to let them compete and 
cooperate on the same formula. Since each worker 
addresses the whole formula, there is no need to 
introduce load-balancing overheads, and coopera -
tion is only achieved through the exchange of con -
flict clauses. With this approach, the crafting of the 
strategies is important, especially with only a few 
workers. The objective is to cover the space of good 
search strategies in the best possible way. 
In general, the interleaving of computation can 
lead to the previously mentioned problem of non -
determinism. This is true for solvers that use a 
Articles 
SUMMER 2013   101 
(2) Implication graph
(3) Con/f_lict-clause
(3) Con/f_lict-clause
(1) Literal
(4) Activity
(5) Con/f_lict
Preprocessing
Boolean Unit
Propagation 

Document 6: 
 the former is to generate instances that are challenging for the
current portfolio, while the aim of the latter is to ﬁnd a new
component solver for the portfolio to better solve the newly gen-
erated instances. Applied to two widely studied problem domains,
that is, the Boolean satisﬁability problems (SAT) and the travel-
ing salesman problems (TSPs), the proposed approach identiﬁed
parallel portfolios with much better generalization than the ones
generated by the existing approaches when the training data
were scarce and biased. Moreover, it was further demonstrated
that the generated portfolios could even rival the state-of-the-art
manually designed parallel solvers.
Index Terms—Automatic portfolio construction (APC), genera-
tive adversarial approach, parallel algorithm portfolio, parameter
tuning.
I. I NTRODUCTION
M
ANY high-performance algorithms for solving com-
putationally hard problems, ranging from the exact
Manuscript received May 17, 2019; revised October 5, 2019 and March
4, 2020; accepted March 23, 2020. Date of publication April 29, 2020; date
of current version February 16, 2022. This work was supported in part by
the National Key Research and Development Program of China under Grant
2017YFB1003102, in part by the Guangdong Provincial Key Laboratory under
Grant 2020B121201001, in part by the Natural Science Foundation of China
under Grant 61672478, in part by the Program for Guangdong Introducing
Innovative and Enterpreneurial Teams under Grant 2017ZT07X386, in part 

","The development of parallel algorithm portfolios was influenced by several factors and challenges. Here are some of the key ones:

1. **Computational complexity**: As computational power increased, algorithms that were previously not feasible to run in parallel became viable options. This led to the development of new parallel algorithms and techniques.
2. **Hardware advancements**: The increasing availability and affordability of high-performance computing (HPC) hardware, such as GPUs and multi-core processors, enabled researchers to develop and deploy parallel algorithms more easily.
3. **Data-intensive applications**: The growth of big data and data-intensive applications like scientific simulations, machine learning, and genomics drove the need for efficient parallel algorithms.
4. **Scalability and portability**: As computing environments became more heterogeneous (e.g., from shared-memory to distributed-memory architectures), researchers focused on developing portable and scalable parallel algorithms that could be easily adapted to various platforms.
5. **Performance and efficiency**: The pursuit of optimal performance and efficiency in parallel algorithms led to the development of novel synchronization strategies, load balancing techniques, and data partitioning methods.
6. **Software frameworks and toolkits**: The emergence of software frameworks like OpenMP, MPI, and CUDA provided a foundation for developing and deploying parallel algorithms.
7. **Interdisciplinary collaboration**: The development of parallel algorithm portfolios required collaboration between computer scientists, engineers, mathematicians, and domain experts from various fields (e.g., physics, chemistry, biology).
8. **Standardization and benchmarking**: Efforts to standardize parallel computing environments and develop benchmarks for evaluating parallel algorithms helped ensure the portability and reliability of parallel code.

Challenges that influenced the development of parallel algorithm portfolios include:

1. **Synchronization and communication overhead**: Efficient synchronization and communication strategies were essential to minimize overhead in parallel algorithms.
2. **Load balancing and task partitioning**: Balancing the load among processors and partitioning tasks effectively to achieve optimal performance was a significant challenge.
3. **Memory management and data locality**: Managing memory efficiently and optimizing data locality to reduce cache misses were crucial for achieving good performance in parallel algorithms.
4. **Code portability and maintainability**: Ensuring that parallel code could be easily adapted to different hardware platforms and maintained over time was a significant challenge.
5. **Error detection and recovery**: Developing robust error detection and recovery mechanisms was essential to ensure the reliability of parallel algorithms.
6. **Scheduling and resource allocation**: Efficient scheduling and resource allocation strategies were necessary to optimize performance in parallel algorithms.

By addressing these challenges and factors, researchers and developers have created a wide range of parallel algorithm portfolios that can be used for various applications, from scientific simulations to machine learning and data analytics.",7.642251253128052,"The development of Parallel Algorithm Portfolios has been influenced by several factors and challenges. Key aspects include:

1. **Complementarity of Algorithms**: The success of parallel portfolios relies on the ability to select algorithm constituents that are complementary. This allows the portfolio to handle a variety of problems effectively, thus reducing optimization risk (Document 4).

2. **Efficient Use of Resources**: With the rise of parallel computing architectures, effective resource allocation that does not significantly increase time complexity compared to single algorithms is crucial (Document 4).

3. **Dynamic Load Balancing**: For parallel search methods, particularly in SAT solving, efficient dynamic load balancing and worker cooperation through conflict clause exchange are vital for performance (Document 5).

4. **Generalization in Learning**: The generation of instances that challenge current portfolios and the identification of new component solvers for better performance in learning are important (Document 6). These factors are critical when existing training data is scarce and biased.

5. **Research Perspectives**: The historical focus of the parallel computing community on combinatorial search, combined with the lack of dedicated research into parallelization from the search community, has created challenges in effectively designing parallel search algorithms (Document 2).

6. **Algorithm Structure Homogeneity**: The homogeneity of the underlying structure of individual-based algorithms allows for easier integration into parallel frameworks without the need for significant modifications (Document 3).

Overall, the interplay of these factors shapes the design, implementation, and performance of parallel algorithm portfolios, addressing challenges related to efficiency, generalization, and collaboration among different solvers."
llama3.2:latest,"What are the main benefits of PAPs?
","Document 1: 
 independently in parallel to obtain multiple solutions. Then, the best solution will be
taken as the ﬁnal output of the PAP . Although a PAP would consume more computational
resources than a single algorithm, it has three important advantages. First, PAPs are
easy-to-implement because they do not necessarily require any resource allocation since
each member algorithm is simply assigned with the same amount of resource. Second,
the performance of a PAP on any problem is the best performance achieved among its
member algorithms on the problem. In other words, a PAP could achieve a much better
overall performance than any of its member algorithms. Third, considering the tremendous
growth of parallel computing architectures [22] (e.g., multi-core CPUs) over the last few
decades, leveraging parallelism has become very important in designing effective solvers
for hard optimization problems [23–27]. PAPs employ parallel solution strategies and thus
allow for the use of modern computing facilities in an extremely simple way.
It is conceivable that any PAP’s effectiveness relies heavily on the diversity and com-
plementarity among its member algorithms. In other words, the manual construction
of high-quality PAPs is generally a challenging task, requiring domain experts (with a
deep understanding of both algorithms and problems) to explore the vast design space
of PAPs, which cannot be carried out manually with ease [28–30]. As an alternative, Tang
and Liu [25,26] proposed a general framework, called automatic construction of PAPs, that 

Document 2: 
 on PAP with more than two constituent algorithms.
Two conclusions regarding the migration scheme of PAP
can be drawn from the superiority of PAP over the compared
algorithms. First, the migration scheme is of great impor-
tance to the success of PAP. Second, 1 andMAX GEN/20
are two appropriate and robust values for the parame-
tersmigration size and migration interval. To further
investigate the inﬂuence of these parameters on PAP, we
carried out a sensitivity analysis to check whether the per-
formance of PAP will change signiﬁcantly with other param-
eter settings. As stated in Section VI-B, 16 different pairs
ofmigration interval and migration size were tested for
every instantiation of PAP. For each pair, 30 independent
runs were executed on all 27 benchmark functions. Then, for
each instantiation on each function, Kruskal–Wallis one-way
analysis of variance by ranks was employed to test whether
the 16 pairs of parameters had led to signiﬁcantly different
performance. After that, for each instantiation of PAP, we
counted the number of the benchmark functions on which
all 16 pairs of parameters made no difference. The larger
the number, the more insensitive an instantiation is to the
parameters. For the sake of brevity, we only summarize in
Table IX these numbers for the 11 PAP instantiations, while
omit the full details. It can be observed that, in the worst case 

Document 3: 
 described as follows:
Ω(P, z) =max{max
θ∈P
Metric (θ, z), Metric ( ¯θ, z)}, (14)
where ¯θ represents the Restructure procedure. Note Equation (14) is slightly different from
Equation (11) (i.e., the performance of conventional PAPs) due to theRestructure procedure.
Initialization
member 
algorithm 
𝜃
1
𝑆
1
member 
algorithm 
𝜃
𝑖
𝑆
𝑖
member 
algorithm 
𝜃
𝑘
𝑆
𝑘
…
…
…
…
𝑆
𝑘
+
1
𝑆
𝑜𝑢𝑡𝑝𝑢𝑡
Comparison
Restructure
Improvement
Figure 1. Illustrations of MOEAs/PAP , the variant form of PAP for MOPs. Si represents the so-
lution set found by member algorithm θi, and Soutput represents the solution set ﬁnally returned
by MOEAs/PAP . The main difference of MOEAs/PAP from conventional PAPs lies in the way of
determining the ﬁnal output, as indicated by the dash box.
Electronics 2023, 12, 4639 7 of 18
4. Automatic Construction of MOEAs/PAP
As aforementioned, the member algorithms of a PAP could be automatically deter-
mined by solving the problem deﬁned in Equation (13). Below, we ﬁrst introduce the 

Document 4: 
 • When constructing MOEAs/PAPs, it is important to maintain the diversity among the
member algorithms. Hence, the population diversity preservation schemes, such as
negatively correlated search [47], can be introduced into the construction approach to
promote cooperation between different member algorithms.
• In real-world applications, one may be unable to collect sufﬁcient MOPs as training
problems. How to automatically build powerful PAPs in these scenarios is also
worth studying.
• The effectiveness of MOEAs/PAP has been primarily demonstrated through exper-
imental evidence, but with an absence of theoretical analysis. A more thorough
investigation of its exceptional performance is crucial for advancing our understand-
ing, which, in turn, can lead to enhancements in its design and the development of a
more comprehensive automatic construction algorithm.
Electronics 2023, 12, 4639 17 of 18
Author Contributions: Conceptualization, S.L.; Methodology, S.L.; Software, X.M.; Validation, X.M.;
Resources, X.M.; Writing—original draft, X.M.; Writing—review and editing, W.H.; Supervision, S.L.
and W.H. All authors have read and agreed to the published version of the manuscript.
Funding: This research was supported by the Strategic Priority Research Program of Chinese
Academy of Science, Grant No. XDB32050100, and the National Natural Science Foundation of
China, Grant No. 91948303.
Data Availability Statement: The data presented in this study are available on request from the 

Document 5: 
 the risks associated with two algorithms. Such a metric can
be used to evaluate how effective our PAP is, together with
other common metrics. Given a set of functions, the proposed
metric essentially measures how likely it is that an algorithm
will ﬁnd a better solution than another algorithm by the end
of a given time budget.
To evaluate the effectiveness of PAP, 11 instantiations of
PAP were implemented based on four existing constituent
algorithms, including SaNSDE, wPSO, G3PCX, and CMA-
ES. The performance of each instantiation was compared to
PENG et al.: POPULATION-BASED ALGORITHM PORTFOLIOS FOR NUMERICAL OPTIMIZATION 799
its constituent algorithms on 27 benchmark functions. Our
experimental results showed that seven out of the 11 PAP in-
stantiations outperformed their constituent algorithms in terms
of solution quality and the proposed risk metric. Furthermore,
7 out of the 11 instantiations even achieved superior or compa-
rable performance in comparison with G-CMA-ES, which was
known to be superior to any of the four constituent algorithms.
Our empirical studies also revealed that PAP is capable of
increasing the probability of ﬁnding the global optimum and
is insensitive to control parameters of the migration scheme.
Further analyses have been conducted to investigate in what
circumstance PAP may outperform its constituent algorithms.
Complementarity was identiﬁed as a key issue.
Though PAP has been shown to be a promising framework, 

Document 6: 
 also presented. The performance of the two instantiations on
TSP and VRPSPDTW problems support the effectiveness of
CEPS in the sense that, in comparison with state-of-the-art
PAP construction approaches, the PAPs obtained by CEPS
achieves better generalization performance.
Since CEPS is a generic framework, some discussions
would help elaborate issues that are of signiﬁcance in practice.
First, although this work assumes CEPS takes a set of initial
training instances as the input, such training instances are
not necessarily real-world instances but could be generated
randomly. In other words, CEPS could be used in a fully cold-
start setting (a.k.a. zero-shot), i.e., no real-world instances are
available for the target problem class. Further, CEPS could
either be run ofﬂine or online, i.e., it could accommodate new
real instances whenever available.
Second, the potential of CEPS could be further explored
by taking advantage of the data generated during its run,
except for the ﬁnal obtained PAP. The data contain all the
sampled conﬁgurations and instances, and the performance
of the former on the latter. Considering that when using
a search method to solve a problem instance, its optimal
parameter values are usually problem-instance dependent and
thus need to be tuned. To tune parameters for a new problem
instance, we can learn from the historical data generated by
CEPS to build a mapping from problem instances to their
optimal parameter values, i.e., a low-cost online parameter- 

","PAPs (Positive Airway Pressure) devices are commonly used to treat various respiratory conditions, including sleep apnea and chronic obstructive pulmonary disease (COPD). The main benefits of PAPs include:

1. **Improved breathing**: PAPs help keep the airways open during sleep or rest, allowing for easier breathing and reducing the risk of hypoxia (low oxygen levels).
2. **Reduced snoring and apnea episodes**: By keeping the airways open, PAPs can significantly reduce snoring and apnea episodes, leading to better sleep quality and overall health.
3. **Increased oxygen saturation**: PAPs help maintain a stable level of oxygen in the blood, which is especially important for people with respiratory conditions or those who are at risk of developing hypoxia.
4. **Reduced symptoms of COPD**: By keeping the airways open, PAPs can help reduce symptoms of COPD, such as shortness of breath and wheezing.
5. **Improved overall health**: By treating underlying sleep disorders and respiratory conditions, PAPs can lead to improved overall health and reduced risk of related complications, such as heart disease and stroke.
6. **Increased energy levels**: With improved breathing and oxygen saturation, people using PAPs often experience increased energy levels and a general sense of well-being.
7. **Reduced daytime sleepiness**: By addressing the underlying cause of excessive daytime sleepiness, PAPs can help reduce fatigue and improve cognitive function.
8. **Improved mental health**: Better sleep quality and reduced symptoms of anxiety and depression can lead to improved mental health and overall well-being.

Overall, PAPs are an effective treatment option for various respiratory conditions, offering numerous benefits for patients' quality of life and overall health.",5.082697629928589,"The main benefits of Population-Based Algorithms (PAPs) are:

1. **Ease of Implementation**: PAPs are straightforward to implement as they do not require complex resource allocation; each member algorithm is assigned an equal amount of resources.

2. **Enhanced Performance**: The performance of a PAP is determined by the best performance achieved among its member algorithms, allowing it to potentially outperform any single algorithm included within it.

3. **Utilization of Parallelism**: With the growth of parallel computing architectures, PAPs can leverage parallel solution strategies effectively, making them suitable for modern computing facilities.

4. **Diversity and Complementarity**: The effectiveness of a PAP relies on the diversity and complementarity among its member algorithms, which can improve the overall solution quality.

5. **Sensitivity to Parameters**: Certain PAP configurations have shown to be insensitive to control parameters, contributing to more robust performance across different scenarios.

6. **Empirical Success**: Experimental results indicate that many PAP instantiations can outperform their individual constituent algorithms and improve the likelihood of finding optimal solutions."
llama3.2:latest,"What is algorithm for mosaic permutation test?
","Document 1: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

Document 2: 
 algorithm and their default values, (i.e. the original values) are
presented in Table II.
The gamma and mu in GA represent the crossover rate
and mutation rate [20]. In DE algorithm, policy represents the
choice of mutation operator andF, CR are two important pa-
rameters in mutation and crossover operators [21]. The policy
candidates contain/rand/1, /best/1, /current−to−best/1,
/best/2 and /rand/2. In PSO algorithm, c1, c2 are two ﬂying
factors for each particle [22].
1852
Authorized licensed use limited to: University College London. Downloaded on July 03,2020 at 02:38:54 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: The tuned parameters for test algorithms with
Dimension d = 30.
Types GA DE PSO
gamma mu policy F CR c1 c2
TB1 0.4294 0.1728 4 0.4155 0.5441 1.7848 0.5908
TB2 0.5972 0.0617 3 0.6553 0.5350 0.1467 3.0000
TB3 0.5493 0.9737 3 0.7791 0.9308 0.7161 1.9731
TB4 0.5329 0.6145 1 0.9584 0.9273 1.7811 1.7866
TB5 0.9964 0.0839 1 0.4453 0.0581 0.4452 3.0000
TC1 0.5972 0.0617 1 0.7791 0.9308 1.2194 2.3496 

Document 3: 
 The mosaic permutation test: an exact and nonparametric
goodness-of-fit test for factor models
Asher Spector ∗ Rina Foygel Barber † Trevor Hastie ∗ Ronald N. Kahn ‡
Emmanuel Cand` es∗§
September 30, 2024
Abstract
Financial firms often rely on fundamental factor models to explain correlations among asset
returns and manage risk. Yet after major events, e.g., COVID-19, analysts may reassess whether
existing risk models continue to fit well: specifically, after accounting for a set of known factor ex-
posures, are the residuals of the asset returns independent? With this motivation, we introduce the
mosaic permutation test, a nonparametric goodness-of-fit test for preexisting factor models. Our
method can leverage modern machine learning techniques to detect model violations while provably
controlling the false positive rate, i.e., the probability of rejecting a well-fitting model, without mak-
ing asymptotic approximations or parametric assumptions. This property helps prevent analysts
from unnecessarily rebuilding accurate models, which can waste resources and increase risk. To
illustrate our methodology, we apply the mosaic permutation test to the BlackRock Fundamental
Equity Risk (BFRE) model. Although the BFRE model generally explains the most significant
correlations among assets, we find evidence of unexplained correlations among certain real estate
stocks, and we show that adding new factors improves model fit. We implement our methods in 

Document 4: 
 the maximum absolute estimated correlation between asset j⋆ and any asset j ∈ Gd.
• Reset Gd⋆ = Gd⋆ ∪ {j⋆}.
Return: Partition G1, . . . , GD.
In particular, let \MaxCorrj := maxj′̸=j | ˆCj,j′| be the maximum absolute estimated correlation between
asset j and asset j′. Let S ⊂ [p] be the subset of indices of [ p] corresponding to the assets with the
ℓ largest values of \MaxCorrj, so |S| = ℓ. Finally, let ˆν denote the top eigenvector of ˆCS,S. We then
define ˆv as follows:
ˆvj =
(
ˆνj j ∈ S
0 else.
In other words, S is the support of ˆ v, and on S, ˆv equals the maximum eigenvalue of ˆCS,S. We
picked this algorithm because it is conceptually simple and computationally cheap, but we have not
explored other algorithms. Of course, the mosaic permutation test could be used in combination with
a maximum bi-cross validation R2 statistic based on any sparse PCA algorithm.
42 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","The Algorithm 1 defines the mosaic permutation test.

Here are the steps:

Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M m=1 and a test statistic S : RT×p → R.

Step 1: For each tile (Bm, Gm), estimate the residuals ˆϵ in that tile.

Step 2: Compute the original meta test-statistic f(˜ϵ(0), ...,˜ϵ(R)) using the permuted variants ˜ϵ(r) of the residual estimates.

Step 3: Sample uniformly random permutations π1, ..., πK : {0, ..., R} → {0, ..., R}.

Step 4: Compute the final adaptive p-value padaptive by checking if any of the permutations decrease the value of f(˜ϵ(r), ...,˜ϵ(R)) for r = 1, ..., R.",4.610363006591797,"The mosaic permutation test is a nonparametric goodness-of-fit test designed for factor models in finance. The algorithm consists of the following steps:

1. **Inputs**: It takes asset returns \( Y \in \mathbb{R}^{T \times p} \), exposures \( L_t \in \mathbb{R}^{p \times k} \) for \( t \in [T] \), and a set of tiles \( \{(B_m, G_m)\}^M_{m=1} \) along with a test statistic \( S : \mathbb{R}^{T \times p} \to \mathbb{R} \).

2. **OLS Estimate**: For each tile \( m = 1, \ldots, M \), the algorithm computes \( \hat{\epsilon}(m) \), which is the ordinary least squares (OLS) estimate of the residuals using only the data in that tile \( Y(m) \). The exposures for the assets in the \( m \)-th tile are denoted as \( L(m) \).

3. **Residual Estimate**: Using these estimates, the algorithm creates a mosaic residual estimate \( \hat{\epsilon} \in \mathbb{R}^{T \times p} \).

4. **Permutation**: The algorithm permutes the order of the residuals multiple times to generate permuted variants \( \tilde{\epsilon}^{(r)} \) for \( r = 1, \ldots, R \) and checks if permuting decreases the value of a chosen meta test-statistic.

5. **P-value Calculation**: Finally, it computes a p-value based on the number of times the permuted residuals yield a test statistic value lower than the original.

Overall, this method allows analysts to check for independence of residuals after accounting for known factor exposures without requiring strong parametric assumptions."
llama3.2:latest,"What test statistic should be used for mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 power compared to the oracle tests, suggesting that the adaptive QMC statistic effectively adapts to
the unknown sparsity level. Furthermore, the MPT oracle and OLS double oracle—which use the
same test statistic but are applied to different residual estimates—have similar power. Indeed, the
average power difference is 3%, and the maximum power difference is 10%. This result should not
be too surprising, since ˆϵols and ˆϵ are estimating the same residuals ϵ, and thus the OLS and mosaic
statistic should be highly correlated. Indeed, in our application (Figure 4), in all three sectors, the
mosaic statistics are empirically ≥ 85% correlated with the OLS statistics. Thus, in this simulation,
the mosaic test is competitive with an oracle method based on OLS residuals.
7 Discussion
This paper introduces the mosaic permutation test, an exact and nonparametric goodness-of-fit test
for factor models with known exposures. In an empirical application to the BlackRock Fundamental
Equity Risk model, we demonstrate how to use the mosaic permutation test to diagnose financial
factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing 

Document 4: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","The test statistic that should be used for the mosaic permutation test is S(ˆϵ), which quantifies the correlations among the columns of ˆϵ.",1.633470058441162,"The mosaic permutation test employs a test statistic denoted as S(ˆϵ), which quantifies the correlations among the columns of the residual estimate ˆϵ. This statistic is central to the test as it allows for the computation of a significance threshold by permuting the observations within the rectangular tiles of ˆϵ. The test is designed to evaluate the independence properties of the residuals, making S(ˆϵ) a key component in assessing the goodness-of-fit in factor models."
llama3.2:latest,"What is restriction for test statistic in mosaic permutation test?
","Document 1: 
 a new estimator ˆϵ ∈ RT×p of the residuals that exactly preserves some of the independence properties
of the true residuals ϵ. To construct ˆϵ, we split the data matrix Y into rectangular tiles (along both
axes) and separately estimate the residuals in each tile, yielding a residual estimate ˆϵ ∈ RT×p. Given a
test statistic S(ˆϵ) quantifying the correlations among the columns of ˆϵ, we can compute a significance
threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ. The idea is illustrated in Figure 2.
We call this a “mosaic permutation test” because the separation of the data into tiles is reminiscent
of a mosaic.
Figure 2: This figure summarizes the main methodology of the mosaic permutation test. Above, rows represent
different observations t = 1, . . . , T, columns represent different assets, and in the second matrix, the shadings
are permuted within each rectangle to illustrate the permutations within tiles.
We now highlight a few key properties of the mosaic permutation test.
1. Exact and nonparametric false positive control. The test yields an exact p-value in finite
samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con- 

Document 2: 
 method above guarantees that no tile contains any missing data; thus, Theorem 3.2 applies. Please
see the proof of Corollary A.2 for more details.
B.2 Sensitivity to the window size
Throughout the main text, we computed test statistics over time using a sliding window of size
350 observations. Although this is a somewhat arbitrary choice, varying the window size does not
substantially change the results. In particular, Figure 11 repeats the analysis from Figure 4 and shows
that the shapes of the curves plotting the mosaic test statistics and the mosaic permutation quantiles
do not significantly change across different window sizes.
37
Figure 11: This figure exactly replicates the analysis from Figure 4, except that Figure 4 uses a window size of
350 and this plot varies the window size. In particular, for three industries, it shows the mosaic MMC statistic
(plotted every two weeks) and its permutation quantile computed using different sliding window sizes. Note
that the shapes of the curves and the relative values of the statistics and their permutation quantiles do not
change substantially with different window sizes.
38
B.3 Results for additional sectors
We now show results for six additional sectors beyond the three from the main text: Consumer
Discretionary, Consumer Staples, Industrials, Materials, Tech, and Utilities. In short, we find similar
results to Section 4: either (i) we do not consistently reject the null or (ii) we can detect violations of 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when
estimating ˆϵ.
6 Do the mosaic residual estimates cause a loss of power?
Our method requires the test statistic S(ˆϵ) to be a function of mosaic residual estimates instead
of a function S(ˆϵols) of OLS residual estimates. We hope that the mosaic statistic is a good proxy
23
for the OLS statistic (which indeed seems to be the case in Figure 4), so this section analyzes via
simulations whether the mosaic test has lower power than an oracle test based on the OLS statistic.
Our simulations also show the effectiveness of the adaptive test statistic introduced in Section 5.1.
We conduct semisynthetic simulations where the exposures Lt are constant over time and equal to
the BFRE exposures for the financial sector on May 21st, 2020. We sample the factor returns Xtk
as i.i.d. t4 variables. The residuals satisfy ϵt = γt + Ztv for γt,j
i.i.d.∼ t4, Zt
i.i.d.∼ t4 and v ∈ Rp. In 

Document 5: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 6: 
 (ϵ1,j, . . . , ϵT,j ) d= (ϵπ(1),j, . . . , ϵπ(T),j). (3.4)
Assumption 3.2 allows the distribution of the residuals to drift between tiles, making this assumption
much weaker than the i.i.d. assumption in Theorem 3.1. Indeed, this assumption is related to the
motivation for many classical procedures for time series data, such as the block bootstrap (Kunsch,
1989). Armed with these assumptions, Algorithm 1 defines the mosaic permutation test.
Algorithm 1: The mosaic permutation test.
Inputs: Asset returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a test
statistic S : RT×p → R.
Step 1: For each tile m = 1, . . . , M, we let ˆϵ(m) denote the OLS estimate of ϵ(m) using only the data
in Y(m). Precisely, let L(m) ∈ R|Gm|×k denote the exposures for the assets in the mth tile
(note by Assumption 3.1 that the exposures are constant over time within the tile). Let
Hm := (I|Gm| − L(m)(L⊤
(m)L(m))−1L⊤ 

","According to the context, there isn't a specific restriction mentioned for the test statistic S(ˆϵ) in the mosaic permutation test. However, it's mentioned that we can compute a significance threshold for S(ˆϵ) by permuting the observations in each tile of ˆϵ.",2.112466812133789,"The restriction for the test statistic in the mosaic permutation test is that it must be a function of the mosaic residual estimates \( \hat{\epsilon} \) instead of a function of the ordinary least squares (OLS) residual estimates \( \hat{\epsilon}_{ols} \). This means that the test statistic \( S(\hat{\epsilon}) \) is calculated using the newly introduced estimator \( \hat{\epsilon} \) which preserves some independence properties of the true residuals, rather than relying on the properties of the OLS residuals which do not meet the same independence criteria."
llama3.2:latest,"What is the trick for allowiing the exposures to change with each observation in Mosaic permutation test?
","Document 1: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 2: 
 bias (≈ 0.17). As a result, the bootstrap Z-statistics are highly inaccurate and have an average value
of ≈ 25 (while we would expect to see an average of ≈ 0 if the test were performing well), leading to
essentially a 100% false positive rate.
1Since the test statistic only depends on Y through ˆϵols, the residual bootstrap is in this case identical to the pairs
bootstrap, which resamples pairs of exposures and returns {(Lt, Yt)}T
t=1.
10
3 Methodology
3.1 Main idea
As discussed in Section 2.1, the key challenge in developing a permutation test for H0 is that the OLS
residual estimates ˆϵols do not satisfy the same independence properties as the true residuals ϵ. Our
solution is to introduce a new estimator ˆϵ that exactly preserves some of the independence properties
of ϵ. To ease readability and build intuition, this subsection introduces the simplest possible variant of
the mosaic permutation test. Section 3.2 then introduces the mosaic permutation test in full generality.
However, Section 3.2 is self-contained, so readers may skip to Section 3.2 if they wish.
For exposition, we make two simplifying assumptions for this subsection only: (1) the vectors of
residuals ϵ1, . . . , ϵT 

Document 3: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 Thus, we can compute a p-value based on any meta test-statistic by randomly permuting the order of
{˜ϵ(r)}R
r=0 and checking if this decreases the value of f. Algorithm 2 formally describes this procedure.
21
Algorithm 2 Adaptive meta-test statistic
Input: Returns Y ∈ RT×p, exposures Lt ∈ Rp×k for t ∈ [T], tiles {(Bm, Gm)}M
m=1 and a meta
test-statistic f : R(R+1)×T×p → R.
Step 1: Construct the mosaic residual estimate ˆ ϵ ∈ RT×p and its permuted variants ˜ϵ(r) ∈ RT×p for
r = 1, . . . , R, as described in Algorithm 1. Set ˜ϵ(0) = ˆϵ.
Step 2: Compute the original meta test-statistic f(˜ϵ(0), . . . ,˜ϵ(R)).
Step 3: Sample uniformly random permutations π1, . . . , πK : {0, . . . , R} → {0, . . . , R}.
Step 4: Compute the final adaptive p-value
padaptive = 1 + PK
ℓ=1 I
","To allow the exposures to change with each observation in the Mosaic Permutation Test, you replace the constant exposure matrix Lt with an augmented exposure matrix L⋆t. This matrix is updated every two observations and consists of a submatrix of Lt with Lt from the current and previous observations, depending on whether t is odd or even.

In more detail, the augmented exposure matrix L⋆t for each observation t is constructed as follows:

- For t = 1 (first observation): L⋆1 = Lt
- For t > 1 (subsequent observations):
  - If t is odd: L⋆t = [Lt, Lt_prev]
  - If t is even: L⋆t = [Lt_prev, Lt]

This way, for every pair of consecutive observations (i.e., when t and t-1 are both odd or both even), the exposure matrix remains constant, mimicking a constant exposure.",4.411861896514893,"The trick for allowing the exposures to change with each observation in the Mosaic permutation test is to augment the exposure matrix. Specifically, instead of simply using the original exposure matrix \( L_t \), an augmented exposure matrix \( L^\star_t \) is defined, which is constructed to vary every two observations. The form of this augmented matrix is as follows:

\[
L^\star_t := 
\begin{cases}
\begin{pmatrix}
L_t \\ L_{t+1}
\end{pmatrix} & \text{if } t \text{ is odd} \\
\begin{pmatrix}
L_{t-1} \\ L_t
\end{pmatrix} & \text{if } t \text{ is even}
\end{cases}
\]

By this construction, \( L^\star_t \) only changes every two observations, and it still maintains that \( L_t \) is a submatrix of \( L^\star_t \). Therefore, the null hypothesis retains its validity for the augmented model, which allows the application of the mosaic permutation test without the need for \( L_t \) to be constant across all observations but instead ensures that it is constant within the tiles created for the permutation test."
llama3.2:latest,"Is there any way of choice of tiling in Mosaic permutation test?
","Document 1: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 2: 
 setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining
provable false positive control. For example, it allows analysts to use regularized estimates of the
covariance matrix of the residuals ϵ, for example, via a graphical lasso (Friedman et al., 2007), and
it also permits the use of cross-validation to choose the regularization strength. The only restriction
5
Figure 3: Semisynthetic simulation with Xtk, ϵtj
i.i.d.
∼ N(0, 1), and the exposures are taken from the BFRE
model for the energy sector on May 21st, 2020. Note T = 350, p = 183, k = 18, and we use the test statistic
from Figure 1. Panel 2(a) shows that a naive residual permutation test (discussed in Section 2.1) inaccurately
simulates the null distribution of the test statistic S(ˆϵols)—in fact, the true null distribution and the estimated
one do not overlap. Panel 2(b) shows that naive bootstrap Z-statistics (discussed in Section 2.2) are not
approximately mean zero, nor do they have unit variance. All p-values based on these two naive methods are 

Document 3: 
 The proof is simple, but the notation is subtle. To ease comprehension, recall that by definition
ϵ(m) = ϵBm,Gm where Bm and Gm are random. In this proof, we will use the notation ϵBm,Gm instead
of ϵ(m) to make the dependence on Bm and Gm explicit.
Let T = {(Bm, Gm)}m∈[M] denote the choice of tiles and let τ = {(βm, γm)}m∈[M] denote an arbitrary
fixed tiling in the support of T . It suffices to show that for any fixed permutation matrices P1 ∈
R|β1|×|β1|, . . . , PM ∈ R|βM |×|βM |,
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ) | T= τ. (A.8)
To show this, we note that Assumption A.1 yields the marginal result that
(P1ϵβ1,γ1 , . . . , PM ϵβM ,γM ) d= (ϵβ1,γ1 , . . . , ϵβM ,γM ). (A.9)
To convert this to a conditional result, note that sinceT is a function of ˆϵ (which itself is a deterministic 

Document 4: 
 In future work, we plan to extend the mosaic permutation test to apply to this new setting (among
others).
• Using regularization: Our methods currently require the use of unregularized OLS regressions within
each tile to estimate the residuals (see Remark 3). However, to increase power, it might be valuable
to develop methods that can use regularization.
• Robustness: It might be valuable to develop tests that are robust to slight inaccuracies in the expo-
sures Lt. Indeed, this could also help relax the assumption that Lt is locally constant (Assumption
3.1), since small within-tile changes in Lt could be viewed as small “inaccuracies.” Similarly, it
would be useful to develop theory quantifying the robustness of the existing test, i.e., by bounding
the excess error in some interpretable way.
25
8 Code and data availability
We implemented our methods in the python package mosaicperm. All other code used in the paper is
available at https://github.com/amspector100/mosaic_factor_paper/. Although we are not able
to make the BFRE model data available, we have provided a publicly available sample dataset that
allows one to obtain qualitatively similar results (see the GitHub repository for more details).
9 Acknowledgements
The authors would like to thank John Cherian, Kevin Guo, Guido Imbens, Lihua Lei, and Bobby Luo 

Document 5: 
 highly computationally efficient procedure compared to one which (e.g.) repeatedly cross-validates an
expensive machine learning algorithm.
5.2 Adaptively choosing the tiling
Although Section 3.3 gives a good default choice of tiling, another option is to learn a good choice
of tiles that “separate” assets whose idiosyncratic returns are correlated. However, in general, if the
tiling is chosen using Y, ˆϵ will not necessarily be invariant to any permutations under the null because
of the dependence between {(Bm, Gm)}M
m=1 and Y. In other words, naive “double dipping” leads to
inflated false positives.
However, we can sequentially choose the mth tile (Bm, Gm) based on the estimated residuals from the
previous m−1 tiles as long as our choice of (Bm, Gm) does not depend on the order of the rows within
each of the previous m − 1 tiles. Precisely, suppose that we can write Bm, Gm as functions bm, gm of
the previous tiles as well as auxiliary randomness Um
i.i.d.∼ Unif(0, 1):
Bm = bm(ˆϵ(1), . . . ,ˆϵ(m−1), Um) and Gm = gm(ˆϵ(1), . . . ,ˆϵ(m−1), Um). (5.5) 

Document 6: 
 returns ˆϵ·,1, ˆϵ·,2 will never be “separated” by different permutations. We address this problem in the
next section.
3.2 The mosaic permutation test
We now introduce the general mosaic permutation test, which is more powerful and stable than
the simple method in Section 3.1. As an added benefit, we will also make the test more robust to
autocorrelation and nonstationarity among the residuals ϵ1, . . . , ϵT .
In Section 3.1, we separated the asset returns Y ∈ RT×p into two disjoint groups, computed residual
estimates ˆϵ separately for each group, and then permuted within each group. Now, we partition the
data Y into an arbitrary number M of rectangles along both axes. Formally, for m = 1, . . . , M, let
Bm ⊂ [T] denote a subset or “batch” of observations and Gm ⊂ [p] denote a subset or “group” of
assets. We say {(Bm, Gm)}M
m=1 is a tiling if for every timepoint t and asset j, there is exactly one pair
(Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we 

","Yes, there is a way of choosing the tiling in the Mosaic permutation test. According to the text, one option is to use Y (the original data) as the basis for tile selection, and another option is to learn a good choice of tiles that ""separate"" assets whose idiosyncratic returns are correlated. However, this second approach can lead to inflated false positives due to the dependence between {(Bm, Gm)}M m=1 and Y.

An alternative approach is to sequentially choose the mth tile (Bm, Gm) based on the estimated residuals from the previous m-1 tiles, as described in section 5.2 of the text.",3.193421602249145,"Yes, the choice of tiling in the mosaic permutation test can be adaptively learned to separate assets whose idiosyncratic returns are correlated. Although a default choice of tiling is provided in Section 3.3, analysts can sequentially choose each tile based on the estimated residuals from previous tiles, ensuring that the choice does not depend on the order of rows within the previous tiles. This process involves writing the pair of sets (Bm, Gm) as functions of previous tiles along with auxiliary randomness, allowing for an adaptive and potentially more effective selection of tiling."
llama3.2:latest,"What is RNN with Long-Short-Term-Memory estimate in deep learning pricing asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper.
Hansen, L. P.(1982): “Large sample properties of generalized method of moments estimators,” Econo-
metrica, 50(4), 1029–1054.
Hansen, L. P., and R. Jagannathan(1997): “Assessing Speciﬁcation Errors in Stochastic Discount
Factor Models,” The Journal of Finance , 52(2), 557–590.
Heaton, J., N. Polson, and J. H. Witte(2017): “Deep learning for ﬁnance: Deep portfolios,” Applied
Stochastic Models in Business and Industry , 33(1), 3–12.
Hochreiter, S., and J. Schmidhuber(1997): “Long short-term memory,” Neural Computation, 9(8),
1735–1780.
Horel, E., and K. Giesecke(2020): “Towards Explainable AI: Signiﬁcance Tests for Neural Networks,”
Journal of Machine Learning Research, forthcoming . 

Document 3: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 4: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 5: 
 Rapach et al. 2013) and lasso (Chinco, Clark-Joseph & Ye 2019; Freyberger, Neuhierl & Weber
2020).
More recently, Gu, Kelly & Xiu (2020) conduct a wide-ranging analysis of machine learning
methods for return prediction, considering not only regularized linear methods but also more
cutting-edge nonlinear methods including random forest, boosted regression trees, and deep
learning.Their research illustrates the substantial gains of incorporating machine learning when
estimating expected returns. This translates into improvements in out-of-sample predictive R2
as well as large gains for investment strategies that leverage machine learning predictions. The
empirical analysis also identifies the most informative predictor variables, which helps facilitate
deeper investigation into economic mechanisms of asset pricing.
Machine learning also makes it possible to improve expected return estimates using predic-
tive information in complex and unstructured data sets. For example, Ke, Kelly & Xiu (2019)
propose a new supervised topic model for constructing return predictions from raw news text
and demonstrate its prowess for out-of-sample forecasting.Jiang,Kelly & Xiu (2021) and Obaid
& Pukthuanthong (2022) demonstrate how to tap return predictive information in image data
using machine learning models from the computer vision literature. Both text and image data
confer particularly strong return forecasting gains at short horizons of days and weeks and are 

Document 6: 
 characteristics. Feng, Polson, and Xu (2019) impose a no-arbitrage constraint by using a set of
pre-speciﬁed linear asset pricing factors and estimate the risk loadings with a deep neural network.
Rossi (2018) uses Boosted Regression Trees to form conditional mean-variance eﬃcient portfolios
based on the market portfolio and the risk-free asset. Our approach also yields the conditional
mean-variance eﬃcient portfolio, but based on all stocks. Gu, Kelly, and Xiu (2019) extend the
linear conditional factor model of Kelly, Pruitt, and Su (2019) to a non-linear factor model using an
autoencoder neural network. 3 We conﬁrm their crucial insight that imposing economic structure
2Other related work includes Sirignano, Sadhwani, and Giesecke (2020) who estimate mortgage prepayments,
delinquencies, and foreclosures with deep neural networks, Moritz and Zimmerman (2016) who apply tree-based
models to portfolio sorting and Heaton, Polson, and Witte (2017) who automate portfolio selection with a deep
neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the 

","I don't know. The text does not mention that Luyang Chen used RNN with Long-Short-Term-Memory (LSTM) in their deep learning pricing asset pricing model. It only mentions Hochreiter and Schmidhuber's 1997 paper ""Long short-term memory"", which introduced the LSTM algorithm, but it is not clear if this is related to Luyang Chen's work.",3.1249728202819824,"The context does not provide specific information about the use of Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) in the asset pricing models developed by Luyang Chen. However, it mentions the use of deep learning techniques for estimating an asset pricing model that accounts for various conditioning information and time-variations. LSTMs are typically utilized in scenarios where data has sequential characteristics, making them suitable for time-series forecasting in finance. They are particularly effective in capturing long-term dependencies in data, which is crucial for modeling the dynamic behavior of asset returns. Therefore, while the specific model by Chen does not detail RNN or LSTM use, these architectures can enhance the processing of temporal data in asset pricing models."
llama3.2:latest,"What is loss function in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 In addition to the various layer-types of CNN architecture introduced in the previous
section, the ﬁnal classiﬁcation is achieved from the output layer that usually the last layer
of the FC layer, as shown in Figure 2. Different loss functions also affect the performance of
the CNN architecture and are applied to different visual tasks (e.g., image classiﬁcation,
face recognition, and object recognition). Here are some commonly used loss functions in
CNN-based image classiﬁcation methods (inherit the content of Section 2.1.2), as shown
in Table 1.
All in all, Softmax+Cross-Entropy has become the usual loss function of the CNN
model. There are also many improved versions based on it, such as center-loss [ 82], L-
Softmax [83], A-Softmax [ 84], AM-Softmax [ 85], PEDCC-loss [ 86], etc., which play an
important role in different visual tasks.
Remote Sens. 2021, 13, 4712 8 of 51
Table 1. Common loss functions for CNN models.
Loss Function Equation Characteristic
L1 (MAE) Loss(y, y∗) = 1
m ×
m
∑
i=1
⏐⏐⏐⏐y∗
i −yi
⏐⏐⏐⏐
This function is widely used in
regression problems. L1 Loss is called
mean absolute error (MAE)
L2 (MSE) Loss(y, y∗) = 1
m ×
m
∑
i=1
( 

Document 3: 
 We choose this speciﬁc linear version of the model as it maps directly into the linear approaches that
have already been successfully used in the literature. This linear framework essentially captures
the class of linear factor models. Appendix C provides a detailed overview of the various models
for conditional SDFs and their relationship to our framework.
II. Estimation
A. Loss Function and Model Architecture
The empirical loss function of our model minimizes the weighted sample moments which can
be interpreted as weighted sample mean pricing errors:
L(ω|ˆg,It,It,i) = 1
N
N∑
i=1
Ti
T

1
Ti
∑
t∈Ti
Mt+1Re
t+1,iˆg(It,It,i)

2
. (4)
13Kozak, Nagel, and Santosh (2020) consider also cross-products of the characteristics. They show that the PCA
rotation of the factors improves the pricing performance. Lettau and Pelger (2020) extend this important insight
to RP-PCA rotated factors. We consider PCA based factors in III.J. Our main analysis focuses on conventional
long-short factors as these are the most commonly used models in the literature.
14As before we deﬁne as tangency portfolio one of the portfolios on the global mean-variance eﬃcient frontier.
12
Electronic copy available at: https://ssrn.com/abstract=3350138 

Document 4: 
 part of network learning. The ﬁrst thing that backpropagation needs to determine is the
loss function of the model. There are many loss functions, see Section 2.2.5. The loss
function in the Figure 2 is L2 Loss:
Loss(y, y∗) = 1
m ×
m
∑
i=1
(y∗
i −yi)2 (2)
By calculating the mean square error between y∗ and y, the network weights w
and bias b can be updated by obtaining the partial derivative of the loss function: w′ =
w −η ×(∂Loss/∂w) and b′= b −η ×(∂Loss/∂b). With a suitable learning rate η, the loss
of the y and y∗can be gradually minimized. That is to make the y closer to y∗, so as to
achieve the effect of network training.
Remote Sens. 2021, 13, 4712 4 of 51
Figure 2. The structure of the MLP . It hasn input values and m output values, including k hidden
units. xn is the input value. The direction of the arrow is the direction in which the input value is
transmitted. The hidden unit is hk, it receives the input value of the previous layer. ym is the output
unit, and the real value is y∗m.
2.2. CNN Architecture
The main structure of CNN is the convolutional layer, pooling layer, nonlinear acti- 

Document 5: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 6: 
 The most common way is to translate the problem into an unconditional asset pricing model
on sorted portfolios. Under additional assumptions one could obtain a valid SDF Mt+1 conditional
on a set of asset-speciﬁc characteristics It,i by its projection on the return space:
Mt+1 = 1 −ω⊤
t Re
t with ωt,i = f(It,i),
where It,i is a vector of q characteristics observed for N stocks and f(·) is a general, potentially
nonlinear and non-separable function. Most of the reduced-form asset pricing models approximate
this function by a (potentially very large) set of simple managed portfolios fj(·), such that f(It,i) ≈∑˜Nbasis
j=1 fj (It,i) ˜wj. The SDF then becomes a linear combination of these managed portfolios with
constant weights ˜ωj:
Mt+1 = 1 −
˜Nbasis
∑
j=1
˜wj ˜Rt+1,j with ˜Rt+1,j =
N∑
i=1
fj (It,i) Re
t+1,i, (A.1)
where ˜Rt+1 are the returns of ˜Nbasis managed portfolios that correspond to diﬀerent basis functions
in the characteristic space. The number of basis portfolios increases by the complexity of the
basis functions and the number of characteristics. The most common managed portfolios are 

","I couldn't find any information about a specific deep learning asset pricing model created by Luyang Chen. It's possible that the model was not widely published or documented, or that the name is incorrect.

However, I can provide some general information about loss functions in deep learning models, including those used for asset pricing.

In general, a loss function is a mathematical function that measures the difference between the predicted output of a model and the actual output. The goal of a loss function is to optimize the model's parameters to minimize this difference.

In the context of asset pricing, a common loss function used in deep learning models is the Mean Absolute Error (MAE) or the Mean Squared Error (MSE). These loss functions measure the average absolute difference between predicted and actual returns, or the average squared difference, respectively.

Another popular loss function for portfolio optimization and risk management is the Value-at-Risk (VaR) loss function. VaR measures the expected loss of a portfolio over a given time horizon with a specified probability.

Some more advanced loss functions that have been used in asset pricing models include:

1. Wasserstein distance: This loss function measures the maximum possible loss of a portfolio over a given time horizon.
2. Mean Absolute Error (MAE): This loss function measures the average absolute difference between predicted and actual returns.
3. Mean Squared Error (MSE): This loss function measures the average squared difference between predicted and actual returns.

If you have any more specific information about Luyang Chen's model, I may be able to provide a more accurate answer.",4.959188222885132,"In the deep learning asset pricing model developed by Luyang Chen and co-authors, the empirical loss function minimizes the weighted sample moments, interpreted as weighted sample mean pricing errors. The loss function is mathematically represented as:

\[ L(\omega|\hat{g}, I_t, I_{t,i}) = \frac{1}{N} \sum_{i=1}^{N} T_i \left\| \frac{1}{T_i} \sum_{t \in T_i} M_{t+1} R_{e, t+1, i} \hat{g}(I_t, I_{t,i}) \right\|^2. \]

This function aims to optimize the model's performance in estimating asset prices based on conditioning information and the dynamic states of the economy."
llama3.2:latest,"What is the model archtecture in deep learning asset pricing model made by Luyang Chen?
","Document 1: 
 Deep Learning in Asset Pricing∗
Luyang Chen† Markus Pelger‡ Jason Zhu§
August 4, 2021
Abstract
We use deep neural networks to estimate an asset pricing model for individual stock returns
that takes advantage of the vast amount of conditioning information, while keeping a fully
ﬂexible form and accounting for time-variation. The key innovations are to use the fundamental
no-arbitrage condition as criterion function, to construct the most informative test assets with
an adversarial approach and to extract the states of the economy from many macroeconomic
time series. Our asset pricing model outperforms out-of-sample all benchmark approaches in
terms of Sharpe ratio, explained variation and pricing errors and identiﬁes the key factors that
drive asset prices.
Keywords: Conditional asset pricing model, no-arbitrage, stock returns, non-linear factor
model, cross-section of expected returns, machine learning, deep learning, big data, hidden
states, GMM
JEL classiﬁcation: C14, C38, C55, G12
∗We thank Doron Avramov, Ravi Bansal, Daniele Bianchi (discussant), Svetlana Bryzgalova, Agostino Capponi, Xiaohong
Chen, Anna Cieslak, John Cochrane, Lin William Cong, Victor DeMiguel, Jens Dick-Nielsen (discussant), Kay Giesecke, Ste- 

Document 2: 
 the most promising direction for future empirical asset pricing research is developing a genuine
fusion of economic theory and machine learning. It is a natural marriage, as asset pricing theory
revolves around price formation through the aggregation of investor beliefs, which undoubtedly
enterpricesinsubtle,complex,andsometimessurprisingways.Atthesametime,machinelearning
constitutes a sophisticated collection of statistical models that flexibly adapt to settings with rich
and complex information sets.
Machine learning factor models are one such example of this fusion. Almost all leading theo-
reticalassetpricingmodelspredictalow-dimensionalfactorstructureinassetprices.Wherethese
models differ is in their predictions regarding the identity of the common factors. Much of the
frontier work in empirical asset pricing can be viewed as using the (widely agreed upon) factor
structure skeleton as a theory-based construct within which various machine learning schemes
are injected to conduct an open-minded investigation into the economic nature of the common
factors.
Our survey is inevitably selective and disproportionally influenced by our own research on
thesetopics.Wehavemainlyfocusedonmethodologicalcontributions,leavingadetailedreviewof
empirical discoveries via these methodologies for future work.A frequently discussed dichotomy
in the literature is observable factor versus latent factor models. While some of the methods we
discuss apply to observable factor settings (or hybrid settings),we have also skewed our coverage 

Document 3: 
 Chen L,Pelger M,Zhu J.2019. Deeplearninginassetpricing. Work.Pap.,Stanford Univ.,Stanford,CA
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 365
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
Chen NF,Roll R,Ross SA.1986.Economic forces and the stock market. J.Bus. 59(3):383–403
Chernozhukov V, Chetverikov D, Demirer M, Duflo E, Hansen C, et al. 2018. Double/debiased machine
learning for treatment and structure parameters. Econom.J. 21(1):C1–68
Chib S,Zeng X,Zhao L.2020.On comparing asset pricing models. J.Finance 75(1):551–77
Chinco A,Clark-Joseph AD,Ye M.2019.Sparse signals in the cross-section of returns. J.Finance 74:449–92
CongLW,TangK,WangJ,ZhangY.2021. Alphaportfolio:directconstructionthroughdeepreinforcementlearning
andinterpretableAI. Work.Pap.,Cornell Univ.,Ithaca,NY 

Document 4: 
 tional autoencoder model,augmented with additional explanatory variables.This model replaces
www.annualreviews.org  FactorModels,MachineLearning,andAssetPricing 341
Downloaded from www.annualreviews.org.  Guest (guest) IP:  188.146.34.35 On: Fri, 27 Dec 2024 20:41:20
the linear beta specification in Equation 6 with a more realistic and flexible beta function. The
Gu,Kelly & Xiu (2021) autoencoder model is the first deep learning model of equity returns that
explicitlyaccountsfortherisk-returntrade-off.Thankstorecentprogressinalgorithmsandcom-
putingpower,deeplearningmodelslikethisarereadilyavailableandincreasinglypopularamong
practitioners. Nevertheless, deep learning models are often criticized for their black-box nature.
Althoughthesemodelsarecomposedofsimplecompositefunctions(notmuchmorecomplicated
than a regression model), training them can be tedious and is sometimes more art than science.
Rigorous theoretical justification still lags far behind the evolution of model architectures and
training algorithms.
Continuous-timefactormodelscansometimesbepreferableformodelingthetime-varyingdy-
namicsofassetreturns,particularlywhenhigh-frequencyreturnsdataareavailable.Returnfactors
havecomplexdynamicssuchasstochasticvolatilityandjumps,andindividualassetreturnsrespond 

Document 5: 
 paper.
Feng, G., N. G. Polson, and J. Xu(2019): “Deep Learning in Characteristics-Sorted Factor Models,”
Working paper.
Freyberger, J., A. Neuhierl, and M. Weber(2020): “Dissecting characteristics nonparametrically,”
Review of Financial Studies, forthcoming , 33(5), 2326–2377.
Goodfellow, I., Y. Bengio, and A. Courville(2016): Deep Learning. MIT Press.
Goodfellow, I., J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville,
and Y. Bengio (2014): “Generative adversarial nets,” in Advances in neural information processing
systems, pp. 2672–2680.
Gu, S., B. Kelly, and D. Xiu(2019): “Autoencoder Asset Pricing Models,” Journal of Econometrics,
forthcoming.
Gu, S., B. T. Kelly, and D. Xiu(2020): “Empirical Asset Pricing Via Machine Learning,” Review of
Financial Studies, 33(5), 2223–2273.
Guijarro-Ordonez, J., M. Pelger, and G. Zanotti(2021): “Deep Learning Statistical Arbitrage,”
Working paper. 

Document 6: 
 neural network. Horel and Giesecke (2020) propose a signiﬁcance test in neural networks and apply it to house price
valuation.
3The intuition behind their and our approach can be best understood when considering the linear special cases.
Our approach can be viewed as a conditional, non-linear generalization of Kozak, Nagel, and Santosh (2020) with the
additional elements of ﬁnding the macroeconomic states and identifying the most robust conditioning instruments.
Fundamentally, our object of interest is the pricing kernel. Kelly, Pruitt, and Su (2019) obtain a multi-factor factor
model that maximizes the explained variation. The linear special case applies PCA to a set of characteristic based
factors to obtain a linear lower dimensional factor model, while their more general autoencoder obtains the loadings
to characteristic based factors that can depend non-linearly on the characteristics. We show in Section III.J how our
SDF framework and their conditional multi-factor framework can be combined to obtain an even better asset pricing
model.
5
Electronic copy available at: https://ssrn.com/abstract=3350138
on a machine learning algorithm can substantially improve the estimation. Bryzgalova, Pelger,
and Zhu (2020) use decision trees to build a cross-section of asset returns, that is, a small set of
basis assets that capture the complex information contained in a given set of stock characteristics.
Their asset pricing trees generalize the concept of conventional sorting and are pruned by a novel 

","I don't have specific information on the exact model architecture used by Luyang Chen's deep learning asset pricing model. However, I can tell you that it likely employs a neural network with a fundamental no-arbitrage condition as its criterion function, and utilizes various techniques such as constructing informative test assets through an adversarial approach to extract economic states from macroeconomic time series.",2.7259411811828613,"The deep learning asset pricing model constructed by Luyang Chen and colleagues utilizes a deep neural network architecture that incorporates a flexible non-linear factor model. This model aims to estimate stock returns and takes into account various conditioning information while accounting for time-variation. The innovations of their approach include the integration of the fundamental no-arbitrage condition in the criterion function, the use of an adversarial approach to construct informative test assets, and the extraction of economic states from macroeconomic time series. The model is designed to outperform traditional methods in terms of Sharpe ratio, explained variation, and pricing errors, ultimately identifying key factors that drive asset prices."
llama3.2:latest,"What is done instead of comparing the variance of the factors before and after the break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 assumption violations.
In the current work, we extend the theory of generalized residuals to common factor
models with continuous MVs. This extension allows us to identify various sources of misfit
in common factor models, such as the nonlinear relationship between MVs and LVs,
heterogeneous variance of MVs conditional on LVs, and nonnormality of LVs. In addition,
we suggest a summary GOF statistic that complements the local statistics conditional on
LV values, which can facilitate an overall judgment on misfit. For all the proposed GOF
statistics, we derive asymptotic reference distributions to construct formal statistical tests.
Ultimately, our proposed framework is expected to complement conventional GOF
diagnostics for common factor models.
The rest of the paper is organized as follows. In Section 2, we introduce the
common factor model and present our extended theory of generalized residuals, along with
example GOF test statistics suitable for testing various model assumptions. In Section 3,
the performance of the proposed test statistics are evaluated by Monte Carlo studies. In
Section 4, our GOF testing methods are illustrated with a real data example. The paper is
concluded with a discussion of the main findings and future directions in Section 5.
2. Theory
2.1. Common Factor Model
2.1.1. Latent Variable Measurement Model
Let Yij ∈ Rbe the individuali’s response for manifest variable (MV)j, and
Xi = (Xi1, . . . , Xid)
⊤ 

Document 5: 
 spectively; and et = [ e1t,· · · ,eN t ]
′
is the N × 1 idiosyncratic error that may have serial and
cross-sectional dependence along with heteroskedasticit y . W e deﬁne π1 = T1/T ∈ (0,1) as the
break fraction, which is assumed to be a ﬁxed constant. This i mplies that T1 is a sequence that
depends on T. F or notational simplicity , we suppress the dependence of T1 on T.
W e are interested in testing the null hypothesis of no struct ural break in the factor loadings,
i.e.,
H0 : λi1 = λi2 ∀i. (2.2)
against the alternative hypothesis that a non-negligible p ortion of the cross sections have a break
in their loadings at a common time, i.e.,
H1 : λi1 ̸= λi2 for i∈ J (2.3)
where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally 

Document 6: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

","Instead of directly comparing the variance of the factors before and after the break, the likelihood ratio test for structural changes in factor models uses a special feature of the estimated factors: the pre-break and post-break variances can be a singular matrix under the alternative hypothesis. This allows for a more powerful LR test that diverges faster than Wald-type tests and simulations confirm its better power in small samples when the break size is moderate.",2.655601501464844,"Instead of directly comparing the variance of the factors before and after the break in the likelihood ratio (LR) test for structural changes in factor models, the approach transforms the structural change problem into a low-dimensional problem by exploring a special feature of the estimated factors. Specifically, the test takes into account that the pre-break and post-break variances can be a singular matrix under the alternative hypothesis, which enhances the power of the LR test compared to Wald-type tests. This method allows for more powerful testing of changes in the estimated factors' variances without requiring a direct comparison of those variances."
llama3.2:latest,"How to determine the number of break in likehood ratio test for structural changes in factor models?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 3: 
 100,500 0.154 0.081 0.017 0.106 0.053 0.017 0.108 0.054 0.01 3 0.063 0.031 0.010
200,200 0.080 0.032 0.005 0.072 0.036 0.013 0.084 0.030 0.00 7 0.052 0.023 0.006
200,500 0.156 0.080 0.022 0.108 0.056 0.018 0.102 0.043 0.01 1 0.066 0.027 0.007
500,500 0.158 0.084 0.020 0.121 0.069 0.020 0.111 0.059 0.01 4 0.066 0.039 0.014
23
T able 4: Size of structural break tests with unknown break da te in a factor model with t-distributed
factors. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.004 0.002 0.000 0.015 0.008 0.003 0.028 0.007 0.00 0 0.128 0.090 0.061
100,200 0.038 0.018 0.002 0.023 0.008 0.001 0.048 0.016 0.00 2 0.032 0.012 0.006 

Document 4: 
 we can set B = C = Ir , and pseudo-factors Gcoincide with the original factors. Under H1 , we can
incorporate diﬀerent types of changes by controlling the ra nks of B and C. F ollowing DBH (2022),
loading changes can be divided into three types: Type (1), in which both B and C are singular
(i.e., both r1 and r2 are less than r, so (2.4) can capture the factor dimension enlargement caus ed
by the break); Type (2), in which only B or C is singular (emerging or disappearing factors); and
Type (3), in which both B and C are nonsingular (rotational change in loadings). In practi ce,
Types (1) and (2) are more common than Type (3).
When H1 is true, the pre- and post-break second moments of the pseudo -factors are BΣF B′
and CΣF C′ , respectively . Thus, various tests (e.g., the sup-W ald and sup-LM type statistics
developed by Chen et al. (2014) and Han and Inoue (2015)) prop ose to compare the subsample
second moments of the factors under the assumption that Σ F is constant over time. Although
these tests are consistent under the alternative hypothesi s (e.g., Han and Inoue, 2015), simulation
6
evidence shows that they have limited power in small samples when the break size is moderate. 

Document 5: 
 In this section, we estimate a factor model for the US industrial employment rates and apply
the proposed tests to check whether the factor loadings have undergone a structural change in
the past decade. Monthly data from January 2010 through Apri l 2022 are available from the US
Department of Labor for 84 industries. The data are a balance d panel with T = 147 ,N = 84.
W e use the information criteria ICp1 and ICp2 of Bai and Ng (2002), the ER (eigenvalue ratio)
and GR (growth ratio) of Ahn and Horenstein (2013), the empirical d istribution estimator of
Onatski (2010), and the bridge estimator of Caner and Han (20 14) to determine the number of
common factors in the data. The maximum number of factors is s et to 10. The estimated number
of factors by ICp1 and ICp2 is three; the methods of Onatski (2010) and Caner and Han (201 4)
detect two factors; and ER and GR of Ahn and Horenstein (2013) choose one common factor.
Therefore, we study the test results for the cases of one, two , and three common factors.
W e apply our LR and LR m tests to examine whether there exists a structural break in f actor
loadings and estimate the break date if the null hypothesis i s rejected. All of the settings for the
two tests are the same as those used in the simulation studies . 

Document 6: 
 References
Ahn, S. and Horenstein, A. 2013. Eigenvalue ratio test for the number of factors. Econometrica 81 ,
pp. 1203–1227.
Andrews, D.W.K., 1993. T ests for parameter instability and structural change with unknown
change point. Econometrica 61 , pp. 821–856.
Bai, J., 1997. Estimaing multiple breaks one at a time. Econometric Theory 13 , pp. 315–352.
Bai, J., 2000. V ector autoregressive models with structura l changes in regression coeﬃcients and
in variance-covariance matrices. Annals of Economics and Finance 1 , pp. 303–339.
43
Bai, J., 2010. Common breaks in means and variances for panel data. Journal of Econometrics 157 ,
pp. 78–92.
Bai, J. and Ng, S. 2002. Determining the number of factors in a pproximate factor models. Econo-
metrica 70 , pp. 191–221.
Bai, J., 2003. Inferential theory for factor models of large dimensions. Econometrica 71 , pp.
135–171.
Bai, J., Han, X., Shi, Y., 2020. Estimation and inference of c hange points in high-dimensional 

","The Break-in-Likelihood Ratio (BIR) test is a statistical method used to detect structural breaks in factor models. To determine the number of breaks in likelihood ratio, you can follow these steps:

1. **Model specification**: Start with an initial model that assumes no breaks in the data.
2. **Estimate the initial model**: Estimate the parameters of the initial model using maximum likelihood estimation (MLE).
3. **Compute the likelihood ratio test statistic**: Calculate the log-likelihood value for both the unrestricted and restricted models (without breaks). The likelihood ratio test statistic is given by:
```r
λ = 2 \* (L_restricted - L_unrestricted)
```
where `L` represents the log-likelihood function.

4. **Determine the critical values**: Obtain the critical values from a standard normal distribution or a chi-squared distribution, depending on the model specification and sample size.
5. **Iterate through different models**: Test for breaks in different models by introducing one break at a time. For example:
* Model 1: No breaks
* Model 2: One break after the first observation
* Model 3: Two breaks after the first and second observations
* ...
6. **Compute the BIR statistic**: For each model, compute the likelihood ratio test statistic using the same formula as in step 3.
7. **Compare to critical values**: Compare the computed BIR statistic to the critical values obtained in step 4. If the statistic exceeds the critical value, reject the null hypothesis of no break. Otherwise, accept it.

The number of breaks can be determined by iterating through different models until the likelihood ratio test statistic is no longer significant (i.e., below a certain threshold or not exceeding the critical value).

Alternatively, you can use software packages such as EViews, R, or Python libraries like `statsmodels` and `pandas` to implement the BIR test. These packages often provide built-in functions for estimating factor models and computing likelihood ratio test statistics.

Here is some example code in R to illustrate the process:
```r
# Load necessary libraries
library(fastM)

# Define the initial model
model <- fastm(data, method = ""Factor"", start_params = TRUE)

# Compute the likelihood ratio test statistic
L_restricted <- fastm(data, method = ""Factor"")
L_unrestricted <- fastm(data, method = ""Factor"", break = 1)
λ <- 2 * (L_restricted - L_unrestricted)

# Determine the critical values
alpha <- 0.05
critical_value <- qnorm(1 - alpha / 2)

# Iterate through different models
breaks <- 1:5
for (i in breaks) {
  # Introduce a break at the i-th observation
  model_break <- fastm(data, method = ""Factor"", start_params = TRUE, break = i)
  
  # Compute the likelihood ratio test statistic
  L_restricted_break <- fastm(data, method = ""Factor"")
  L_unrestricted_break <- fastm(data, method = ""Factor"", break = i)
  λ_break <- 2 * (L_restricted_break - L_unrestricted_break)
  
  # Compare to critical values
  if (λ_break > critical_value) {
    cat(""Reject the null hypothesis of no breaks.\n"")
  } else {
    cat(""Accept the null hypothesis of no breaks.\n"")
  }
}
```
Note that this is a simplified example and may not be representative of all scenarios. In practice, you should consider factors such as model specification, sample size, and data characteristics when implementing the BIR test.",10.365628719329834,"To determine the number of breaks in the likelihood ratio (LR) test for structural changes in factor models, the procedure involves the following steps:

1. **Initial Break Point**: Identify the initial break point, denoted as \( \hat{k}_1 \), using a method such as the Quasi-Maximum Likelihood (QML) method.

2. **Subsample Division**: Split the entire sample into two subsamples: one from the start to \( \hat{k}_1 \) and the other from \( \hat{k}_1 \) to the end of the sample.

3. **Conduct Sup-LR Tests**: For each subsample, conduct a sup-LR test with \( m = 1 \). If the null hypothesis is rejected at a specified significance level \( \alpha \), this suggests the presence of additional breaks.

4. **Estimate Further Break Points**: For the subsample where the null is rejected, estimate a new break point, and repeat the process by further dividing the subsample at this newly estimated break point. 

5. **Continue Until Non-Rejection**: Continue this splitting and testing process until the LR test does not reject the null hypothesis for all subsamples. 

6. **Count Breaks**: The number of break points is determined by counting the number of subsamples minus one. This gives a consistent estimate of the number of breaks when the significance level \( \alpha \) converges to zero slowly.

This method effectively tracks structural changes and their locations by continuously refining the sample based on the results of the statistical tests at each step."
llama3.2:latest,"What is equivalent of a factor model with structural brak in factor loading?
","Document 1: 
 where |J | /N → b0 ∈ (0,1] as N → ∞ .
Under H0, (2.1) is a standard factor model with time-invariant facto r loadings and r denotes
the number of original factors. Under H1 , it is well known that the factor model is observationally
equivalent to a model with time-invariant loadings and pote ntially more pseudo-factors (e.g., HI,
2015; Baltagi et al., 2017). T o capture the factor dimension augmentation caused by the break,
we follow the framework of DBH (2022) and set r as the number of pseudo-factors in (2.1). W e
set
Λ1 = Λ B, Λ2 = Λ C,
5
where Λ is an N× rmatrix with full column rank r, B and C are r× rmatrices, rank( B) = r1 ≤ r,
and rank( C) = r2 ≤ r.
F or a given split point k, deﬁne
X(1)
k = [ x1 ,...,x k ]′, X(2)
k = [ xk+1,...,x T ]′, F(1)
k = [ f1,...,f k ]′ , F(2)
k = [ fk+1,...,f T ]′ ,
e(1) 

Document 2: 
 loadings βSDF as a function of characteristics on diﬀerent stocks for the estimation and evaluation.
Therefore, we can evaluate the model on stocks that are not available in the ﬁrst part of the data.
Importantly, the conditional model also allows us to directly study the economic sources of risk in
terms of ﬁrm-characteristics.
Appendix C.D. Conditional Factor Models
A conditional factor model assumes that the SDF is spanned by a linear combination of con-
ditional risk factors, and hence it restricts the basis assets that span the SDF. In contrast to the
60
Electronic copy available at: https://ssrn.com/abstract=3350138
unconditional model, the SDF weights and loadings of the conditional risk factors are a function
of the characteristics and hence time-varying. We use the Instrumented Principal Component
Analysis (IPCA) of Kelly, Pruitt, and Su (2019) to illustrate this setup. This conditional factor
model directly models individual stock returns as a function of characteristics given by Re
t+1,i =
b⊤
t,ifIPCA
t+1 + ϵt+1,i with bt,i = I⊤
i,tΓb. Instead of allowing the SDF to be Mt+1 = 1 −∑N
i=1 ωt,iRe
t+1,i,
it is restricted to Mt+1 = 1 −∑K
k=1 wIPCA
t,k fIPCA 

Document 3: 
 EC08CH03-Bai ARI 17 October 2016 11:32
where λ = vec
(
/Lambda1′)
. Consider the following restriction on the factor loadings:
λ = Bδ + C, (19)
where δ is a vector of free parameters with dim( δ) ≤ dim( λ). In general, B and C are known ma-
trices and vectors deﬁned by either identifying restrictions or other structural model restrictions.
In view of Equation 19, we may rewrite the restricted factor model in Equation 18 as
y = Zδ + vec( E),
where y = vec( X) − (IN ⊗ F)C and Z = [( IN ⊗ F)B]. If we impose some distributional assump-
tions on the error terms, for example, vec (E|Z) ∼ N(0 ,R ⊗ IT −s )f o rs o m e N × N positive
deﬁnite matrix R, such models can be estimated using the Bayesian algorithm from Bai & Wang
(2015).
11.3. Structural Vector Autoregression and Restricted Dynamic Factor Models
The dynamic factor models also bring new insight into the estimation of structural vector autore-
gression (SVAR) models with measurement errors. Consider a traditional SVAR given by
A( L)Zt = at , 

Document 4: 
 100,200 0.998 0.984 0.792 0.861 0.704 0.424 1.000 1.000 1.00 0 1.000 1.000 0.997
100,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
200,200 1.000 0.980 0.784 0.860 0.702 0.419 1.000 1.000 1.00 0 1.000 1.000 0.999
200,500 1.000 1.000 1.000 1.000 1.000 0.998 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.00 0 1.000 1.000 1.000
24
T able 6: Power of structural break tests with unknown break d ate as the magnitude of the break in factor
loadings increases. ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
λi2 = λi1 + εi, εi
i.i.d.∼ N (0, b · Ir0 )
N = 100 , T = 100 , ρ = 0
b
0.0 0.015 0.005 0.000 0.026 0.007 0.002 0.055 0.018 0.003 0.158 0.111 0.074 

Document 5: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 6: 
 model that specifies the relationship between LVs and MVs.
A widely used LV measurement model is the common factor model (e.g., Bollen,
1989; Jöreskog, 1969; Kaplan, 2008; Kline, 2023; Lawley and Maxwell, 1971), particularly
when analyzing continuous data. The common factor model requires MVs to be linearly
dependent on LVs, aiming to attribute the dependencies among MVs to LVs. This model
has been widely used for testing theories about the number of LVs and the pattern of
MV-LV dependency (i.e., confirmatory factor analysis), or for learning these structures
from data (i.e., exploratory factor analysis).
Most of the commonly used measurement models, however, rely on strong
parametric assumptions. For example, in the common factor model, the conditional mean
of an MV given LVs is a linear function of LVs, and the conditional variance of an MV is
constant. In addition, it is often assumed that the MVs and LVs are normally distributed.
These restrictive assumptions may be violated when analyzing real-world data, resulting in
poor model-data fit and potentially erroneous inferences.
To prevent fallacious inferences, various goodness-of-fit (GOF) assessment tools
have been developed for common factor models. A majority of existing GOF diagnostics
rely on residual means, residual covariances, or deviances to reflect the discrepancies 

","In factor analysis, the factor loading represents the correlation between an observed variable and a latent variable (factor). When you have a structural brake (or constraint) on the factor loading, it means that you want to restrict the magnitude or sign of the loading.

The equivalent concept to a factor model with structural break in factor loading is a factor model with structural constraints or regularization. Here are some ways to interpret this:

1. **Factor loading constraints**: Instead of having all possible factor loadings (0 ≤ λ ≤ 1), you can impose additional constraints, such as:
	* Zero-loading: λ = 0
	* Unit-load: λ = 1
	* Sign constraint: λ ≥ 0 or λ ≤ 0
2. **Regularization**: You can add a penalty term to the objective function (e.g., sum of squared errors) to discourage large factor loadings.
3. **Partial correlations**: Instead of estimating direct factor loadings, you can estimate partial correlations between variables and the latent variable(s). This approach allows for indirect relationships between variables.
4. **Factor mixture models**: You can model a mixture of factors with varying levels of loading constraints.

Some common techniques to handle structural breaks in factor analysis include:

1. **Change-point detection**: Identify significant changes in factor loadings over time or across different groups.
2. **Structural equation modeling (SEM)**: Estimate the relationships between variables while accounting for latent variables and structural constraints.
3. ** Bayesian approaches**: Use prior distributions to incorporate uncertainty about factor loadings and structural breaks.

These techniques can be used to handle structural breaks in factor analysis, but they may require more advanced statistical knowledge and computational resources.",5.133805990219116,"A factor model with a break in its factor loadings is observationally equivalent to a model without changes in the loadings but with a change in the variance of its factors. This transformation simplifies the analysis of structural changes by reducing it from a high-dimensional problem to a low-dimensional problem, allowing for the likelihood ratio (LR) test to be more powerful in detecting variance changes in the estimated factors."
llama3.2:latest,"What is null hypothesis is likehood ratio test for structural changes in factor model?
","Document 1: 
 arXiv:2206.08052v2  [econ.EM]  5 Dec 2023
The likelihood ratio test for structural changes in factor m odels†
Jushan Bai 1, Jiangtao Duan 2, Xu Han 3
1Columbia University, 2Xidian University and 3City University of Hong Kong
Abstract:
A factor model with a break in its factor loadings is observat ionally equivalent to a
model without changes in the loadings but with a change in the variance of its factors.
This approach eﬀectively transforms a high-dimensional st ructural change problem into
a low-dimensional problem. This paper considers the likeli hood ratio (LR) test for
a variance change in the estimated factors. The LR test impli citly explores a special
feature of the estimated factors: the pre-break and post-br eak variances can be a singular
matrix under the alternative hypothesis, making the LR test diverging faster and thus
more powerful than W ald-type tests. The better power proper ty of the LR test is
also conﬁrmed by simulations. W e also consider mean changes and multiple breaks.
W e apply this procedure to the factor modeling of the US emplo yment and study the
structural change problem using monthly industry-level da ta.
Key words and phrases: High-dimensional factor models, Str uctural breaks, LR test
JEL classiﬁcation: C12, C38, C55 

Document 2: 
 LQ(k) = klog(| ˜Σ1 (k)|) + ( T − k) log( | ˜Σ2(k)|)
= klog(|Q′ | · | ˆΣ1 (k)| · | Q|) + ( T − k) log( |Q′| · | ˆΣ2(k)| · | Q|)
= 2 Tlog(|Q|) + klog(| ˆΣ1 (k)|) + ( T − k) log( | ˆΣ2 (k)|)
The log-likelihood of no change for the entire sample become s
LQ0 = Tlog(|T−1
T∑
t=1
˜gt,Q ˜g′
t,Q|) = Tlog(|Q′ | · | T−1
T∑
t=1
ˆgt ˆg′
t| · | Q|) = 2 Tlog(|Q|)
because T−1 ∑ T
t=1 ˆgt ˆg′
t = Ir under (3.1). Thus, the likelihood ratio test under this alte rnative
identiﬁcation condition can be expressed as
LRQ(k) ≡ − klog(| ˜Σ1(k)|) − (T − k) log( | ˜Σ2 (k)|) + LQ0 

Document 3: 
 in Section 3.
2 Performance of default bootstrap and permutation methods
To review from Section 1, the problem statement is to test the following factor model:
Yt = LtXt + ϵt for t = 1, . . . , T, (2.1)
for outcomes Yt ∈ Rp, fixed and known exposures Lt ∈ Rp×k, unobserved factor returns Xt ∈ Rk and
residuals ϵt ∈ Rp. We seek to test the null hypothesis H0 that the time series of residuals for each
asset are independent:
H0 : ϵ·,1, ϵ·,2, . . . , ϵ·,p ∈ RT are jointly independent. (2.2)
Sections 2.1-2.2 now explain why naive permutation and bootstrap tests can yield false positive rates
of up to 100%, as in Figure 3. The main challenge is that the estimated OLS residuals ˆ ϵols do not
satisfy the same independence properties as the true residuals.
2.1 Naive residual permutation tests are invalid
For simplicity, we assume for this section that the vectors of residuals ϵ1, . . . , ϵT
i.i.d.∼ Pϵ are i.i.d. This
assumption plus H0 together imply that separately permuting the residuals of each asset does not
change the joint law of all of the residuals:
ϵ :=

 

Document 4: 
 aresettozero.
• Scenario2:Changeinonecolumn.
Let1−1 = (ρ(−)
ij ) with
ρ(−)
1j = ρ(−)
j1 =



sign(ρj−1)|ρ|
1+ 3(p− 1)ρ2/2
for j> 1,
1 for j= 1.
Theremainingcorrelationcoefficientsarezero.
• Scenario3:Allcorrelationcoefficientsarechanged.
Here,weput ρij = ρ|j−i| fori, j∈ {1, . . . ,p}.
• Scenario4:Changeinthenumberoffactors.
Thenumberoffactorsinthetruemodelisincreasedto K+ ˜Kwith˜K∈ {1, . . . ,10}.
Thesefourscenariosleadtodifferenttypesoffactormodelsunderthealternativehypothesis.Forinstance,incaseof
Scenario1,asinglechangeinthecorrelationmatrixofresidualsisassumed,whereasScenario2leadstochangesinthefirst
column(row)of 11.Scenario3correspondstochangesinallelementsof 11althoughtheirmagnitudebecomessmaller 

Document 5: 
 However, it can be shown that H′
0gt = R′ ηt for some orthonormal matrix R. Because a pre-
multiplication by R does not change the distribution of an independent standard normal vector,
we can still replace Ω with ˆΩ when simulating the limiting null distribution.
Note that the presence of Ω in (3.5) is due to the potentially m isspeciﬁed likelihood function
(3.2), which assumes that gt,t = 1 ,· · · ,T are i.i.d. Gaussian under the null hypothesis. If (3.2)
correctly speciﬁes the likelihood, then the limiting distr ibution reduces to
sup
[ǫT ]≤k≤[(1−ǫ)T ]
LR(k) ⇒ sup
π∈[ǫ,1−ǫ]
[W1(π) − πW1(1)]′[W1(π) − πW1(1)]
π(1 − π) (3.7)
where W1(π) is an r(r+ 1) /2 vector of independent Brownian motions. The distribution in (3.7)
is the same as that used in conventional supreme type tests fo r a structural break, and the critical
values can be found in Andrews (1993).
If gt,t = 1 ,· · · ,T are i.i.d. Gaussian and the null hypothesis is true, the sup- LR statistic is 

Document 6: 
 200,200 0.844 0.677 0.333 0.736 0.498 0.187 1.000 1.000 0.99 9 1.000 0.998 0.956
200,500 0.999 0.999 0.984 0.999 0.997 0.951 1.000 1.000 1.00 0 1.000 1.000 1.000
500,500 1.000 0.999 0.985 0.998 0.995 0.934 1.000 1.000 1.00 0 1.000 1.000 1.000
T able 9: Power of structural break tests with unknown break d ate in the case of factor mean change.
ǫ = 0 .15.
N, T sup-W ald(HAC) sup-LM(HAC) sup-LR sup-LR m
10% 5% 1% 10% 5% 1% 10% 5% 1% 10% 5% 1%
ρ = 0
100,100 0.048 0.014 0.000 0.071 0.026 0.005 0.128 0.049 0.00 5 0.196 0.106 0.037
100,200 0.202 0.103 0.018 0.215 0.109 0.022 0.269 0.141 0.02 8 0.551 0.335 0.101
100,500 0.653 0.499 0.221 0.649 0.508 0.222 0.678 0.531 0.25 6 0.967 0.919 0.611
200,200 0.199 0.096 0.020 0.215 0.107 0.019 0.277 0.149 0.03 2 0.556 0.341 0.115 

","The null hypothesis likelihood ratio (NHLR) test is a statistical method used to detect structural changes in a factor model. Here's an overview:

**Null Hypothesis:** The null hypothesis states that there are no significant structural changes in the factor model. In other words, it assumes that the factor loading parameters remain constant across all time periods.

**Alternative Hypothesis:** The alternative hypothesis suggests that there are significant structural changes in the factor model, i.e., the factor loading parameters have changed over time.

**Null Hypothesis Likelihood Ratio (NHLR) Test:**

The NHLR test compares the likelihood of observing the data under two sets of models:

1. **Null Model:** A factor model with no structural changes (no changes in factor loading parameters).
2. **Alternative Model:** A factor model with significant structural changes (changes in factor loading parameters).

The test statistic is calculated as the ratio of the log-likelihood of the null model to the log-likelihood of the alternative model:

ΔL = ln(L_null) - ln(L_alt)

where L_null and L_alt are the likelihoods of the null and alternative models, respectively.

**Interpretation:**

If the test statistic is significant (usually at a certain significance level, e.g., 5%), it suggests that there are significant structural changes in the factor model. In other words, the data are more likely to have been generated by an alternative model with changing factor loading parameters than by the null model.

**Steps:**

1. Estimate the factor model using the null model.
2. Estimate the factor model using the alternative model.
3. Calculate the test statistic (ΔL) and its p-value.
4. Compare the p-value to a predetermined significance level (e.g., 5%).

If the p-value is below the significance level, you reject the null hypothesis and conclude that there are significant structural changes in the factor model.

**Advantages:**

1. Robustness to sample size issues
2. Efficient use of data information
3. Easy interpretation

**Disadvantages:**

1. Assumes a specific distribution for the errors (e.g., normal)
2. May not perform well with non-linear structural changes or complex relationships between factors.

**Software:**

The NHLR test can be implemented in various statistical software packages, such as:

* R: `nlr.test()` function
* Python: `statsmodels` library
* SAS: `PROC REG` and `PROC IML` procedures

Keep in mind that the specific implementation may vary depending on the software package you choose.",7.469177484512329,"The null hypothesis (H0) for the likelihood ratio test for structural changes in a factor model typically posits that there are no structural changes in the factor loadings or variances across the observed period. In the context of the provided snippets, this means that the time series of residuals for each asset are jointly independent, indicating that any dependencies occurring in the data can be attributed to the specified factor model without any structural shifts. The alternative hypothesis posits that there is a structural change, which may include changes in the factor loadings or variances of the factors."
llama3.2:latest,"Is it possible to test mutliple change with sup-LR test?
","Document 1: 
 limit is similar to that of the usual W ald test,
Waldm(k) ⇒ ∥ W(π) − πW(1)∥ 2
π(1 − π)
where W(π) is an r+ r(r+ 1) /2 = r(r+ 3) /2 standard Brownian motion. This test is nuisance-
parameter free.
Consider the simple weighting by k(T − k)/T2, let WWald = k(T − k)/T2 · Waldm(k). T ake
16
the maximum over the range say k= r+ 1,r + 2,· · · ,T − r− 1. The limiting distribution is simply
sup
π∈[0,1]
∥ W(π) − πW(1)∥ 2 ,
and the critical values do not depend on trimming parameter ǫ.
4.2 T esting multiple changes
In this section, we extend the sup-LR test to multiple changes. W e consider testing the null
hypothesis of no change versus the alternative hypothesis o f a prespeciﬁed number of changes. T o
allow for mchanges under the alternative hypothesis, let 0 = k0 <k1 <k2 <· · · <km <km+1 = T
and deﬁne
LR(k1 ,...,k m) = −
m∑
j=0 

Document 2: 
 the null hypothesis. As the value of a increases, all of the tests become more powerful, except
for W ald(HAC) and LM(HAC) when N = T = 100. The LR test has the highest power under
this DGP . The power of the LR m test increases at a slower rate as a increases. Our Theorem
3 demonstrates that the sup-LR diverges at the same rate as th e conventional sup-W ald when
both pre-break and post-break pseudo-factors have nonsing ular variances. However, our simulation
results indicate that sup-LR remains more powerful than sup -W ald under this scenario, as reported
in T able 7. Speciﬁcally , when the loading matrix undergoes a rotational change, sup-LR is notably
more powerful than sup-W ald with HAC variance. While we ackn owledge that the theoretical
power comparison under this setup may require analysis unde r local alternatives, we leave this as
a future research topic.
T able 8 presents the power against changes in the number of fa ctors. The post-break loadings
are equal to pre-break loadings multiplied by an r × r matrix, i.e., Λ 2 = Λ 1C. W e set C = 

Document 3: 
 Motivated by our real applications, our analysis so far assumes that the exposures Lt ∈ Rp×k are
constant within tiles. However, if Lt changes with every observation, a fix is to replace Lt with an
augmented exposure matrix L⋆
t :
L⋆
t :=



h
Lt Lt+1
i
t is oddh
Lt−1 Lt
i
t is even
∈ Rp×2k. (5.7)
By construction, L⋆
t only changes every two observations. E.g., for the first two time points, L⋆
1 =
L⋆
2 =

L1 L2

∈ Rp×2k. Furthermore, since Lt is a submatrix of L⋆
t , if the null holds for the original
model Yt = LtXt +ϵt, it also holds for the augmented model Yt = L⋆
t X⋆
t +ϵt, since we can set X⋆
t ∈ R2k
to equal (Xt, 0) for even t and (0, Xt) for odd t. Thus, after augmenting the exposures, we can apply
the mosaic permutation test (all tiles will contain exactly two observations to ensure L⋆
t is constant
within tiles). However, the cost is that we must estimate twice as many nuisance parameters when 

Document 4: 
 factor models. Additionally, our simulations and theory show the power and flexibility of the mosaic
permutation test, which can be used in combination with a wide variety of test statistics to quickly
detect unexplained correlations among variables. Lastly, although this paper focuses on applications
to financial factor models, our methods can also be applied to test the goodness-of-fit of pre-existing
factor models in any domain, including psychology (e.g., McCrae and John, 1992) and genetics (e.g.,
Gain and Fran¸ cois, 2021). That said, our work leaves open several possible directions for future
research.
24
Figure 10: This figure shows the power of the mosaic permutation test with an adaptive QMC statistic as well
as the power of the two oracles from Section 6. It shows that (1) the adaptive QMC statistic from Section
5.1 effectively adapts to the unknown sparsity of the alternative, and (2) the MPT does not lose much power
compared to an oracle procedure using the OLS residuals ˆ ϵols in place of the mosaic residuals ˆϵ. For example,
the average power difference between the MPT oracle and OLS double oracle (which use the same test statistic
but applied to different residual estimates) is 3%, and the maximum power difference is 10%. The dotted black
line shows the nominal level α = 0.05. All methods control the false positive rate when ρ = 0 and H0 holds.
• Confidence intervals. It would be interesting to produce confidence intervals that quantify how 

Document 5: 
 LRm (k1,...,k m) ⇒ sup
(π1 ,··· ,πm)∈Θ π,ǫ
m∑
j=0
∥ ¯Ω 1/2 [B(πj+1 )−B(πj )]∥ 2
πj+1 −πj
, (4.3)
where B(π) = U(π) − πU(1) is an (r+ r2) Brownian bridge process, and ¯Ω is deﬁned in Theorem
4.
These results depend on m, but mdoes not have to be correctly speciﬁed. W e can also consider
the double max type of test and the conditional test for multi ple breaks as in Bai and Perron (1998),
but we leave these as future research topics.
4.3 Determining the number of breaks
In practice, the number of breaks in a factor model is often unknown. W e follow Bai’s (1997)
sequential testing procedure to provide a consistent estim ate for the true number of breaks. The
procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples: 

Document 6: 
 factors can stil l be estimated consistently up to a rotation , and the sample factor variance wil l
remain asymptotical ly the same before and after the break, l eading to a loss of power in the test.
4 Extensions
4.1 Allowing mean change ingt
First we note that testing a change in variance also has power for changes in the mean. Nevertheless,
it might be interesting to explicitly allow for a shift in the mean of gt. It is common practice to
ﬁrst demean the data and then extract the principal componen ts so that the full sample average
T−1 ∑ T
t=1 ˆgt is always exactly zero by construction. This property is use d here. T o incorporate a
mean change in gt, we simply redeﬁne the pre-break and post-break variance es timators:
˜Σ1 (k) = 1
k
k∑
t=1
(ˆgt − ¯
ˆg1)(ˆgt − ¯
ˆg1)′, with ¯
ˆg1 = 1
k
k∑
t=1
ˆgt,
˜Σ2 (k) = 1
T − k
T∑
t=k+1
(ˆgt − ¯
ˆg2)(ˆgt − ¯
ˆg2)′ with ¯
ˆg2 = 1 

","Yes, the Sup-LR (Supervised Local False Discovery Rate) test is capable of testing multiple changes. In fact, it can be extended to test for multiple breaks in a factor model, as mentioned in Section 4.3 of the text. This involves identifying the initial break point and then using an additional test procedure to determine if there are any additional breaks.",2.7362096309661865,"Yes, it is possible to test for multiple changes using the sup-LR test. The context indicates that the sup-LR test has been extended to allow for multiple changes under the alternative hypothesis, enabling the testing of the null hypothesis of no change against a prespecified number of changes, as outlined in Document 1 and Document 5."
llama3.2:latest,"What is misbehavior of the residual bootstrap in high dimension?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 ϵ = 2. Each simulation uses the standard estimate of σ2
ϵ from
the generated ϵi’s. rρ(κ; G) was computed using a ﬁrst run of simulations with
ϵi
iid∽ G. The Huber loss in this plot is Huber 1 and not the default Huber 1.345 of
the rlm function.
to bootstrapping from the residuals. In the case of random design, it makes also a lot
of intuitive sense to use the pairs bootstrap, since resampling the predictors might be
interpreted as mimicking the data generating process.
However, as in residual bootstrap, it is clear that the pairs bootstrap will have problems,
at least in quite high dimensions. In fact, when resampling the Xi’s from ˆF, the number
of times a certain vector Xi0 is picked has asymptotically Poisson(1) distribution. So the
expected number of diﬀerent vectors appearing in the bootstrapped design matrix X∗ is
n(1 −1/e). When p/n is large, with increasingly high probability the bootstrapped design
matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that 

Document 3: 
 accurate conﬁdence intervals in moderately high dimensions.
In Section 4, we discuss another resampling scheme, the jackknife. We focus on the
jackknife estimate of variance and show that it has similarly poor behavior in high dimen-
sions. In the case of L2 loss with Gaussian design matrices, we further prove that the
jackknife estimator over estimates the variance of our estimator by a factor of 1 /(1 −p/n);
we also provide corrections for other losses that improve the jackknife estimate of variance
in moderately high dimensions.
We rely on simulation results to demonstrate the practical impact of the failure of the
bootstrap. The settings for our simulations and corresponding theoretical analyses are ide-
alized, without many of the common settings of heteroskedasticity, dependency, outliers and
so forth that are known to be a problem for bootstrapping. This is intentional, since even
these idealized settings are suﬃcient to demonstrate that the standard bootstrap methods
have poor performance. For brevity, we give only brief descriptions of the simulations in
what follows; detailed descriptions can be found in AppendixD.1.
Similarly, we focus on the basic implementations of the bootstrap for linear models.
While there are many proposed alternatives (often for speciﬁc loss functions or types of
data), the standard methods we study are most commonly used and recommended in prac-
tice. Furthermore, to our knowledge none of the alternative bootstrap methods we have 

Document 4: 
 regression, by making assumptions on the diagonal entries of the hat matrix. In our context,
these assumptions would be satisﬁed only if p/n →0. Hence those interesting results do
not apply to the present study. We also note that Hall (1992, p. 167) contains cautionary
notes about using the bootstrap in high-dimension.
While there has not been much theoretical work on the bootstrap in the setting where
p/n →κ ∈(0,1), one early work of Bickel and Freedman (1983) considered bootstrap-
ping scaled residuals for least-squares regression when κ > 0. They show that when
p/n → κ ∈ (0,1), there exists a data-dependent direction c, such that c′ˆβ∗
LS does not
have the correct asymptotic distribution (Bickel and Freedman, 1983, Theorem 3.1, p.39),
i.e., its distribution is not conditionally in probability close to the sampling distribution of
c′ˆβLS. Furthermore, they show that when the errors in the model are Gaussian, under the
assumption that the diagonal entries of the hat matrix are not all close to a constant, the
empirical distribution of the residuals is a scaled-mixture of Gaussian, which is not close to
the original error distribution.
As we previously explained, in this work we instead only consider inference forpredeﬁned
contrasts υ′β. The important and interesting problems pointed out in Bickel and Freedman 

Document 5: 
 Journal of Machine Learning Research 19 (2018) 1-66 Submitted 1/17; Revised 12/17; Published 08/18
Can We Trust the Bootstrap in High-dimensions?
The Case of Linear Models
Noureddine El Karoui nkaroui@berkeley.edu, n.elkaroui@criteo.com
Criteo AI Lab
32 Rue Blanche
75009 Paris, France
and
Department of Statistics
University of California
Berkeley, CA 94270, USA
Elizabeth Purdom epurdom@stat.berkeley.edu
Department of Statistics
University of California
Berkeley, CA 94270, USA
Editor: Guy Lebanon
Abstract
We consider the performance of the bootstrap in high-dimensions for the setting of linear
regression, where p<n but p/nis not close to zero. We consider ordinary least-squares as
well as robust regression methods and adopt a minimalist performance requirement: can
the bootstrap give us good conﬁdence intervals for a single coordinate of β (where β is the
true regression vector)?
We show through a mix of numerical and theoretical work that the bootstrap is fraught
with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","The misbehavior of the residual bootstrap in high dimensions is that it gives anti-conservative estimates (inflated Type I error) and very conservative estimates (severe loss of power), specifically when the ratio p/n grows, where p is the number of features and n is the sample size.",2.0032007694244385,"The misbehavior of the residual bootstrap in high dimensions is primarily due to the fact that the residuals tend to have a very different distribution compared to the true errors. Specifically, their distributions differ not only in basic properties such as variances but also in more general aspects, including their marginal distributions. This discrepancy can lead to anti-conservative estimates, resulting in inflated Type I error rates. When the ratio of predictors to observations (p/n) is large, the residual bootstrap fails to provide accurate inferences, highlighting the challenges of using standard bootstrap methods in high-dimensional regression settings."
llama3.2:latest,"How misbehavior of the residual in high dimension can be explained?
","Document 1: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 2: 
 decent fit of the linear normal one-factor model, our method detected misfit in various
model assumptions: misfit in the LV density, the MV-level mean function, and the
MV-level variance function.
In factor analysis, it is common to assume the joint multivariate normality of MVs
and LVs in order to perform ML estimation. Various well-known methods exist for
assessing normality at the level of observed variables (e.g., Mardia, 1970; Mardia, 1980);
however, to our knowledge, assessing the normality assumption for LVs is less common.
This LV-level misfit can be conveniently assessed by generalized residuals, which sometimes
GENERALIZED RESIDUALS 29
can explain non-normality at the MV level, as is the case in our empirical example.
Furthermore, graphical plots such as the right panel of Figure 8 provide intuitive guidance
for model modification.
In the MV-level fit assessment, our method helps to identify curvilinear
relationships in the mean and/or variance functions. In the case of item 7 in our empirical
example, the misfitting linear normal factor model tended to overestimate the conditional
mean of log-RT while underestimating the conditional variance at the high end of the LV.
Such misfits can be particularly problematic if researchers’ focus is on investigating
extremely slow responders. Similar discussions can also be found in Liu and Wang (2022),
where similar patterns of curvatures in the conditional moment functions have been
identified using a semiparametric approach. 

Document 3: 
 P
→λ⋆. This gives that 2Λj is a
multiple of aχ2
1 variable with multiplicative factor given byκσ2
⋆/λ⋆.
This rough analysis shows that the distribution of the LLR in high dimensions deviates from aχ2
1 due
to the coupled eﬀects of two high-dimensional phenomena. The ﬁrst is the inﬂated variance of the MLE,
which is larger than classically predicted. The second comes from the termλ⋆, which is approximately equal
to Tr
(
H−1( ˆβ)
)
/n, where H( ˆβ) = ∇2ℓ( ˆβ) is the Hessian of the negative log-likelihood function. In the
classical setting, this Hessian converges to a population limit. This is not the case in higher dimensions and
the greater spread in the eigenvalues also contributes to the magnitude of the LRT.
5 Broader Implications and Future Directions
This paper shows that in high-dimensions, classical ML theory is unacceptable. Among other things, classical
theory predicts that the MLE is approximately unbiased when in reality it seriously overestimates eﬀect
magnitudes. Since the purpose of logistic modeling is to estimate the risk of a speciﬁc disease given a patient’s
observed characteristics, say, the bias of the MLE is extremely problematic. As we have seen, an immediate
consequence of the strong bias is that the MLE either dramatically overestimates, or underestimates, the
chance of being sick. The issue becomes increasingly severe as either the dimensionality or the signal 

Document 4: 
 data IRT (e.g., Wang, 2010).
Finally, exploring efficient ways to compute and visualize generalized residuals when
dealing with high-dimensional LVs is a potential topic for future research. When the latent
dimensionality is high, it is impossible to construct evaluation points using an outer
product grid as the number of evaluation points grows exponentially fast. For example, in
a five-dimensional case, examining residuals at just five different points per LV leads to
55 = 3, 125 evaluations, making it computationally intensive and posing a challenge for
generating graphical plots. Additionally, selecting which grids of LVs to summarize the
results would also be challenging. Formulating partially marginalized residuals conditioned
only on one or two LV(s) may yield more informative diagnostics.
GENERALIZED RESIDUALS 30
References
Alexander, P. A., Dumas, D., Grossnickle, E. M., List, A., & Firetto, C. M. (2016).
Measuring relational reasoning.The Journal of Experimental Education, 84(1),
119–151.
Bentler, P. M. (1990). Comparative fit indexes in structural models.Psychological bulletin,
107(2), 238.
Bentler, P. M. (1995).Eqs structural equations program manual(Vol. 6). Multivariate
software Encino, CA.
Bickel, P. J., & Doksum, K. A. (2015).Mathematical statistics: Basic ideas and selected 

Document 5: 
 pressed sensing. IEEE Transactions on Information Theory, 57(10):6920–6941, 2011.
[26] Noureddine El Karoui. Asymptotic behavior of unregularized and ridge-regularized high-dimensional
robust regression estimators: rigorous results.arXiv preprint arXiv:1311.2445, 2013.
[27] Noureddine El Karoui. On the impact of predictor geometry on the performance on high-dimensional
ridge-regularized generalized robust regression estimators.Probability Theory and Related Fields, pages
1–81, 2017.
23
[28] Noureddine El Karoui, Derek Bean, Peter J Bickel, Chinghway Lim, and Bin Yu. On robust regression
with high-dimensional predictors. Proceedings of the National Academy of Sciences, 110(36):14557–
14562, 2013.
[29] Jianqing Fan and Jinchi Lv. Nonconcave penalized likelihood with NP-dimensionality.IEEE Transac-
tions on Information Theory, 57(8):5467–5484, 2011.
[30] Yingying Fan, Emre Demirkaya, and Jinchi Lv. Nonuniformity of p-values can occur early in diverging
dimensions. https://arxiv.org/abs/1705.03604, May 2017.
[31] David Firth. Bias reduction of maximum likelihood estimates.Biometrika, 80(1):27–38, 1993. 

Document 6: 
 (∑ψ(e′
j))2 , (2)
where hi is the i-th diagonal entry of the hat matrix, e′
j = ej/s, sis a estimate of σ, and ψ
is the derivative of ρ, assuming ψ is a bounded and odd function (see Davison and Hinkley,
1997 for a complete description of its implementation for the bootstrap and McKean et al.,
1993 for a full description of the regularity conditions).
Unlike the correction for L2 loss mentioned earlier, however, the scaling described in
Equation (2) for the residuals is an approximate variance correction, and the approximation
depends on assumptions that do not hold true in higher dimensions. The error rate of
conﬁdence intervals in our simulations based on this rescaling show no improvement in high
dimensions over that of simple bootstrapping of the residuals. This could be explained by
the fact that standard perturbation analytic methods used for the analysis of M-estimators
in low-dimension, which are at the heart of the correction in Equation (2), fail in high-
dimensions.
2.2 Understanding the Behavior of the Residual Bootstrap
This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements 

","The concept of ""residual"" in machine learning refers to the difference between the predicted output and the actual output. In high-dimensional spaces, the misbehavior of residuals can be attributed to several factors:

1. **Overfitting**: When a model is too complex for the amount of training data available, it may fit the noise in the training data rather than the underlying patterns. This leads to overestimation of the residual values.
2. **High-dimensional curse**: As the number of features (dimensionality) increases, the capacity of the model to represent the data also increases. However, this comes at the cost of increased sensitivity to noise and outliers in the data, leading to unstable residuals.
3. **Non-linear relationships**: In high-dimensional spaces, non-linear relationships between variables can lead to complex patterns that are difficult for models to capture. This can result in residuals that do not accurately represent the underlying dynamics.
4. **Noise and variability**: High-dimensional data often contains more noise and variability than lower-dimensional data. This can lead to residuals that are more prone to fluctuations and less reliable.
5. **Model complexity**: Models with too many parameters or interactions can be prone to overfitting and poor generalization, leading to misbehavior of residuals in high dimensions.

To explain the misbehavior of residuals in high dimension, consider the following:

* **Residual plots**: Visualizing residual plots (e.g., residual vs. fitted values) can help identify patterns and outliers that may indicate issues with the model or data.
* **Dimensionality reduction**: Techniques like PCA (Principal Component Analysis), t-SNE (t-Distributed Stochastic Neighbor Embedding), or other dimensionality reduction methods can help reduce the number of features while preserving important information, potentially improving residuals.
* **Regularization techniques**: Regularization techniques like L1, L2, or elastic net regularization can help control overfitting and improve model performance on high-dimensional data.
* **Ensemble methods**: Ensemble methods like bagging, boosting, or stacking can combine multiple models to reduce the impact of individual model misbehavior and improve overall residuals.

By understanding the factors that contribute to misbehavior of residuals in high dimension, you can choose the most suitable approach to address these issues and improve your model's performance.",6.369244337081909,"The misbehavior of the residual bootstrap in high dimensions can be explained by the fact that the residuals tend to have a very different distribution compared to that of the true errors. This discrepancy is evident in both simple properties, such as variances, and more general aspects, including the marginal distributions. The work of El Karoui et al. (2013) characterizes the asymptotic relationship between the residuals and true errors in high dimensions, which reveals that as the dimensionality increases, the classical assumptions of the bootstrap no longer hold, leading to a poor approximation of confidence intervals and other statistical inference derived from bootstrap methods."
llama3.2:latest,"What is a alternative residual bootstrap procedure called deconvolution?
","Document 1: 
 This explains why the scaled ˜ ri(i) is probably a good error distribution ϵ∗ to use in the
residual bootstrap when κis close to 0 or 1. We note that when κis close to 1, ˜ri(i) gives an
error distribution that is in general very diﬀerent from the distribution of ϵ. Our numerical
work of Section 2.3 shows that it is nonetheless a good error distribution from the point of
view of the residual bootstraps we consider.
Appendix B. Deconvolution Bootstrap
In the main text, we considered situations where our predictorsXi are i.i.d with an elliptical
distribution and assume for instance that Xi = λiξi, where ξi ∼N (0,Σ) and λi are i.i.d
scalar random variables with E
(
λ2
i
)
= 1. As described in the main text, if X is elliptical,
˜ei(i) is a convolution of the correct G distribution and a Normal distribution,
˜ei(i) ≃ϵi + ˜Zi,
where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the 

Document 2: 
 problem, and experiments demonstrate that increasing cardinality C is more effective
than going deeper or wider when we increase the capacity. The authors compared the
completely equivalent structures of the three mathematical calculations in Figure 18.
The experimental results show that block Figure 18c with grouped convolution is
more succinct and faster than the other two forms, and ResNeXt uses this structure as
a basic block.
5. Dilated Residual Networks (DRN). To solve the decrease in the resolution of the
feature map and the loss of feature information caused by downsampling. However,
simply removing subsampling steps in the network will reduce the receptive ﬁeld. So,
Yu et al. [120] introduced dilated convolutions that are used to increase the receptive
ﬁeld of the higher layers and replaced a subset of the internal downsampling layer
Remote Sens. 2021, 13, 4712 19 of 51
based on the residual network, compensating for the reduction in receptive ﬁeld
induced by removing subsampling. Compared to ResNet with the same parameter
amount, the accuracy of DRN is signiﬁcantly improved in image classiﬁcation.
6. Other models. Veit et al. [ 121] drops some of the layers of a trained ResNet and
still have comparable performance. Resnet in Resnet (RiR) [ 122] proposed a deep
dual-stream architecture that generalizes ResNets and standard CNNs and is easily
implemented with no computational overhead. DropBlock [123] technique discards 

Document 3: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 4: 
 and mild moment requirements. In implementing the two alternative residual-bootstrap
methods described above, we use ˆσϵ,LS as our estimate of σϵ, including for bootstrapping
robust regression where ρ(x) ̸= x2.
Performance in bootstrap inferenceIn Figure 2 we show the error rate of conﬁdence intervals
based on the two residual-bootstrap methods we proposed above. We see that both methods
control the Type I error, unlike bootstrapping directly from the residuals, and that both
methods are conservative. There is little diﬀerence between the two methods with this
sample size (n= 500), though with n= 100, we observe the deconvolution performance to
be worse in L1 (data not shown).
The deconvolution strategy, however, depends on the distribution of the design matrix,
which in these simulations we assumed was Gaussian (so we did not have to estimate λi’s).
For elliptical designs (λi ̸= 1), the error rate of the deconvolution method described above,
with no adaptation for the design, was similar to that of uncorrected residuals in high
dimensions (i.e., > 0.25 for p/n = 0 .5). Individual estimates of λi might improve the
deconvolution strategy, but this problem points to the general reliance of the deconvolution
method on precise knowledge about the design matrix. The bootstrap using standardized
predicted errors, on the other hand, had a Type I error for an elliptical design only slightly 

Document 5: 
 where
˜Zi
iid∽ N(0,λ2
i∥ˆβρ(i) −β∥2
2)
and are independent of ϵi.
We proposed in Section 2.3 of the main text an alternative bootstrap method based on
using deconvolution techniques to estimate G (Method 1). Speciﬁcally, we proposed the
following bootstrap procedure:
1. Calculate the predicted errors, ˜ei(i)
2. Estimate |λi|∥ˆβρ(i) −β∥2 (the standard deviation of the ˜Zi)
3. Deconvolve in ˜ei(i) the error term ϵi from the ˜Zi term ;
4. Use the resulting estimates of G as the estimate of ˆG in residual bootstrapping.
34
Can We Trust the Bootstrap in High-dimensions?
B.1 Estimating ∥ˆβρ −β∥and the Variance of the Zi
Deconvolution methods that deconvolve ϵ from the ˜Zi require an estimate of the variance
of the ˜Zi. Equation (3) gives the variance as λ2
i∥ˆβρ(i) −β∥2
2, and we need to estimate this
quantity from the data. We use the approximation
∥ˆβρ(i) −β∥2 ≃∥ˆβρ −β∥2.
See AppendixA and references therein for justiﬁcation of this approximation.
Furthermore, as we note in the main text, in our implementation of this deconvolution in 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","Yes, the alternative residual bootstrap procedure proposed in the text is indeed called ""deconvolution"". It involves deconvolving the error term ϵi from the estimated ˜Zi term to obtain an estimate of G.",1.8353774547576904,"The alternative residual bootstrap procedure called deconvolution involves several steps to estimate the distribution of the error term while accounting for the influence of the predictors. Specifically, the procedure includes:

1. Calculating the predicted errors \(\tilde{e}_i(i)\).
2. Estimating \(|\lambda_i|\| \hat{\beta}_{\rho}(i) - \beta \|_2\), which represents the standard deviation of the \(\tilde{Z}_i\) term.
3. Deconvolving in \(\tilde{e}_i(i)\) the error term \(\epsilon_i\) from the \(\tilde{Z}_i\) term.
4. Using the resulting estimates of \(G\) as the estimate of \(\hat{G}\) in residual bootstrapping.

This procedure is particularly useful in situations where the predictors have an elliptical distribution and is intended to improve the accuracy of bootstrap inference in high-dimensional settings."
llama3.2:latest,"What is a alternative residual bootstrapinf from sttandarized residual?
","Document 1: 
 ∀i,yi = β′Xi + ϵi ,1 ≤i≤n, where Xi ∈Rp .
We are interested in the bootstrap or resampling properties of the estimator deﬁned as
ˆβρ = argminb∈Rp
n∑
i=1
ρ(yi −X′
ib) , where ρ is a convex function.
We consider the two standard methods for resampling to create a bootstrap distribution
in this setting. The ﬁrst is pairs resampling, where bootstrap samples are drawn from
the empirical distribution of the pairs ( yi,Xi). The second resampling method is residual
resampling, where the bootstrapped data consists of y∗
i = ˆβ′
ρXi+ˆϵ∗
i, where ˆϵ∗
i is drawn from
the empirical distribution of the estimated residuals, ei. We also consider the jackknife,
a resampling method focused speciﬁcally on estimating the variance of functionals of ˆβρ.
These three methods are extremely ﬂexible for linear models regardless of the method of
ﬁtting β or the error distribution of the ϵi.
The high dimensional setting: p/n→ κ∈ (0,1) In this work we call a high-dimensional
setting one where the number of predictors, p, is of the same order of magnitude as the 

Document 2: 
 samples under only the assumption that the residuals for each asset are locally exchangeable (defined
in Section 3). In particular, we make no assumptions about the marginal distributions of {ϵt}T
t=1 and
{Xt}T
t=1, allowing them to be arbitrarily heavy-tailed. Furthermore, to allow for changing market con-
ditions, our results allow the factor returns {Xt}T
t=1 to be arbitrarily non-stationary and the residuals
{ϵt}T
t=1 to be non-stationary across tiles. For instance, in our empirical application, we require that
the residuals are stationary within each week, but their distributions can change arbitrarily between
weeks.
To illustrate this contribution, we conduct semisynthetic simulations using the exposure matrix Lt
from the BFRE model for energy stocks on May 21st, 2020. For simplicity, we generate data Y from
Eq. 1.1 after sampling the residuals and factor returns as i.i.d. standard Gaussians, with T = 350
observations. We use the test statistic from Section 1.2. Figure 3 shows that in this simple Gaussian
setting, naive bootstrap and permutation testing methods yield essentially a 100% false positive rate
(we review intuition for this result in Section 2). In contrast, the mosaic permutation test has provable
validity in finite samples.
2. Power and flexibility. Our method permits the use of a wide set of test statistics while retaining 

Document 3: 
 15
El Karoui and Purdom
Implications for the Bootstrap For the purposes of the residual-bootstrap, Theorem 1 im-
plies that diﬀerent methods of estimating the residual distribution ˆG will result in similar
residual-bootstrap conﬁdence intervals as p/n→1, if ˆGhas the same variance. This agrees
with our simulations, where both of our proposed bootstrap strategies set the variance of ˆG
equal to ˆσ2
ϵ,LS and both had similar performance in our simulations for large p/n. Further-
more, as we noted, for p/n closer to 1, they both had similar performance to a bootstrap
procedure that simply sets ˆG = N(0,ˆσ2
ϵ,LS) (Figure 2) (see also AppendixA.3 for further
discussion of residual bootstrap methods which draw from the “wrong” distribution, i.e.,
forms of wild bootstraps (Wu, 1986)).
We return speciﬁcally to the bootstrap based on ˜ri(i), the standardized predicted errors.
Equation (3) tells us that the marginal distribution of ˜ei(i) is a convolution of the distribution
of ϵi and a normal, with the variance of the normal governed by the term∥ˆβρ−β∥2. Theorem
1 makes rigorous our previous assertion that as p/n →1, the normal term will dominate
and the marginal distribution of ˜ei(i) will approach normality, regardless of the distribution 

Document 4: 
 well beyond the expected 5%: we observe error rates of 10-15% for p/n= 0.3 and approxi-
mately 20% for p/n= 0.5. We see similar error rates for other robust-regression methods,
such as L1 and Huber loss, and also for diﬀerent error distributions and distributions of X
(Supplementary Figures A-1 and A-2). We explain some of the reasons for these problems
in Subsection 2.2 below.
2.1 Bootstrapping from Corrected Residuals
While resampling directly from the uncorrected residuals is widespread and often given
as a standard bootstrap procedure (e.g., Koenker, 2005; Chernick, 1999), the discrepancy
between the distribution of ϵi and ei has spurred more reﬁned recommendations in the
8
Can We Trust the Bootstrap in High-dimensions?
case of least-squares: form corrected residuals ri = ei/√1 −hi and sample the ϵ∗
i from the
empirical distribution of the ri −¯r (see e.g., Davison and Hinkley, 1997).
This correction is known to exactly align the variance of ri with that of ϵi regardless
of the design vectors Xi or the true error distribution, using simply the fact that the hat
matrix is a rank min(n,p) orthogonal projection matrix. We see that for L2 loss it corrects 

Document 5: 
 This misbehavior of the residual bootstrap can be explained by the fact that in high-
dimension, the residuals tend to have a very diﬀerent distribution from that of the true
errors. Their distributions diﬀer not only in simple properties, such as their variances, but
in more general aspects, such as their marginal distributions. To make these statements
precise, we make use of the previous work of El Karoui et al. (2013) and El Karoui (2013).
These papers do not discuss bootstrap or resampling issues, but rather are entirely focused
on providing asymptotic theory for the behavior of ˆβρ as p/n→κ∈(0,1); in the course of
doing so, they characterize the asymptotic relationship of ei to ϵi in high-dimensions. We
make use of this relationship to characterize the behavior of the residual bootstrap and to
suggest an alternative estimates of ˆG for bootstrap resampling.
Behavior of residuals in high-dimensional regression We now summarize the asymptotic
relationship between ei and ϵi in high-dimensions given in the above cited work (see Ap-
pendixA for a more detailed and technical summary). Let ˆβ(i) be the estimate of βbased on
ﬁtting the linear model of Equation (1) without using observation i, and ˜ej(i) be the error of
observation j from this model (the leave-one-out or predicted error), i.e., ˜ej(i) = yj−X′
jˆβ(i) 

Document 6: 
 is excluded from the regression, and ˜ ei(i) ≜ yi −X′
iˆβ(i) is the i-th predicted error (based
on the leave-one-out estimate of ˆβ). We also use the notation ˜ej(i) ≜ yj −X′
jˆβ(i). The hat
matrix is of course H = X(X′X)−1X′. o P denotes a “little-oh” in probability, a standard
notation (see van der Vaart, 1998). When we say that we work with a Gaussian design
with covariance Σ, we mean that Xi
iid∽ N(0,Σ). Throughout the paper, the loss function
ρ is assumed to be convex, R ↦→R+. We use the standard notation ψ = ρ′. We ﬁnally
assume that ρ is such that there is a unique solution to the robust regression problem—an
assumption that applies to all classical losses in the context of our paper.
2. Residual Bootstrap
We ﬁrst focus on the method of bootstrap resampling where ˆF is the conditional distribution
y|ˆβ,X. In this case the distribution of ˆβ∗ under ˆF is formed by independent resampling
of ϵ∗
i from an estimate ˆG of the distribution G that generated ϵi. Then new data y∗
i are
formed as y∗
i = X′ 

","According to the text, an alternative estimate of ˆG for bootstrap resampling is proposed by using the fact that the hat matrix H is a rank min(n,p) orthogonal projection matrix. Specifically, the authors suggest sampling from the empirical distribution of ri - ¯r, where ri = ei/√(1-Hi).

This correction aligns the variance of ri with that of ϵi, regardless of the design vectors Xi or the true error distribution.",2.723299980163574,"An alternative to the standard residual bootstrap is to form corrected residuals instead of resampling directly from the uncorrected residuals. For instance, the corrected residuals can be defined as \( r_i = \frac{e_i}{\sqrt{1 - h_i}} \), where \( e_i \) are the residuals from the model and \( h_i \) are the leverage values derived from the hat matrix. By sampling \( \epsilon^*_i \) from the empirical distribution of the corrected residuals \( r_i - \bar{r} \), the variance of the corrected residuals can be aligned with the variance of the true errors \( \epsilon_i \) regardless of the distribution of the design vectors \( X_i \) or the true error distribution. This method aims to address the discrepancy between the distribution of the residuals and the true errors, especially in high-dimensional settings."
llama3.2:latest,"What is pair bootstrap?
","Document 1: 
 i = X′
iˆβρ + ϵ∗
i, from which ˆβ∗
ρ is computed. This
method of bootstrapping assumes that the linear model is correct for the mean of y (i.e.,
that E (yi) = X′
iβ); it also assumes ﬁxed Xi design vectors because the sampling is con-
ditional on the Xi. In the second method, called pairs bootstrap, ˆF is an estimate of the
joint distribution of the vector ( yi,Xi) ∈Rp+1 given by the empirical joint distribution
of {(yi,Xi)}n
i=1; the corresponding resampling method resamples the pairs ( yi,Xi). This
method makes no assumption about the mean structure of yand, by resampling the Xi, also
does not condition on the values of Xi. For this reason, pairs resampling is often considered
to be more generally applicable than residuals resampling (see e.g., Davison and Hinkley,
1997).
1.3 Background: High-dimensional Inference of Linear Models
Recent research shows that ˆβρ has very diﬀerent asymptotic properties when p/nhas a limit
κ that is bounded away from zero than it does in the classical setting where p/n→0 (see 

Document 2: 
 B.4 Bootstrap Estimates ˆβ∗from ˆG . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.5 Estimation of λ2
i . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
C Alternative Weight Distributions for Pairs Bootstrap 38
C.1 Case Σ ̸= Idp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D Description of Numerics 39
D.1 Simulation Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 

Document 3: 
 v′ˆβ∗over repeated resampling from the pairs ( yi,Xi).
18
Can We Trust the Bootstrap in High-dimensions?
1 1
1
1
Ratio ( κ )
% Increase in Average CI Width
2
2
2
2
3
3
3
3
0.01 0.10 0.30 0.50
1
10
50
100
350 1
2
3
Normal
Ellip. Normal
Ellip. Exp
Figure 4: Comparison of width of 95% conﬁdence intervals of β1 for L2 loss:
Here we demonstrate the increase in the width of the conﬁdence interval due to
pairs bootstrapping. Shown on the y-axis is the percent increase of the average
conﬁdence interval width based on simulation ( n = 500), as compared to the
average for the standard conﬁdence interval based on normal theory in L2; the
percent increase is plotted against the ratio κ = p/n (x-axis). Shown are three
diﬀerent choices in simulating the entries of the design matrix X: (1) Xij ∼
N(0,1) (2) elliptical Xij with λi ∼ N(0,1) and (3) elliptical Xij with λi ∼
Exp(
√
2). The methods of simulation are the same as described in Figure 1;
exact values are given in Table A-2 in AppendixI. 

Document 4: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 5: 
 bootstrap and other resampling plans perform well when p is comparable to n.
Deﬁning success: accurate inference on β1 The common theoretical deﬁnition of whether
the bootstrap “works” is that the bootstrap distribution of the entire bootstrap estimate
ˆβ∗
ρ converges conditionally almost surely to the sampling distribution of the estimator ˆβρ
(see e.g., van der Vaart, 1998). The work of Bickel and Freedman (1983) on the residual
bootstrap for least squares regression, which we discuss in the background section 1.2, shows
that this theoretical requirement is not fulﬁlled even for the simple problem of least squares
regression.
In this paper, we choose to focus only on accurate inference for the projection of our
parameter on a pre-speciﬁed direction υ. More speciﬁcally, we concentrate only on whether
the bootstrap gives accurate conﬁdence intervals for υ′β. We think that this is the absolute
minimal requirement we can ask of a bootstrap inferential method, as well as one that is
meaningful from the standpoint of applied statistics. This is of course a much less stringent
requirement than performing well on complicated functionals of the whole parameter vector,
which is the implicit demand of standard deﬁnitions of bootstrap success. For this reason,
we focus throughout the exposition on inference for β1 (the ﬁrst element of β) as an example
of a pre-deﬁned direction of interest (where β1 corresponds to choosing υ = e1, the ﬁrst
canonical basis vector). 

Document 6: 
 matrix X∗ will no longer be of full rank. For example, if p/n >(1 −1/e) ≈0.63 then
with probability tending to one as n→∞, the bootstrapped design matrix X∗is singular,
even when the original design matrix X is of rank p < n. Bootstrapping the pairs in that
situation makes little statistical sense (El Karoui, 2010, Subsection 4.4; Zheng et al., 2014).
For smaller ratios of p/n, we evaluate the performance of pairs bootstrapping on sim-
ulated data. We see that the performance of the bootstrap for inference also declines
17
El Karoui and Purdom
dramatically as the dimension increases, becoming increasingly conservative (Figure 1). In
pairs bootstrapping, the error rates of 95%-conﬁdence-intervals drop far below the nominal
5%, and are essentially zero for the ratio of p/n= 0.5. Like residual bootstrap, this overall
trend is seen for all the settings we simulated under (Supplemental Figures A-1, A-2). For
L1 loss, even ratios as small as 0 .1 yield incredibly conservative bootstrap conﬁdence inter-
vals for ˆβ1, with the error rate dropping to less than 0.01. For Huber and L2 losses, the
severe loss of power in our simulations starts for ratios of 0 .3.
A minimal requirement for the distribution of the bootstrapped data to give reasonable
inferences is that the variance of the bootstrap estimator ˆβ∗ 

","Pair bootstrap is a resampling method used in statistics, where pairs (yi,Xi) are resampled instead of individual residuals or observations. This method makes no assumption about the mean structure of y and does not condition on the values of Xi, making it more generally applicable than residuals resampling.",1.998497486114502,"Pairs bootstrap is a resampling method used in statistical inference that involves resampling pairs of observations from a dataset, specifically the pairs of outcome variables (yi) and design vectors (Xi). Unlike residuals resampling, which assumes a specific mean structure for the outcome, pairs bootstrap does not make such assumptions about the mean of the outcome variable and allows for the resampling of the design vectors as well. This method is often considered to be more generally applicable because it does not condition on the values of Xi. It aims to estimate the joint distribution of the pairs (yi, Xi) based on their empirical distribution from the original sample."
llama3.2:latest,"How central limit theorem for hyperrectangles work?
","Document 1: 
 using the empirical or multiplier bootstraps.
The results in this paper substantially extend those obtain ed in [17] where
we considered the class A = Am of sets of the form A = {w ∈ Rp :
maxj∈J wj ≤ a} for some a∈ R and J ⊂ { 1,...,p }, but in order to obtain
much better dependence on n, we employ new techniques. Most notably, as
the main ingredient in the new proof, we employ an argument in spired by
Bolthausen [10]. Our paper builds upon our previous work [17 ], which in
turn builds on a number of works listed in the bibliography (s ee [18] for a
detailed review and links to the literature).
The organization of this paper is as follows. In Section 2, we derive
a Central Limit Theorem (CLT) for hyperrectangles in high di mensions;
that is, we derive a bound on ρn(A) for A = Are being the class of all
hyperrectangles and show that the bound converges to zero un der certain
conditions even when pis potentially larger or much larger than n. In Section
3, we extend this result by showing that similar bounds apply for A ⊂
Asi(a,d) being a class of simple convex sets and for A = Asp(s) being the
class of all s-sparsely convex sets. In Section 4, we derive high dimensio nal 

Document 2: 
 Assumption F1. Assumption 6(ii) strengthens Bai’s (2003) A ssumption F3 and ensures the con-
sistency of the heteroskedasticity and autocorrelation co nsistent (HAC) covariance estimator of
ˆg′
t ˆgt − Ir . Assumption 7 follows from Assumption 10 of DBH (2022) or Ass umption 8 of HI (2015).
Assumption 8 states that a basic functional central limit th eorem holds for the sums of gt under
the null hypothesis of no break.
9
3.2 Limiting distribution of sup-LR under the null hypothes is
It is well known that ˆGis an estimator of GH, where H = (Λ
′
Λ/N)(G
′ ˆG/T)V−1
N T and VN T denotes
the eigenvalues of XX′/NT. Bai’s (2003) Proposition 1 shows that G
′ ˆG/T →p Σ−1/2
Λ ΥV1/2 , where
V is the probability limit of VN T and an r× r diagonal matrix of the eigenvalues of Σ 1/2
Λ ΣG Σ1/2
Λ ,
and Υ is the eigenvector of Σ 1/2
Λ ΣGΣ1/2
Λ . Thus, 

Document 3: 
 where the ﬁrst three lines follow from the properties of Fβ(w) and g(t) (recall
that m(w) = g(Fβ(w))), and the last inequality follows from Lemma A.1.
Here the constant C depends only on b. Likewise we have
P(Vn ≤ y− φ−1) ≥ P(SW
n ≤ y− φ−1) − Cφ−1 log1/2 p− |E[In]|.
The conclusion of the lemma follows from combining these ine qualities with
the bound on |E[In]| derived in Step 1. ■
Proof of Corollary 5.1. Pick any hyperrectangle
A= {w∈ Rp : wj ∈ [aj,bj] for all j = 1 ,...,p }.
For i = 1 ,...,n , consider the random vectors ˜Xi and ˜Yi in R2p deﬁned
by ˜Xij = Xij and ˜Yij = Yij for j = 1 ,...,p , and ˜Xij = −Xi,j−p and
˜Yij = −Yi,j−p for j = p+ 1,..., 2p. Then
P(SX
n ∈ A) = P( S
˜X
n ≤ y), P(SY
n ∈ A) = P( S
˜Y 

Document 4: 
 ) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely
convex sets are sets that can be represented as intersection s of many
convex sets whose indicator functions depend only on a small subset of
their arguments, with hyperrectangles being a special case .
1. Introduction
Let X1,...,X n be independent random vectors in Rp where p≥ 3 may be
large or even much larger than n. Denote by Xij the j-th coordinate of Xi,
so that Xi = ( Xi1,...,X ip)′. We assume that each Xi is centered, namely
E[Xij] = 0, and E[ X2
ij] <∞ for all i= 1 ,...,n and j = 1 ,...,p . Deﬁne the
normalized sum
SX
n := ( SX
n1,...,S X
np)′ := 1
√n
n∑
i=1
Xi.
We consider Gaussian approximation to SX
n , and to this end, let Y1,...,Y n 

Document 5: 
 by ψα(x) := exp( xα) − 1, and for a real-valued random variable ξ, we deﬁne
∥ξ∥ψα := inf {λ> 0 : E[ ψα(|ξ|/λ)] ≤ 1}.
For α∈ [1,∞), ∥ · ∥ψα is an Orlicz norm, while for α∈ (0,1), ∥ · ∥ψα is not a
norm but a quasi-norm, that is, there exists a constant Kα depending only
on α such that ∥ξ1 + ξ2∥ψα ≤ Kα(∥ξ1∥ψα + ∥ξ2∥ψα ). Throughout the paper,
we assume that n≥ 4 and p≥ 3.
2. High Dimensional CLT for Hyperrectangles
This section presents a high dimensional CLT for hyperrecta ngles. We
begin with presenting an abstract theorem (Theorem 2.1); th e bound in
Theorem 2.1 is general but depends on the tail properties of t he distri-
butions of the coordinates of Xi in a nontrivial way. Next we apply this
theorem under simple moment conditions and derive more expl icit bounds
(Proposition 2.1).
Let Are be the class of all hyperrectangles in Rp; that is, Are consists of
all sets A of the form 

Document 6: 
 arXiv:1412.3661v4  [math.ST]  8 Mar 2016
CENTRAL LIMIT THEOREMS AND BOOTSTRAP IN
HIGH DIMENSIONS
VICTOR CHERNOZHUKOV, DENIS CHETVERIKOV, AND KENGO KATO
Abstract. This paper derives central limit and bootstrap theorems for
probabilities that sums of centered high-dimensional rand om vectors hit
hyperrectangles and sparsely convex sets. Speciﬁcally, we derive Gauss-
ian and bootstrap approximations for probabilities P( n− 1/ 2 ∑ n
i=1 Xi ∈
A) where X1, . . . , Xn are independent random vectors in Rp and A is
a hyperrectangle, or, more generally, a sparsely convex set , and show
that the approximation error converges to zero even if p = pn → ∞ as
n → ∞ and p ≫ n; in particular, p can be as large as O(eCnc
) for some
constants c, C >0. The result holds uniformly over all hyperrectangles,
or more generally, sparsely convex sets, and does not requir e any re-
striction on the correlation structure among coordinates o f Xi. Sparsely 

","The paper discusses a Central Limit Theorem (CLT) for hyperrectangles in high dimensions. Here's a simplified explanation:

1. **Hyperrectangles**: A hyperrectangle is a set of points in R^p such that each coordinate j takes values between aj and bj, i.e., {w ∈ Rp: wj ∈ [aj,bj] for all j = 1,...,p}.

2. **High-dimensional CLT**: The authors derive a CLT for the probability that a sum of n independent random vectors X_i (in R^p) hits a hyperrectangle A. Specifically, they want to bound P(n^(1/2) ∑_{i=1}^n Xi ∈ A).

3. **Key idea**: The authors use an argument inspired by Bolthausen [10] and apply the HAC covariance estimator to derive bounds on the probability.

4. **Approximation**: They show that this bound converges to zero uniformly over all hyperrectangles, even if p is large compared to n (i.e., p ≫ n).

5. **Bootstrap theorem**: The authors also provide a bootstrap approximation for probabilities P(n^(1/2) ∑_{i=1}^n Xi ∈ A), which can be used in practice.

The CLT works by establishing the asymptotic normality of the sum of random vectors, which allows them to apply standard CLT results. This enables the derivation of a bound on the probability that the sum hits a specific region (in this case, a hyperrectangle).

While I couldn't provide a detailed mathematical proof here, the authors' work builds upon established techniques in high-dimensional statistics and CLT theory, demonstrating how these concepts can be applied to derive useful results for high-dimensional data.",6.625782251358032,"The Central Limit Theorem (CLT) for hyperrectangles in high dimensions, as described in the provided snippets, involves the analysis of sums of independent random vectors and how these sums behave when they are constrained to lie within hyperrectangles or sparsely convex sets. The key points are as follows:

1. **Definition of Hyperrectangles**: A hyperrectangle in \( \mathbb{R}^p \) is defined as a set \( A = \{ w \in \mathbb{R}^p : w_j \in [a_j, b_j] \text{ for all } j = 1, \ldots, p \} \).

2. **Random Vectors**: The CLT applies to independent random vectors \( X_1, \ldots, X_n \) in \( \mathbb{R}^p \), where the number of dimensions \( p \) can be much larger than the number of observations \( n \).

3. **Normalized Sums**: The normalized sum \( S_n^X = \frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \) is considered. The paper derives Gaussian approximations for the probabilities that \( S_n^X \) lies in a specified hyperrectangle.

4. **Convergence of Approximation Error**: The approximation error (the difference between the true distribution of \( S_n^X \) and the Gaussian distribution) converges to zero under certain conditions, even when \( p \) grows significantly with \( n \).

5. **Generalization to Sparsely Convex Sets**: The results hold not just for hyperrectangles but also for more general sparsely convex sets, which are combinations of several convex sets.

6. **Independence of Correlation Structure**: The results are robust in that they do not require any specific assumptions about the correlation structure of the random variables involved.

In summary, the CLT for hyperrectangles establishes that even in high dimensions, under appropriate conditions, the sum of independent centered random vectors will converge in distribution to a multivariate normal distribution when constrained to fall within hyperrectangles or certain convex shapes."
llama3.2:latest,"What is high dimensional CLT for simple and sparsely conver sets?
","Document 1: 
 In addition, denote
D(1)
n =
(B2
nlog7(pn)
n
)1/6
, D(2)
n,q =
(B2
nlog3(pn)
n1−2/q
)1/3
. (9)
An application of Theorem 2.1 under these conditions leads t o the following
proposition.
Proposition 2.1(High Dimensional CLT for Hyperrectangles) . Suppose
that conditions (M.1) and (M.2) are satisﬁed. Then under (E.1), we have
ρn(Are) ≤ CD(1)
n ,
where the constant C depends only on b; while under (E.2), we have
ρn(Are) ≤ C{D(1)
n + D(2)
n,q},
where the constant C depends only on b and q.
3. High Dimensional CLT for Simple and Sparsely Convex Sets
In this section, we extend the results of Section 2 by conside ring larger
classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather 

Document 2: 
 mT = max i≤N
∑
j≤N
1
(
σij ̸= 0
)
.
The sparsity assumption puts an upper bound assumption on mT in the sense that
m2
T = o
( T
r2 log ( N )
)
.
In this formulation, the number of factors r is allowed to be large and grows with T .U s i n g
principal components estimators under the normalization 1
T
∑T
t=1 Ft F′
t = Ir , one can decompose
the sample covariance of Xt as
SX = ˆ/Lambda1ˆ/Lambda1+
N∑
i=r+1
ˆμi ˆξi ˆξ
′
i ,
where ˆμi and ˆξi are the i-th leading eigenvalues and eigenvectors of SX , respectively. In the high-
dimensional setup, the sample covariance might be singular and provides a poor estimator for the
population covariance. For example, when N > T , the rank of SX can never exceed T ,w h e r e a s
the theoretical covariance /Sigma1X always has rank N . To overcome this problem, we may apply the
thresholding technique to the component ∑N 

Document 3: 
 kw≤ ak for all k= 1 ,...,m }
for some unit vectors vk ∈ Sp−1 and coeﬃcients ak, k = 1 ,...,m . If the
number of non-zero elements of each vk does not exceed s, this A is an
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 9
s-sparsely convex set. Yet another example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj for all j = 1 ,...,p and w2
1 + w2
2 ≤ c}
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , j = 1 ,...,p , and 0 <c ≤ ∞ . This
A is a 2-sparsely convex set. A more complicated example is the set
A= {w∈ Rp : aj ≤ wj ≤ bj,w2
k + w2
l ≤ ckl, for all j,k,l = 1 ,...,p }
for some coeﬃcients −∞ ≤ aj ≤ bj ≤ ∞ , 0 < ckl ≤ ∞ , j,k,l = 1 ,...,p .
This A is a 2-sparsely convex set. Finally, consider the set 

Document 4: 
 classes of sets; in particular, we consider classes of simpl e convex sets, and
obtain, under certain conditions, bounds that are similar t o those in Section
2 (Proposition 3.1). Although an extension to simple convex sets is not
diﬃcult, in high dimensional spaces, the class of simple con vex sets is rather
large. In addition, it allows us to derive similar bounds for classes of sparsely
convex sets. These classes in turn may be of interest in stati stics where
sparse models and techniques have been of canonical importa nce in the past
years.
CLT AND BOOTSTRAP IN HIGH DIMENSIONS 7
3.1. Simple convex sets. Consider a closed convex set A⊂ Rp. This set
can be characterized by its support function:
SA : Sp−1 → R ∪ {∞}, v ↦→ SA(v) := sup {w′v: w∈ A},
where Sp−1 := {v ∈ Rp : ∥v∥ = 1 }; in particular, A = ∩v∈Sp− 1 {w ∈ Rp :
w′v≤ S A(v)}. We say that the set Ais m-generated if it is generated by the
intersection of m half-spaces (that is, A is a convex polytope with at most
m facets). The support function SA of such a set A can be characterized 

Document 5: 
 CLT AND BOOTSTRAP IN HIGH DIMENSIONS 3
because they allow us to approximate the distributions of va rious key sta-
tistics that arise in inference for high-dimensional model s. For example,
the probability that a collection of Kolmogorov-Smirnov ty pe statistics falls
below a collection of thresholds
P
(
max
j∈Jk
SX
nj ≤ tk for all k= 1 ,...,κ
)
= P
(
SX
n ∈ A
)
can be approximated by P( SY
n ∈ A) within the error margin ρn(Are); here
{Jk} are (non-intersecting) subsets of {1,...,p }, {tk} are thresholds in the
interval ( −∞,∞), κ ≥ 1 is an integer, and A ∈ A re is a hyperrectangle of
the form {w∈ Rp : max j∈Jk wj ≤ tk for all k= 1 ,...,κ }. Another example
is the probability that a collection of Pearson type statist ics falls below a
collection of thresholds
P
(
∥(SX
nj)j∈Jk ∥2 ≤ tk for all k= 1 ,...,κ
)
= P
(
SX 

Document 6: 
 It is worthwhile to mention that a notable example where the t ransformed
variables ˜Xi = ( v′Xi)v∈V(Am) satisfy condition (E.1 ′) is the case where each
Xi obeys a log-concave distribution. Recall that a Borel proba bility measure
µ on Rp is log-concave if for any compact sets A1,A2 in Rp and λ∈ (0,1),
µ(λA1 + (1 − λ)A2) ≥ µ(A1)λµ(A2)1−λ,
where λA1 + (1 − λ)A2 = {λx+ (1 − λ)y: x∈ A1,y ∈ A2}.
Corollary 3.1 (High Dimensional CLT for Simple Convex Sets with Log–
concave Distributions) . Suppose that each Xi obeys a centered log-concave
distribution on Rp and that all the eigenvalues of E[XiX′
i] are bounded from
below by a constant k1 >0 and from above by a constant k2 ≥ k1 for every
i= 1 ,...,n . Then
ρn(Asi(a,d)) ≤ Cn−1/6 log7/6(pn),
where the constant C depends only on a,b,d,k 1, and k2. 

","The High Dimensional CLT (Central Limit Theorem) for simple and sparsely convex sets extends the standard Central Limit Theorem to high-dimensional spaces by providing bounds on the rate of convergence. Specifically, it provides a bound on the rate at which the empirical distribution function of a random sample converges to its theoretical limit, under certain conditions.

For simple convex sets, the High Dimensional CLT states that if each Xi obeys a centered log-concave distribution and has bounded eigenvalues, then the convergence rate is O(n^(-1/6)log(7/pn)).

For sparsely convex sets, which are defined by the number of non-zero elements in their support being less than s, the High Dimensional CLT states that if each Xi obeys a centered log-concave distribution and has bounded eigenvalues, then the convergence rate is O(n^(-1/6)log(7/pn)) for sets with s-sparsity.",4.546256065368652,"The high-dimensional Central Limit Theorem (CLT) for simple and sparsely convex sets refers to the analysis and approximation of the distributions of statistics arising from high-dimensional models, particularly in the context of simple convex sets, which can be characterized by their support functions and generated by half-spaces. Under certain conditions, the theorem establishes bounds on the convergence rates of these statistics, such as the probability that specific statistical measures fall within certain thresholds. 

Proposition 3.1 outlines that the results extend to larger classes of sets, including sparsely convex sets, where the number of non-zero elements of vector representations is limited. This finding is significant in statistics, especially for sparse models, as it allows practitioners to infer key properties from high-dimensional distributions, thus providing a framework for data analysis where traditional low-dimensional assumptions do not hold. 

Moreover, the theorem can also specifically address distributions of transformed variables satisfying log-concave properties, enhancing the application of high-dimensional CLT in various statistical inference contexts."
llama3.2:latest,"What is multiplier bootstrap?
","Document 1: 
 bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn
1 = {X1,...,X n}. Let ¯X := ( ¯X1,..., ¯Xp)′ := En[Xi], and
consider the normalized sum:
SeX
n := ( SeX
n1 ,...,S eX
np )′ := 1√n
n∑
i=1
ei(Xi − ¯X).
We are interested in bounding
ρMB
n(A) := sup
A∈A
|P(SeX
n ∈ A| Xn
1 ) − P(SY
n ∈ A)|
for A = Are, Asp(s), or A ⊂ A si(a,d).
We begin with the case A ⊂ A si(a,d). Let
ˆΣ := n−1
n∑
i=1
(Xi − ¯X)(Xi − ¯X)′, Σ := n−1
n∑
i=1 

Document 2: 
 search trajectory followed by the conﬁguration procedure. The key idea behind
this generalisation (dubbed aggressive capping) is to additionally bound the time
allowed for evaluating conﬁgurations based on the performance observed for the
current incumbent, i.e., the best-performing conﬁguration encountered since the
beginning of the ParamILS run. The additional bound is obtained by multiplying
the performance estimate of the incumbent by a constant, bm, called the bound
multiplier. Formally, for bm = ∞, the additional bound becomes inactive (assum-
ing the performance measure is to be minimised), and the behaviour of trajectory-
preserving capping is obtained. Forbm = 1, on the other hand, a very aggressive
heuristic is obtained, which limits the evaluation of any conﬁguration to the time
spent on evaluating the current incumbent. In practice,bm = 2 appears to result
in good performance and is used as a default setting in ParamILS. Despite its
heuristic nature, this modiﬁed capping mechanism preserves the PAC property of
FocusedILS.
Although Hutter et al. [ 37] spelled out their adaptive capping mechanisms for
the performance objective of minimising a target algorithm’s mean run-time only,
these mechanisms generalise to other objectives in a rather straightforward way (a
discussion of capping in the context of minimising quantiles of run-time is found
in Ch. 7 of [32]). We note, however, that – especially when several target algorithm 

Document 3: 
 with problems. Both of the most commonly used methods of bootstrapping for regression—
residual bootstrap and pairs bootstrap—give very poor inference on β as the ratio p/n
grows. We ﬁnd that the residual bootstrap tend to give anti-conservative estimates (inﬂated
Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of
power) as the ratio p/n grows. We also show that the jackknife resampling technique for
estimating the variance of ˆβ severely overestimates the variance in high dimensions.
We contribute alternative procedures based on our theoretical results that result in
dimensionality adaptive and robust bootstrap methods.
Keywords: Bootstrap, high-dimensional inference, random matrices, resampling
1. Introduction
The bootstrap (Efron, 1979) is a ubiquitous tool in applied statistics, allowing for infer-
ence when very little is known about the properties of the data-generating distribution.
The bootstrap is a powerful tool in applied settings because it does not make the strong
assumptions common to classical statistical theory regarding this data-generating distri-
bution. Instead, the bootstrap resamples the observed data to create an estimate, ˆF, of
the unknown data-generating distribution, F. The distribution ˆF then forms the basis of
further inference.
c⃝2018 Noureddine El Karoui and Elizabeth Purdom. 

Document 4: 
 ρMB
n(A) ≤ C{D(1)
n (α) + D(1)
n,q(α)},
where the constant C depends only on a,b,d,q , and s.
Remark 4.2 (Bootstrap theorems in a.s. sense) . Proposition 4.1 leads to
the following multiplier bootstrap theorem in the a.s. sens e. Suppose that
A is a subclass of Asi(a,d) as in Proposition 4.1 and that (M.2) is satisﬁed.
We allow p= pn → ∞ and Bn → ∞ as n→ ∞ but assume that a,b,d,q,s
are all ﬁxed. Then by applying Proposition 4.1 with α= αn = n−1(log n)−2,
together with the Borel-Cantelli lemma (note that ∑ ∞
n=4 n−1(log n)−2 <∞),
we have with probability one
ρMB
n(A) =
{
O{D(1)
n (αn)} under (E.1)
O{D(1)
n (αn) ∨ D(2)
n,q(αn)} under (E.2) ,
and it is routine to verify that D(1)
n (αn) = o(1) if B2 

Document 5: 
 10 CHERNOZHUKOV, CHETVERIKOV, AND KATO
4. Empirical and Multiplier Bootstrap Theorems
So far we have shown that the probabilities P( SX
n ∈ A) can be well
approximated by the probabilities P( SY
n ∈ A) under weak conditions for hy-
perrectangles A∈ A re, simple convex sets A∈ A si(a,d), or sparsely convex
sets A∈ A sp(s). In practice, however, the covariance matrix of SY
n is typi-
cally unknown, and direct computation of P( SY
n ∈ A) is infeasible. Hence,
in this section, we derive high dimensional bootstrap theor ems which allow
us to approximate the probabilities P( SY
n ∈ A), and hence P( SX
n ∈ A),
by data-dependent techniques. We consider here multiplier and empirical
bootstrap methods (we refer to [32] for various versions of b ootstraps).
4.1. Multiplier bootstrap. We ﬁrst consider the multiplier bootstrap.
Let e1,...,e n be a sequence of i.i.d. N(0,1) random variables that are
independent of Xn 

Document 6: 
 Buja, A. and Eyuboglu, N. (1992). Remarks on parallel analysis. Multivariate Behavioral Research,
27(4):509 – 540.
Campbell, J. Y. and Yogo, M. (2006). Efficient tests of stock return predictability.Journal of Financial
Economics, 81(1):27–60.
Chamberlain, G. and Rothschild, M. (1983). Arbitrage, factor structure, and mean-variance analysis
on large asset markets. Econometrica, 51(5):1281–1304.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2013). Gaussian approximations and multiplier
bootstrap for maxima of sums of high-dimensional random vectors. The Annals of Statistics ,
41(6):2786 – 2819.
Chernozhukov, V., Chetverikov, D., and Kato, K. (2017). Central limit theorems and bootstrap in
high dimensions. The Annals of Probability , 45(4):2309 – 2352.
Chernozhukov, V., Chetverikov, D., Kato, K., and Koike, Y. (2023). High-dimensional data bootstrap.
Annual Review of Statistics and Its Application , 10(1):427–449. 

","The multiplier bootstrap is a type of bootstrap method used in statistics that involves resampling data with an additional step to adjust for the original distribution's scaling factor. In this case, it multiplies the original values by a constant (called the bound multiplier) to create new samples. The purpose of the multiplier bootstrap is to bound the probability of certain events or quantiles in the resampled distribution, allowing for more accurate inference and robustness against various types of data distributions.",2.3421192169189453,"The multiplier bootstrap is a resampling method used to make statistical inferences about a population when very little is known about the underlying data-generating distribution. In this procedure, one generates a sequence of independent and identically distributed (i.i.d.) standard normal random variables and uses them to create a normalized sum that helps in approximating the distribution of interest. Specifically, it involves computing a standardized version of the data, leveraging the concept of a sample mean and variance to form a new sample distribution that can then be utilized to derive estimates and confidence intervals for parameters of interest, such as regression coefficients. This technique is particularly important in high-dimensional statistics where traditional inference approaches may fail due to the dimensionality of the data."
llama3.2:latest,"What are the methods of examining impact of imputed values on XAI methods?
","Document 1: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 2: 
 k (e.g., Onatski, 2009, 2010; Alessi et al., 2010; Owen and Wang, 2016; Dobriban and Owen, 2018;
Dobriban, 2020), tests for changepoints in the factor loadings (e.g., Breitung and Eickmeier, 2011; Bai
et al., 2022), tests for whether observed proxies of the factor returns Xt are good proxies (Bai and Ng,
2006), bootstrap methods to debias OLS estimates of the exposures (Gon¸ calves and Perron, 2020),
covariance matrix estimation (e.g., Fan et al., 2008, 2011), estimation and inference for dynamic factor
models (e.g., Bates et al., 2013; Stock and Watson, 2016) and more—see Bai and Wang (2016) for a
review. Many of these techniques leverage key results from random matrix theory (Johnstone, 2001;
Paul, 2007; Bai and Silverstein, 2010), sometimes in combination with permutation-based methods
(Buja and Eyuboglu, 1992).
Factor models are also widely discussed in the asset pricing literature (see Giglio et al. (2022) for
review), including methods for explaining excess returns (Fama and French, 1992, 2008; Welch and
Goyal, 2007; Lewellen, 2015; Freyberger et al., 2020; Gu et al., 2020), estimating factors and exposures
7 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 Journal of Clinical Epidemiology 56 (2003) 968–976
Imputation of missing longitudinal
data: a comparison of methods
Jean Mundahl Engels *, Paula Diehr
Departments of Biostatistics and Health Services, University of Washington, 1959 Northeast Paciﬁc Avenue, Box 357232, Seattle, WA 98195, USA
Accepted 11 October 2002
Abstract
Background and Objective: Missing information is inevitable in longitudinal studies, and can result in biased estimates and a loss
of power. One approach to this problem is to impute the missing data to yield a more complete data set. Our goal was to compare the
performance of 14 methods of imputing missing data on depression, weight, cognitive functioning, and self-rated health in a longitudinal
cohort of older adults.
Methods: We identiﬁed situations where a person had a known value following one or more missing values, and treated the known
value as a “missing value.” This “missing value” was imputed using each method and compared to the observed value. Methods were
compared on the root mean square error, mean absolute deviation, bias, and relative variance of the estimates.
Results: Most imputation methods were biased toward estimating the “missing value” as too healthy, and most estimates had a variance
that was too low. Imputed values based on a person’s values before and after the “missing value” were superior to other methods, followed
by imputations based on a person’s values before the “missing value.” Imputations that used no information speciﬁc to the person, such 

Document 5: 
 follows a VAR of ﬁnite order,
/Phi1( L)Ht = ut ,
where /Phi1( L) = /Phi10 − ∑h
j=1 /Phi1j Lh , with /Phi10 being possibly not an identity matrix. Bernanke et al.
(2005) propose two ways to analyze FAVAR. The ﬁrst is based on a two-step principal components
method, in which in the ﬁrst step, the method of principal components is employed to form
estimates of the space spanned by bothFt and Wt . In the second step, various identiﬁcation
schemes, such as Cholesky ordering, can be applied to obtain estimates of latent factors ˆFt ,w h i c h
are treated as observed when conducting VAR analysis of [ ˆF′
t,W′
t]′. Under suitable identiﬁcation
conditions, Bai et al. (2016) show that inferential theory can be developed for such a two-step
estimator, which differs from a standard large factor model. Conﬁdence bands for the impulse
responses can be readily constructed using the theory therein. The second method involves a
one-step likelihood approach, implemented by Gibbs sampling, which leads to joint estimation of
both the latent factors and impulse responses. The two methods can complement each other, with
the ﬁrst one being computationally simple and the second providing possibly better inference in
ﬁnite samples although with increased computational cost. 

Document 6: 
 van Buuren, 2018), which is veryﬂexible and can be implemented
with different models. While some applications can beneﬁt from
this uncertainty information, integrating this uncertainty
information in data pipelines can be challenging. From a
practitioner’s point of view, point estimates are much simpler
to integrate into conventional data pipelines. This is why we
restrict our analysis to point estimate imputations. Note,
however, that all the experiments conducted in this work
could, in principle, also be evaluated with respect to their
uncertainty estimates in a MICE setting, using the examined
imputation methods as the model underlying the MICE
estimator.
More recently, also ML approaches have increasingly been
used for imputation. Popular methods include k-nearest
neighbors (k-NNs) ( Batista and Monard, 2003 ), matrix
factorization (Troyanskaya et al., 2001 ; Koren et al., 2009 ;
Mazumder et al., 2010 ), random-forest –based approaches
(Stekhoven and Bühlmann, 2012), discriminative deep learning
methods (Biessmann et al., 2018), and generative deep learning
methods (Shang et al., 2017; Yoon et al., 2018; Li et al., 2019;
Nazábal et al., 2020; Qiu et al., 2020).
Most imputation studies provide solid experimental
evidence that the respective proposed method in the
application setting investigated outperforms other 

","There are several methods to examine the impact of imputed values on Explainable AI (XAI) methods:

1. **Sensitivity Analysis**: This involves analyzing how sensitive XAI models are to changes in imputed values. By varying the imputed values and observing the effect on the explanations, researchers can identify which features are most influential.
2. **Partial Dependence Plots (PDPs)**: PDPs are a graphical representation of the relationship between a feature and the predicted output of a model. Imputing different values for a feature can alter the shape and orientation of the PDP, revealing how sensitive the model is to changes in that feature.
3. **SHAP (SHapley Additive exPlanations)**: SHAP assigns a value to each feature contribution to the overall output, indicating the impact of each feature on the predicted outcome. By analyzing these values for different imputed features, researchers can understand how sensitive the model is to changes in those features.
4. **Feature Importance Methods**: Techniques like permutation importance or recursive feature elimination can be used to evaluate the importance of each feature in the XAI model. Imputing different values for a feature can alter its importance ranking, revealing how sensitive the model is to changes in that feature.
5. **Uncertainty Analysis**: This involves analyzing the uncertainty associated with the predictions made by the XAI model. By examining the distribution of uncertainty for different imputed features, researchers can understand how sensitive the model is to changes in those features.
6. **Model-Agnostic Explanation Methods**: Techniques like LIME (Local Interpretable Model-agnostic Explanations) and Anchors use model-agnostic explanations that are not dependent on the specific model architecture. These methods can provide insights into how imputed values affect the explanations generated by these models.
7. **Quantile Regression Analysis**: This involves analyzing the relationship between a feature and the predicted output of a model using quantiles instead of mean values. By examining the relationships for different imputed features, researchers can understand how sensitive the model is to changes in those features.
8. **Conditional Entropy Analysis**: This involves analyzing the entropy of the conditional distribution of the predictions given a particular feature. By examining the conditional entropy for different imputed features, researchers can understand how sensitive the model is to changes in those features.

These methods can be used individually or in combination to provide insights into the impact of imputed values on XAI methods.",7.771514415740967,"The impact of imputed values on XAI (explainable artificial intelligence) methods can be examined using various strategies as outlined in the provided snippets. Specifically, the methods of examination include:

1. **Comparison of Imputation Methods**: Document 1 emphasizes exploring the effects of different imputation methods on the calculation of Shapley values. By systematically contrasting various imputation strategies, researchers can evaluate how these methods influence the interpretability and robustness of outcomes in machine learning models.

2. **Feature Relevance Evaluation**: Document 3 mentions approaches that evaluate features based on their average expected marginal contribution. This can be affected by the imputed values, as the way in which features are weighted during decision-making may change depending on the imputation method used.

3. **Visual Explanations**: As noted in Document 3, using visualization techniques may also help in interpreting predictions when imputed values are involved. Visual analytics can reveal how different imputed values alter the model's predictions and subsequently affect explanations.

4. **Local Explanations**: Local explanation methodologies can provide insights into how specific imputed values affect the model's behavior around certain instances, as discussed in Document 3. This involves examining the model's workings around inputs that have been altered by different imputation methods.

5. **Statistical Comparison of Estimates**: Document 4 details comparing various imputation methods, evaluating their performance based on metrics like root mean square error and bias. This approach can also prove useful for assessing how different imputed values interact with XAI methodologies.

6. **Integration of Uncertainty**: Document 6 mentions that while point estimates are often used in practice, incorporating uncertainty estimates from multiple imputation techniques (like MICE) can provide a more thorough understanding of how imputation influences model interpretability and insights.

Through these methodological approaches, researchers can gain a better understanding of how imputation methods affect XAI techniques and the interpretability of machine learning models."
llama3.2:latest,"What is MSE SHAP? 
","Document 1: 
 The following section proposes a uniﬁed approach that improves previous methods, preventing them
from unintentionally violating Properties 1 and 3.
4 SHAP (SHapley Additive exPlanation) Values
We propose SHAP values as a uniﬁed measure of feature importance. These are the Shapley values
of a conditional expectation function of the original model; thus, they are the solution to Equation
4
Figure 1: SHAP (SHapley Additive exPlanation) values attribute to each feature the change in the
expected model prediction when conditioning on that feature. They explain how to get from the
base value E[f(z)] that would be predicted if we did not know any features to the current output
f(x). This diagram shows a single ordering. When the model is non-linear or the input features are
not independent, however, the order in which features are added to the expectation matters, and the
SHAP values arise from averaging the φi values across all possible orderings.
8, where fx(z′) = f(hx(z′)) = E[f(z) |zS], and Sis the set of non-zero indexes in z′(Figure 1).
Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values 

Document 2: 
 the MIT license and is available at:
https://bitbucket.org/pl-edu-pw-mini-optimization/basic-pso-de-hybrid/ .
3.2. Samples archive
In order to store and eﬃciently retrieve samples, M-GAPSO utilizes a
multi-dimensional R-Tree index [34]. Due to performance reasons capacity of
the R-Tree index is limited and set by the user. After reaching the maximum
capacity, the index is restarted from scratch.
Samples archive is utilized in two scenarios. Mainly, for eﬃcient retrieval
of the nearest samples as described in Section 2.3. Subsequently those sam-
ples are utilized to ﬁt a quadratic or polynomial function model. Secondly,
samples archive serves as a cache memory, so that in the event of the algo-
rithm trying to sample the same location (as it does happen when the swarm
has nearly collapsed), it retrieves the function value from memory, saving
some budget of the ﬁtness function computations.
3.3. Restart management
M-GAPSO uses an enhanced version of JADE [5] restart manager. In
M-GAPSO the RestartManager registers iteration count intervals between
global optimum updates, considers a spread of personal best locations of
particles (eq. (10)) and additionally a spread of personal best locations val-
ues (eq. (11)). The last feature was added in order to better handle step 

Document 3: 
 ⏐⏐
(19)MSE = 1
m
m∑
i=1
(yi −¯yi)2
(20)RMSE =
√1
m
m∑
i−1
(yi −¯yi)2
(21)TPR = TP
TP + FN
Page 24 of 37Emmanuel et al. J Big Data           (2021) 8:140 
The major advantages of the MSE and RMSE is that they provide a quadratic loss func -
tion. Also, uncertainty in forecasting is measured when they are used. However, MSE 
and RMSE are highly influenced by extreme values [145]. While MAE is not influenced 
by extreme values, also a more natural measure and unambiguous [146]. Most stud -
ies in research are found to mostly use the RMSE for missing value imputation evalu -
ation [147–149]. Although some studies have proposed valid evidence against the use 
of RMSE in favour of MAE due to its less sensitive to extreme values [150]. The authors 
further advised against the reporting of RMSE in literature and strongly recommended 
the use of MAE [146, 150]. However, Chai and Draxler [145] partially disputed the con -
clusions and introduced arguments against avoiding RMSE. They contended that RMSE 
was appropriate to represent model performance than the MAE. The AUC like other 

Document 4: 
 ELSHADE-SPACMA 1.52 1.40 1.38 1.45 1.72 1.83 2.13 
GSKA 4.10 4.20 4.17 4.17 4.17 4.17 4.20 
PSO 6.03 5.97 5.97 5.97 5.93 5.93 5.87 
SSA 5.27 5.40 5.43 5.47 5.60 5.63 5.67 
jSO 2.70 2.40 2.35 2.37 2.47 2.35 2.05 
(b) 40–100% Evaluations 
ﬁtness evaluations, including the previous winners of CEC compe- 
titions (EBOwithCMAR and jSO) and the most competitive recent 
algorithm (ELSHADE-SPACMA). 
• EBOwithCMAR is the algorithm with the best results for dimensions 
10 and 30 (very close to ELSHADE-SPACMA in those dimensions), 
whereas ELSHADE-SPACMA is the best performing algorithm for di- 
mensions 50 and 100, closely followed by EBOwithCMAR and jSO. 
• For dimensions 30, 50 and 100, ELSHADE-SPACMA is the best one 
since the 40% of the budget of evaluations. Although Table 3 indi- 
cates that EBOwithCMAR obtains the best ﬁnal results, ELSHADE- 
SPACMA achieves better results during most of the search, being 
only improved by the former at the end. 
• Deciding which algorithm should be applied to a speciﬁc problem 
strongly depends on the eﬀort that can be devoted to the search. In 
this benchmark, GSKA is better when less evaluations are allowed, 

Document 5: 
 open-source innvestigate package. In the case of expected gradients, we bench-marked our imple-
mentation against a public one in the SHAP python package.
Surrogate Approaches.We consider:
• LIME (Ribeiro et al., 2016) locally approximate F around xi with a simple function, g,
that is then interpreted. LIME corresponds to: arg ming∈GL(f,g, pert(xi)) + Ω(g), where
pert(xi) local perturbations of the inputxi, and Ω(g) is a regularizer. Overall, recent work
hasshownthat, inthetabularsetting, LIMEapproximatesthecoeﬃcientsofablack-boxlinear
model with high probability. In our empirical implementation we follow the open source lime-
image package. Here to account for high dimensions, the input image is ﬁrst segmented into
50 segments and the local approximationg is ﬁt around input perturbations with50. We
experimented with 5,10,15,&25 dimensions as well. Overall, we found the LIME with50
segments to be more stable for the input data sizes that we consider. We use1000 samples in
model ﬁtting.
• SHAP (Lundberg and Lee, 2017) Similar to LIME, SHAP provides a local approximation
around a single input. The local model is then interpreted as a form of explanation. SHAP
uniﬁes LIME and several under methods under the same umbrella and turns out to be a 

Document 6: 
 4.1 | Features-oriented methods
SHapley Additive exPlanation (SHAP) (Lundberg & Lee, 2017) is a game-theoretic approach to explain ML predictions.
SHAP seeks to deduce the amount each feature contributed to a decision by representing the features as players in a
ANGELOV ET AL. 5o f1 3
 19424795, 2021, 5, Downloaded from https://wires.onlinelibrary.wiley.com/doi/10.1002/widm.1424 by Cochrane Poland, Wiley Online Library on [28/12/2024]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 

","I don't know. The provided context does not mention MSE SHAP specifically, but it mentions Mean Squared Error (MSE). It also discusses SHAP values and their application in feature importance measures.",1.6733293533325195,"The term ""MSE SHAP"" does not appear in the provided documents specifically. However, MSE (Mean Squared Error) is a common metric used to measure the accuracy of predictions in a model, calculated as the average of the squared differences between predicted and actual values. SHAP (SHapley Additive exPlanation) values, on the other hand, are a measure of feature importance that explains the contribution of each feature to the model's predictions based on game-theoretic principles. 

While there is no direct mention of ""MSE SHAP"" in the snippets, it could imply a context where both MSE (or other error metrics) are being evaluated alongside SHAP values to provide insights into model performance and feature contributions. In practice, researchers may analyze how different features (using SHAP) influence the MSE or other error metrics, but a specific definition or methodology termed ""MSE SHAP"" is not delineated in the provided excerpts."
llama3.2:latest,"What are observations on impact of different imputation methods on explainability of models?
","Document 1: 
 relationships between observed and missing variables.
In recent years, more attention has been paid to the explainability of
the imputation method under missing data. For example, [37] introduces
DIMV, an imputation method that provides explainability and uncertainty
of imputation via conditional distribution, or [1] analyzes the effects of vari-
ous imputation and direct parameter estimation methods to the correlation
plot. Next, Hans et al. [38] introduce an explainable imputation method
that leverages association constraints within the data. However, up to our
5
knowledge, so far, there is no work on the effects of missing data on Shapley
values.
2.3. Direct missing data handling techniques without imputation
Different from the imputation approaches, methods that directly handle
missing data can have clearer implications in terms of explainability.
Specifically, Nguyen et al. [35] introduced the EPEM algorithm to esti-
mate the maximum likelihood estimates (MLEs) for multiple class monotone
missing data when the covariance matrices of all classes are assumed to be
equal. Additionally, DPER [39] addresses a more general case where miss-
ing data can occur in any feature by using pairs of features to estimate the
entries in the mean and covariance matrices. The implication to model ex-
plainability of using direct parameter estimation methods, like EPEM and
DPER, includes improved transparency and interpretability of the model’s
behavior, as these methods provide clear estimates of means and covariances, 

Document 2: 
 details of how these methods works is described by White, Royston, and
Wood (2011).
The related work shows that there exists several literature describing different
methods of data imputation. From the implementation perspective, it is also
very important to understand and evaluate performance of different imputation
methods so that appropriate method can be used while performing data mining
task. Though there exist some literatures that analyzed performance of different
imputation methods, in this paper, we intend to analyze performance of differ-
ent imputation method for numeric datasets that uses single and multiple
imputation methods namely mean imputation, median imputation, kNN impu-
tation, predictive mean matching, Bayesian Linear Regression (norm), Linear
Regression, non-Bayesian (norm.nob), and random sample.
Research Methodology
This section describes procedure followed for analyzing performance of the
imputation methods. Objective of this study is to analyze performance of imputa-
tion methods that includes: (a) Single Imputation Methods: Mean Imputation,
Median Imputation, KNN Imputation and (b) Multiple Imputation Methods:
Predictive Mean Matching (pmm), Bayesian Linear Regression (norm), Linear
Regression non-Bayesian (norm.nob), and Sample method. All these imputation
methods are applied only on numeric datasets. The datasets used in this study are
obtained from UCI Machine Learning Repository (Lichman2013). The descrip- 

Document 3: 
 The main limitations of ou r work were as follows. 
We used a data set contained only four categorical 
variables. Therefore questions whether number and 
nature of variables affected our conclusions remains 
to be addressed. In regression imputation, E-M algo-
rithm, and the MICE methods regression modelling 
has been used to draw the imputation. It is clear that, 
generally speaking, the more the number of variables 
the better the prediction. In addition, in regression 
and E-M imputations, it is as sumed that the data fol-
lows a normal distribution. This was not the case in 
our data set. This might partly explain poorer perfor-
mance of these two methods. Another limitation of 
our work was that we di d not compare performance 
of the imputation methods under different missing 
rates. One last issue was the process of model build-
ing. We developed all four models using ENTER 
method. This method fits a model including all varia-
bles offered to. Therefore, the behavior of imputation 
models under different variable selection methods 
(i.e. Backward and Forward) should be investigated. 
All issues noted affect the conclusions 5,25-28 and their 
influence should be explored in future studies.  
 Although the MICE method does not provide 
unique estimates,29 and communication of results with 
clinical audiences is not simple, evidence from litera-
ture suggested that the MICE method is the best ap-
proach to impute missing data. However, our literature 

Document 4: 
 Shapley values attribute the contribution of each feature to the final predic-
tion, offering insights into feature importance and interaction. However, the
accuracy and reliability of Shapley values can be affected by the choice of
the imputation method, as the imputed values can introduce biases or distor-
tions. Despite the importance of this issue, little attention has been paid to
the effect of imputation on the Shapley values of the downstream model. For
example, in [3], the authors examine various imputation methods to choose
the best one and then use Shapley values to explain the prediction of the
downstream task. However, the effects of imputation methods on Shapley
values have not been thoroughly examined.
This paper aims to explore the effects of various imputation methods on
the calculation of Shapley values. By comparing different imputation strate-
gies, we seek to understand how they influence the interpretability of machine
learning models and the robustness of the insights derived from Shapley’s val-
ues. The study provides a comprehensive evaluation of imputation methods
2
in the context of model interpretation, offering guidance for practitioners on
selecting appropriate techniques for their specific datasets and objectives.
The rest of the paper is organized as follows. In Section 2, we briefly
review works on explainable AI and missing data to gain insights into the
development of missing data in relation to explainable AI and highlight open
issues for future consideration. Next, in Section 3, we describe the methods
being examined in this work. Following that, we theoretically analyze the 

Document 5: 
 of 10–20% in more than 75% of our experiments. This holds
for most imputation methods; we did not observe a clear
advantage for an imputation method overall. Taking into
account the considerable differences in wall-clock run time,
our results indicate that also when choosing an imputation
method that is both fast and improves downstream predictive
performance random forests would be the preferred
imputation method.
The positive impact of imputation on downstream
performance is most pronounced when the imputation
methods were trained on fully observed data. When
imputation methods were trained on incomplete data, the
positive impact of imputing missing values in the test data was
substantially lower, sometimes even negative. While this might
seem a disadvantage, we emphasize that, in many application use
cases, we can ensure that the training data be fully observed, for
instance, by acquiring more data before training the imputation
and the downstream ML model.
6.3 Limitations
Because one of the main goals of this study is a comprehensive
comparison of imputation methods on a large number of datasets
and missingness conditions, we made some decisions that limit
our results.
First, we focus on point estimates of imputed values rather than
multiple imputations because it is 1) easier to handle in automated
pipelines and 2) can be considered a more relevant scenario in real-
world applications of imputation methods. Thus, we do not
consider the inherent uncertainty of the imputation process. We
decided to measure and compare the impact imputation methods 

Document 6: 
 that is not positive definite, which is can possibly prevent further analysis such as calcu -
lating coefficients estimates [36]. Finally, pairwise deletion also known to produce low 
bias results for MCAR or MAR data [34].
Imputation
The process of imputation involves replacing missing values by some predicted values. 
The non-missing values data set is normally used to predict the values used to replace 
the missing values [8]. In the following we cover some of the most used imputation 
methods in the literature.
Simple imputation
Simple imputation approach entails replacing missing values for each individual value by 
using a quantitative attribute or qualitative attribute of all the non-missing values [37]. 
With simple imputation, missing data is handled by different methods such as, mode, 
mean, or median of the available values. In most studies simple imputation methods are 
used because of their simplicity and that they can be used as an easy reference technique 
[38]. However, simple imputation methods may produce bias or unrealistic results on a 
high-dimensional data sets. Also, with the generation of big data emerging, this method 
seems to be performing poorly and therefore is inadequate to be implemented on such 
data sets [39].
Regression imputation
Regression is one of the preferred statistical technique for handling missing values. This 
method is also termed conditional mean imputation, here missing values are replaced 
with a predicted value created on a regression model if data is missing at random. The 
overall regression process is a two-phase method: the first step, uses all the complete 

","Research has shown that different imputation methods can significantly affect the explainability of machine learning models. For example, studies have found that some imputation methods may introduce biases or distortions in the data, which can negatively impact model interpretability.

In particular, this paper notes that various imputation methods can influence the calculation of Shapley values, a technique used to provide insights into the contribution of each feature to a model's predictions. The authors found that different imputation methods can lead to varying levels of bias and distortion in the data, which can affect the accuracy and reliability of Shapley values.

The study suggests that some imputation methods may not be suitable for all datasets, and that practitioners should carefully consider their specific dataset and objectives when selecting an imputation method. The authors also found that, even when choosing an imputation method that is both fast and improves downstream predictive performance, random forests would still be a preferred choice for many cases.

Overall, the research highlights the importance of considering the impact of different imputation methods on model explainability and suggests that practitioners should carefully evaluate their options to ensure accurate and reliable insights from machine learning models.",4.943673133850098,"Different imputation methods have significant implications for the explainability of models, especially regarding the calculation of Shapley values, which attribute the contribution of features to model predictions. The choice of imputation method can introduce biases or distortions that affect the accuracy and reliability of Shapley values. 

1. **Effect on Shapley Values**: The impact of imputation methods on Shapley values has not been extensively studied despite its importance. Different imputation strategies can influence the interpretability of machine learning models and the robustness of the insights derived from Shapley values.

2. **Comparison of Methods**: Research has shown that the performance of imputation methods varies, and while some methods like random forests can improve downstream predictive performance, their effectiveness can diminish if trained on incomplete data. This highlights the need for careful selection of imputation methods based on the nature of the dataset.

3. **Explainability versus Performance**: Methods that provide clearer estimates of means and covariances, like direct parameter estimation techniques (e.g., EPEM and DPER), tend to offer better transparency and interpretability. On the other hand, simpler imputation methods might lead to unrealistic results in high-dimensional datasets, which could impact the explainability in a negative manner.

4. **Limitations of Imputation Methods**: Specific techniques such as regression and multiple imputation methods can perform poorly if the data does not meet the underlying assumptions (e.g., normal distribution) or if the imputation method does not adequately capture the complexity of the dataset. This suggests that the imputation methods used have a direct correlation with the explainability and interpretation of the resulting models.

Overall, selecting the appropriate imputation method is crucial for maintaining the integrity and interpretability of machine learning models that rely on imputed data."
llama3.2:latest,"Does usage of different imputation methods on missing values has impact on SHAP feature importance?
","Document 1: 
 especially at higher missing rates. MICE and DIMV often show similar
patterns, possibly due to the fact that MICE is based on regression and
DIMV is based on a conditional Gaussian formula. MissForest and SOFT-
IMPUTE sometimes preserve feature rankings better than simpler methods,
but this is not consistent across all scenarios.
The variability in results between methods and missing rates underscores
the need to evaluate imputation effects when using Shapley values for model
interpretation.
The following discussion is structured around our result and the specific
pitfalls that may arise due to incomplete understanding of the relationship
between missing data, imputation methods, and Shapley values. We high-
23
light how different approaches can lead to vastly different interpretations, how
dataset characteristics and missing rates affect results, and the importance
of considering both imputation accuracy and interpretability preservation.
Pitfall 1: Assume the neutrality of the imputation method.Our
study reveals that different imputation methods can significantly alter Shap-
ley values and, consequently, the interpretability of the model. For instance,
mean imputation tends to distort feature importances, especially at higher
missing rates, while methods like MICE and DIMV often show similar pat-
terns. This underscores the importance of carefully considering the imputa-
tion method when using Shapley values for model explanation, as the choice
can lead to vastly different interpretations of feature importance.
Pitfall 2: Overlooking data set dependency. We observed that 

Document 2: 
 Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that
adheres to Properties 1-3 and uses conditional expectations to deﬁne simpliﬁed inputs. Implicit in this
deﬁnition of SHAP values is a simpliﬁed input mapping, hx(z′) = zS, where zS has missing values
for features not in the set S. Since most models cannot handle arbitrary patterns of missing input
values, we approximate f(zS) with E[f(z) |zS]. This deﬁnition of SHAP values is designed to
closely align with the Shapley regression, Shapley sampling, and quantitative input inﬂuence feature
attributions, while also allowing for connections with LIME, DeepLIFT, and layer-wise relevance
propagation.
The exact computation of SHAP values is challenging. However, by combining insights from current
additive feature attribution methods, we can approximate them. We describe two model-agnostic
approximation methods, one that is already known (Shapley sampling values) and another that is
novel (Kernel SHAP). We also describe four model-type-speciﬁc approximation methods, two of
which are novel (Max SHAP, Deep SHAP). When using these methods, feature independence and
model linearity are two optional assumptions simplifying the computation of the expected values
(note that ¯Sis the set of features not in S):
f(hx(z′)) = E[f(z) |zS] SHAP explanation model simpliﬁed input mapping (9) 

Document 3: 
 dependsonthesizeofthedatasetandmissingvaluespresent
inthem.Here,theauthorsanalyzedoftheperformanceoffour
imputationpackagesinRontwodatasetsusingthreepredictive
algorithms-LogisticRegression,SupportVectorMachineandAr-
tificialNeuralNetwork.Theauthorsmeasuretheperformanceof
thepackagesintermsofimputationtime,imputationefficiency
andeffectonthevariance.Theauthorsalsoshowthattheper-
formance of an imputation method depends on the predictive
algorithm which is a function of dataset characteristics. There
aredifferentwaysofhandlingmissingdata[25–27,29–32].The
mostsimpleapproachistouseonlythesubsetthatiscomplete;
inotherwords,suchamethodignoresallfeaturevectorswith
missing values and the remaining subset is used for analysis.
Althoughverysimple,suchanapproachwillbeusefulonlywhen
asmallfractiondatapointshavemissingvalues.Amoreuseful
and popular approach is to impute the missing values using
differenttechniquesandthenusethedatasetconsistingofboth
featurevectorswithoutmissingvaluesandthefeaturevectors
with imputed missing values. These techniques often depend
ontheproblemthatonelikestosolveusingthedataset.For
example,toclusteranincompletedataset,twotypesofframe-
works may be adopted: single view clustering and multi-view 

Document 4: 
 ley values, ranging from (−0.5) to around (+1.5) across all missing rates, em-
phasizing its strong influence on the model outputs. The imputation methods
show similar patterns with Shapley values for key features slightly reduced
compared to the XGBoost model, but they still highlight the importance of
Latitude, Longitude, and MedInc in the predictive modeling.
Considering the results at each missing rate, we observed similar distribu-
tions in the Shapley values for Latitude, Longitude, and MedInc on both the
15
(a) Original
 (b) Xgboost
 (c) Mean Imputation
(d) MICE
 (e) DIMV
 (f) missForest
(g) SOFT-IMPUTE
Figure 1: Global feature importance plot on the California dataset with the missing rate
r = 0.2
original and imputed data using various imputation methods, generally fluc-
tuating around zero. However, XGBoost shows a distribution that is more
skewed from zero. For example, at a missing rate of 20% in figure 5, the
Shapley values in the original data for Latitude and Longitude range from
(−0.75) to (+0 .75), and for MedInc from ( −0.5) to (+0 .7). Likewise, the
ranges for models with imputation methods are [ −0.6; 0.6] and [ −0.4; 0.6],
respectively, indicating that the imputation does not significantly alter the 

Document 5: 
 dataset. By displaying each data point as a dot and arranging these
dots to show the distribution of Shapley values for each feature, the
beeswarm plot offers a comprehensive overview of feature importance
and interaction effects. It allows for the identification of patterns and
outliers, helping to understand the behavior of the model with respect
to individual features. Additionally, the plot can reveal how specific
features interact with others, providing insights into complex depen-
dencies within the data.
3.2. Imputation techniques
In this section, we briefly summarize the missing data handling methods
that we will examine for the effects on Shapley values. The methods being
investigated consist of a method that can directly learn from missing data,
such as XGBoost, to a simple imputation method as Mean Imputation, as
well as the widely used or recently developed imputation techniques, such as
8
MICE, DIMV, missForest, and SOFT-IMPUTE. The details of the methods
are as follows:
• XGBoost (Extreme Gradient Boosting) [47] is a powerful and efficient
algorithm that belongs to the family of gradient boosting techniques.
It builds an ensemble of decision trees, where each tree corrects errors
made by the previous ones, enhancing predictive accuracy. XGBoost
stands out for its speed and performance, employing advanced features
like tree pruning, regularization, and parallel processing, which help
in reducing overfitting and handling large-scale data. XGBoost can
handle missing data directly. 

Document 6: 
 2018). However, in that work, the authors only considered text data
as an inputﬁeld to an imputation method, not as a column that
could be imputed. Generally, most modern ML applications that
involve text data are based on rather sophisticated natural language
models. Combinations of such models with tabular data are an
importantﬁeld of research (Yin et al., 2020) but beyond the scope of
most imputation research so far.
Third, to measure the imputation impact on the downstream
performance, we discarded and imputed values in only a single
column. Therefore, the impact depends heavily on the chosen
column’s importance (e.g., see the work ofSchelter et al. (2021)).
Generally, the impact when using an imputation model could
vary when multiple columns are affected by missing values.
7 CONCLUSION
In this study, we developed an experimental protocol and
conducted a comprehensive benchmark for imputation
methods comparing classical and modern approaches on a
large number of datasets under realistic missingness conditions
with respect to the imputation quality and the impact on the
predictive performance of a downstream ML model. We also
evaluated how the results changed when the imputation and
downstream model were trained on incomplete data.
Our results can be summarized in two mainﬁndings. First, we
demonstrate that imputation helps to increase the downstream
predictive performance substantially regardless of the
missingness conditions. When training data are fully observed,
our results demonstrate that, in more than 75% of our 

","Yes, according to the text, using different imputation methods on missing values can have an impact on SHAP (SHapley Additive exPlanations) feature importance. The study found that the Shapley values for key features such as Latitude, Longitude, and MedInc are reduced when using certain imputation methods compared to the XGBoost model.",2.240588665008545,"Yes, the usage of different imputation methods on missing values does have an impact on SHAP feature importance. Various imputation techniques can significantly alter the calculated SHAP values, leading to different interpretations of feature importance. For instance, mean imputation can distort feature importances, especially at higher missing rates, while methods like MICE and DIMV often produce similar patterns. Therefore, it is crucial to consider the choice of imputation method when interpreting SHAP values for model explanations, as different methods can yield vastly different results."
llama3.2:latest,"How sampling for LIME for single observation is done?
","Document 1: 
 πx. We sample instances around x′ by drawing nonzero
elements of x′ uniformly at random (where the number of
such draws is also uniformly sampled). Given a perturbed
sample z′∈{0, 1}d′
(which contains a fraction of the nonzero
elements of x′), we recover the sample in the original repre-
sentation z ∈Rd and obtain f(z), which is used as a label for
the explanation model. Given this dataset Zof perturbed
samples with the associated labels, we optimize Eq. (1) to
get an explanation ξ(x). The primary intuition behind LIME
is presented in Figure 3, where we sample instances both
in the vicinity of x (which have a high weight due to πx)
and far away from x (low weight from πx). Even though
the original model may be too complex to explain globally,
LIME presents an explanation that is locally faithful (linear
in this case), where the locality is captured by πx. It is worth
noting that our method is fairly robust to sampling noise
since the samples are weighted byπ
x in Eq. (1). We now
present a concrete instance of this general framework.
3.4 Sparse Linear Explanations
For the rest of this paper, we let G be the class of linear
models, such that g(z′) = wg ·z′. We use the locally weighted 

Document 2: 
 (Bm, Gm) such that t ∈ Bm and j ∈ Gm. See Figure 5 for an illustration of this definition.
We will soon discuss how to choose the tiling (in Section 3.3). For now, given an arbitrary tiling, we
refer to the submatrices Y(m) = YBm,Gm and ϵ(m) := ϵBm,Gm as the mth tiles of the matrices Y and ϵ,
respectively. Before presenting the mosaic permutation test, we make two assumptions, generalizing
the assumptions from Section 3.1.
First, in Section 3.1, we assumed that the exposures Lt did not change with time. Now, we ask that
the analyst chooses the tiles such that the exposure matrices {Lt}T
t=1 may change across tiles, but not
within tiles. For example, in our application to the BFRE model, the exposures change every week.
As a result, we choose the tiles such that each tile contains data from only one week. If Lt takes
unique values at every observation, then testing H0 is possible but requires an extension introduced
in Section 5.3.
Assumption 3.1. The exposures {Lt}t∈Bm in each tile are all equal.
Second, in Section 3.1, we assumed that the idiosyncratic returns ϵ1, . . . , ϵT were i.i.d. We now relax 

Document 3: 
 DATA REDUCTION
Data reduction is typically conducted in two directions, i.e., row-
wise for data sample reduction and column-wise for data variable
reduction. Various data sampling techniques can be applied for
row-wise data reduction, such as random and stratiﬁed sampling
(Fan et al., 2015b). Random sampling is typically used to simulate
a random process in data sample selection. By contrast, stratiﬁed
sampling is performed to maintain the proportions of data
samples corresponding to different categories. For instance,
assuming there are 100 and 50 data samples in the original
data set for category A and B respectively, a strati ﬁed
sampling process with a selection rate of 0.7 will randomly
select 70 out of 100, and 35 out of 50 data samples in
category A and B for further analysis. Such techniques can be
applied for sampling time series data as well. In such a case, it is
recommended to transform the original sequence data into
shorter subsequences before data sampling, since direct
sampling may lead to temporal information loss.
There are three main methods of column-wise data variable
reduction. Theﬁrst is to use domain knowledge to directly select
variables of interests. The second is to use statistical feature
selection methods to select important variables for further
analysis. The third is to adopt feature extraction methods to
construct useful features for data analysis. The details of the latter
two methods are shown inFeature Selection Techniques, Feature
Extraction Techniques.
Feature Selection Techniques
As shown inTable 2, feature selection methods can be broadly 

Document 4: 
 the change in the conditional expectation of all features given
a subset of features. In other words, this calculation ignores
the nodes of features that are not present in the sample subset
and calculate change on the conditional expectation, given
this subset sample.
5 Experiments
The main steps of the experiments consisted of data acqui-
sition, data pre-processing, model tuning, model evaluation,
and interpretation of models and their predictions.
All the experiments ran in a free tier Google Colab
environment, which consisted of an Intel Xeon 2.34Ghz
quad-core processor, 25Gb of RAM, and Linux Ubuntu 18.04
operational system. The algorithms were implemented in
Python 3.7 using PIP packages. Implementation details can
be found here.
5.1 Data Acquisition
The experiment was executed with data provided by the
COVID-19 Data Sharing/BR, which is an initiative of the São
Paulo Research Foundation (FAPESP) in collaboration with
the University of São Paulo. The dataset consisted of sus-
pected COVID-19 patients admitted to the Hospital Israelita
Albert Einstein from February 2020 to June 2020. Addition-
ally, we conﬁrmed that it is possible to have two samples from
the same patient but from different days. It happens because
a patient can be admitted more than once in the same unit,
and also, a doctor can request a COVID-19 for the patient
more than once while they are still under treatment. 

Document 5: 
 procedure involves treating the model as if there is only one change point at each time.
T o be speciﬁc, we ﬁrst identify the initial break point, ˆk1 , using the QML method in DBH. T o
determine the presence of any additional breaks, we split th e entire sample into two subsamples:
[1,ˆk1] and [ ˆk1,T]. F or each subsample, we conduct the sup-LR test with m= 1 as outlined in (4.2)
or (4.3), and employ the QML method to estimate a break point f or the subsample where the null
hypothesis is rejected at a signiﬁcance level of α. W e then split the corresponding subsample into
further subsamples at the newly estimated break point and re peat the LR tests for each subsample.
This process continues until the LR test is not rejected for a ll subsamples. The number of break
points is equal to the number of subsamples minus one, and the location of the change points can
be estimated in the procedure. The following corollary show s that the number of breaks can be
consistently estimated when α converges to zero slowly .
18
Corollary 1. Suppose that the size of the test αconverges to zero slowly ( α→ 0 yet lim inf T →∞ Tα>
0), then under the assumptions of Theorems 5 and 6, 

Document 6: 
 Method
Used for Data Imputation 10% 20% 30% 40% 50%
Mean Imputation 0.068072 0.106639 0.120149 0.142043 0.160996
Median Imputation 0.069302 0.108637 0.123685 0.141440 0.165698
KNN Imputation 0.013803 0.025372 0.035587 0.047773 0.066471
Predictive Mean Matching(pmm) 0.017844 0.035328 0.044072 0.057366 0.081049
Bayesian Linear Regression(norm) 0.017917 0.033387 0.046552 0.057673 0.077796
Linear Regression, non-Bayesian (norm.nob) 0.017933 0.033459 0.044446 0.055386 0.072130
Random sample from observed values(sample) 0.072683 0.119587 0.131316 0.153484 0.169020
Figure 1. Plot of imputation method versus mean NRMSE for wine dataset.
Figure 2. Plot of imputation method versus mean NRMSE for glass dataset.
924 A. JADHAV ET AL.
given in ascending order of Mean NRMSE. It means that lowest Mean
NRMSE value get first rank.
Tables 7– 11 provide rank of imputation method for varying percentage of
missing data for five different datasets. Each table indicates performance of
imputation method on different datasets for given percentage of imputed
data. The reason for doing this is to assess consistency in performance of the 

","For LIME (Local Interpretable Model-agnostic Explanations), single observation sampling is typically done using a method called ""stratified bootstrap sampling"" or ""resampling"". 

The process involves resampling the dataset with replacement, stratifying it by class (or category) and then taking a random sample from each stratum. This ensures that each class is represented in the sampled data set.

For a single observation, LIME uses a subset of the training data to approximate the local behavior around the target observation. The size of this subset can be adjusted as needed for good performance.",2.8131117820739746,"In LIME (Local Interpretable Model-agnostic Explanations), sampling for a single observation, denoted as x, involves drawing instances around a perturbed version of that observation (x′). The process includes sampling nonzero elements of x′ uniformly at random, with the number of draws also sampled uniformly. A perturbed sample z′ is created, which includes a fraction of the nonzero elements from x′, and this is then rephrased back to the original representation as z. The output of the original model for this representation, f(z), serves as the label for the explanation model. The sampling process captures instances close to x (high weight from the locality function πx) and farther away (lower weight), ensuring that the explanation remains locally faithful even in complex models."
llama3.2:latest,"How can LIME lead to improvement of model by non-experts?
","Document 1: 
 In this paper, we propose providing explanations for indi-
vidual predictions as a solution to the “trusting a prediction”
problem, and selecting multiple such predictions (and expla-
nations) as a solution to the “trusting the model” problem.
Our main contributions are summarized as follows.
•LIME, an algorithm that can explain the predictions ofany
classiﬁer or regressor in a faithful way, by approximating
it locally with an interpretable model.
•SP-LIME, a method that selects a set of representative
instances with explanations to address the “trusting the
model” problem, via submodular optimization.
•Comprehensive evaluation with simulated and human sub-
jects, where we measure the impact of explanations on
trust and associated tasks. In our experiments, non-experts
using LIME are able to pick which classiﬁer from a pair
generalizes better in the real world. Further, they are able
to greatly improve an untrustworthy classiﬁer trained on
20 newsgroups, by doing feature engineering using LIME.
We also show how understanding the predictions of a neu-
ral network on images helps practitioners know when and
why they should not trust a model.
2. THE CASE FOR EXPLANATIONS
By“explaining a prediction”, we mean presenting textual or
visual artifacts that provide qualitative understanding of the
relationship between the instance’s components (e.g. words 

Document 2: 
 that is available.
2.3 Improving existing data
In addition to searching and labeling datasets, one can
also improve the quality of existing data and models. This
approach is useful in several scenarios. Suppose the target
application is novel or non-trivial where there are no relevant
datasets outside, or collecting more data no longer beneﬁts
the model’s accuracy due to its low quality. Here, a better
option may be to improve the existing data. One effective
approach is to improve the labels through re-labeling. Sheng
et al. [ 146] demonstrates the importance of improving labels
by showing the model accuracy trends against more training
examples for datasets with different qualities. As the data
quality decreases, even if more data is used, the accuracy of
the model does not increase from some point and plateaus. In
this case, the only way to improve the model accuracy is to
improve the label quality, which can be done by re-labeling
and taking majority votes on multiple labels per example. In
fact, one could clean the entire data including labels, which
naturally leads to the next section where we cover data vali-
dation, cleaning, and integration.
3 Data validation, cleaning, and integration
It is common for the training data to contain various errors.
Machine learning platforms like TensorFlow Extended
(TFX) [13] have data validation [ 117] components to detect
such data errors in advance using data visualization and
schema generation techniques. Data cleaning can be used 

Document 3: 
 that the differences between the mosaic test statistics and their significance thresholds double or more
compared to the analysis of the full BFRE model. Overall, this gives more (heuristic) evidence that
the BFRE model accounts for the most significant correlations among assets, even though it does not
fit perfectly. Soon, we will see more evidence for this claim.
17
Figure 7: This figure repeats the analysis in Figure 1, except now we perform an ablation test where we remove
the twelve style factors from the BFRE model.
4.3 Improving the model
The main contribution of our paper is a method for testing the goodness-of-fit of a factor model.
That said, another important question is whether one can improve a model. We now illustrate how
the mosaic permutation test can help answer this question. Consider a simple alternative where the
BFRE model is missing at least one component:
Yt = LtXt + vZt + ϵt, (4.2)
where Lt ∈ Rp×k are the pre-existing BFRE factor exposures, Xt are the BFRE factors, Zt ∈ R is an
additional factor, and v ∈ Rp is an unknown (missing) factor exposure. We ask: can we estimate a
new exposure v so that the model fit is improved?
To measure model performance, we split the data into two folds. On the first fold, we estimate new
exposures ˆv. On the second fold, we check whether ˆv allows us to better predict each residual ˆϵt,j from 

Document 4: 
 commonly held misconception is that ML is a one-step training pro-
cedure that accepts data as input and generates a model as output.
Many research efforts, both within the DB community and in other
communities, focus on this simplified view and aim to improve the
effectiveness of ML training, e.g., by generating more powerful mod-
els through better training algorithms, or by reducing its resource
footprint through various software and hardware optimizations.
This view is reinforced by various leaderboard-style competitions
popular among practitioners (e.g., Kaggle), and academics (e.g., ML
benchmarks and KDD competitions).
At the same time, there is evidence from practitioners [16, 19, 48]
that ML deployments in production are significantly more compli-
cated. Specifically, ML in production involves pipelines with many
interlocking steps, only one of which is training. This has spurred
on the development of many end-to-end ML systems (e.g., TFX [16],
MLFlow [52], Microsoft Azure ML [8], AWS Sagemaker [34]) and
open-source ML libraries (e.g., MLlib [6], MetaFlow [2], and Scikit-
Learn [7]), all of which provide native support for data pre-processing,
data validation, model validation, and model deployment, in addi-
tion to model training, all within a single environment.
As an example, TFX [16] includes pipeline steps that perform 

Document 5: 
 that is, regardless of the model class of the original black-box. Usually,G is chosen to be a constrained class of intrinsi-
cally interpretable models (Section4.2), for example, linear models or GAMs. Put simply, LIME trains many interpret-
able surrogate models to approximate a black-box modelf /C1ðÞ locally. During training, instances are sampled around
each data pointxi weighted byπxi . In addition to local explanations given byξ /C1ðÞ , Ribeiro et al. (2016) introduce a pro-
cedure for obtaining a global understanding of the modelf /C1ðÞ : given a limited budget, their algorithm picks several
explanations based on greedy submodular optimization (Krause & Golovin,2014) and aggregates them into global vari-
able importances, similar to the random forest feature importance (Breiman,2001).
DeepLIFT
Shrikumar et al. (2017) introduce an efficient method for disentangling contributions of inputs in a neural network—
deep learning important features (DeepLIFT). Compared to LIME, DeepLIFT is not model-agnostic since it is explicitly
tailored to neural networks; it also requires a reference, or baseline, data point. While in natural images an all-black
image is typically used as a baseline input, the choice of a reference might not be so trivial for more specialized datasets 

Document 6: 
 SP-LIME
RP-LIME
No cleaning
Figure 10: Feature engineering experiment. Each
shaded line represents the average accuracy of sub-
jects in a path starting from one of the initial
10 sub-
jects. Each solid line represents the average across
all paths per round of interaction.
person inspects ( B) to 6. The position of each algorithm
and the order of the instances seen are randomized between
subjects. After examining the explanations, users are asked
to select which algorithm will perform best in the real world.
The explanations are produced by either greedy (chosen
as a baseline due to its performance in the simulated user
experiment) or LIME, and the instances are selected either
by random (RP) or submodular pick (SP). We modify the
greedy step in Algorithm 2 slightly so it alternates between
explanations of the two classiﬁers. For each setting, we repeat
the experiment with 100 users.
The results are presented in Figure 9. Note that all of
the methods are good at identifying the better classiﬁer,
demonstrating that the explanations are useful in determining
which classiﬁer to trust, while using test set accuracy would
result in the selection of the wrong classiﬁer. Further, we see
that the submodular pick (SP) greatly improves the user’s
ability to select the best classiﬁer when compared to random
pick (RP), with LIME outperforming greedy in both cases.
6.3 Can non-experts improve a classiﬁer? 

","According to the text, LIME can lead to model improvement by non-experts through two mechanisms: 

1. Feature engineering: Non-experts using LIME are able to pick which classifier from a pair generalizes better in the real world. 
2. Improving an untrustworthy classifier: Non-experts using LIME further improve an untrustworthy classifier trained on 20,000 instances of text data by selecting features that are important for the model's predictions, thus potentially leading to better performance.",2.6289350986480717,"LIME (Local Interpretable Model-agnostic Explanations) allows non-experts to understand and interpret the predictions of any classifier or regressor. By providing clear explanations for individual predictions, non-experts can identify which features are most influential in the model's decisions. This understanding enables them to engage in feature engineering by modifying the input features or improving the labeling of the data based on the insights gained from LIME. In experiments, it was shown that non-experts could effectively select better-performing classifiers by using LIME to analyze the models, ultimately leading to improved model accuracy and trustworthiness. The ability to explain predictions empowers users to make informed decisions about model improvement, even without deep expertise in machine learning."
llama3.2:latest,"What is Explainable Boosting Machine model?
","Document 1: 
 feature x2 there is a weight β2 and β1 is ten times greater than
β2, it is reasonable to say that the feature x1 is considerably
more important to the decision of the model. That is, x1 will
have ten times more impact than x2. Therefore, it is possible
to make sense of a global feature importance by analyzing
the values of the β feature weights. One limitation of this
model is that due to its linear nature, local explanations can-
not be directly assessed, that is because the model adjusts the
feature importances as being the same for the entire features
distribution.
4.2 Explainable Boosting Machine
The explainable boosting machine (EBM) proposed in Nori
et al. ( 2019) is a glass-box-type model that leverages state-of-
the-art performing algorithms, such as boosting and bagging,
while maintaining intrinsic interpretability. The main idea
of this model is to build a generalized additive model with
interactions ( GA 2 M) by executing a round-robin training
procedure on one feature at a time, using a meager learning
rate. Its general is given in the form of:
g(E[y]) = β0 + /Sigma1f j (x j ) + /Sigma1fij (xi , x j ) (3)
where g is the link function and f j and fij are called smooth
functions. This approach brings two major improvements 

Document 2: 
 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
https://doi.org/10.1007/s40313-021-00858-y
Explainable Artiﬁcial Intelligence for COVID-19 Diagnosis Through
Blood Test Variables
Lucas M. Thimoteo 1 · Marley M. Vellasco 1 · Jorge Amaral 2 · Karla Figueiredo 3 · Cátia Lie Yokoyama 4 ·
Erito Marques 2
Received: 30 April 2021 / Revised: 1 October 2021 / Accepted: 20 October 2021 / Published online: 3 January 2022
© Brazilian Society for Automatics–SBA 2021
Abstract
This work proposes an explainable artiﬁcial intelligence approach to help diagnose COVID-19 patients based on blood test
and pathogen variables. Two glass-box models, logistic regression and explainable boosting machine, and two black-box
models, random forest and support vector machine, were used to assess the disease diagnosis. Shapley additive explanations
were used to explain predictions for the black-box models, while glass-box models feature importance brought insights into
the most relevant features. All global explanations show the eosinophils and leukocytes, white blood cells are among the
essential features to help diagnose the COVID-19. Moreover, the best model obtained an AUC of 0.87.
Keywords COVID-19 diagnosis · Machine learning · Explainability · Interpretability · Shapley additive explanations ·
Explainable boosting machine 

Document 3: 
 machine learning(Vol. 37, pp. 1718–1727). PMLR.
Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2020). Explainable AI: A review of machine learning interpretability methods.
Entropy, 23(1), 18.https://doi.org/10.3390/e23010018
Lipovetsky, S., & Conklin, M. (2001). Analysis of regression in game theory approach.Applied Stochastic Models in Business and Industry,
17(4), 319–330. https://doi.org/10.1002/asmb.446
Lipton, Z. C. (2018). The mythos of model interpretability.Queue, 16(3), 31–57. https://doi.org/10.1145/3236386.3241340
Liu, K., Sadoune, N., Rao, N., Greitemann, J., & Pollet, L. (2021). Revealing the phase diagram of Kitaev materials by machine learning:
Cooperation and competition between spin liquids.Physical Review Research, 3(2), 023016. https://doi.org/10.1103/physrevresearch.3.
023016
Liu, S., Kailkhura, B., Loveland, D., & Han, Y. (2019). Generative counterfactual introspection for explainable deep learning. In2019 IEEE 

Document 4: 
 atively to predict and impute each missing feature
from the other features until the imputed values con-
verge. MissForest is a popular imputation method
as it is capable of capturing non-linear and interac-
tion eﬀects between features to improve imputation
accuracy, and can be applied to mixed data types
(continuous and discrete). Note that, the framework
of MissForest is similar to that of MICE (Van Buuren
and Groothuis-Oudshoorn, 2011) — the only diﬀer-
ence is MissForest uses random forest while MICE
uses linear model as base model for imputation.
KNN imputation imputes the missing values by the
mean value of its K nearest neighbors in the training
set. The distance of two samples is measured on the
non-missing features in both samples. KNN imputa-
tion is fast and accurate but requires choosing a good
distance metric and tuning the hyperparameter K.
3.3. Explainable Boosting Machines
The methods proposed in this work are based on one
interpretable machine learning model, the Explain-
able Boosting Machine (EBM).
Suppose an input sample is denoted as (x,y), where
x is the p dimensional feature vector and y is the
target. Denote the jth dimension of the feature vector
as xj. Then a generalized additive model (GAM), 

Document 5: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 6: 
 show examples where all models correctly predicted the
123
634 Journal of Control, Automation and Electrical Systems (2022) 33:625–644
Fig. 1 Global interpretation for logistic regression. Top: dataset with blood tests and pathogens; bottom: dataset with only blood tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 635
Fig. 2 Global interpretation for explainable boosting machine. Top: dataset with blood tests and pathogens; Bottom: dataset with only blood tests
outcome, some models failed, while others succeeded, all
models failed.
Thus, we will show the reader how one can interpret local
explanations for the COVID-19 diagnosis in a myriad of sit-
uations. In other words, we want to show that although the
overall results are really good, the models still make mistakes
and get confused with the features, and local explanations
can help with identify that. If the reader wants to check more
examples, we encourage seeing our GitHub, provided at the
beginning of the Experiments section.
Figure 5 shows explanations for a given patient that led to a
true-positive diagnosis and Fig. 6 for a true-negative patient.
Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis. 

","The Explainable Boosting Machine (EBM) is a glass-box-type model that leverages state-of-the-art performing algorithms, such as boosting and bagging, while maintaining intrinsic interpretability. It builds a generalized additive model (GAM) by modeling the interaction between features and predicting outcomes. EBM achieves better results and narrower confidence intervals compared to other models in certain scenarios, especially when considering blood test and pathogen data for COVID-19 diagnosis.",2.467742681503296,"The Explainable Boosting Machine (EBM) is a type of glass-box model that combines boosting and bagging techniques while maintaining interpretability. It builds a generalized additive model with interactions (GA2M) by training on one feature at a time using a small learning rate. The model is defined mathematically as g(E[y]) = β0 + Σ f_j(x_j) + Σ f_ij(x_i, x_j), where g represents the link function, and f_j and f_ij are smooth functions associated with the features. EBMs provide useful insights into feature importance, allowing for an understanding of the relevance of different features in model predictions, while being capable of achieving high predictive performance similar to that of black-box models."
llama3.2:latest,"How can EBMs help with healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 icians tend to focus on abnormal ﬁndings.
Figure 3: EBM shape function of “heart rate” for
predicting pneumonia mortality risk. Blue curve is
the original shape function; red curve is the edited
shape function.
For example, this happens to a pneumonia mortal-
ity risk dataset (Cooper et al., 2005). The blue curve
in Figure 3 shows what an EBM model has learned
for predicting pneumonia mortality as a function of
heart rate. As expected, risk is elevated for patients
with abnormally low (10-30) or high heart rate (125-
200). The graph, however, shows a surprising region
of ﬂat risk between heart rate 38 and 125, which is a
normal heart rate for patients in a doctor’s oﬃce.
Moreover, the model surprisingly predicts patients
who have normal heart rate are at elevated risk: it
adds 0.22 to the risk for patients in this region.
On further inspection, it turns out that there are
no patients in the data set with heart rates between
38 and 125, and 91% of patients are missing their
heart rate which has then been coded as zero. In
other words, there are no data to support the model
in the normal range of heart rate 38-125, and instead
the patients who would be in this range are all coded
as zero in the data and on the graph. This explains
why the model predicts the lowest risk = -0.04 for
patients with heart rate = 0, because these are the
patients who actually have normal heart rates. 

Document 3: 
 indicate the existence of the disease.
Our results indicated that the proposed system could
indeed be used to detect COVID-19 patients using clinical
and laboratory data that is commonly used in clinical prac-
tice. In terms of healthcare, our approach could represent a
new method that can help by quickly screening patients with
COVID-19. This can be specially advantageous to expand
testing protocols to areas where there is not available tests
123
Journal of Control, Automation and Electrical Systems (2022) 33:625–644 641
Fig. 8 Local explanations for
failed diagnosis on hemogram
dataset. Top: EBM; middle: RF;
bottom: SVM
for the local population. Additionally, situations where lab-
oratory workloads are heavy, and RT-PCR tests are scarce
could also beneﬁt from rapid blood sample tests. Therefore,
it would help reduce the burden in the healthcare system
and promote the optimal utilization of healthcare resources.
Besides, we think that is possible to improve the results with
the addition of the clinical signs such as the proportion of
cough, hyperthermia, myalgia, asthenia, diarrhea and confu-
sion.
We look forward to including more COVID-19 test types
as well as data from different geographical and socioeco-
nomic regions from Brazil. Due to the continental size of the
country, the epidemic dynamics changes drastically between
regions and our proposed approach might show different
performance and also different explanations. Moreover, we
would like to increase more blood test features in order 

Document 4: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 5: 
 Recalling that SVM and RF explanations are obtained
through SHAP values, while EBM explanations result from
its smooth functions for each feature.
The pink arrows in the SHAP values waterfall plots indi-
cate values contributing to the positive diagnosis, while the
blue arrows contribute to the negative diagnosis.
For the true positive, low levels of white blood cells and
elevated values of red cells greatly impact the positive out-
come for this patient, for all models.
For the true negative, the white blood cells are also respon-
sible for the majority impact on models output. Interestingly,
the SVM was the only model that took in consideration the
monocytes and sex of the patient as relevant features.
Figure 7 shows a negative patient where the EBM cor-
rectly identiﬁed and RF and SVM classiﬁed as positive. Even
though EBM got the classiﬁcation right, we notice that sev-
eral variables have positive contributions, similarly to the
SVM and RF models. Also, we see that the model output for
SVM and RF are nearly 0.5. Although not reported in the
ﬁgure, the model output for the EBM is 0.48. Therefore, this
patient lies in a low conﬁdence zone for all models.
Figure 8 shows a positive patient that all the models failed
to diagnose, for the hemogram dataset. We notice that all
models predicted a score near to 0 probability of a positive
case and most of the features actually contributed to a neg-
ative diagnosis in all cases. This implies that there might be 

Document 6: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

","EBMs (Elastic Net-based Bridging Models) can help with healthcare data by providing interpretable and explainable predictions. They can detect pairwise interaction terms, which increases accuracy while maintaining intelligibility. Additionally, EBMs can automatically identify and include relevant features, making them useful for understanding the consequences of different imputation methods on model performance.",2.108060598373413,"EBMs (Explainable Boosting Machines) can significantly assist with healthcare data by providing interpretable and editable models that detect potential issues arising from missing values or imputation. They capture subtle anomalies in data that may go unnoticed by simpler models like linear regression or decision trees. EBMs are particularly beneficial in high-stakes environments, such as medical care, where the costs of poor predictions can be severe.

One key advantage of EBMs is their ability to reveal the consequences of different imputation methods, thereby illuminating the influence of missing data on model predictions. For instance, when analyzing datasets related to pneumonia mortality risk, EBMs can highlight particular patterns, such as the abnormal prediction of risk for patients with normal heart rates due to missing data representation. This interpretability helps healthcare professionals make informed decisions based on model outputs.

Furthermore, EBMs can automatically include interaction terms among features, thereby enhancing accuracy while maintaining intelligibility. Their flexibility and enhancement over traditional generalized additive models mean they can adapt to various healthcare applications, improving patient screenings and enabling better resource allocation in testing protocols. Overall, the use of EBMs equips healthcare practitioners with insights that promote safer and more effective clinical decision-making."
llama3.2:latest,"How can be EDMs used in imputing missing values in healthcare data?
","Document 1: 
 ize the potential impact of these imputation methods
(Appendix B), and show that it helps detect potential
problems that otherwise might have remained invisi-
ble and led to suboptimal healthcare decisions.
6. Discussion
We found many potential risks in models that were
introduced by missing values or imputation. Because
EBMs are interpretable and editable, once the prob-
lem is detected, we can often edit the model to ﬁx
these issues using existing model editing tools for
GAMs (Wang et al., 2021). Because edits only af-
fect model behavior on small subsets of samples and
for a few features (e.g., samples near the mean in the
case of mean imputation), the change in accuracy is
small. However, these changes can still be critical in
high-stakes tasks like medical care, where the poten-
tial cost for bad predictions is very high.
The proposed methods are all based on EBM. We
chose EBMs because the shape functions are good at
capturing subtle anomalies in the data, compared to
linear models and decision trees. In the future, it is
worth investigating if other interpretability methods
can handle the same missing value tasks. For exam-
ple, a sparse decision tree model (Lin et al., 2020)
95
Interpretable Missing Values in Healthcare
might be able to learn complex feature interactions
when predicting missingness from other features.
7. Conclusion 

Document 2: 
 and class median impute the missing value as the mean/
median of known Depression values at that time point, in the
appropriate class. Thehot deck [12] method, which is used
to impute labor force items in the Current Population Survey
[13], involves a recipient (the person with missing data) and
a donor (another person in the same imputation class, whose
value is known). We selected a donor by sampling without
replacement from the recipient’s class, and replaced the re-
cipient’s missing value with the donor’s value at the appro-
priate time point. These class imputations imply that persons
with missing data are a random sample of the persons in
their class[14].
Another baseline approach is regression imputation. The
variable of interest (known Depression value for the year in
which the “missing value” occurred) is regressed on baseline
covariates, and the resulting equation is used to estimate
the missing values for that year. We regressed Depression
on baseline Depression, age, gender, and health status. Data
for persons missing any covariate information were not im-
puted. Two different types of regression imputation were
performed.Regressionassigns the person’s predicted value to
the “missing value.” Persons with the same covariates will
have exactly the same imputed value. This can lead to the
variance of the imputed data set being too small, yielding
inappropriately small standard errors and P-values at the 

Document 3: 
 orange) when predicting whether HR is missing. In
the plot, the curve of the imputed group is signiﬁ-
cantly higher than that of the observed group, again
suggesting that when the blood pressure of the pa-
tients is missing, their heart rate is also more likely
to be missing. This eﬀect is strong, as the maximum
gap between the two curves is approximately 2.5 (1.5
in orange curve and -1.0 in blue curve) of predicted
log odds. Again the blue curve is constant.
92
Interpretable Missing Values in Healthcare
Figure 4(c) shows the shape function for tempera-
ture when predicting if urea is missing or not. There
is no missing value for temperature, so there is no
orange curve. The bump at temperature ∈[36,37.5)
indicates that urea is more likely to be missing, which
suggests when a patient has normal body tempera-
ture, doctors may be less likely to order a blood test
to measure urea.
To test how well can EBM predict the missing-
ness, we generate some semi-synthetic datasets with
ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out- 

Document 4: 
 column values. But this approach is appropriate only when there are a couple of missing values in the 
data set. Although as the quantity of missing values increase, significant data may be lost by deletion of 
the incomplete instances [13].
 – Mean/mode substitution: This is a simple way to impute the missing data. It replaces the missing values 
by the mean or mode of all the observations or a subgroup at the same variable. It consists of replacing 
the unknown value for a given attribute by the mean (quantitative attribute) or mode (qualitative attrib-
ute) of all known values of that attribute. But replacing all missing records with a single value distorts 
the input data distribution [21].
 – Maximum likelihood procedures [3]: These methods are used for estimation of parameters of model by 
making use of the expectation–maximization algorithm [13, 21]. These methods make assumption of 
model distribution for the variables, such as a multivariate normal model, which are very much sensitive 
to outliers [21].
 – Multiple imputation [5, 18]: This method imputes the value of a missing variable more than once. Then, 
analysis is done by averaging them [21].
 – Machine learning-based imputation [13]: There are a number of approaches for missing value imputation 
include choosing the most common value among the neighbors, weighted imputation based on k-nearest 
neighbors, decision tree based, association based, fuzzy logic, neural networks, SVM, and clustering- 

Document 5: 
 Meanwhile, the importance of explainable AI is increasingly recognized across
various sectors. Taking medical imaging is an example where physicians rely on
explainability to evaluate and potentially improve diagnoses based on machine
outputs. However, a significant challenge arises when these explanation methods
encounter missing data in the dataset, an aspect they often overlook. The
occurrence of missing data inherently introduces additional uncertainty into the
model. Depending on the imputation values used to fill in the missing data,
explanations and results can vary, potentially leading to inaccurate evaluations
of a patient’s health status. Such issues are not exclusive to healthcare but
extend to other domains, including banking, biology, and more.
Thus, being able to explain the imputation of missing values in the data
is an essential step toward more trustworthy data analysis. This motivates
us to introduceconditional Distribution based Imputation of Missing Values
(DIMV) algorithm, an algorithm that is able to find the conditional distribution
of the features with missing values based on fully observed features in a fashion
2
similar to the Expectation-Maximization algorithm. However, DIMV only
conditionsuponselectedrelevantfeatures, andmoreover, themeanandcovariance
matrix are estimated directly based on the DPER algorithm [2], which gives
DIMV an advantage in speed compared to EM. In addition, the imputation
step of DIMV can be interpreted as Ridge regression, with coefficients giving
direct explainability as regression coefficients. Furthermore, note that by the 

Document 6: 
 ground-truth missing mechanism. Again, these semi-
synthetic datasets start from MIMIC-II imputed by
MissForest, and then apply ﬁxed models (linear,
curvilinear and quadratic models) plus an Gaussian
noise to decide which entry in the “Age” feature is
missing. The feature value is missing when the out-
put score is higher than the threshold. The diﬀer-
ence between MAR and MNAR is whether the target
feature value is considered as an input of the miss-
ing models. Table 2 compares EBM’s the test accu-
racy of predicting missingness with machine learning
models commonly used for missing value imputation.
EBM predicts missingness better than other methods
in cases of MAR and MNAR values generated from
linear and curvilinear models and is not far behind
Random Forest in case of quadratic model.
Summary: We use EBMs to predict the missing-
ness of features from other input features. EBM pre-
dicts the missingness accurately. The interpretability
of EBMs can help users understand the relationship
between the features and missingness and thus bring
more insight for the cause(s) of missingness.
5. Detecting and avoiding potential
risks of missing value imputations
5.1. Imputation With the Mean
Because many machine learning methods cannot na-
tively handle missing values, it is common for data
scientists to impute missing values before training
models. There are many diﬀerent ways to do this 

","I don't have information on EDMs (Exponential Distribution Models) being specifically mentioned or utilized for imputing missing values in healthcare data. However, the text does mention that the proposed methods are based on EBM (Expected Value Model), not EDM. The EBM is a type of model used to predict missing values by capturing subtle anomalies in the data.",2.230294942855835,"EBMs (Explainable Boosting Machines) can be used in imputing missing values in healthcare data by effectively predicting the missingness of features based on the relationships with other input features. Given their interpretable nature, EBMs can uncover subtle anomalies and provide insights into the causes of missing data, whether they follow a Missing At Random (MAR) or Missing Not At Random (MNAR) mechanism. The ability of EBMs to model the shape functions allows them to capture complex interactions between features and missing values, which makes them more proficient than traditional methods like mean or regression imputation. Furthermore, since EBMs are editable, any detected issues related to missing values can be rectified using model editing tools, contributing to more reliable healthcare decisions and potentially mitigating the risks associated with erroneous predictions derived from incomplete data."
llama3.2:latest,"How can EBMs help detect observations poorly imputed by other methods?
","Document 1: 
 concatenation of the two curves corresponding to the
observed group and the missing group. Also, because
we know the oﬀset we added to the imputed value,
we can subtract it during visualization, and show the
two curves on the same plot and original x-axis.
Figure 9 shows the EBM shape functions of the
imputed group and the observed group separated us-
ing the method proposed above. Figure 9(a) shows
that the risk of the RF imputed group is much lower
than the risk of the observed group which corrobo-
rates what we found in Figure 6(a). Similarly, the
eﬀects of the imputed group in Figure 9(b) also diﬀer
signiﬁcantly from the observed group, which explains
why there exist spikes in the RF imputed EBM shape
function in Figure 6(b). Using interpretable methods
like EBMs allows one to understand the consequence
of diﬀerent imputation methods that otherwise would
be invisible.
(a) Shape functions for P/F ratio
(b) Shape functions for Urea
Figure 9: EBM shape functions when the eﬀects of
imputation group (imputed by MissForest, denoted
as RF imputed) and observed (non missing) groups
are separated. The plots suggests how the two groups
are diﬀerent in terms of predicting the ICU mortality
risk, and suggests how MissForest imputation might
result in problematic models.
99 

Document 2: 
 comparable to state-of-the-art machine learning methods like Random Forest and Boosted
Trees, while being highly intelligibile and explainable. EBM is a generalized additive model
(GAM) of the form:
g(E[y]) = β0 +
∑
fj (xj )
where g is the link function that adapts the GAM to diﬀerent settings such as regression
or classiﬁcation. EBM has a few major improvements over traditional GAMs (Hastie and
Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning
techniques such as bagging and gradient boosting. The boosting procedure is carefully
restricted to train on one feature at a time in round-robin fashion using a very low learning
rate so that feature order does not matter. It round-robin cycles through features to mitigate
the eﬀects of co-linearity and to learn the best feature function f j for each feature to show
how each feature contributes to the model’s prediction for the problem. Second, EBM can
automatically detect and include pairwise interaction terms of the form:
g(E[y]) = β0 +
∑
fj (xj ) +
∑
fij (xi,xj )
which further increases accuracy while maintaining intelligibility. EBM is a fast imple-
mentation of the GA 2M algorithm (Lou et al., 2013), written in C++ and Python. The 

Document 3: 
 the mean value of the feature is the same before and
after mean imputation, we can directly ﬁnd the bin
(of EBM) covering the mean value, and detect if the
bin is a spike or not. This also works for median im-
putation — the median of a feature does not change
by imputing the missing values with the median.
To address the second problem, we need an algo-
rithm to distinguish spikes resulting from mean im-
putation and ﬂuctuations that naturally occur in the
EBM shape functions. We formulate this as an out-
lier detection problem. First, we calculate the second
order diﬀerences for all bins in all shape functions
(excluding ﬁrst and last bins), since spikes usually
have extreme second order diﬀerences. We denote
the function values of the kth bin and its neighbour-
ing bins as fk, fk−1, and fk+1. The corresponding
bin sizes are denoted as hk, hk−1, and hk+1. The
second order diﬀerence is
f′′
k (x) ≈
fk+1−fk
(hk+1+hk)/2 − fk−fk−1
(hk+hk−1)/2
hk + hk+1/2 + hk−1/2 . (5)
We then run an outlier detection algorithm (Isolation
Forest (Liu et al., 2008)) on these second order diﬀer- 

Document 4: 
 and generative models like deep generative models
(Yoon et al., 2018). Since most of these methods are
based on black-box machine learning methods and
the accuracy and behavior of the ﬁnal model depends
on the imputed values, it is diﬃcult for users to recog-
nize and understand the potential harms that might
be introduced by these imputation methods.
Recently developed interpretable machine learning
methods have been shown to be useful for debugging
models and detecting issues with datasets (Adebayo
et al., 2020; Koh and Liang, 2017). Interpretable ma-
chine learning methods provide a new opportunity to
study missing values and revisit some of the classi-
cal methods for handling missing values. In this pa-
per, we propose novel methods based on the Explain-
able Boosting Machine (EBM) (Lou et al., 2012, 2013;
Nori et al., 2019), a high-accuracy, fully-interpretable
glass-box machine learning method, to answer the fol-
lowing questions: (1) how interpretability can help
users gain insights on the causes of missingness, and
(2) how interpretability can help detect and avoid po-
tential risks introduced by diﬀerent imputation meth-
ods. We show that the glass-box models provide new
insights into missingness mechanisms, and in some
settings, suggest alternate ways of handling missing
values, as well as new tools that can alert users when 

Document 5: 
 a method to test MCAR based on EBM shape func-
tions. The testing process of the proposed method
88
Interpretable Missing Values in Healthcare
can be directly visualized on the shape function plots,
which is not achievable by Little’s test. We will also
show that EBM can bring additional insights beyond
simply testing for MCAR.
4.1.1. Testing for MCAR with EBM
To test for MCAR, we use the common trick of encod-
ing missing values with a unique value for the feature,
e.g., -1 for a feature with positive values or a sepa-
rate category for a categorical feature. After ﬁtting
an EBM that predicts the target, we get a shape func-
tion representing the contribution of diﬀerent feature
values for predicting the target, including the unique
value denoting missingness. Note that the leaf nodes
in EBM split the feature values into many bins, where
each bin has a prediction score. These bins and scores
together form the shape function. Therefore, the
EBM shape function fj(·) of feature j can be rewrit-
ten as a linear combination of a series of indicator
variables denoting if the feature values are within the
bins, and the coeﬃcients are the corresponding scores
of the bins, i.e.,
fj(xj) =
Bj−1∑
k=0 

Document 6: 
 be similar to uniformly imputing the heart rates in
the interval 38-125 and retraining the model.) This
approach has the following advantages:
1. Editing shape functions provides an opportunity
for experts to use their professional training to
correct and improve models in ways that may not
be adequately represented in the training data.
2. Editing the model may not only improve the ac-
curacy of the model in the real world where it
will be used (instead of just on held-aside test
data from the train set), but also make the shape
plots more “reasonable” and trusted by experts.
3. Editing an EBM shape function can be done
without retraining the model and potentially in-
troducing new problems.
Summary: We show that EBM shape function
can help identify the case when feature values are
missing because they are assumed to be normal. We
also show how editing the EBM graphs can help ad-
dress issues resulting from missing assumed normal.
4.3. Predicting the Missingness
Most missing values are not MCAR, but as mentioned
in Section 2, MNAR and MAR can be diﬃcult to dis-
tinguish (Van Buuren, 2018). For both cases, inter-
pretable models like EBM can still be useful in pro-
viding insights on possible missingness mechanisms.
(a) “Bilirubin” shape function when predicting missing-
ness of “Na”
(b) “Systolic blood pressure” shape function when pre- 

","EBMs (Explainable Boosting Machines) can help detect observations that have been poorly imputed by other methods in several ways:

1. **Shape functions**: EBMs produce a shape function that represents the contribution of different feature values to predicting the target variable. By visualizing these shape functions, experts can identify which observations are being poorly imputed by other methods.
2. **Indicator variables**: The shape functions can be broken down into indicator variables, which indicate whether an observation is within a particular bin or not. These indicators can help identify observations that have been poorly imputed by other methods, as they may not match the expected pattern of the data.
3. **Score values**: The scores associated with each feature value in the shape function can also provide clues about which observations are being poorly imputed. High score values may indicate that an observation is being predicted correctly, while low score values may suggest poor imputation.

By using these visualization tools and techniques, EBMs can help experts identify observations that have been poorly imputed by other methods, allowing them to take corrective action to improve the quality of their data.",4.825688123703003,"EBMs can help detect observations poorly imputed by other methods by providing a clear visualization of the contribution and behavior of feature values through their shape functions. When different imputation methods are applied, such as MissForest, EBMs can reveal discrepancies between the imputed groups and the observed groups, highlighting significant differences in predicted outcomes, such as ICU mortality risk. 

By analyzing the shape functions, researchers can identify spikes or unusual patterns that may indicate problematic imputation results. Additionally, EBMs allow for the calculation of outlier detection based on second-order differences in feature values, making it possible to distinguish between natural fluctuations and those caused by flawed imputation methods. This interpretability enables users to gain insights into the implications of missing data handling, allowing them to address issues arising from imputation and potentially leading to more accurate and reliable model performance."
llama3.2:latest,"How can one distinct terms intepretabilitry and explainability in XAI?
","Document 1: 
 our approach shows better results and a narrower conﬁdence
interval. When considering the blood test and the pathogens,
we obtained an AUC measure of AUC = 0.874 (CI 95%
±0.04) for RF, and they obtained 0.778 (CI 95% ±0.7). It is
also worth mentioning that the explainable boosting model
achieved an AUC = 0.873 with the beneﬁt of explaining. We
believe that the results could be further improved with the
addition of the clinical signs such as the proportion of cough,
hyperthermia, myalgia, asthenia, diarrhea and confusion.
7 Interpretation and Explanations
Explainable artiﬁcial intelligence (XAI) is a new topic of
study that focuses on machine learning interpretability and
aspires to create a more transparent AI. The major goal is
to develop a set of interpretable models and methodologies
that result in more understandable models while maintaining
excellent prediction performance (Adadi and Berrada2018).
Regrettably, there is no universally accepted deﬁnition of
explainable. Some researchers use the terms interpretability
and explainability interchangeably, while others distinguish
between the two. Authors Doshi-V elez and Kim ( 2017)
deﬁne interpret as “to explain or present in language that
humans can understand.” Authors in Samek et al. (2019)
deﬁne interpretation as the translation of abstract concepts
into a domain humans can understand, whereas explanation
is the collection of the features of the interpretable domain
123 

Document 2: 
 tant to distinguish between features and interpretable data
representations. As mentioned before, interpretable expla-
nations need to use a representation that is understandable
to humans, regardless of the actual features used by the
model. For example, a possible interpretable representation
for text classiﬁcation is a binary vector indicating the pres-
ence or absence of a word, even though the classiﬁer may
use more complex (and incomprehensible) features such as
word embeddings. Likewise for image classiﬁcation, an in-
terpretable representationmay be a binary vector indicating
the “presence” or “absence” of a contiguous patch of similar
pixels (a super-pixel), while the classiﬁer may represent the
image as a tensor with three color channels per pixel. We
denote x∈Rd be the original representation of an instance
being explained, and we use x′∈{0, 1}d′
to denote a binary
vector for its interpretable representation.
3.2 Fidelity-Interpretability Trade-off
Formally, we deﬁne an explanation as a model g ∈G,
where G is a class of potentially interpretable models, such
as linear models, decision trees, or falling rule lists [ 27], i.e. a
model g∈Gcan be readily presented to the user with visual
or textual artifacts. The domain of g is {0,1}d′
, i.e. g acts 

Document 3: 
  Explanation by feature relevance: This idea is similar to simplification. Roughly, this type of XAI approaches attempts
to evaluate a feature based on its average expected marginal contribution to the model's decision, after all possible
combinations have been considered (Chen et al., 2019; Pedreschi et al., 2019).
 Visual explanation: This type of XAI approach is based on visualization (Chattopadhay et al., 2018). As such, the fam-
ily of data visualization approaches can be exploited to interpret the prediction or decision over the input data.
 Local explanation: Local explanations (Selvaraju et al., 2017) approximate the model in a narrow area, around a spe-
cific instance of interest, and offer information about how the model operates when encountering inputs that are
similar to the one we are interested in explaining.
The ML literature predominantly uses the term“interpretability” as opposed to“explainability,” but according to
Burkart and Huber (2020),interpretability itself is insufficient as it does not cover all possible problems associated with
understanding “black-box” models. To gain the trust of users, and acquire meaningful insights about the causes, rea-
sons, and decisions of“black-box” approaches, explainability is required rather than simple interpretability. Although,
explainable models are interpretable by default, the opposite is not always true. The existing literature (Adadi &
Berrada, 2018) divides XAI taxonomy by: 

Document 4: 
 9.8.1735
Hohman, F., Srinivasan, A., & Drucker, S. M. (2019). TeleGam: Combining visualization and verbalization for interpretable machine learn-
ing. InIEEE visualization conference (VIS). IEEE.https://doi.org/10.1109/visual.2019.8933695
Holzinger, A., Langs, G., Denk, H., Zatloukal, K., & Müller, H. (2019). Causability and explainability of artificial intelligence in medicine.
WIREs Data Mining and Knowledge Discovery, 9(4), e1312.https://doi.org/10.1002/widm.1312
Jain, S., & Wallace, B. C. (2019). Attention is not explanation. InProceedings of the 2019 conference of the North American chapter of the Asso-
ciation for Computational Linguistics: Human language technologies(pp. 3543–3556). Association for Computational Linguistics.https://
doi.org/10.18653/v1/N19-1357
Jesus, S., Belém, C., Balayan, V., Bento, J., Saleiro, P., Bizarro, P., & Gama, J. (2021). How can I choose an explainer? An application-
grounded evaluation of post-hoc explanations. InProceedings of the 2021 ACM conference on fairness, accountability, and transparency 

Document 5: 
 In general, there is no agreement within the ML community on the definition ofinterpretability and thetask of inter-
pretation (Doshi-Velez & Kim,2017; Lipton,2018). For example, Doshi-Velez and Kim (2017) define interpretability of
ML systems as“the ability to explain or to present in understandable terms to a human.” This definition lacks mathemat-
ical rigor (Lipton, 2018). Nevertheless, the notion of interpretability often depends on the domain of application
(Rudin, 2019) and the targetexplainee (Carvalho et al.,2019), that is, the recipient of interpretations and explanations.
Therefore, an all-purpose definition might be infeasible (Rudin,2019) or unnecessary. Other terms that are synonymous
with interpretability and also appear in the ML literature are“intelligibility” (Caruana et al.,2015; Lou et al.,2012) and
“understandability” (Lipton, 2018). These concepts are often used interchangeably.
Yet another term prevalent in the literature is“explainability,” giving rise to the direction ofexplainable artificial
intelligence (XAI) (Gunning & Aha,2019). This concept is closely tied with interpretability; and many authors do not
differentiate between the two (Carvalho et al.,2019). Doshi-Velez and Kim (2017) provide a definition ofexplanation 

Document 6: 
 Kirrane, 2020)).
In recognition of the growing importance of this topic, NIST published in August 2020Four principles of XAI
(Phillips et al., 2020), which define the following fundamental principles which an AI must honor to be considered an
XAI as follows:
 Explanation: this principle states that an AI system must supply evidence, support; or reasoning for each decision
made by the system.
 Meaningful: this principle states that the explanation provided byt h eA Is y s t e mm u s tb eu n d e r s t a n d a b l eb y ,a n dm e a n i n g f u l
to, its users. As different groups of users may have different necessities and experiences, the explanation provided by the AI
system must be fine-tuned to meet the various characteristics and needs of each group.
 Accuracy: this principle states that the explanation provided by the AI system must reflect accurately the system's processes.
 Knowledge limits: this principle states that AI systems must identify cases that they were not designed to operate in
and, therefore, their answers may not be reliable.
Figure 3 depicts the ontology of the XAI taxonomy. Transparent models can easily achieve explainability, while
opaque models require post hoc approaches to make them explainable. The categories of post hoc approaches are illus-
trated accordingly. 

","The terms ""interpretability"" and ""explainability"" in Explainable AI (XAI) often overlap, but they have distinct connotations and implications. Here's a concise summary:

Interpretability refers to the ability of a model to be understood or comprehended by humans, often through mathematical or algorithmic means. It implies that the model's decisions or outputs can be explained in terms of its internal workings or parameters.

Explainability, on the other hand, is more focused on the transparency and justifiability of a model's decisions. An explainable AI system provides evidence, reasoning, or justification for its conclusions, making it easier for users to understand why certain decisions were made.

To distinguish between interpretability and explainability:

* Interpretability is about understanding the ""how"" (the inner workings of the model), while
* Explainability is about understanding the ""why"" (justification or rationale behind the model's decisions).

In other words, interpretability aims to provide insight into a model's internal mechanisms, whereas explainability seeks to provide context and justification for its output.",4.310437917709351,"Interpretability and explainability are terms that are often used interchangeably within the field of explainable artificial intelligence (XAI), but they can be distinguished based on their definitions and implications. Interpretability refers to the degree to which a user can understand the cause of a decision made by an AI system, typically implying that the model is transparent and its workings can be easily explained in human terms. Doshi-Velez and Kim (2017) define interpretability as “the ability to explain or present in understandable terms to a human.”

On the other hand, explainability encompasses a broader scope. It pertains to how an AI system provides reasoning or support for its decisions and aims to convey this information meaningfully to users. Explainability is necessary when dealing with complex or ""black-box"" models where the internal processes are not straightforward or easily understood. As Burkart and Huber (2020) note, while explainable models are interpretable by default, the opposite is not necessarily true, highlighting that interpretability alone may not address all aspects of understanding machine learning decisions.

In summary, while interpretability focuses on the clarity and transparency of the model itself, explainability relates more to the quality and relevance of the information provided to users regarding the model's decision-making process."
llama3.2:latest,"What issues in machine learning can be categorized as 'model bugs'?
","Document 1: 
 2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n
i, wherex∈X and y∈Y, a classiﬁer’s goal is to learn a function,fθ : X→Y ,
that generalizes. fθ is then used to predict test examples,xtest ∈X, as ytest = fθ(xtest). Given a
loss functionL, and model parameter,θ, for a model family, we provide a categorization of bugs as
model, data and test-time contamination:
Learning: arg min
θ
Model Contamination
L(
Data Contamination  
(Xtrain,Ytrain) ; θ);
Prediction: ytest = fθ(
Test-Time Contaminationxtest ).
1We will provide code to replicate our ﬁndings at: https://github.com/adebayoj/explaindebug.git.
3
Bug Category Speciﬁc Examples tested Formalization
Data Contamination Spurious Correlation arg min
θ
L(Xspurious artifact,Ytrain;θ)
Labelling Errors arg min
θ 

Document 2: 
 lyzing all of this data together requires some form of data
integration. In machine learning, two relevant integration
techniques are alignment and co-learning. Alignment is to
ﬁnd relationships of sub-components of instances that have
multiple modalities. For example, if there are multi-view time
series, one can perform subsampling, forward or backward
ﬁlling, or aggregate in time windows so that the time series
can be better integrated. Co-learning is to train better on a
modality using a different modality. For example, if there
are embeddings from different modalities, one approach is
to concatenate them together for a multimodal representa-
tion. In general, data integration is by itself a large research
area that has been studied for decades [ 46,152], although not
all techniques are relevant to machine learning.
4 Robust model training
Even after collecting the right data and cleaning it, data
quality may still be an issue during model training. It is
widely agreed that real-world datasets are dirty and erro-
neous despite the data cleaning process. As summarized in
Table2, these ﬂaws in datasets can be categorized depending
on whether data values are noisy or missing and depending
on whether these ﬂaws exist in data features (attributes) or
labels.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 799
Table 2 Types of data poisoning covered in this survey
Noisy Missing 

Document 3: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 4: 
 attributions, and attributions for these samples on other models. As seen in Table 3, we observe
visual similarity, particularly for the VGG-16 model on ImageNet, but essentially no correlation in
feature ranking.
10
Insights from Human Subject Study: users use prediction labels, not the attributions.
For the domain shift study, we show participants attribution of dogs that were not used during
training, and whose breeds diﬀered from those that the model was trained to predict. We ﬁnd that
users do not recommend a model under this setting due to wrong prediction labels (Figure 5).
6 Discussion & Conclusion
Debugging machine learning models remains a challenging endeavor, and model explanations could
be a useful tool in that quest. Even though a practitioner or a researcher may have a large class of
explanation methods available, it is still unclear which methods are useful for what bug type. This
work aims to address this gap by ﬁrst, categorizing model bugs into: data, model, and test-time
contamination bugs, then testing feature attribution methods, a popular explanation approach for
DNNs trained on image data, against each bug type. Overall, we ﬁnd that feature attribution
methods are able to diagnose the spatial spurious correlation bug tested, but do not conclusively
help to distinguish mislabelled examples for normal ones. In the case of model contamination, we
ﬁnd that certain feature attributions that perform positive aggregation while computing feature
relevance with modiﬁed back-propagation produce attributions that are invariant to the parameters 

Document 5: 
 that the humans exploit label information and Feng and Boyd-Graber (2019) demonstrate how to
assess explanations in a natural language setting. Similarly, Alqaraawi et al. (2020) ﬁnd that the
LRP explanation method (see Section 2.2) improves participant understanding of model behavior
for an image classiﬁcation task, but provides limited utility to end-users when predicting the model’s
output on new inputs.
Feature attributions can be easily manipulated, providing evidence for a collective ‘weakness’ of
current approaches (Ghorbani et al., 2019; Heo et al., 2019; Lakkaraju and Bastani, 2020; Slack
et al., 2020). While susceptibility is an important issue, our work focuses on providing insights
when model bugs are ‘unintentionally’ created.
2 Bug Characterization, Explanation Methods, & User Study
We now present our characterization of model bugs, provide an overview of the explanation methods
assessed, and close with a background on the human subject study.1
2.1 Characterizing Model Bugs.
We deﬁne modelbugs as contamination in the learning and/or prediction pipeline that causes the
model to produce incorrect predictions or learn error-causing associations. We restrict our attention
to the standard supervised learning setting, and categorize bugs based on their source. Given input-
label pairs,{xi,yi}n 

Document 6: 
 model as a way to provide a global understanding.
There are several ways a model or its evaluation can go
wrong. Data leakage, for example, deﬁned as the uninten-
tional leakage of signal into the training (and validation) data
that would not appear when deployed [ 14], potentially in-
creases accuracy. A challenging example cited by (author?)
[14] is one where the patient ID was found to be heavily corre-
lated with the target class in the training and validation data.
This issue would be incredibly challenging to identify just by
observing the predictions and the raw data, but much easier
if explanations such as the one in Figure 1 are provided, as
patient ID would be listed as an explanation for predictions.
Another particularly hard to detect problem is dataset shift
[5], where training data is diﬀerent than test data (we give
an example in the famous 20 newsgroups dataset later on).
The insights given by explanations are particularly helpful in
identifying what must be done to convert an untrustworthy
model into a trustworthy one – for example, removing leaked
data or changing the training data to avoid dataset shift.
Machine learning practitioners often have to select a model
from a number of alternatives, requiring them to assess
the relative trust between two or more models. In Figure
Figure 2: Explaining individual predictions of com-
peting classiﬁers trying to determine if a document
is about “Christianity” or “Atheism”. The bar chart
represents the importance given to the most rele- 

","According to the text, model bugs are categorized into three types: 

1. Model Contamination
2. Data Contamination
3. Test-Time Contamination.

These categories refer to contamination or errors in the learning and/or prediction pipeline of a machine learning model, which can cause incorrect predictions or learn error-causing associations.",2.143232583999634,"Model bugs in machine learning can be categorized as contamination in the learning and/or prediction pipeline that leads to incorrect predictions or the learning of error-causing associations. These bugs can be classified into three primary categories based on their source:

1. **Model Contamination**: Issues arising from the model itself that can affect its learning process or predictions.
  
2. **Data Contamination**: Problems related to the dataset used for training, such as:
   - Spurious correlations in the training data.
   - Labeling errors, where incorrect or noisy labels are present due to manual annotation errors or software mistakes.

3. **Test-Time Contamination**: Problems that occur during the model's prediction phase, potentially due to issues like data leakage, where unintended signals from the training set could artificially inflate accuracy in the prediction phase, or dataset shift, where the training data differs from the test data.

These model bugs highlight the complexities and challenges in ensuring model reliability and accuracy in machine learning frameworks."
llama3.2:latest,"What XAI techniques can be heloful in detecting mislabeled observation in image classification?
","Document 1: 
 query is generally much smaller than the entire database.
Thus, the computation spent for imputation is signiﬁ-
cantly saved.
4.3 Noisy labels
Because data labeling is done manually in many cases,
incorrect or missing labels are, in fact, very common;
the proportion of incorrect labels is reported to be 8–
38% in several real-world datasets [ 150]. As an example of
ANIMAL-10N [149], which is real-world noisy data with
human annotation errors, human annotators mistakenly clas-
siﬁed the Cheetah images as other animals like Jaguars
instead of Cheetahs. In this example, it may be difﬁcult to
distinguish the patterns of Cheetahs and Jaguars, resulting
in noisy labels in training data. So wrong annotations can be
caused by such human errors. Similarly, labeling errors occur
with data types other than images. For sentiment analysis,
annotators often disagree on the polarity (e.g., positive or neg-
ative) of the sentiment expressed in the text [ 166]. Another
type of error is software error. If there are many images to
annotate, one may use automatic object recognition software.
However, the object recognition itself may have errors. Thus,
many deep learning techniques have been developed to con-
sider the existence of label noises and errors, which are more
critical in deep learning than in conventional machine learn-
ing as a deep neural network completely memorizes such 

Document 2: 
 In the ILSVRC2012 ImageNet competition, it surpassed the second-best entry by a huge
advantage. After AlexNet achieved excellent results in the ImageNet image classiﬁcation
competition, researchers began to study CNN more deeply, Zeiler and Fergus proposed
a visualization technique to understand CNNs and proposed ZFNet [ 20]. Min Lin et al.
proposed NIN network [21], which contributed to the control of the parameter amount
and the number of channels. Next, refs. [ 22–27] yielded high performance during the
ILSVRC2014-2017 classiﬁcation challenge, they all made great innovations on the original
basis. From 2017 to the present, more models with superior performance have appeared
one after another. CNNs have increasingly demonstrated irreplaceable superiority in image
classiﬁcation.
With the successful application of CNN to large-scale visual classiﬁcation tasks, around
2015, the application of CNNs has ﬁnally taken off in the remote sensing image analysis
ﬁeld [28,29]. A variety of CNN-based scene classiﬁcation methods have emerged by using
different strategies of exploiting CNNs [ 30–36]. Generally, CNN-based remote sensing
image scene classiﬁcation methods can be divided into three types: (i) The pre-trained
CNNs is used as a feature extractor [ 37–44]. (ii) Fine-tune the pre-trained CNNs on the
dataset [30,45–52]. (iii) Globally initialize the weights of CNNs for training [ 31,53–55].
As we all know, the CNN-based image classiﬁcation method was originally designed for 

Document 3: 
 ence, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence,
EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 6021–6029. AAAI Press, 2020. URL
https://aaai.org/ojs/index.php/AAAI/article/view/6064.
C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011
Dataset. Technical Report CNS-TR-2011-001, California Institute of Technology, 2011.
17
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from
error visibility to structural similarity.IEEE transactions on image processing, 13(4):600–612,
2004.
Gezheng Wen, Brenda Rodriguez-Niño, Furkan Y Pecen, David J Vining, Naveen Garg, and Mia K
Markey. Comparative study of computational visual attention models on two-dimensional medical
images. Journal of Medical Imaging, 4(2):025503, 2017.
Chih-Kuan Yeh, Joon Kim, Ian En-Hsu Yen, and Pradeep K Ravikumar. Representer point selection
for explaining deep neural networks. InAdvances in neural information processing systems, pages
9291–9301, 2018. 

Document 4: 
 self-supervised learning and generative models, and we
will cover a self-supervised learning technique called Jig-
sawNet [108].
In Mean-Teacher [ 156], the teacher model is the average
of consecutive student models. Both the student and teacher
models evaluate the input in a training batch. The softmax
output of the student model is compared with the one-hot
label using a classiﬁcation cost. Additionally, the output is
compared with the teacher output using the consistency loss.
After the weights of the student models are updated via gra-
dient descent, the teacher model weights are updated as an
exponential moving average of the student model weights.
123
Data collection and quality challenges in deep learning: a data-centric AI perspective 803
Fig. 10 Example of the jigsaw puzzle task for a given unlabeled
image [108]
A training step with unlabeled examples is done without the
classiﬁcation cost.
In MixMatch [ 17], to exploit an unlabeled dataset, it per-
forms label guessing where stochastic data augmentation is
applied to an unlabeled imageK times; then, each augmented
image is fed through the classiﬁer. The average of these
K predictions is sharpened by adjusting the distribution’s
temperatures. The MixMatch algorithm mixes both labeled
examples and unlabeled example with label guesses. In more
detail, when mixing two images, the images are overlayed,
and the labels are averaged, following the MixUp augmen- 

Document 5: 
 which was veriﬁed via3 questions.
Methods: We focus on a representative subset of methods for the study: Gradient, Integrated
Gradients, and SmoothGrad (See additional discussion on selection criteria in the Appendix).
Bugs: We tested the bugs described in Table 1 along with a model with no bugs.
3 Debugging Data Contamination
Overview. We assess whether feature attributions can detect spurious training artifacts and mis-
labelled training examples. Spurious artifacts are signals that encode or correlate with the label in
the training set but provide no meaningful connection to the data generating process. We induce
a spurious correlation in the input background and test whether feature attributions are able di-
agnose this eﬀect. We ﬁnd that the methods considered indeed attribute importance to the image
background for inputs with spurious signals. However, despite visual evidence in the attributions,
5
participants in the human subject study were unsure about model reliability for the spurious model
condition; hence, did not out-rightly reject the model.
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
Spurious 
Image
Spurious 
Background 
only
A
B
C
D
Figure 3: Feature Attributions for Spurious Correlation Bugs.Figure shows attributions 

Document 6: 
 3.2 Mislabelled Training Examples
Bug Implementation.We train a BVD-CNN model on a birds-vs-dogs dataset where10 percent
of training samples have their labels ﬂipped. The model achieves a93.2, 91.7, 88 percent accuracy
on the training, validation, and test sets.
Correct  
Label
Incorrect  
Label
Input
Correct  
Label
Incorrect  
Label
Figure 6: Diagnosing Mislabelled Training Examples.The Figure shows two training inputs
along with feature attributions for each method. The correct label row corresponds to feature
attributions derived from a model with the correct label in the training set. The incorrect-label row
shows feature attributions derived from a model with the wrong label in the training set. We see
that the attributions under both settings are visually similar.
Results. We ﬁnd that attributions from mislabelled examples for a defective model are visually
similar to attributions for these same examples but derived from a model with correct input labels
(examples in Figure 6). We ﬁnd that the SSIM between the attributions of a correctly labeled
instance, and the corresponding incorrectly labeled instance, are in the range0.73 −0.99 for all
methods tested. These results indicate that the attribution methods tested might be ineﬀective for
identifying mislabelled examples. We refer readers to Section I of the Appendix for visualizations
of additional examples.
Insights from Human Subject Study: users use prediction labels, not attribution meth- 

","Based on the provided context, two XAI (Explainability and Interpretability) techniques that can be helpful in detecting mislabeled observations in image classification are:

1. **Integrated Gradients**: This technique can help detect mislabelled examples by analyzing how changes in the input affect the output of the model. If a mislabelled example has similar feature attributions to correctly labelled examples, it may indicate that the label is incorrect.
2. **SmoothGrad**: Similar to Integrated Gradients, SmoothGrad can also be used to analyze how the input affects the output of the model. By comparing the feature attributions for correctly and incorrectly labelled examples, we can identify potential mislabelled observations.

Additionally, other XAI techniques such as:

* **Feature attribution maps** (e.g., SHAP values or LIME)
* **Model interpretability methods** (e.g., saliency maps or attention weights)

may also be useful in detecting mislabeled observations by providing insights into how the model's outputs are generated and which input features contribute most to those outputs.",4.3729541301727295,"The context suggests that several explainable AI (XAI) techniques can be utilized to help detect mislabeled observations in image classification. Some of these methods include:

1. **Feature Attribution Methods**: Techniques like Gradient, Integrated Gradients, and SmoothGrad can be used to assess the importance of various features in the model's predictions. However, evidence from studies indicates that these feature attribution methods may struggle to effectively differentiate between correctly and incorrectly labeled instances, as the attributions derived from mislabeled data can be visually similar to those from correctly labeled instances.

2. **Diagnostic Models**: Training models on datasets with known proportions of mislabeled examples, such as a BVD-CNN on a birds-vs-dogs dataset with flipped labels, can help evaluate how models respond to mislabeling. This approach can provide insights into the model's performance and the impact of label noise.

3. **Spurious Correlation Detection**: Techniques that assess attributions for spurious signals (artifacts unrelated to true data generating processes) can be employed to identify misleading patterns in training data, including mislabeled examples.

These techniques highlight that while XAI methods can provide insights into model behavior, identifying mislabeled observations may require careful design and additional strategies beyond traditional attribution approaches."
llama3.2:latest,"How can deep learning methods be used in imputation of missing data?
","Document 1: 
 least square methods [23] for imputation. For large datasets, deep learn-
ing imputation techniques have gained popularity due to their performance
4
[24, 25, 26]. It’s important to note that different imputation methods may
produce varying values for each missing entry, making the modeling of un-
certainty for each missing value crucial. Bayesian and multiple imputation
techniques, such as Bayesian principal component analysis-based imputa-
tion [27] and multiple imputations using Deep Denoising Autoencoders [28],
are particularly useful in these scenarios. Furthermore, certain tree-based
methods can naturally handle missing data through prediction, including
missForest [29], the DMI algorithm [30], and DIFC [31]. Methods that can
manage mixed data types have also emerged, such as SICE [32], FEMI [33],
and HCMM-LD [34].
Although various missing data methods have been developed [35], most
of them to date do not have a built-in explanation mechanism for imputed
values, although how a value is imputed can have a profound impact on the
performance and interpretability of downstream machine learning models.
If one uses a prediction model such as a regression or classification model,
then one can use Shapley values or some other XAI technique to explain the
imputation of missing values. However, many times, one may want to use
a more complicated imputation method that requires building regression or
classification models and looping through the data multiple times to improve
imputation accuracy. 

Document 2: 
 forinitializationofthemissingvalues.Moreimportantly,almost
allofthesemethodsuseabottlenecklayerasthelatentspace,
whileweuseanencodinglayerofsizemuchbiggerthanthatof
inputwithtwopurposes:Firsttoensureabetterreconstruction
oftheinputandsecondthedataintheprojectedspaceofhigher
dimensionarelikelytobeflattened[51].In[51]authorsremarked
that unfolding of class manifolds using deep networks results
inhigherdimensionofthedatawhichmakestheclassesmore
separable.Moreover,likethesemethods,ourobjectiveisnotjust
therestorationofmissingvalues,buthowwecanimputemissing
valuessothatwecanusedatawithmissingvaluesindesigning
moreeffectiveclassifiers.
Theselforganizingmap(SOM)[30,31]andmultilayerpercep-
tron(MLP)[32]aretheothertwomachinelearningtechniques
that have also been used to impute missing values. There are
several other machine learning techniques to impute missing
data[52–56].Aknowledge-basedmethodhasalsobeendevel-
opedthatcandealwithmissingvalueswhichisnon-numericin
nature[57].Chih-Fongetal.[58]proposedamodifiedformofthe
meanimputationmethod.First,thedatasetisdividedintocom- 

Document 3: 
 SN Computer Science
[175] 2021 Springer Data imputation in wireless 
sensor network using deep 
learning techniques
Data Analytics and Manage-
ment
[176] 2020 Sustainable and Resilient 
Infrastructure
Handling incomplete and 
missing data in water net-
work database using imputa-
tion methods
Taylor and Francis
Page 28 of 37Emmanuel et al. J Big Data           (2021) 8:140 
missing to 5%, 10% and 15% across all observations. RMSE performance measure was 
then used to help quantify the accuracy of the estimated values compared to the actual 
non-missing data.
After simulation of missing values, KNN imputation was implemented to replace the 
missing values. Firstly, when implementing the imputation method, the nearest neigh -
bors (K) must be chosen. The value of K was chosen based on experimental results start -
ing with K = 1 and stopped at K = 5, the best accurate estimation value of K was then 
used for the experiment which was K = 4. The Euclidean distance measure was used on 
the KNN imputation algorithm. The  RF missForest algorithm was then implemented, 
which is a nonparametric imputation method based on the random forest. For every 
variable missForest fits a random forest on the observed values and then predicts the 
missing variables. The process of training and predicting of missForest is repeated in an 
iterative process until a number of iterations are reached. The missForest ran for three 

Document 4: 
 this in a concise way. This is the area to employ meta-learning model to capture these high-level interactions. In future
work, we aim to extend this analysis with adding new data sets and training surrogate model to deeper understanding
the complex interactions between the structure of missing data in tasks, the assumption of imputation method and
ML algorithms (Wo´ znica and Biecek, 2020). Another extension may be considering hyperparameters optimization in
machine learning models as well as imputation methods.
References
B. Bischl, G. Casalicchio, M. Feurer, F. Hutter, M. Lang, R. G. Mantovani, J. N. van Rijn, and J. Vanschoren. OpenML
Benchmarking Suites. 8 2017. URL http://arxiv.org/abs/1708.03731.
M. L. Brown and J. F. Kros. Data mining and the impact of missing data. Industrial Management and Data Systems,
103(8-9):611–621, 2003. ISSN 02635577. doi: 10.1108/02635570310497657.
T. Hastie, R. Mazumder, J. D. Lee, and R. Zadeh. Matrix Completion and Low-Rank SVD via Fast Alternating Least
Squares. Technical report, 2014.
J. Honaker, G. King, and M. Blackwell. Amelia II: A program for missing data. Journal of Statistical Software, 45(7): 

Document 5: 
 missing rate is high and the samples are inadequate. Another
solution is to ﬁll in the missing values with substituted val-
ues, which is known asdata imputation . However, these
methods do not capture variable correlations and may not
capture complex patterns to perform imputation. Combin-
ing the imputation methods with prediction models often
results in a two-step process where imputation and prediction
models are separated; the missing patterns are not effectively
explored in the prediction model, thus leading to suboptimal
analysis results.
GRU-D [29] is a deep learning model based on the gated
recurrent unit (GRU) to effectively exploit two representa-
tions of informative missingness patterns—masking and time
interval. Masking informs the model of which inputs are
observed or missing, while time interval encapsulates the
input observation patterns. GRU-D captures the observa-
tions and their dependencies by applying masking and time
interval, which are implemented using a decay term, to the
inputs and network states of the GRU, and jointly train all
model components through back-propagation. GRU-D not
only captures the long-term temporal dependencies of time-
series observations, but also utilizes the missing patterns to
improve the prediction results.
We elaborate on the two components of GRU-D: making
and time interval. The value of a missing variable tends to be
close to some default value if its last observation happened 

Document 6: 
 bayesian ridge regression, Symmetry 12 (2020) 1594.
[23] P. Keerin, W. Kurutach, T. Boongoen, An improvement of missing value
imputation in dna microarray data using cluster-based lls method, in:
2013 13th International Symposium on Communications and Informa-
tion Technologies (ISCIT), IEEE, 2013, pp. 559–564.
[24] S. J. Choudhury, N. R. Pal, Imputation of missing data with neural net-
works for classification, Knowledge-Based Systems 182 (2019) 104838.
[25] A. Garg, D. Naryani, G. Aggarwal, S. Aggarwal, Dl-gsa: a deep learning
metaheuristic approach to missing data imputation, in: International
Conference on Sensing and Imaging, Springer, (2018), pp. 513–521.
[26] K. Mohan, J. Pearl, Graphical models for processing missing data,
Journal of the American Statistical Association (2021) 1–42.
[27] V. Audigier, F. Husson, J. Josse, Multiple imputation for continuous
variables using a bayesian principal component analysis, Journal of
statistical computation and simulation 86 (2016) 2140–2156.
[28] L. Gondara, K. Wang, Multiple imputation using deep denoising au-
toencoders, arXiv preprint arXiv:1705.02737 (2017). 

","Deep learning methods, such as neural networks and recurrent neural networks, can be used to impute missing data by leveraging the patterns and relationships in the data. These methods can capture complex patterns and correlations between variables, leading to more accurate imputations.

Some common deep learning approaches for missing data imputation include:

1. Masking: This involves identifying which inputs are observed or missing, and using this information to inform the model.
2. Time interval: This involves capturing the temporal dependencies of observations by applying a decay term to the inputs and network states of the GRU (Gated Recurrent Unit) or other recurrent neural networks.

Deep learning methods can be used for imputation in various ways, including:

1. Predicting missing values based on patterns learned from training data.
2. Combining imputation with prediction models to improve analysis results.
3. Utilizing multiple representations of informative missingness patterns, such as masking and time interval, to capture complex relationships.

Some examples of deep learning-based methods for missing data imputation include:

1. GRU-D (Gated Recurrent Unit for Deep Learning): A deep learning model based on the GRU that effectively exploits two representations of informative missingness patterns - masking and time interval.
2. Dl-gsa: A deep learning metaheuristic approach to missing data imputation that uses neural networks to find the optimal solution.

Overall, deep learning methods offer a promising approach for handling high-dimensional and complex datasets with missing values, leading to more accurate and reliable imputations.",5.496887922286987,"Deep learning methods can be utilized in the imputation of missing data through various techniques that leverage the capabilities of neural networks to model complex patterns and relationships within the data. For example, models such as Deep Denoising Autoencoders can be employed to perform multiple imputations by reconstructing missing values based on learned representations of the data. Additionally, Gated Recurrent Unit based models like GRU-D exploit informative missingness patterns by incorporating mechanisms for masking and time intervals, allowing them to capture long-term dependencies in time-series data and thus improve prediction results. Moreover, traditional deep learning architectures, such as multilayer perceptrons (MLP), and self-organizing maps (SOM) can also be adapted for the imputation task, alongside ensemble methods like random forest imputation (missForest), which iteratively trains on observed values to predict missing entries. Overall, deep learning approaches often allow for more effective handling of missing data by integrating the imputation process with classification or prediction models, facilitating better overall model performance."
