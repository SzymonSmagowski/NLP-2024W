{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\OneDrive\\Desktop\\PW\\Natural Language Processing\\SciBot\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import ollama\n",
    "import numpy as np\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 384\n",
    "CHUNK_OVERLAP = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load pdfs from the given file path. Each pdf is loaded and converted to a Document object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the folder containing pdfs\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of Document objects\n",
    "    \"\"\"\n",
    "    pdfs = os.listdir(file_path)\n",
    "    pdfs = [os.path.join(file_path, pdf) for pdf in pdfs if pdf.endswith(\".pdf\")]\n",
    "\n",
    "    docs = []\n",
    "    for pdf in tqdm(pdfs):\n",
    "        pages = []\n",
    "        loader = PyPDFLoader(pdf)\n",
    "        for page in loader.load():\n",
    "            pages.append(page)\n",
    "\n",
    "        text = \"\\n\".join(page.page_content for page in pages)\n",
    "        doc = Document(page_content=text, metadata={\"source\": page.metadata[\"source\"]})\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_docs(docs: List[Document], tokenizer: AutoTokenizer) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split the documents into chunks of text using the RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        docs (List[Document]): List of Document objects\n",
    "        tokenizer (AutoTokenizer): Huggingface tokenizer\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of Document objects\n",
    "    \"\"\"\n",
    "    docs_all = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "        for idx, chunk in enumerate(doc_chunks):\n",
    "            chunk.metadata.update({\"chunk_idx\": idx})\n",
    "            docs_all.append(chunk)\n",
    "\n",
    "    return docs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 74/75 [01:08<00:01,  1.64s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 111 0 (offset 0)\n",
      "100%|██████████| 75/75 [01:08<00:00,  1.09it/s]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../../data/pdfs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hkunlp/instructor-xl\")\n",
    "docs = load_docs(file_path)\n",
    "docs_all = split_docs(docs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_document_as_json(doc, filename):\n",
    "    doc_data = {\n",
    "        \"page_content\": doc.page_content,\n",
    "        \"metadata\": doc.metadata\n",
    "    }\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(doc_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, len(docs_all), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Modifying Documents: 100%|██████████| 100/100 [14:26<00:00,  8.66s/it]\n"
     ]
    }
   ],
   "source": [
    "for cnt, i in tqdm(enumerate(idx), total=len(idx), desc=\"Modifying Documents\"):\n",
    "    doc = docs_all[i]\n",
    "    response = ollama.chat(model='llama3.1:latest', messages=[{\n",
    "        'role': 'user',\n",
    "        'content': f'Slightly change and rephrase the provided piece of scientific paper. PLEASE RESPOND ONLY WITH THE MODIFIED DOCUMENT, NO OTHER TEXT!!! Document: {doc.page_content}',\n",
    "    }])\n",
    "    \n",
    "    doc_modified = Document(page_content=response[\"message\"][\"content\"], metadata=doc.metadata)\n",
    "    save_document_as_json(doc_modified, f\"../../data/mod/doc_{cnt}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.array([2677, 1188, 2608, 2097,  417, 2196, 1442,  486,  481,  873, 2693,\n",
    "       2519, 3068, 2472,  760, 1111, 1672, 1062,   63,  896, 1227, 2376,\n",
    "       1043,   69,  411,  730, 2064, 1009, 3164, 1912,  814,    1, 2826,\n",
    "       2041, 2363, 3073, 2447,  448, 2484,  879, 2945,  465,   95, 2798,\n",
    "       1522, 1128, 2251,  741, 2552,  923, 1755,  686,  424,  136, 1333,\n",
    "       2636, 1076, 2254, 2722,  160, 2971, 2224, 1540,  447,  243, 2786,\n",
    "       1604, 3147,  796,  607, 2576, 1341, 2200, 2077,  923, 1148, 2988,\n",
    "       1357,  211, 1243, 3100, 1649, 2278,   10, 2842, 2243, 2803,  998,\n",
    "        800,  490, 2600, 3068,  174, 2548, 1684, 1180, 2936, 1306, 1401,\n",
    "         17])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
