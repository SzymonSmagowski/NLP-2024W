{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 384\n",
    "CHUNK_OVERLAP = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(file_path: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load pdfs from the given file path. Each pdf is loaded and converted to a Document object.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the folder containing pdfs\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of Document objects\n",
    "    \"\"\"\n",
    "    pdfs = os.listdir(file_path)\n",
    "    pdfs = [os.path.join(file_path, pdf) for pdf in pdfs if pdf.endswith(\".pdf\")]\n",
    "\n",
    "    docs = []\n",
    "    for pdf in tqdm(pdfs):\n",
    "        pages = []\n",
    "        loader = PyPDFLoader(pdf)\n",
    "        for page in loader.load():\n",
    "            pages.append(page)\n",
    "\n",
    "        text = \"\\n\".join(page.page_content for page in pages)\n",
    "        doc = Document(page_content=text, metadata={\"source\": page.metadata[\"source\"]})\n",
    "        docs.append(doc)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "def split_docs(docs: List[Document], tokenizer: AutoTokenizer) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split the documents into chunks of text using the RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        docs (List[Document]): List of Document objects\n",
    "        tokenizer (AutoTokenizer): Huggingface tokenizer\n",
    "\n",
    "    Returns:\n",
    "        List[Document]: List of Document objects\n",
    "    \"\"\"\n",
    "    docs_all = []\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        tokenizer, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_chunks = text_splitter.split_documents([doc])\n",
    "\n",
    "        for idx, chunk in enumerate(doc_chunks):\n",
    "            chunk.metadata.update({\"chunk_idx\": idx})\n",
    "            docs_all.append(chunk)\n",
    "\n",
    "    return docs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 74/75 [03:15<00:03,  3.50s/it]Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 48 0 (offset 0)\n",
      "Ignoring wrong pointing object 53 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 71 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 84 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n",
      "Ignoring wrong pointing object 91 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 111 0 (offset 0)\n",
      "100%|██████████| 75/75 [03:17<00:00,  2.64s/it]\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../../data/pdfs\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hkunlp/instructor-xl\")\n",
    "docs = load_docs(file_path)\n",
    "docs_all = split_docs(docs, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://localhost:11434\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vectorstore with: 6612 docuemnts\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(base_url=url, model=\"snowflake-arctic-embed:latest\")\n",
    "db = FAISS.from_documents(docs_all, embeddings)\n",
    "print(f\"Created vectorstore with: {db.index.ntotal} docuemnts\")\n",
    "db.save_local(folder_path=\"../../data/dbs/db_snowflake\", index_name=\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vectorstore with: 6612 docuemnts\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(base_url=url, model=\"mxbai-embed-large:latest\")\n",
    "db = FAISS.from_documents(docs_all, embeddings)\n",
    "print(f\"Created vectorstore with: {db.index.ntotal} docuemnts\")\n",
    "db.save_local(folder_path=\"../../data/dbs/db_mxbai\", index_name=\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vectorstore with: 6612 docuemnts\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(base_url=url, model=\"nomic-embed-text:latest\")\n",
    "db = FAISS.from_documents(docs_all, embeddings)\n",
    "print(f\"Created vectorstore with: {db.index.ntotal} docuemnts\")\n",
    "db.save_local(folder_path=\"../../data/dbs/db_nomic\", index_name=\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "embeddings = HuggingFaceInstructEmbeddings(\n",
    "    model_name=\"hkunlp/instructor-xl\", model_kwargs={\"device\": device}\n",
    ")\n",
    "db = FAISS.from_documents(docs_all, embeddings)\n",
    "print(f\"Created vectorstore with: {db.index.ntotal} docuemnts\")\n",
    "db.save_local(folder_path=\"../../data/dbs/db_instructor\", index_name=\"faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
