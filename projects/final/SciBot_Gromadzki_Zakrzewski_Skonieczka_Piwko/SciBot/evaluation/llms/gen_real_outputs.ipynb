{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from langchain.chains import (create_history_aware_retriever,\n",
    "                              create_retrieval_chain)\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.embeddings import HuggingFaceInstructEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "class SciBot:\n",
    "    def __init__(self, llm: str) -> None:\n",
    "        self.store = {}\n",
    "\n",
    "        self.llm = ChatOllama(model=llm)\n",
    "        # ===============================================\n",
    "\n",
    "        ### Contextualize question ###\n",
    "        self.contextualize_q_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "\n",
    "        ### Answer question ###\n",
    "        self.system_prompt = (\n",
    "            \"You are an assistant for question-answering tasks. \"\n",
    "            \"Use the following pieces of retrieved context to answer \"\n",
    "            \"the question. If you don't know the answer, say that you \"\n",
    "            \"don't know. Use three sentences maximum and keep the \"\n",
    "            \"answer concise.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "    def ingest(self, db_path: str) -> None:\n",
    "        \"\"\"\n",
    "        Load the database and create the conversational chain.\n",
    "        \"\"\"\n",
    "        model = \"hkunlp/instructor-xl\"\n",
    "        kwargs = {\"device\": \"cpu\"}\n",
    "        embeddings = HuggingFaceInstructEmbeddings(\n",
    "            model_name=model,\n",
    "            model_kwargs=kwargs,\n",
    "        )\n",
    "\n",
    "        db = FAISS.load_local(\n",
    "            folder_path=db_path,\n",
    "            index_name=\"faiss_index\",\n",
    "            embeddings=embeddings,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "\n",
    "        self.retriever = db.as_retriever(\n",
    "            search_type=\"mmr\",  # “similarity” (default), “mmr”, or “similarity_score_threshold”\n",
    "            search_kwargs={\"k\": 6},\n",
    "        )\n",
    "\n",
    "        contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "        history_aware_retriever = create_history_aware_retriever(\n",
    "            self.llm, self.retriever, contextualize_q_prompt\n",
    "        )\n",
    "\n",
    "        qa_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                (\"system\", self.system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.question_answer_chain = create_stuff_documents_chain(self.llm, qa_prompt)\n",
    "\n",
    "        self.rag_chain = create_retrieval_chain(\n",
    "            history_aware_retriever, self.question_answer_chain\n",
    "        )\n",
    "\n",
    "        self.conversational_rag_chain = RunnableWithMessageHistory(\n",
    "            self.rag_chain,\n",
    "            self.get_session_history,\n",
    "            input_messages_key=\"input\",\n",
    "            history_messages_key=\"chat_history\",\n",
    "            output_messages_key=\"answer\",\n",
    "        )\n",
    "\n",
    "    def get_session_history(self, session_id: str) -> ChatMessageHistory:\n",
    "        \"\"\"\n",
    "        Get the chat history for a given session ID.\n",
    "        \"\"\"\n",
    "        if session_id not in self.store:\n",
    "            self.store[session_id] = ChatMessageHistory()\n",
    "        return self.store[session_id]\n",
    "\n",
    "    def ask(self, query: str, session_id: str = \"abc123\") -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Ask a question and get a response.\n",
    "        \"\"\"\n",
    "        response = self.conversational_rag_chain.invoke(\n",
    "            {\"input\": query},\n",
    "            config={\n",
    "                \"configurable\": {\"session_id\": session_id},\n",
    "            },\n",
    "        )\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../data/llm_eval/questions.txt\", \"r\") as f:\n",
    "    questions = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = [\"qwen2.5:3b\", \"qwen2.5:7b-instruct-q4_0\", \"llama3.1:latest\", \"llama3.2:latest\"]\n",
    "db_path = \"../../data/dbs/db_instructor\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2.5:3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\OneDrive\\Desktop\\PW\\Natural Language Processing\\SciBot\\.venv\\Lib\\site-packages\\InstructorEmbedding\\instructor.py:7: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\OneDrive\\Desktop\\PW\\Natural Language Processing\\SciBot\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n",
      "100%|██████████| 60/60 [04:04<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2.5:7b-instruct-q4_0\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\OneDrive\\Desktop\\PW\\Natural Language Processing\\SciBot\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n",
      "100%|██████████| 60/60 [06:31<00:00,  6.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1:latest\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\OneDrive\\Desktop\\PW\\Natural Language Processing\\SciBot\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n",
      "100%|██████████| 60/60 [05:37<00:00,  5.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.2:latest\n",
      "load INSTRUCTOR_Transformer\n",
      "max_seq_length  512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jakub\\OneDrive\\Desktop\\PW\\Natural Language Processing\\SciBot\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n",
      "100%|██████████| 60/60 [03:36<00:00,  3.61s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for llm in llms:\n",
    "    print(llm)\n",
    "    chat = SciBot(llm=llm)\n",
    "    chat.ingest(db_path)\n",
    "    sample_ans = chat.ask(\"What is the capital of France?\") # Sample question to eliminate cold start\n",
    "    \n",
    "    for i, question in enumerate(tqdm(questions)):\n",
    "        start = time.time()\n",
    "        response = chat.ask(question, session_id=str(i))\n",
    "        ex_time = time.time() - start\n",
    "        context = \"\".join(f\"Document {i+1}: \\n {doc.page_content} \\n\\n\" for i, doc in enumerate(response[\"context\"]))\n",
    "        results.append([llm, question, context, response[\"answer\"], ex_time])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>llm</th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>answer</th>\n",
       "      <th>ex_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>What does the term \"learn to optimize\" mean?\\n</td>\n",
       "      <td>Document 1: \\n NatlSciRev ,2024,Vol.11,nwae132...</td>\n",
       "      <td>The term \"learn to optimize\" (L2O) refers to a...</td>\n",
       "      <td>2.757843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>Please give some examples of metaheuristics.\\n</td>\n",
       "      <td>Document 1: \\n usually provide only sub-optima...</td>\n",
       "      <td>Metaheuristics are high-level methodologies or...</td>\n",
       "      <td>2.751359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>What is the \"no free lunch\" theorem about?\\n</td>\n",
       "      <td>Document 1: \\n IEEE TRANSACTIONS ON EVOLUTIONA...</td>\n",
       "      <td>The \"No Free Lunch\" (NFL) theorem states that ...</td>\n",
       "      <td>2.572277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>What is the concept behind Parallel Algorithm ...</td>\n",
       "      <td>Document 1: \\n training set as well as for con...</td>\n",
       "      <td>The concept of a Parallel Algorithm Portfolio ...</td>\n",
       "      <td>2.925742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen2.5:3b</td>\n",
       "      <td>Please provide some approaches to how Parallel...</td>\n",
       "      <td>Document 1: \\n algorithms, and thereby combine...</td>\n",
       "      <td>To construct parallel algorithm portfolios, ef...</td>\n",
       "      <td>3.219646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>How can EBMs help detect observations poorly i...</td>\n",
       "      <td>Document 1: \\n concatenation of the two curves...</td>\n",
       "      <td>EBMs (Explainable Boosting Machines) can help ...</td>\n",
       "      <td>4.825688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>How can one distinct terms intepretabilitry an...</td>\n",
       "      <td>Document 1: \\n our approach shows better resul...</td>\n",
       "      <td>The terms \"interpretability\" and \"explainabili...</td>\n",
       "      <td>4.310438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>What issues in machine learning can be categor...</td>\n",
       "      <td>Document 1: \\n 2.1 Characterizing Model Bugs.\\...</td>\n",
       "      <td>According to the text, model bugs are categori...</td>\n",
       "      <td>2.143233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>What XAI techniques can be heloful in detectin...</td>\n",
       "      <td>Document 1: \\n query is generally much smaller...</td>\n",
       "      <td>Based on the provided context, two XAI (Explai...</td>\n",
       "      <td>4.372954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>llama3.2:latest</td>\n",
       "      <td>How can deep learning methods be used in imput...</td>\n",
       "      <td>Document 1: \\n least square methods [23] for i...</td>\n",
       "      <td>Deep learning methods, such as neural networks...</td>\n",
       "      <td>5.496888</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>240 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 llm                                           question  \\\n",
       "0         qwen2.5:3b     What does the term \"learn to optimize\" mean?\\n   \n",
       "1         qwen2.5:3b     Please give some examples of metaheuristics.\\n   \n",
       "2         qwen2.5:3b       What is the \"no free lunch\" theorem about?\\n   \n",
       "3         qwen2.5:3b  What is the concept behind Parallel Algorithm ...   \n",
       "4         qwen2.5:3b  Please provide some approaches to how Parallel...   \n",
       "..               ...                                                ...   \n",
       "235  llama3.2:latest  How can EBMs help detect observations poorly i...   \n",
       "236  llama3.2:latest  How can one distinct terms intepretabilitry an...   \n",
       "237  llama3.2:latest  What issues in machine learning can be categor...   \n",
       "238  llama3.2:latest  What XAI techniques can be heloful in detectin...   \n",
       "239  llama3.2:latest  How can deep learning methods be used in imput...   \n",
       "\n",
       "                                               context  \\\n",
       "0    Document 1: \\n NatlSciRev ,2024,Vol.11,nwae132...   \n",
       "1    Document 1: \\n usually provide only sub-optima...   \n",
       "2    Document 1: \\n IEEE TRANSACTIONS ON EVOLUTIONA...   \n",
       "3    Document 1: \\n training set as well as for con...   \n",
       "4    Document 1: \\n algorithms, and thereby combine...   \n",
       "..                                                 ...   \n",
       "235  Document 1: \\n concatenation of the two curves...   \n",
       "236  Document 1: \\n our approach shows better resul...   \n",
       "237  Document 1: \\n 2.1 Characterizing Model Bugs.\\...   \n",
       "238  Document 1: \\n query is generally much smaller...   \n",
       "239  Document 1: \\n least square methods [23] for i...   \n",
       "\n",
       "                                                answer   ex_time  \n",
       "0    The term \"learn to optimize\" (L2O) refers to a...  2.757843  \n",
       "1    Metaheuristics are high-level methodologies or...  2.751359  \n",
       "2    The \"No Free Lunch\" (NFL) theorem states that ...  2.572277  \n",
       "3    The concept of a Parallel Algorithm Portfolio ...  2.925742  \n",
       "4    To construct parallel algorithm portfolios, ef...  3.219646  \n",
       "..                                                 ...       ...  \n",
       "235  EBMs (Explainable Boosting Machines) can help ...  4.825688  \n",
       "236  The terms \"interpretability\" and \"explainabili...  4.310438  \n",
       "237  According to the text, model bugs are categori...  2.143233  \n",
       "238  Based on the provided context, two XAI (Explai...  4.372954  \n",
       "239  Deep learning methods, such as neural networks...  5.496888  \n",
       "\n",
       "[240 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results, columns=[\"llm\", \"question\", \"context\", \"answer\", \"ex_time\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../../data/llm_eval/real_outputs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
